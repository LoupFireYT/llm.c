[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Little Book of llm.c",
    "section": "",
    "text": "Content\n\nChapter 1 — Orientation\n\nWhat llm.c Is (scope, goals, philosophy)\nRepository Tour (folders, files, structure)\nMakefile Targets & Flags (CPU, CUDA, options)\nQuickstart: CPU Reference Path (train_gpt2.c)\nQuickstart: 1-GPU Legacy Path (train_gpt2_fp32.cu)\nQuickstart: Modern CUDA Path (train_gpt2.cu)\nStarter Artifacts & Data Prep (dev/download_starter_pack.sh, dev/data/)\nDebugging Tips & IDE Stepping (-g, gdb, lldb, IDEs)\nProject Constraints & Readability Contract\nCommunity, Discussions, and Learning Path\n\n\n\nChapter 2 — Data, Tokenization, and Loaders\n\nGPT-2 Tokenizer Artifacts (gpt2_tokenizer.bin)\nBinary Dataset Format (.bin with header + tokens)\nDataset Scripts in dev/data/ (Tiny Shakespeare, OpenWebText)\nDataLoader Design (batching, strides, epochs)\nEvalLoader and Validation Workflow\nSequence Length and Memory Budgeting\nReproducibility and Seeding Across Runs\nError Surfaces from Bad Data (bounds, asserts)\nTokenization Edge Cases (UNKs, EOS, BOS)\nData Hygiene and Logging\n\n\n\nChapter 3 — Model Definition & Weights\n\nGPT-2 Config: vocab, layers, heads, channels\nParameter Tensors and Memory Layout\nEmbedding Tables: token + positional\nAttention Stack: QKV projections and geometry\nMLP Block: linear layers + activation\nLayerNorm: theory and implementation (doc/layernorm)\nResidual Streams: skip connections explained\nLoss Head: tied embeddings and logits\nCheckpoint Loading from PyTorch\nParameter Counting and Sanity Checks\n\n\n\nChapter 4 — CPU Inference (Forward only)\n\nForward Pass Walkthrough\nToken and Positional Embedding Lookup\nAttention: matmuls, masking, softmax on CPU\nMLP: GEMMs and activation functions\nLayerNorm on CPU (step-by-step)\nResidual Adds and Signal Flow\nCross-Entropy Loss on CPU\nPutting It All Together: The gpt2_forward\nOpenMP Pragmas for Parallel Loops\nCPU Memory Footprint and Performance\n\n\n\nChapter 5 — Training Loop (CPU Path)\n\nSkeleton of Training Loop\nAdamW Implementation in C\nLearning Rate Schedulers (cosine, warmup)\nGradient Accumulation and Micro-Batching\nLogging and Progress Reporting\nValidation Runs in Training Loop\nCheckpointing Parameters and Optimizer State\nReproducibility and Small Divergences\nCommand-Line Flags and Defaults\nExample CPU Training Logs and Outputs\n\n\n\nChapter 6 — Testing, Profiling, & Parity\n\nDebug State Structs and Their Role\ntest_gpt2.c: CPU vs PyTorch\ntest_gpt2cu.cu: CUDA vs PyTorch\nMatching Outputs Within Tolerances\nProfiling with profile_gpt2.cu\nMeasuring FLOPs and GPU Utilization\nReproducing Known Loss Curves\nCommon CUDA Pitfalls (toolchain, PTX)\ncuDNN FlashAttention Testing (USE_CUDNN)\nFrom Unit Test to Full Training Readiness\n\n\n\nChapter 7 — CUDA Training Internals (train_gpt2.cu)\n\nCUDA Architecture Overview (streams, kernels)\nMatrix Multiplication via cuBLAS/cuBLASLt\nAttention Kernels: cuDNN FlashAttention\nMixed Precision: FP16/BF16 with Master FP32 Weights\nLoss Scaling in Mixed Precision Training\nActivation Checkpointing and Memory Tradeoffs\nGPU Memory Planning: params, grads, states\nKernel Launch Configurations and Occupancy\nCUDA Error Handling and Debugging\ndev/cuda/: From Simple Kernels to High Performance\n\n\n\nChapter 8 — Multi-GPU & Multi-Node Training\n\nData Parallelism in llm.c\nMPI Process Model and GPU Affinity\nNCCL All-Reduce for Gradient Sync\nBuilding and Running Multi-GPU Trainers\nMulti-Node Bootstrapping with MPI\nSLURM and PMIx Caveats\nDebugging Multi-GPU Hangs and Stalls\nScaling Stories: GPT-2 124M → 774M → 1.6B\nNCCL Tuning and Overlap Opportunities\nCommon Multi-GPU Errors and Fixes\n\n\n\nChapter 9 — Extending the Codebase\n\nThe dev/cuda Library for Custom Kernels\nAdding New Dataset Pipelines (dev/data/*)\nAdding a New Optimizer to the Codebase\nAdding a New Scheduler (cosine, step, etc.)\nAlternative Attention Mechanisms\nProfiling and Testing New Kernels\nUsing PyTorch Reference as Oracle\nExploring Beyond GPT-2: LLaMA Example\nPorting Playbook: C → Go/Rust/Metal\nKeeping the Repo Minimal and Clean\n\n\n\nChapter 10 — Reproductions, Community, and Roadmap\n\nReproducing GPT-2 124M on Single Node\nReproducing GPT-2 355M (constraints and tricks)\nReproducing GPT-2 774M (scaling up)\nReproducing GPT-2 1.6B on 8×H100 (24h run)\nCPU-only Fine-Tune Demo (Tiny Shakespeare)\nCost and Time Estimation for Runs\nHyperparameter Sweeps (sweep.sh)\nValidating Evaluation and Loss Curves\nFuture Work: Kernel Library, Less cuDNN Dependence\nCommunity, GitHub Discussions, and Suggested Learning Path",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Content</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html",
    "href": "books/en-US/book.html",
    "title": "The Book",
    "section": "",
    "text": "Chapter 1. Orientation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html#chapter-1.-orientation",
    "href": "books/en-US/book.html#chapter-1.-orientation",
    "title": "The Book",
    "section": "",
    "text": "1. What llm.c Is\nImagine you wanted to peek inside a modern AI model-not by reading thousands of lines of optimized C++ or CUDA hidden inside a giant framework, but by opening a small, neat folder and seeing the entire training pipeline laid out in front of you. That is what llm.c gives you.\nAt its heart, llm.c is a reference implementation of how to train and run a GPT-2 style language model, written in pure C (and CUDA). The key word is reference: the code is meant to be minimal, readable, and educational. You don’t need to wade through abstraction layers or device-specific macros. Instead, you get a version that looks almost like pseudocode, but still compiles and runs on your computer.\n\nWhy This Project Exists\nDeep learning frameworks like PyTorch and TensorFlow are amazing for getting models to work quickly, but they hide most of the actual mechanics. Under the hood, there’s a lot happening: tensors are allocated in memory, gradients are computed through backpropagation, optimizer states are updated, and schedulers adjust the learning rate. Most of us never see those details, because the framework handles them for us.\nllm.c flips this around. It says: what if we removed the black box and showed you exactly how a GPT-2 model is trained, line by line? It’s not about speed or production deployment. It’s about clarity, education, and demystifying how large language models work.\n\n\nKey Characteristics\n\nMinimalism: The CPU version (train_gpt2.c) avoids complicated optimizations so that beginners can follow the logic. Even the CUDA version tries to stay simple, with only necessary calls to cuBLAS/cuDNN.\nSelf-contained: No external frameworks. The code defines its own tokenizer, dataloader, optimizer, and scheduler. Everything you need is in the repository.\nParallels to PyTorch: Each function in the C/CUDA implementation has a counterpart in PyTorch. The repo even ships with Python test files to prove that the outputs match within tolerance.\nStep-by-step scalability: You can start with a tiny model on CPU and, once you understand the basics, switch to GPU, multi-GPU, or even multi-node training. The structure remains the same, just faster.\n\n\n\nWhat You Can Do With It\n\nTrain GPT-2 from scratch: Start with a small dataset (like Tiny Shakespeare) and see the model learn patterns in language.\nExperiment with configurations: Change number of layers, sequence length, or hidden size, then watch how memory and training time scale.\nLearn GPU training internals: Move from CPU to CUDA, and later to multi-GPU with MPI/NCCL, to see how real distributed training works under the hood.\nProfile performance: The repo includes profiling tools so you can measure FLOPs, memory bandwidth, and kernel execution times.\nReproduce big models: With enough hardware, you can actually retrain GPT-2 124M or larger versions, using the exact same setup described in the README.\n\n\n\nWhy You Should Care\nIf you’ve ever used a large language model and thought “this feels like magic”, llm.c is your chance to peel back the curtain. You’ll see:\n\nHow text is converted into numbers (tokenization).\nHow those numbers are fed into matrices and transformed.\nHow attention actually looks in code.\nHow gradients are computed and weights updated.\nHow scaling from one GPU to many is managed.\n\nIt’s not magic anymore-it’s math plus code you can read.\n\n\nThe Big Picture\nThink of llm.c as a teaching lab for modern AI:\n\nOn day one, you can run a small demo on CPU and see loss values dropping as the model learns.\nOn day ten, you might be profiling CUDA kernels or debugging NCCL synchronization across GPUs.\nAt every step, the code remains approachable because it avoids unnecessary complexity.\n\nThis philosophy makes llm.c unique: it’s not trying to be the fastest or the most feature-rich. It’s trying to be the clearest window into how large language models actually work when stripped down to their essentials.\nThe Takeaway: llm.c is a minimal, transparent re-implementation of GPT-2 training and inference, designed to teach you how language models operate under the hood. It’s not about production performance-it’s about giving you a guided tour through the machinery of modern AI, one C file at a time.\nWould you like me to now expand Section 2 (“Repository Tour”) in the same friendly beginner tone, showing what’s inside each folder and file?\n\n\n\n2. Repository Tour\nWhen you first open the llm.c repository, it may look like a random collection of .c, .cu, .py, and shell scripts. But there’s a clear structure hiding underneath. Each part has a purpose, and once you know where things live, navigating the repo feels natural. Let’s take a guided walk through the main pieces.\n\nRoot Directory: The Entry Points\nAt the top level, you’ll find the core training programs. These are the files you actually compile and run:\n\ntrain_gpt2.c - The CPU reference implementation. This is the simplest, most readable version of GPT-2 training. It avoids special optimizations so you can follow the math and logic step by step.\ntrain_gpt2.cu - The CUDA implementation. Faster, uses GPU kernels, cuBLAS, and optional cuDNN FlashAttention. This is the version you’d use for serious training runs.\ntrain_gpt2_fp32.cu - A legacy CUDA path, using plain FP32 precision instead of mixed precision. It’s slower but useful as a debugging baseline.\ntrain_gpt2.py - The PyTorch reference. This is the oracle: a tiny script in Python/PyTorch that trains the same GPT-2 so you can compare outputs and verify correctness.\n\nOther important root-level files:\n\nMakefile - Defines how to build different versions. Targets like make train_gpt2 or make train_gpt2cu are your entry points.\nREADME.md - The main guide for running experiments, installing dependencies, and reproducing models.\n\n\n\nllmc/ Directory: Utilities and Building Blocks\nThis folder holds reusable C utilities that the main training files include:\n\nutils.h - Safety wrappers (fopenCheck, mallocCheck) and helper functions.\ntokenizer.h - Implements GPT-2’s tokenizer in C: encoding text into token IDs and decoding back to text.\ndataloader.h - Defines how training batches are loaded and served, handling dataset splits and iteration.\nrand.h - Random number utilities, mirroring PyTorch’s manual_seed and normal distributions.\nschedulers.h - Learning rate scheduling, like cosine decay with warmup.\nsampler.h - Implements softmax sampling for text generation and helper RNG.\nlogger.h - Minimal logging functionality for tracking progress.\n\nThink of llmc/ as the library that keeps the main files clean and readable. Instead of cluttering train_gpt2.c with helpers, everything is modularized here.\n\n\ndev/ Directory: Scripts and Extras\nThis folder is full of supporting tools that make experiments easier:\n\ndev/download_starter_pack.sh - Fetches the GPT-2 124M weights, tokenizer, and datasets. This is the quickest way to get started.\ndev/data/ - Contains scripts for preparing datasets like Tiny Shakespeare or OpenWebText in the binary format that llm.c expects.\ndev/cuda/ - A place for experimenting with standalone CUDA kernels. This is where you’d go if you want to tinker with custom GPU code beyond the main trainer.\n\n\n\ndoc/ Directory: Learning Resources\nDocumentation that digs deeper into specific topics. For example:\n\ndoc/layernorm/layernorm.md - A tutorial-style explanation of Layer Normalization, complete with math and code. It helps you understand one of GPT-2’s core components before diving into the C implementation.\n\nThis folder is a learning aid. Whenever a concept feels too dense, check here for a more gentle walkthrough.\n\n\nTest Files\nTesting is taken seriously in llm.c, because the goal is to prove that the C/CUDA implementation is correct compared to PyTorch:\n\ntest_gpt2.c - Runs forward passes and training steps on CPU and compares outputs to PyTorch.\ntest_gpt2cu.cu - Same idea but for CUDA, including both FP32 and mixed-precision runs.\n\nThese files keep everything honest: you can always verify that your build produces the same results as the canonical PyTorch model.\n\n\nProfiling Tools\nFor performance deep dives:\n\nprofile_gpt2.cu - A CUDA profiling harness that benchmarks kernels and measures throughput.\nprofile_gpt2cu.py - Python-side profiler for analyzing GPU utilization, memory bandwidth, and FLOPs.\n\nIf you’re curious about where time is being spent in training, these files show you how to measure it.\n\n\nDatasets and Artifacts\nWhen you run download_starter_pack.sh, you’ll get:\n\ngpt2_tokenizer.bin - GPT-2’s byte-pair encoding tokenizer, serialized in binary.\nDataset .bin files - Training and validation sets, tokenized and ready for the dataloader.\n\nThese files are not in the repo by default but are downloaded or generated locally.\n\n\nPutting It Together\nThe repository is structured like a teaching lab:\n\nRoot files are the main experiments.\nllmc/ is the library of building blocks.\ndev/ provides extra tools and scripts.\ndoc/ explains tricky concepts in tutorial form.\nTests and profilers make sure everything matches PyTorch and runs efficiently.\n\nOnce you see the pattern, the repo feels less intimidating. Every file has a role in telling the story of how a GPT-2 model is built from scratch in C and CUDA.\n\n\n\n3. Makefile Targets & Flags\nEvery C or CUDA program needs a build system, and in llm.c that role is handled by a simple but powerful Makefile. If you’ve never used make before, think of it as a recipe book: you type make &lt;target&gt; in your terminal, and it follows the instructions for compiling the code into an executable. In llm.c, this file is your control center for choosing which trainer to build, whether to enable GPUs, and which optional features to turn on.\n\nWhy a Makefile?\nInstead of memorizing long gcc or nvcc compile commands with dozens of flags, the Makefile captures those instructions once and gives them a short name. For example, building the CPU trainer is as easy as:\nmake train_gpt2\nBehind the scenes, this calls gcc, sets optimization flags, includes the right headers, and links everything together. The same applies to CUDA builds with nvcc.\n\n\nCore Targets\nHere are the most important build targets you’ll find:\n\ntrain_gpt2 - Builds the CPU-only reference trainer. Uses gcc (or clang) and links against OpenMP for parallel loops.\ntrain_gpt2cu - Builds the CUDA trainer with mixed precision and optional cuDNN FlashAttention. Uses nvcc.\ntrain_gpt2_fp32 - Builds the legacy CUDA trainer that stays in pure FP32 (slower but simpler).\ntest_gpt2 - Compiles the CPU test program to compare results against PyTorch.\ntest_gpt2cu - Compiles the CUDA test program to check GPU parity with PyTorch.\nprofile_gpt2.cu - Compiles the CUDA profiler harness, used to benchmark kernels and FLOPs.\n\nEach of these produces a binary you can run directly, for example:\n./train_gpt2\n./train_gpt2cu\n./test_gpt2\n\n\nKey Flags You Can Toggle\nThe Makefile also exposes several switches that let you customize the build. You set them when running make, like this:\nmake train_gpt2cu USE_CUDNN=1\nHere are the most important flags:\n\nUSE_CUDNN - Enables cuDNN FlashAttention if your system has cuDNN installed. This can give big speedups for attention, but it’s optional. By default, it’s off.\nOMP=1 - Tells the CPU trainer to compile with OpenMP enabled. This allows multithreaded execution, making CPU runs much faster. Usually on by default if OpenMP is detected.\nDEBUG=1 - Compiles with debugging symbols (-g) instead of maximum optimization. Useful when stepping through code in an IDE or using a debugger.\nPROFILE=1 - Adds profiling hooks, helping you analyze execution time and performance.\n\n\n\nOptimization Choices\nThe default build uses -O3 optimization, which makes the code run fast but sometimes harder to debug. If you’re just learning and want clarity, you can switch to:\nmake train_gpt2 DEBUG=1\nThis creates a binary that runs slower but lets you step through line by line in a debugger. For performance benchmarking, stick with the optimized default.\n\n\nMulti-GPU and MPI Support\nWhen building the CUDA trainer, the Makefile can also link against MPI and NCCL if they’re installed. That’s what enables multi-GPU and multi-node training. You usually don’t need to change anything-the Makefile automatically detects these libraries and includes them if available.\n\n\nPutting It All Together\nThink of the Makefile as a switchboard for the whole project:\n\nWant to run the simple CPU demo? → make train_gpt2\nWant to train faster on GPU? → make train_gpt2cu\nWant to debug kernels? → make train_gpt2cu DEBUG=1\nWant to test parity with PyTorch? → make test_gpt2 or make test_gpt2cu\n\nWith just a few keystrokes, you control whether you’re running a beginner-friendly CPU demo, a high-performance GPU build, or a debugging session.\nThe takeaway: The Makefile is your control center. It abstracts away complicated compiler commands and gives you a clean menu of options: CPU vs GPU, FP32 vs mixed precision, debug vs optimized, and single vs multi-GPU. Mastering it is the first step to feeling comfortable experimenting inside llm.c.\n\n\n\n4. Quickstart: CPU Reference Path (train_gpt2.c)\nThe simplest way to begin exploring llm.c is with the CPU-only reference implementation. This file, train_gpt2.c, is deliberately designed to be minimal, readable, and approachable. It doesn’t hide complexity behind libraries or macros. Instead, it shows you exactly how a GPT-2 model is trained, step by step, using plain C and a sprinkle of OpenMP for speed.\n\nWhy Start with CPU?\n\nClarity first: GPUs add layers of complexity (CUDA kernels, memory transfers, cuBLAS). On CPU, you can focus on the core algorithm without distraction.\nPortability: Any machine with a C compiler can run it-no special hardware required.\nDebuggability: Errors are easier to trace, and you can single-step through the code in an IDE.\n\nThe CPU version is slower, but that’s a feature here-it forces you to really see what’s happening under the hood.\n\n\nBuilding the CPU Trainer\nFrom the root of the repository, you just type:\nmake train_gpt2\nThis compiles train_gpt2.c into an executable named train_gpt2. If your system has OpenMP, the Makefile will detect it and add the right flags.\n\n\nRunning Your First Training Run\nBefore running, download the starter pack (tokenizer, dataset, configs):\n./dev/download_starter_pack.sh\nNow launch training:\n./train_gpt2\nYou’ll see output like:\n[GPT-2]\nmax_seq_len: 1024\nvocab_size: 50257\npadded_vocab_size: 50304\nnum_layers: 12\nnum_heads: 12\nchannels: 768\nnum_parameters: 124475904\ntrain dataset num_batches: 1192\nval dataset num_batches: 128\nnum_activations: 73347840\nval loss 5.325529\nstep 0: train loss 4.677779 (took 1987.546000 ms)\nstep 1: train loss 5.191576 (took 1927.230000 ms)\n...\nEach line tells you:\n\nModel size and config (sequence length, vocabulary size, layers, heads, channels).\nDataset stats (how many batches for training and validation).\nActivation memory size (a measure of how big the intermediate states are).\nTraining progress (step number, train loss, validation loss, time per step).\n\n\n\nInside the Training Loop\nAlthough you don’t need to dive into the code yet, here’s the high-level flow in train_gpt2.c:\n\nLoad tokenizer and dataset → turns text into tokens.\nInitialize model parameters → embeddings, attention weights, MLPs, norms.\nFor each batch:\n\nForward pass → compute logits and loss.\nBackward pass → compute gradients.\nUpdate parameters → optimizer step.\n\nLog progress → print losses, occasionally run validation.\n\nThis mirrors exactly what happens in PyTorch, just spelled out in C.\n\n\nPerformance Notes\nOn CPU, don’t expect speed. Training GPT-2 124M can take days or weeks. But that’s not the point. The CPU reference path is like a glass box: everything is visible, no shortcuts. You’ll use this to learn the mechanics and to verify that your GPU runs match the same results.\nIf you want to speed things up slightly, you can:\n\nIncrease OpenMP threads:\nOMP_NUM_THREADS=8 ./train_gpt2\nUse a smaller dataset (Tiny Shakespeare) to see faster progress.\nReduce model size by changing config values (fewer layers, smaller channels).\n\n\n\nWhen to Move On\nOnce you’re comfortable with how training looks on CPU-loss values going down, checkpoints being written, logs appearing-you’ll be ready to graduate to the GPU version (train_gpt2.cu). That’s where performance and scaling come in, but the CPU run gives you the conceptual foundation.\nThe takeaway: Running train_gpt2.c is your first hands-on encounter with GPT-2 training in llm.c. It’s slow, transparent, and designed for learning. You’ll see every piece of the model at work, one step at a time, before diving into the complexity of CUDA.\n\n\n\n5. Quickstart: 1-GPU Legacy Path (train_gpt2_fp32.cu)\nOnce you’ve seen the CPU trainer in action, the natural next step is to try training on a GPU. The file train_gpt2_fp32.cu is the simplest GPU entry point. It predates the more advanced mixed-precision trainer (train_gpt2.cu), and it runs everything in full 32-bit floating point (FP32) precision. That makes it easier to follow and debug, even though it’s slower than modern approaches. Think of it as the “training wheels” for GPU training in llm.c.\n\nWhy This Path Exists\nModern GPU training almost always uses mixed precision (FP16/BF16 for speed and memory savings, FP32 for stability). But mixed precision introduces extra complexity: scaling losses, maintaining master weights, checking for overflows. For beginners, all that can be distracting.\nThe FP32 path avoids those complications:\n\nEvery tensor (activations, weights, gradients) is stored as 32-bit floats.\nNo special handling of loss scaling is needed.\nDebugging mismatches with PyTorch is straightforward.\n\nThe trade-off is performance-this version runs significantly slower and uses more memory.\n\n\nBuilding the FP32 CUDA Trainer\nFrom the root of the repository:\nmake train_gpt2_fp32\nThis invokes nvcc (the NVIDIA CUDA compiler) and links against cuBLAS for matrix multiplications. The output is an executable named train_gpt2_fp32.\n\n\nRunning It\nJust like the CPU version, make sure you’ve downloaded the starter pack first:\n./dev/download_starter_pack.sh\nThen launch training on your GPU:\n./train_gpt2_fp32\nIf CUDA is installed correctly, the program will detect your GPU and start training. You’ll see logs that look similar to the CPU trainer’s, but with much shorter step times. For example, a training step that took ~2 seconds on CPU might take ~50 milliseconds on GPU.\n\n\nUnder the Hood\nAlthough the training loop looks the same on the surface, a lot changes under the hood when running on GPU:\n\nTensors are allocated in GPU memory (not system RAM).\nMatrix multiplications (the core of attention and MLP layers) are executed by cuBLAS, NVIDIA’s high-performance linear algebra library.\nKernels for elementwise operations (like adding residuals, applying softmax, or normalizing) are written in CUDA or use built-in primitives.\nGradients and optimizer states are updated entirely on the device, with minimal CPU↔︎GPU transfers.\n\nThis makes training dramatically faster, but the structure of the code is still recognizable compared to the CPU version.\n\n\nWhen to Use FP32 vs Mixed Precision\n\nUse FP32 (this path) when:\n\nYou’re learning how GPU training works step by step.\nYou want a clean comparison with the CPU trainer.\nYou’re debugging correctness issues without worrying about loss scaling.\n\nUse Mixed Precision (train_gpt2.cu) when:\n\nYou want real performance (2–4× faster training).\nYou’re training larger models (774M, 1.6B parameters) where memory efficiency matters.\nYou’re aiming to reproduce published GPT-2 runs on modern GPUs.\n\n\n\n\nCommon Pitfalls\n\nCUDA not installed → If nvcc isn’t found, the Makefile will fail. You’ll need the CUDA Toolkit installed.\nDriver mismatch → Your NVIDIA driver must match the CUDA version.\nOut of memory errors → FP32 uses more GPU memory, so you may need to lower batch size if you’re on a smaller GPU.\n\n\n\nWhy This Step Matters\nThe FP32 trainer is like a bridge:\n\nOn one side is the CPU reference path, slow but crystal-clear.\nOn the other is the mixed-precision CUDA path, fast but more complex.\n\nBy walking across this bridge, you learn how GPU acceleration works without being overwhelmed by optimizations.\nThe takeaway: train_gpt2_fp32.cu is your first taste of real GPU training in llm.c. It skips advanced tricks and shows you a clean, one-GPU, full-precision implementation. It’s not the fastest, but it’s the friendliest way to understand how training moves from CPU to GPU.\n\n\n\n6. Quickstart: Modern CUDA Path (train_gpt2.cu)\nThis is the high-performance trainer most people use day to day. It runs on a single NVIDIA GPU (and also forms the basis for multi-GPU), uses mixed precision (FP16/BF16 where safe, FP32 where needed), and can optionally enable cuDNN FlashAttention for fast attention. Compared to the FP32 legacy path, it’s significantly faster and uses less memory, while keeping the training loop easy to follow.\n\nWhat “mixed precision” means (in plain words)\n\nWeights & activations: stored/processed in FP16 or BF16 for speed and lower memory.\nMaster weights: a FP32 copy of parameters kept for stable updates.\nLoss scaling: multiply the loss before backward to avoid underflow; unscale the grads before the optimizer step.\nAutocast-like behavior: the code picks safe dtypes for each op (GEMMs in tensor cores, reductions in FP32, etc.).\n\nYou get 2–4× speedups on many GPUs, and the same final accuracy when configured properly.\n\n\nBuild the modern CUDA trainer\nmake train_gpt2cu\nCommon variants:\n\nWith cuDNN FlashAttention (if available):\nmake train_gpt2cu USE_CUDNN=1\nWith debug symbols (slower, but easier to step through):\nmake train_gpt2cu DEBUG=1\n\nThis produces an executable named train_gpt2cu.\n\n\nOne-time data & artifacts\nIf you haven’t already:\n./dev/download_starter_pack.sh\nThis fetches the tokenizer and a small dataset so you can run immediately.\n\n\nRun your first GPU training session\n./train_gpt2cu\nYou should see a config header (model dims, vocab, sequence length) followed by step-by-step loss prints. Step times will be much shorter than CPU and noticeably faster than FP32, especially on tensor-core GPUs (Turing and newer).\nSpeed tips right away:\n\nUse a larger global batch if memory allows-it improves GPU utilization.\nSet environment threads for any CPU preprocessing:\nOMP_NUM_THREADS=8 ./train_gpt2cu\n\n\n\nWhat’s different under the hood vs. FP32\n\nTensor cores: GEMMs run in FP16/BF16 paths via cuBLAS/cuBLASLt for big throughput.\nScaled loss & unscale pass: Forward computes the loss, multiplies it by a scale factor; backward divides gradients by the same factor before updates.\nMaster FP32 copy: Optimizer (AdamW) updates this copy, then casts back to low precision for the next forward.\nFused/fast attention (optional): With USE_CUDNN=1, attention may route through cuDNN FlashAttention backends.\n\nYou still recognize the same loop: load batch → forward → loss → backward → AdamW step → log.\n\n\nChoosing FP16 vs. BF16\n\nFP16: best speed, needs loss scaling; widely supported.\nBF16: more numerically forgiving (often needs little/no scaling), requires hardware support (Ampere+); slightly larger memory than FP16 but often simpler.\n\nThe trainer picks what your GPU supports or what the code defaults to; you can expose a flag later if you want to force one.\n\n\nCommon command patterns\n\nSmall GPU (less VRAM):\n./train_gpt2cu --batch_size 4 --micro_batch_size 1 --seq_len 512\nFaster warmup with cosine schedule:\n./train_gpt2cu --warmup_steps 1000 --lr 6e-4 --scheduler cosine\nPeriodic eval to sanity-check:\n./train_gpt2cu --eval_interval 200 --eval_batches 50\n\n(Flag names above mirror typical patterns; adjust to match the binary’s printed help.)\n\n\nValidating correctness (highly recommended)\n\nRun the CUDA test binary to compare against the PyTorch reference on small batches:\nmake test_gpt2cu\n./test_gpt2cu\nCheck that logits/loss match within a small tolerance. If mismatches happen, recompile without optimizations or disable cuDNN fast paths (USE_CUDNN=0) to isolate the issue.\n\n\n\nEnabling FlashAttention (when available)\nmake train_gpt2cu USE_CUDNN=1\n./train_gpt2cu\nGood signs: faster attention time and lower step latency. If you hit build/runtime errors, ensure your CUDA, cuDNN, and driver versions are compatible; fall back to USE_CUDNN=0 while you sort it out.\n\n\nMemory & performance tuning checklist\n\nBatching: Increase micro_batch_size until you reach ~90% GPU utilization without OOM.\nSequence length: Longer sequences increase compute quadratically in attention; reduce --seq_len if memory is tight.\nGrad accumulation: Keep global batch size large by accumulating over multiple micro-batches.\nPinned host memory & async copies: Already used where sensible; keep CPU↔︎GPU transfers minimal.\nProfiler: Once it runs, profile hotspots to confirm GEMMs dominate (as expected) and attention isn’t a bottleneck unless FlashAttention is off.\n\n\n\nTroubleshooting\n\ncudaErrorNoKernelImageForDevice: Toolkit too new/old for your GPU; rebuild with proper -arch= or update drivers.\nCUBLAS_STATUS_ALLOC_FAILED / OOM: Lower batch size, sequence length, or switch to BF16 if supported.\nDiverging loss with FP16: Increase loss scale (if configurable) or try BF16; confirm master-weight updates are in FP32.\ncuDNN errors: Rebuild without USE_CUDNN to verify the base path works, then revisit versions/paths.\n\nThe takeaway: train_gpt2.cu is the practical, fast trainer: mixed precision, optional FlashAttention, and ready to scale. You keep the same readable training loop while tapping your GPU’s tensor cores for large speedups and much better memory efficiency.\n\n\n\n7. Starter Artifacts & Data Prep (dev/download_starter_pack.sh, dev/data/)\nBefore you can actually train or test a model in llm.c, you need a few essential artifacts: the tokenizer, a dataset, and a config. These files aren’t stored in the repo directly (they’re too large and often under different licenses), so the project provides scripts to fetch or generate them. This is where the dev/ folder comes into play.\n\nThe Starter Pack Script\nThe easiest way to get going is with:\n./dev/download_starter_pack.sh\nThis script pulls down a ready-made bundle containing:\n\ngpt2_tokenizer.bin - The GPT-2 byte-pair encoding (BPE) tokenizer in binary format.\ntrain.bin / val.bin - Pre-tokenized training and validation datasets, often based on OpenWebText or Tiny Shakespeare for demos.\nModel configs - A JSON or header file that sets hyperparameters like layers, hidden size, and number of heads for GPT-2 124M.\n\nThink of this as your “starter kit”: it contains just enough to run a demo and see training loss decreasing without setting up a full-scale dataset pipeline yourself.\n\n\nThe Tokenizer File (gpt2_tokenizer.bin)\nThis is a binary representation of GPT-2’s tokenizer vocabulary. It maps raw text (like \"Hello world\") into integer token IDs, which are the actual inputs to the model.\n\nWhy binary? It’s faster to load in C than parsing a text-based vocabulary.\nSize? ~500 KB, representing ~50,000 tokens.\nRole in training? Used in both the dataloader (to prepare inputs) and the sampler (to decode outputs).\n\nWithout this file, the model can’t understand text at all-it would just be manipulating meaningless numbers.\n\n\nDataset Files (train.bin, val.bin)\nEach dataset file is a binary blob containing:\n\nA header (about 1 KB) describing sequence length, vocab size, and other metadata.\nA stream of token IDs (uint16), representing the text corpus already tokenized.\n\nThis design means the C dataloader can simply fread() chunks of tokens into memory, without needing to tokenize text on the fly. It’s fast and memory-efficient, perfect for a lean project like llm.c.\nThe script usually fetches two versions:\n\nTraining set (train.bin)\nValidation set (val.bin)\n\nThat way, the training loop can occasionally switch to validation mode and report a validation loss, helping you track overfitting.\n\n\nThe dev/data/ Folder\nIf you want to generate your own datasets, this is where you’ll find the tools:\n\nScripts for Tiny Shakespeare, OpenWebText, or other corpora.\nUtilities to tokenize text using the GPT-2 tokenizer and write out the .bin format.\nSmall Python snippets to check dataset statistics (like number of tokens or average sequence length).\n\nFor example, if you wanted to try fine-tuning GPT-2 on your own text files, you’d:\n\nRun a preprocessing script in dev/data/ to tokenize and save your corpus.\nPoint train_gpt2.c or train_gpt2.cu to your new train.bin and val.bin.\nKick off training as usual.\n\n\n\nWhy Preprocessing Matters\nTokenization and dataset preparation can be surprisingly heavy in Python, especially for large corpora. By precomputing everything into compact .bin files, llm.c keeps the runtime training loop as simple as possible-just reading arrays of integers and feeding them into the model.\nThis separation of concerns (preprocessing vs. training) is what makes the training code clean and focused.\n\n\nQuick Sanity Check\nAfter running download_starter_pack.sh, you should see these files in your working directory:\ngpt2_tokenizer.bin\ntrain.bin\nval.bin\nIf any are missing, re-run the script. Without them, the trainer will exit with a file-not-found error.\nThe takeaway: The starter pack is your ticket to running llm.c right away. It gives you a tokenizer and datasets in exactly the format the C code expects. Later, when you’re ready to train on your own text or scale up, the dev/data/ folder shows you how to prepare custom datasets the same way.\n\n\n\n8. Debugging Tips & IDE Stepping (-g)\nEven though llm.c is designed to be small and readable, training a transformer model is still a big program with lots of moving parts. When something goes wrong-whether it’s a segmentation fault, NaN losses, or unexpected results-you’ll want to be able to debug effectively. That’s where debug builds and IDE stepping come in.\n\nWhy Debug Mode Exists\nBy default, the Makefile compiles with heavy optimization (-O3). That makes the code run fast, but it also makes debugging harder:\n\nVariables may be optimized away.\nFunctions might get inlined so you can’t step through them clearly.\nThe debugger may jump around unpredictably.\n\nAdding the -g flag (enabled with DEBUG=1) tells the compiler to include extra information in the binary so you can see exactly what the code is doing at runtime.\n\n\nBuilding a Debug Binary\nTo build with debug info:\nmake train_gpt2 DEBUG=1\nThis produces a slower executable, but one that works seamlessly with tools like:\n\ngdb - the classic GNU debugger.\nlldb - default on macOS.\nVS Code / CLion / Xcode - IDEs with integrated debuggers and GUI interfaces.\n\n\n\nUsing gdb on Linux/macOS\nStart your program under gdb:\ngdb ./train_gpt2\nInside gdb:\n\nRun the program: run\nSet a breakpoint at main: break main\nStep through line by line: step or next\nInspect variables: print loss, print i\nQuit: quit\n\nThis is the fastest way to see exactly where a crash happens.\n\n\nUsing an IDE\nIf command-line debugging feels intimidating, you can use an IDE like VS Code or CLion:\n\nOpen the project folder.\nConfigure the debugger (choose gdb or lldb backend).\nAdd breakpoints by clicking next to line numbers.\nRun the debug build (train_gpt2 with DEBUG=1).\nStep through forward pass, backward pass, or optimizer updates.\n\nThis way, you can visually watch variables update with each step.\n\n\nDebugging CUDA Code\nCUDA debugging is a bit trickier, but still possible:\n\ncuda-gdb - NVIDIA’s GPU debugger, works like gdb but supports stepping into kernels.\nNsight Systems / Nsight Compute - graphical profilers/debuggers that let you trace kernel launches, memory transfers, and GPU utilization.\n\nIf your CUDA code crashes with cryptic messages like illegal memory access, cuda-gdb can help pinpoint the kernel and even the exact line.\n\n\nDebugging Common Issues in llm.c\n\nFile not found → Make sure gpt2_tokenizer.bin, train.bin, and val.bin are downloaded.\nSegfault at malloc/fread → Check file paths and dataset sizes.\nLoss becomes NaN →\n\nOn CPU: check for division by zero in normalization.\nOn GPU: check loss scaling (mixed precision) or try FP32 path for comparison.\n\nMismatch with PyTorch tests → Run test_gpt2 or test_gpt2cu and compare outputs; this usually isolates whether the bug is in forward pass, backward pass, or optimizer.\n\n\n\nLogging & Sanity Checks\nWhen debugging, it helps to add extra logging. The repo already has a lightweight logger, but you can also sprinkle printfs (on CPU) or cudaDeviceSynchronize(); printf(...) (on GPU) to track values. For example:\nprintf(\"Step %d: loss=%f\\n\", step, loss);\nSometimes the quickest fix is just to print and see what’s going on.\n\n\nBest Practices for Beginners\n\nStart with the CPU build when learning-it’s easier to debug than CUDA.\nAlways keep a small dataset (like Tiny Shakespeare) for fast iteration.\nCompare against the PyTorch reference for the same batch to catch subtle errors.\nUse DEBUG=1 whenever you hit strange behavior-you’ll trade speed for clarity, which is usually worth it when learning.\n\nThe takeaway: Debug builds (-g) turn llm.c from a black box into a step-through learning tool. With gdb, lldb, or an IDE, you can pause at any line, inspect variables, and understand exactly how GPT-2 training works inside C or CUDA. It’s slower, but it’s the clearest way to learn and fix issues.\n\n\n\n9. Project Constraints & Readability Contract\nThe llm.c project isn’t trying to be the fastest or most feature-rich GPT-2 trainer. Instead, it has a very deliberate set of constraints-rules the author imposes on the codebase to keep it approachable and educational. You can think of these as the “contract” between the code and the reader: certain things are kept simple on purpose, even if they cost some performance.\n\nMinimalism over Optimizations\n\nNo processor-specific intrinsics: You won’t see AVX, NEON, or other hardware-tuned assembly calls in the CPU path.\nNo fancy template metaprogramming: Unlike in C++ frameworks, here you get plain C structs and functions.\nNo exotic libraries: Aside from cuBLAS/cuDNN for GPU acceleration, most functionality is implemented directly.\n\nThis means the code runs almost anywhere, and you don’t need to understand deep compiler tricks to follow what’s going on.\n\n\nTransparency over Abstraction\n\nEvery operation is visible in the source. For example, instead of calling a framework function like nn.CrossEntropyLoss, you’ll find an explicit forward and backward pass coded in C.\nData loading, tokenization, optimizer steps, and schedulers are all implemented as separate, small modules in llmc/.\nYou don’t need to guess what’s happening-if you’re curious, you can open the corresponding .h file and see the exact code.\n\nThe guiding idea: if something is central to training GPT-2, you should be able to read and understand it.\n\n\nPerformance Where It Matters (but No More)\n\nOpenMP pragmas are allowed in CPU builds, because they give large speedups with minimal extra code.\ncuBLAS/cuDNN are used for GPU matmuls and attention, because re-implementing them would be a distraction and would make the project impossibly large.\nBut the project avoids unnecessary complexity-no kernel fusion, no elaborate caching layers, no half-implemented “framework” abstractions.\n\nThis balance ensures you can still run experiments at a reasonable speed, but the code never sacrifices readability.\n\n\nEducational First\nThe code is written to teach, not to win benchmarks. That means:\n\nVariable names are descriptive, not cryptic.\nComments explain not just what happens, but also why.\nFiles are kept small and focused, rather than sprawling across dozens of layers of abstraction.\nThere’s a matching PyTorch reference implementation so you can always check your understanding against a familiar baseline.\n\n\n\nLimitations You Should Expect\n\nTraining is slower than PyTorch/XLA/JAX or DeepSpeed-tuned runs.\nMulti-GPU scaling is functional but not heavily optimized.\nOnly GPT-2 architectures are covered-don’t expect GPT-3 or transformer variants.\nFeatures like dataset streaming, checkpoint sharding, or advanced distributed tricks are intentionally left out.\n\nThese are not bugs-they’re conscious trade-offs to keep the codebase small, sharp, and didactic.\n\n\nWhy This Matters for You\nIf you’re learning how transformers work, this contract is a gift:\n\nYou won’t get lost in performance hacks.\nYou won’t fight through an abstraction jungle.\nYou’ll always know that what you’re reading is close to the “pure” algorithmic idea.\n\nOn the flip side, if you’re aiming for production-grade speed, you’ll need to layer more on top. But that’s outside the mission of llm.c.\nThe takeaway: llm.c is bound by a readability contract: clarity over raw speed, transparency over abstraction, minimalism over complexity. These constraints keep the project small enough to fit in your head, while still powerful enough to reproduce GPT-2 training. It’s a teaching lab, not a racing car-and that’s exactly why it’s valuable.\n\n\n\n10. Community, Discussions, and Learning Path\nThe last piece of the quickstart isn’t about code at all-it’s about the people and resources around the project. llm.c has grown into more than just a single repository; it has become a meeting point for learners, tinkerers, and researchers who want to strip large language models down to their essentials. Understanding this community layer is just as important as understanding the code itself.\n\nDiscussions and Issues on GitHub\nThe project’s Discussions tab is full of valuable context:\n\nDevelopers asking about build errors on different platforms (Linux, macOS, Windows).\nExplorations of how to extend llm.c to train larger GPT-2 models (355M, 774M, 1.6B).\nReports on multi-GPU and MPI runs, including troubleshooting NCCL hangs and performance bottlenecks.\nDebates on mixed precision vs FP32 vs BF16 stability.\n\nReading these threads is like looking over the shoulders of hundreds of other learners. You’ll see not only the official answers but also the thought process of people solving problems in real time.\n\n\nRoadmap and Contributions\nThe README and issues sometimes hint at where the project might grow:\n\nMaking the CUDA kernels more modular in dev/cuda/.\nSimplifying multi-GPU startup for clusters.\nAdding small tutorial-style docs (like the LayerNorm walkthrough).\n\nThe project is open to contributions, but it follows the same minimalist philosophy. If you’re thinking of contributing, remember: the goal is clarity first, performance second.\n\n\nExternal Learning Resources\nWhile llm.c is self-contained, it pairs nicely with outside material:\n\nThe PyTorch reference implementation in train_gpt2.py is your canonical oracle for correctness.\nThe GPT-2 paper gives the architecture background.\nCUDA and cuBLAS/cuDNN docs explain the GPU APIs that the project calls into.\nCommunity blog posts often walk through specific sections of the code in plain English, making it easier to digest.\n\nBy combining the code, the paper, and these resources, you can triangulate a much deeper understanding.\n\n\nA Suggested Learning Path\nIf you’re coming to llm.c as a beginner, here’s a natural progression:\n\nRun the CPU trainer (train_gpt2.c) on Tiny Shakespeare. Watch the loss decrease.\nStep through the code with DEBUG=1, confirming that you understand forward, backward, and optimizer steps.\nMove to the FP32 CUDA trainer to see how the same loop runs on GPU.\nSwitch to the modern CUDA trainer (train_gpt2.cu) and learn how mixed precision works.\nExperiment with dataset scripts in dev/data/-try your own text corpus.\nRead the LayerNorm doc in doc/ to deepen your theory-practice connection.\nExplore multi-GPU runs with MPI/NCCL if you have access to multiple GPUs.\nFollow GitHub Discussions for real-world debugging and scaling stories.\n\n\n\nWhy This Matters\nCode alone is not enough. The community context, the discussions, and the learning path make llm.c a living project. By engaging with them, you avoid the feeling of learning in isolation. You’ll see others wrestling with the same challenges, and you’ll have a clearer sense of what to try next.\nThe takeaway: Beyond the files and scripts, llm.c is a community-driven learning environment. GitHub issues, discussions, reference docs, and external tutorials all form part of the “extended classroom.” If the code is the lab bench, the community is the set of lab partners who help you figure things out along the way.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html#chapter-2.-data-tokenization-and-loaders",
    "href": "books/en-US/book.html#chapter-2.-data-tokenization-and-loaders",
    "title": "The Book",
    "section": "Chapter 2. Data, Tokenization, and Loaders",
    "text": "Chapter 2. Data, Tokenization, and Loaders\n\n11. GPT-2 Tokenizer Artifacts (gpt2_tokenizer.bin)\nA language model like GPT-2 doesn’t directly understand English, Vietnamese, or any other natural language. Instead, it understands numbers. These numbers are called tokens. A tokenizer is the tool that translates between human text and tokens. In llm.c, the GPT-2 tokenizer is stored in a small file called gpt2_tokenizer.bin. This file is the key that lets the model read input text and produce output text that we can understand.\n\nWhat This File Contains\nThe file gpt2_tokenizer.bin is a binary version of GPT-2’s tokenizer. It includes:\n\n\n\n\n\n\n\nComponent\nPurpose\n\n\n\n\nByte vocabulary (0–255)\nMakes sure every possible character can be represented.\n\n\nMerge rules (BPE)\nCombines frequent sequences like “ing” or ” the” into single tokens for efficiency.\n\n\nVocabulary size (~50,257)\nDefines how many distinct tokens GPT-2 can work with.\n\n\nMapping IDs ↔︎ text\nLets the program turn model outputs (numbers) back into human-readable strings.\n\n\n\nInstead of being written as JSON or text, the tokenizer is stored in binary form. This allows llm.c to load it very quickly using a simple file read, which keeps the code clean and fast.\n\n\nWhere It Comes From\nYou don’t need to build this file by hand. The repository provides a script to download it, along with small training and validation datasets:\n./dev/download_starter_pack.sh\nAfter running the script, you should see gpt2_tokenizer.bin, train.bin, and val.bin in your working directory. If the tokenizer is missing, the program cannot run because it won’t know how to interpret text.\n\n\nHow the Code Uses It\nDuring training, the tokenizer is not active because the datasets (train.bin and val.bin) are already pre-tokenized into integers. This keeps the training loop fast and simple.\nDuring sampling or evaluation, the tokenizer becomes important again. After the model predicts a sequence of token IDs, the tokenizer translates those numbers back into text that you can read on your screen.\nThe C API for the tokenizer, defined in llmc/tokenizer.h, provides just three main functions:\nint tokenizer_init(Tokenizer *t, const char *filename);\nint tokenizer_decode(Tokenizer *t, const int *ids, int n, char *out);\nvoid tokenizer_free(Tokenizer *t);\nThis is all you need: initialize the tokenizer from the file, decode tokens into text, and free memory when done.\n\n\nExample Workflow in Practice\n\nInitialize the tokenizer:\nTokenizer t;\ntokenizer_init(&t, \"gpt2_tokenizer.bin\");\nDecode a sequence of tokens back to text:\nchar buf[512];\ntokenizer_decode(&t, tokens, ntokens, buf);\nprintf(\"%s\\n\", buf);\nClean up memory when you no longer need it:\ntokenizer_free(&t);\n\nThis small cycle is enough to turn model outputs into readable sentences.\n\n\nWhy It Matters\nWithout the tokenizer, the model cannot communicate. The tokenizer is like a shared dictionary between humans and the neural network. If you give the model text, the tokenizer converts it into numbers the model understands. When the model responds, the tokenizer converts its numbers back into text. If the tokenizer does not match the dataset, the model’s predictions will come out as gibberish. Keeping the tokenizer and dataset in sync is essential for correct training and evaluation.\n\n\nTry It Yourself\nHere are a few small exercises you can do to understand the tokenizer better:\n\nCheck that the file exists: After running the starter pack script, verify that gpt2_tokenizer.bin is in your directory. Try running the trainer without it and observe the error message.\nInspect vocab size: Run the trainer and look for the line that prints vocab_size: 50257. Compare this with padded_vocab_size: 50304. Why do you think padding helps GPUs?\nDecode a sequence manually: Write a short C program that loads the tokenizer and decodes a fixed list of token IDs (for example [464, 3290, 318]). Observe what text you get.\nMismatch experiment: If you build your own dataset with a different tokenizer (say, a custom vocabulary), try decoding it with gpt2_tokenizer.bin. Notice how the output becomes meaningless, showing why consistency matters.\nDataset + tokenizer link: Open train.bin in a hex viewer. You’ll see it’s just numbers. Use the tokenizer to decode the first few hundred tokens and see real text emerge.\n\n\n\nThe Takeaway\ngpt2_tokenizer.bin is a tiny but vital file. It is the bridge that allows the model and humans to speak the same language. Training is efficient because all data is pre-tokenized, and when you want to see what the model has written, the tokenizer turns raw numbers back into words. Without it, the entire system would be silent.\n\n\n\n12. Binary Dataset Format (train.bin and val.bin)\nJust like the tokenizer turns text into numbers, the datasets in llm.c are stored as numbers too. Instead of reading plain text files like .txt, the training and validation data are kept in simple binary files: train.bin and val.bin. These files are the fuel for the training loop.\n\nWhat These Files Look Like\nAt first glance, train.bin and val.bin look like unreadable blobs if you open them in a text editor. That’s because they are not meant to be human-readable. They contain:\n\n\n\n\n\n\n\nPart\nDescription\n\n\n\n\nA tiny header (about 1 KB)\nStores metadata such as sequence length and vocab size.\n\n\nA stream of token IDs (uint16)\nEvery tokenized word piece from the dataset, saved as 16-bit integers.\n\n\n\nEach integer represents one token from the tokenizer’s vocabulary. Since GPT-2 has a vocabulary of about 50,000 tokens, 16-bit integers (uint16_t) are enough to store them all.\n\n\nWhy Binary Format?\n\nEfficiency: Instead of re-tokenizing text every time, the data is pre-tokenized once and stored as numbers. The trainer just reads them directly.\nSpeed: Reading integers from a file is faster than parsing and processing raw text.\nSimplicity: The training loop only has to deal with arrays of integers-no string handling, no parsing, no surprises.\n\nThis choice makes the training code in llm.c much cleaner and faster.\n\n\nHow the Dataloader Uses Them\nWhen training starts, the dataloader reads chunks of numbers from train.bin. Each chunk corresponds to one batch of size B × T:\n\nB = batch size (number of examples in a batch).\nT = sequence length (number of tokens per example).\n\nFor example, if B = 8 and T = 1024, the dataloader will read 8 × 1024 = 8192 token IDs from the file, reshape them into sequences, and feed them to the model.\nThe validation file (val.bin) works the same way but is only used occasionally during training to measure validation loss. This helps detect overfitting.\n\n\nWorkflow in Code\nInside the repo, you’ll see functions like these in llmc/dataloader.h:\nint dataloader_init(Dataloader *loader, const char *filename, int B, int T);\nint dataloader_next_batch(Dataloader *loader, int *inputs, int *targets);\nvoid dataloader_reset(Dataloader *loader);\nvoid dataloader_free(Dataloader *loader);\nHere’s what happens step by step:\n\nInitialize with the binary file and batch/sequence sizes.\nNext batch reads the next B × T tokens into an input array and a target array.\nReset allows re-reading from the beginning when you start a new epoch.\nFree cleans up resources when training ends.\n\nThe target array is simply the same sequence shifted by one token-because language modeling predicts the next token.\n\n\nWhy It Matters\nThe dataset format is what makes llm.c practical. Without it, the code would need to handle messy text, encodings, and tokenization during every training step. By storing clean arrays of token IDs, the training loop becomes very short and easy to follow. It’s a design decision that keeps the project minimal yet faithful to real training pipelines.\n\n\nTry It Yourself\n\nCheck file size: Run ls -lh train.bin and notice how large it is compared to a plain .txt file. Why is it smaller or larger?\nPeek inside: Use a hex viewer (xxd train.bin | head) to see raw numbers. They won’t look like text, but they are the tokens the model trains on.\nCount tokens: Write a short Python or C script to count how many token IDs are stored in train.bin. This gives you a sense of dataset size.\nMini-dataset: Try generating your own dataset from a small .txt file using the scripts in dev/data/. See how the .bin file is created.\nValidation experiment: During training, reduce the validation set to only a few batches and observe how the validation loss stabilizes or fluctuates compared to training loss.\n\n\n\nThe Takeaway\ntrain.bin and val.bin may look like gibberish, but they are carefully prepared binary files containing token IDs. They make training faster, simpler, and more reproducible. The dataloader in llm.c reads these numbers in neat chunks and serves them directly to the model, letting you focus on learning how transformers work instead of wrestling with raw text parsing.\n\n\n\n13. Dataset Scripts in dev/data/\nThe repository doesn’t just give you ready-made binary datasets like train.bin and val.bin. It also provides scripts inside the dev/data/ folder that show you how to create your own. These scripts are important because they demonstrate how raw text gets transformed into the binary format that the dataloader in llm.c expects.\n\nWhat’s Inside dev/data/\nThis folder contains small Python scripts that:\n\n\n\n\n\n\n\nScript\nPurpose\n\n\n\n\nprepare_shakespeare.py\nTurns the Tiny Shakespeare dataset into train.bin and val.bin.\n\n\nprepare_openwebtext.py\nPrepares a large-scale dataset similar to the one GPT-2 was trained on.\n\n\nOther helpers\nTokenize raw .txt files, split them into train/val, and save to binary.\n\n\n\nEach script follows the same basic recipe:\n\nRead raw text from a source file.\nApply the GPT-2 tokenizer to turn text into token IDs.\nSplit the tokens into training and validation portions.\nWrite the IDs into binary files that llm.c can read directly.\n\n\n\nWhy Preprocessing Happens Outside C\nIn C, handling text files with Unicode, punctuation, and different encodings is messy. Instead, preprocessing is done once in Python, where tokenizers are easier to use. The results are saved in a simple binary format (uint16 IDs). From then on, C only has to deal with arrays of integers-clean and efficient.\nThis design keeps the training loop minimal: no text parsing, no string handling, just numbers.\n\n\nExample: Tiny Shakespeare\nOne of the simplest datasets is Tiny Shakespeare, about 1 MB of text from Shakespeare’s plays. The script prepare_shakespeare.py will:\n\nRead input.txt (the raw text).\nUse the GPT-2 tokenizer (gpt2_tokenizer.bin) to turn every word and symbol into token IDs.\nSplit 90% of the data into train.bin and 10% into val.bin.\n\nAfter running the script, you’ll have small binary files that let you train GPT-2 from scratch in minutes on CPU or GPU.\n\n\nExample: OpenWebText\nThe script prepare_openwebtext.py shows how to tokenize a much larger dataset, closer to what GPT-2 was originally trained on. This is heavier and requires more disk space, but it’s useful if you want to try scaling up training to bigger models.\n\n\nWhy It Matters\nThese scripts are more than convenience tools-they are examples of how to adapt llm.c to your own data. If you have a collection of emails, poems, or programming code, you can:\n\nPut them into a single .txt file.\nModify one of the scripts in dev/data/.\nGenerate new train.bin and val.bin files.\nTrain GPT-2 on your own text.\n\nBy separating dataset creation from training, llm.c keeps the C code small and makes experimentation flexible.\n\n\nTry It Yourself\n\nRun the Shakespeare script:\npython dev/data/prepare_shakespeare.py\nThen check that train.bin and val.bin were created.\nOpen the binary files with a hex viewer and confirm that they contain only numbers.\nModify the script to tokenize a different text file (for example, your own writing).\nCompare dataset sizes: Tiny Shakespeare is tiny (MBs), OpenWebText is huge (GBs). Observe how training speed changes depending on dataset size.\nRe-run training with your custom dataset and watch how the model starts generating text in your style.\n\n\n\nThe Takeaway\nThe dev/data/ scripts are the bridge between raw human text and the binary datasets used in training. They let you prepare small demo datasets or scale up to larger corpora. By experimenting with these scripts, you learn how to bring your own data into llm.c and train a GPT-style model on anything you like.\n\n\n\n14. DataLoader Design (Batching, Strides, Epochs)\nNow that the datasets are prepared as .bin files, we need a way to feed them into the model during training. This is the job of the DataLoader in llm.c. You’ll find its interface in llmc/dataloader.h, and its purpose is very simple: take a big stream of token IDs from train.bin or val.bin, cut it into manageable chunks, and serve those chunks to the training loop as batches.\n\nThe Core Idea\nTraining a language model requires two arrays for every batch:\n\nInputs: a sequence of token IDs, like [The, cat, sat, on]\nTargets: the same sequence shifted by one, like [cat, sat, on, the]\n\nThe model learns to predict each next token in the sequence. The DataLoader automates slicing these arrays out of the giant dataset file.\n\n\nThe Interface\nIn the code you’ll see function declarations like these:\nint dataloader_init(Dataloader *loader, const char *filename, int B, int T);\nint dataloader_next_batch(Dataloader *loader, int *inputs, int *targets);\nvoid dataloader_reset(Dataloader *loader);\nvoid dataloader_free(Dataloader *loader);\nHere’s what each does:\n\ndataloader_init: opens the dataset file, remembers batch size B and sequence length T.\ndataloader_next_batch: returns the next chunk of B × T tokens (inputs) and their shifted version (targets).\ndataloader_reset: rewinds to the start of the file when an epoch ends.\ndataloader_free: closes the file and releases memory.\n\nThis design keeps the training loop clean: just call next_batch and you get the data ready for forward/backward passes.\n\n\nB × T Explained\nThe two most important parameters are:\n\n\n\nSymbol\nMeaning\nExample\n\n\n\n\nB\nBatch size (how many sequences per step)\n16\n\n\nT\nSequence length (how many tokens per sequence)\n1024\n\n\n\nSo one batch contains B × T tokens. For example, with B = 16 and T = 1024, each batch holds 16,384 tokens. The DataLoader simply reads that many numbers from the binary file and arranges them in memory.\n\n\nStrides Through the Dataset\nAs you call dataloader_next_batch, the loader moves forward through the dataset by B × T tokens each time. When it reaches the end of the dataset file, it either:\n\nResets back to the beginning (dataloader_reset), or\nSwitches from training to validation, depending on the training loop’s needs.\n\nThis stride-based reading is efficient: no random access, just sequential reads from a file.\n\n\nEpochs and Shuffling\nIn deep learning, an epoch means one full pass through the dataset. The DataLoader in llm.c is simple: it goes linearly from start to finish. It doesn’t shuffle data like PyTorch’s DataLoader. Why? Because language data is already very diverse, and the project values minimal code over extra features. If you want shuffling, you can preprocess the dataset differently before creating .bin files.\n\n\nWhy It Matters\nThe DataLoader is the quiet workhorse of training. It ensures that every step sees a fresh batch of token sequences, always with matching inputs and targets. By separating dataset reading from the training loop, the code stays clean and focused. This design also makes it easy to swap datasets-once you generate a .bin file, the loader doesn’t care where it came from.\n\n\nTry It Yourself\n\nPrint the first batch: Modify the code to print the first 20 input tokens and their targets. See how each input token aligns with the next target token.\nExperiment with B and T: Set B = 2 and T = 8 and observe how the loader slices the dataset into tiny chunks. Then try larger values and see how memory usage changes.\nCheck epoch length: Write a small loop to count how many batches you get before dataloader_reset is called. Does this match the total tokens divided by B × T?\nValidation check: Observe how often the training loop switches to val.bin. How does validation loss compare to training loss over time?\nCustom stride: Modify the code so the DataLoader skips some tokens between batches. What effect does this have on training?\n\n\n\nThe Takeaway\nThe DataLoader in llm.c is intentionally simple. It streams token IDs in fixed-sized batches, moves forward stride by stride, and resets when done. This straightforward design avoids complexity and keeps the focus on the model itself, while still teaching you the essential mechanics of batching and sequence handling in language model training.\n\n\n\n15. EvalLoader and Validation Workflow\nTraining a model isn’t just about watching the training loss go down. To know whether the model is actually learning patterns that generalize-and not just memorizing the training data-you need to run validation. In llm.c, validation is handled by a component called the EvalLoader, which works just like the DataLoader but reads from the validation dataset (val.bin) instead of the training dataset (train.bin).\n\nWhy We Need Validation\nImagine teaching a student only by drilling them with the same math problems over and over. They might get really good at those problems, but fail completely when given new ones. Validation is like giving the student a pop quiz with unseen questions. If they do well, you know they’ve actually learned the concepts.\nFor language models, validation helps detect overfitting: when the training loss keeps improving but the validation loss stays flat or even gets worse.\n\n\nHow EvalLoader Works\nEvalLoader lives in the same code file as the DataLoader (llmc/dataloader.h), but it points to a different dataset file. Its workflow is nearly identical:\n\nOpen val.bin and prepare for reading.\nServe up batches of size B × T (batch size × sequence length).\nProvide inputs and targets the same way as the training DataLoader.\nReset after one full pass through the file.\n\nThe training loop typically calls the EvalLoader at intervals-for example, every few hundred steps-so you get a snapshot of validation loss during training.\n\n\nWhat Happens During Validation\nWhen validation is triggered:\n\nThe current model parameters are frozen (no gradient updates).\nA few batches are read from val.bin.\nThe model runs forward passes only, computing the loss on each batch.\nThe losses are averaged and reported as validation loss.\n\nThis doesn’t take long because it usually samples just a subset of the validation dataset, not the entire file.\n\n\nTraining Loop with Validation\nIn pseudocode, the loop looks like this:\nfor (step = 0; step &lt; max_steps; step++) {\n    dataloader_next_batch(&train_loader, inputs, targets);\n    forward_backward_update(model, inputs, targets);\n    \n    if (step % eval_interval == 0) {\n        float val_loss = 0.0f;\n        for (int i = 0; i &lt; eval_batches; i++) {\n            evalloader_next_batch(&val_loader, inputs, targets);\n            val_loss += forward_only(model, inputs, targets);\n        }\n        val_loss /= eval_batches;\n        printf(\"step %d: val loss %.4f\\n\", step, val_loss);\n    }\n}\nThis is simplified, but it shows the idea: the validation loop is nested inside the training loop, running occasionally instead of every step.\n\n\nWhy It Matters\nValidation is the reality check of training. Without it, you could train forever and celebrate low training losses, only to discover that your model produces nonsense on new text. By tracking validation loss, you can:\n\nDetect overfitting early.\nAdjust hyperparameters (like learning rate or batch size).\nKnow when training has plateaued and it’s time to stop.\n\nIn professional setups, validation curves are often plotted live, but in llm.c, the minimalist approach is to just print numbers to the console.\n\n\nTry It Yourself\n\nWatch val loss: Run training and note how validation loss compares to training loss. Do they both decrease together?\nOverfitting demo: Train on a very tiny dataset (like 10 KB of text). Notice how training loss plummets but validation loss stalls or rises.\nChange eval interval: Reduce eval_interval so validation runs every step. How much slower does training feel?\nChange eval batches: Set eval_batches to 1 vs 100. What difference does this make in the stability of the reported validation loss?\nValidation as stopping rule: Stop training when validation loss stops improving for many intervals. How does this affect final performance?\n\n\n\nThe Takeaway\nThe EvalLoader is a twin of the DataLoader, but for validation. It feeds the model data it has never seen during training, and the resulting validation loss tells you whether your model is learning useful patterns or just memorizing. It’s the simplest safeguard against wasted compute, and it’s an essential part of every training loop-even in the stripped-down world of llm.c.\n\n\n\n16. Sequence Length and Memory Budgeting\nWhen training GPT-2 in llm.c, one of the most important decisions you make is choosing the sequence length (often called T). This value determines how many tokens the model processes in a single forward pass. It might sound like just another parameter, but sequence length has a huge impact on what the model can learn, how much memory it uses, and how fast training runs.\n\nWhat Sequence Length Means\nSequence length is simply the number of tokens per training example. If T = 1024, the model reads 1,024 tokens in a row (like words or subwords) and tries to predict the next token at each position.\nThink of it like this: if you give the model a paragraph of text, sequence length is how much of that paragraph it sees at once. Shorter lengths give the model less context, while longer lengths allow it to capture bigger patterns, like whole paragraphs or even multiple pages.\n\n\nWhere It Appears in the Code\nIn the logs, you’ll often see lines like:\nmax_seq_len: 1024\nThis number is defined in the GPT-2 configuration and passed into the DataLoader. The DataLoader slices chunks of exactly T tokens from train.bin and val.bin. The model itself has fixed positional embeddings of size T, so it cannot process sequences longer than this maximum.\n\n\nMemory Costs of Longer Sequences\nTransformers are powerful but expensive. The attention mechanism compares every token to every other token in the sequence. This means memory and compute scale with the square of sequence length:\n\n\n\nSequence Length (T)\nRelative Attention Cost\n\n\n\n\n256\n1×\n\n\n512\n4×\n\n\n1024\n16×\n\n\n2048\n64×\n\n\n\nSo doubling T doesn’t just double the cost-it multiplies it by four. That’s why training at long context lengths requires a lot of GPU memory.\n\n\nTrade-offs\n\nShorter sequences: Faster, less memory, but limited context. Good for quick experiments or tiny datasets like Tiny Shakespeare.\nLonger sequences: More memory, slower, but the model can understand larger spans of text. Required for large-scale GPT-2 training.\n\nYou can think of sequence length as a dial: turning it up increases the model’s ability to “remember,” but it also makes training much heavier.\n\n\nPractical Choices in llm.c\n\nTiny Shakespeare example: often trained with T = 64 or 128 for speed.\nGPT-2 small (124M): typically uses T = 1024, the same as the original paper.\nIf your GPU has limited memory, you might need to shrink T and/or batch size B.\n\n\n\nWhy It Matters\nChoosing sequence length is about balancing learning power against hardware limits. A too-small sequence length can prevent the model from capturing long-term dependencies. A too-large one can make training impossible on your hardware. Every run of llm.c is a negotiation between what you’d like the model to see and what your system can handle.\n\n\nTry It Yourself\n\nShort vs long: Train Tiny Shakespeare with T = 64 and then T = 256. Compare both the speed and the coherence of generated text.\nMemory test: Increase T step by step until you hit an out-of-memory (OOM) error. Note the maximum your GPU can handle.\nBatch trade-off: Try reducing batch size B while increasing T. Can you keep GPU memory stable while giving the model more context?\nValidation impact: Run with different T values and watch how validation loss behaves. Does longer context always help?\nInspect embeddings: Print out the shape of the positional embeddings. Notice how they are always tied to T.\n\n\n\nThe Takeaway\nSequence length (T) controls how much context the model sees. It directly determines the size of the positional embeddings, the structure of batches, and the memory required for attention. In llm.c, adjusting T is one of the fastest ways to explore the trade-offs between speed, memory, and model capability.\n\n\n\n17. Reproducibility and Seeding Across Runs\nWhen training machine learning models, it’s common to notice that two runs-using the same code and the same dataset-don’t produce exactly the same results. This happens because many parts of training involve randomness. In llm.c, reproducibility is controlled by random seeds. A seed is a starting point for a random number generator. If you always start from the same seed, the sequence of “random” numbers will be identical, and so will the training run.\n\nWhere Randomness Appears\nEven in a small project like llm.c, randomness shows up in several places:\n\n\n\n\n\n\n\nComponent\nRandom Role\n\n\n\n\nWeight initialization\nThe model’s parameters (like attention matrices) are set randomly at the start.\n\n\nOptimizer states\nSome optimizers use random noise (though AdamW is mostly deterministic).\n\n\nSampling outputs\nWhen generating text, randomness decides which token to pick if probabilities are close.\n\n\nParallelism\nOn GPU, threads may execute in slightly different orders, sometimes introducing small nondeterminism.\n\n\n\nWithout a fixed seed, every training run can drift apart, even if all settings look the same.\n\n\nHow llm.c Handles Seeds\nThe repository provides a small random utilities module: llmc/rand.h. Inside you’ll find functions such as:\nvoid manual_seed(uint64_t seed);\nfloat normal_(float mean, float std);\n\nmanual_seed sets the seed for the internal random number generator, ensuring reproducibility.\nnormal_ is used for initializing weights with Gaussian noise, similar to PyTorch’s torch.nn.init.normal_.\n\nWhen you call manual_seed(1337);, the model weights will be initialized the same way every time.\n\n\nWhy Seeds Don’t Guarantee Perfect Reproducibility\nEven with a fixed seed, you may still see small differences:\n\nGPU kernels sometimes use parallel algorithms that are not bitwise deterministic.\nFloating-point math can produce slightly different rounding on different hardware.\nMulti-GPU runs (via NCCL/MPI) may introduce nondeterministic reduce operations.\n\nThese differences are usually tiny-validation loss might vary by 0.001-but they exist. For most educational purposes, llm.c seeds are enough to make experiments repeatable.\n\n\nTypical Defaults\nIn many examples, you’ll see:\nmanual_seed(1337);\nThis “magic number” 1337 is just a convention. You can change it to any integer. Using the same seed across runs guarantees the same starting weights, which helps when comparing hyperparameters.\n\n\nWhy It Matters\nReproducibility is crucial in machine learning because it lets you:\n\nDebug effectively: If a bug appears, you want it to appear consistently.\nCompare settings: You can test learning rates or batch sizes fairly by keeping everything else the same.\nShare results: Other people can run your exact setup and see the same outcomes.\n\nWithout seeds, it becomes hard to tell whether a difference came from your hyperparameter change or just random luck.\n\n\nTry It Yourself\n\nRun twice with same seed: Train GPT-2 with manual_seed(1337) set. Do you get identical training loss curves?\nChange the seed: Try manual_seed(42) and compare the loss curve. How similar are they? Do they converge to about the same final validation loss?\nRemove seeding: Comment out the seed line and run again. Notice how runs diverge.\nSampling experiment: With a fixed seed, generate text multiple times. Then change the seed and generate again. See how outputs change.\nMulti-GPU test: If you have more than one GPU, run the same seed across devices. Do results stay exactly the same or only approximately?\n\n\n\nThe Takeaway\nReproducibility in llm.c comes from setting seeds for random number generators. While floating-point quirks mean you can’t always get perfect bit-for-bit matches, seeds let you control the biggest source of randomness: weight initialization and sampling. With seeding, you can debug, compare, and share results confidently.\n\n\n\n18. Error Surfaces from Bad Data (Bounds, Asserts)\nWhen training a model in llm.c, everything depends on the quality and correctness of the data you feed in. If the dataset or batches contain mistakes, the training process can go off track quickly-sometimes by crashing outright, other times by producing strange loss values like NaN. To guard against this, the code uses bounds checks and asserts that catch problems early.\n\nWhat Can Go Wrong with Data\nThere are several common data issues:\n\n\n\n\n\n\n\nProblem\nWhat Happens\n\n\n\n\nToken ID out of range\nThe model expects IDs between 0 and vocab_size-1. A wrong ID can cause array indexing errors.\n\n\nEmpty or short dataset\nThe DataLoader may run out of tokens before filling a batch.\n\n\nMismatched tokenizer\nIf you build a dataset with a different tokenizer, IDs may not correspond to the GPT-2 tokenizer in gpt2_tokenizer.bin. This produces nonsense outputs.\n\n\nCorrupt .bin files\nIf files are incomplete or written incorrectly, the DataLoader might read garbage values.\n\n\n\nThese errors show up as segfaults, invalid memory access, or exploding losses during training.\n\n\nHow llm.c Defends Against Bad Data\nThe repository makes heavy use of asserts-simple checks that stop the program immediately if something unexpected happens. For example, in llmc/utils.h, functions like freadCheck and mallocCheck ensure that file reads and memory allocations succeed. If not, they print an error message and abort instead of silently failing.\nInside the DataLoader, token IDs are often validated to make sure they fall inside the expected vocabulary range. If you try to access an invalid index in the embedding table, the program will crash quickly, which is better than continuing with corrupted values.\n\n\nExample: Vocab Range Check\nDuring training, every input token is used to look up a row in the embedding matrix. If a token ID is too large, you’d access memory outside the matrix. This is why checking 0 &lt;= id &lt; vocab_size is essential. In C, asserts provide this safety net.\nassert(id &gt;= 0 && id &lt; vocab_size);\nThis kind of check may look simple, but it saves hours of debugging mysterious crashes.\n\n\nError Surfaces in Loss\nEven if your program doesn’t crash, bad data can create “error surfaces” in the loss function:\n\nNaNs: Appear when invalid values propagate through softmax, layernorm, or division operations.\nFlat loss: If the dataset is empty or repetitive, the model never improves.\nMismatch behavior: Training loss decreases but validation loss stays high if training and validation sets use inconsistent tokenization.\n\nThese are signs that something is wrong with the dataset or preprocessing.\n\n\nWhy It Matters\nC is a low-level language with very little safety by default. One out-of-range index can corrupt memory and cause unpredictable bugs. By aggressively checking assumptions (file sizes, vocab bounds, token IDs), llm.c turns hard-to-find errors into immediate, clear failures. For learners, this makes it much easier to understand what went wrong.\n\n\nTry It Yourself\n\nCorrupt a dataset: Open train.bin and delete a few bytes. Run training and see what error appears. Notice how quickly asserts catch it.\nForce a bad ID: Modify the DataLoader to add +100000 to a token. Does the model crash with an assertion?\nSkip asserts: Temporarily disable checks and rerun. Compare how much harder it is to figure out what went wrong.\nValidation mismatch: Tokenize a file with a different tokenizer and save it as val.bin. Watch how the validation loss behaves compared to training loss.\nPrint debug info: Add logging to display the first 20 tokens of each batch. Can you spot bad data before it crashes?\n\n\n\nThe Takeaway\nBad data can silently sabotage training, but llm.c uses asserts and bounds checks to make errors loud and clear. This design choice helps learners focus on the real logic of transformers instead of chasing hidden bugs caused by corrupted or mismatched datasets. In machine learning, good data hygiene and strict validation are as important as the model itself.\n\n\n\n19. Tokenization Edge Cases (UNKs, EOS, BOS)\nTokenization looks simple at first: take text, split it into tokens, and assign each token an ID. But in practice, there are always tricky situations. llm.c inherits the quirks of the GPT-2 tokenizer, which is byte-level BPE (Byte Pair Encoding). This design mostly avoids “unknown” tokens, but it still has details you need to understand when preparing datasets or interpreting outputs.\n\nNo True “UNK” in GPT-2\nSome tokenizers, like those used in earlier NLP systems, include a special UNK (unknown) token for words that aren’t in the vocabulary. GPT-2 avoids this problem by working at the byte level:\n\nEvery possible byte (0–255) is in the base vocabulary.\nIf the tokenizer doesn’t know how to split a character or word, it just falls back to raw bytes.\n\nThat means you will never see an UNK token in llm.c. Any input text is always representable. This is one of the main reasons GPT-2’s tokenizer is so robust.\n\n\nSpecial Tokens: EOS and BOS\nEven though GPT-2 doesn’t use UNK, it does use other special tokens:\n\n\n\n\n\n\n\n\nToken\nID\nPurpose\n\n\n\n\nEOS (End of Sequence)\n50256\nMarks the end of a text segment. Used during training and sampling.\n\n\nBOS (Beginning of Sequence)\nNot explicit in GPT-2\nGPT-2 doesn’t use a fixed BOS token. Instead, the model assumes generation starts at position 0.\n\n\n\nIn llm.c, you’ll often see EOS at the end of training sequences or when sampling text. If you generate text and see strange endings, it’s usually because the model predicted EOS.\n\n\nWhitespace Quirks\nThe tokenizer also handles whitespace in a slightly unusual way. For example, the word “hello” and the word ” hello” (with a leading space) map to different tokens. This is why generated text sometimes starts with a space-it’s part of the token definition.\nExample:\n\n\"hello\" → token ID 31373\n\" hello\" → token ID 15496\n\nThis is normal behavior for GPT-2. It helps the model capture spacing and punctuation consistently.\n\n\nUnicode and Rare Characters\nBecause it’s byte-level, GPT-2 can encode emojis, accented characters, or even binary junk data. But the BPE merges are optimized for English, so rare characters often get split into multiple byte tokens. That means sequences with lots of rare symbols (like Chinese or emojis) will use more tokens than plain English text.\n\n\nWhy It Matters\nEdge cases in tokenization affect both dataset preparation and model outputs. If you see weird spacing or early EOS tokens, it’s not a bug-it’s just how the tokenizer works. Understanding these quirks helps you debug outputs and prepare datasets without surprises.\n\n\nTry It Yourself\n\nEOS inspection: Open val.bin with a hex viewer and look for token ID 50256. These mark the ends of text segments.\nWhitespace check: Use the tokenizer to encode \"hello\" and \" hello\". Compare the token IDs.\nEmoji test: Encode a string with emojis (e.g., \"🙂🙂🙂\") and see how many tokens it becomes.\nRare character dataset: Create a small .txt file with accented characters and tokenize it. How many bytes does each character consume?\nSampling experiment: Generate text until you see the EOS token appear. Notice how the model “knows” to stop.\n\n\n\nThe Takeaway\nTokenization in GPT-2 is robust, but it has quirks. There are no unknown tokens thanks to byte-level encoding, but whitespace and special tokens like EOS play important roles. By experimenting with these edge cases, you’ll develop an intuition for how raw text is mapped into the numbers that drive training and generation in llm.c.\n\n\n\n20. Data Hygiene and Logging\nWhen training with llm.c, having clean data is just as important as having the right model code. If the dataset contains errors, duplicates, or formatting issues, the model may waste capacity memorizing noise instead of learning useful patterns. This is where data hygiene comes in-making sure your training and validation sets are prepared properly. Alongside this, logging ensures you can monitor what’s happening during training and catch problems early.\n\nWhat Data Hygiene Means\nData hygiene is about making sure your dataset is both valid and useful. For language models, this includes:\n\n\n\n\n\n\n\nCheck\nWhy It Matters\n\n\n\n\nCorrect tokenization\nMust match the tokenizer (gpt2_tokenizer.bin), otherwise IDs won’t line up.\n\n\nNo corrupt files\nBinary .bin files must be complete; partial writes cause crashes.\n\n\nBalanced splits\nTraining and validation sets should come from the same distribution.\n\n\nReasonable size\nToo small → overfitting. Too large → slow or infeasible.\n\n\nDeduplication\nRepeated passages (e.g., web scrapes) make models memorize instead of generalize.\n\n\n\nThe scripts in dev/data/ handle basic hygiene by tokenizing consistently and splitting into train/val sets. But if you bring your own dataset, you are responsible for cleaning it first.\n\n\nLogging During Training\nOnce training starts, logging becomes your window into what’s happening. llm.c uses a minimal logging system (llmc/logger.h) to print progress to the console. Typical logs include:\nstep 0: train loss 5.19, val loss 5.32\nstep 100: train loss 4.87, val loss 5.01\nstep 200: train loss 4.62, val loss 4.88\nThese numbers let you track:\n\nTraining loss: Is the model fitting the data?\nValidation loss: Is it generalizing, or overfitting?\nStep timing: How long each batch takes, useful for profiling.\n\nEven in such a small project, this logging loop gives you most of what you need to debug runs.\n\n\nWhy Hygiene and Logging Go Together\nBad data often reveals itself in the logs. For example:\n\nIf validation loss is much higher than training loss, your validation set may be mismatched.\nIf loss suddenly becomes NaN, your dataset might contain corrupt tokens.\nIf loss plateaus at a high value, you may have too little data or poor preprocessing.\n\nBy keeping your data clean and watching logs closely, you can detect these issues early instead of wasting hours of compute.\n\n\nTry It Yourself\n\nDirty dataset test: Take a .txt file, add random symbols or binary junk, and prepare a .bin dataset. What happens to training loss?\nDuplicate passages: Copy the same paragraph 100 times into a training file. Does validation loss improve, or does the model just memorize?\nLog frequency: Modify the code to log every step instead of every N steps. How noisy are the results?\nCustom logger: Extend the logger to also print gradient norms or learning rate values. Does this help you understand training dynamics better?\nCompare splits: Build two datasets with different train/val splits. Which one gives more stable validation losses?\n\n\n\nThe Takeaway\nData hygiene ensures the model learns from clean, consistent input, while logging ensures you can see whether learning is actually happening. Together, they form the foundation of reliable experiments in llm.c. If you clean your data carefully and pay attention to the logs, you’ll catch most problems before they become serious.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html#chapter-3.-model-definition-and-weights",
    "href": "books/en-US/book.html#chapter-3.-model-definition-and-weights",
    "title": "The Book",
    "section": "Chapter 3. Model Definition and Weights",
    "text": "Chapter 3. Model Definition and Weights\n\n21. GPT-2 Config: Vocab, Layers, Heads, Channels\nEvery GPT-2 model, no matter how large or small, is defined by a handful of configuration numbers. These numbers decide how big the model is, how much memory it needs, and how powerful it can become. In llm.c, these settings are stored in a simple config struct and printed at the start of training. They describe the “blueprint” of the transformer.\n\nThe Core Parameters\nHere are the most important values you’ll see in the logs:\n\n\n\n\n\n\n\n\nParameter\nMeaning\nExample (GPT-2 Small)\n\n\n\n\nvocab_size\nNumber of distinct tokens (from tokenizer).\n50,257\n\n\npadded_vocab_size\nVocab size rounded up to nearest multiple (for GPU efficiency).\n50,304\n\n\nmax_seq_len\nLongest sequence of tokens the model can handle.\n1,024\n\n\nnum_layers\nNumber of transformer blocks stacked on top of each other.\n12\n\n\nnum_heads\nNumber of attention heads per block.\n12\n\n\nchannels\nWidth of hidden states (embedding dimension).\n768\n\n\nnum_parameters\nTotal trainable weights in the model.\n~124M\n\n\n\nTogether, these values define both the structure and the capacity of the model.\n\n\nWhat They Control\n\nVocabulary size connects the model to the tokenizer. Every input token ID must be less than vocab_size. The padded version makes GPU matrix multiplications easier.\nMax sequence length fixes the size of the positional embeddings. If you set this to 1024, the model can’t read beyond 1024 tokens in one pass.\nLayers control model depth. Each layer contains an attention block and an MLP. More layers = more representational power.\nHeads divide attention into parallel “subspaces.” With 12 heads, the model can track different types of relationships in the text at the same time.\nChannels set the dimensionality of embeddings and hidden vectors. Larger channels mean more expressive representations but also more computation.\nParameters are the sum of it all. This number tells you how heavy the model is to train and how much memory it will consume.\n\n\n\nConfigs Across GPT-2 Sizes\nThe original GPT-2 models come in several sizes:\n\n\n\nModel\nLayers\nHeads\nChannels\nParameters\n\n\n\n\nGPT-2 Small\n12\n12\n768\n124M\n\n\nGPT-2 Medium\n24\n16\n1024\n355M\n\n\nGPT-2 Large\n36\n20\n1280\n774M\n\n\nGPT-2 XL\n48\n25\n1600\n1.6B\n\n\n\nllm.c can scale between these by just changing a few numbers in the config struct.\n\n\nWhere Config Appears in the Code\nIn train_gpt2.c and train_gpt2.cu, you’ll see something like:\nGPT2Config config = {\n    .vocab_size = 50257,\n    .max_seq_len = 1024,\n    .num_layers = 12,\n    .num_heads = 12,\n    .channels = 768,\n};\nLater, the model is initialized using this struct, and the log prints all the derived information (like num_parameters).\n\n\nWhy It Matters\nThe config is the contract between your dataset and your model.\n\nIf vocab_size doesn’t match your tokenizer, you’ll get crashes.\nIf max_seq_len is too small, you’ll lose context.\nIf num_layers or channels are too large for your GPU, you’ll run out of memory.\n\nBy tweaking the config, you decide whether you want a tiny model for learning or a massive one closer to GPT-2 XL.\n\n\nTry It Yourself\n\nPrint config: Run the trainer and note the printed values. Compare them with the GPT-2 sizes in the table.\nShrink the model: Change num_layers = 4, num_heads = 4, and channels = 256. Train on Tiny Shakespeare and see how fast it runs.\nIncrease sequence length: Try setting max_seq_len = 2048. Does your GPU still handle it, or do you get out-of-memory errors?\nParameter count check: Compute how many parameters your custom config has. Compare it to the reported num_parameters.\nTokenizer mismatch test: Intentionally set vocab_size = 30000 and watch what error appears when loading the tokenizer.\n\n\n\nThe Takeaway\nThe GPT-2 config struct in llm.c is small but powerful. It defines everything about the model’s architecture: vocabulary, sequence length, depth, width, and total parameters. By adjusting just a few integers, you can scale from a toy model that runs on CPU to a billion-parameter giant (if your hardware allows it). Understanding these numbers is the first step to understanding how transformer capacity is controlled.\n\n\n\n22. Parameter Tensors and Memory Layout\nOnce the GPT-2 configuration is set, the next big step is to allocate the parameters of the model. These are the trainable numbers-weights and biases-that define how the model processes input tokens. In llm.c, parameters are stored in flat arrays of floats rather than in deeply nested objects like in PyTorch. This choice makes the code easier to read and keeps memory access predictable.\n\nWhat Are Parameters?\nEvery part of the transformer has its own trainable weights:\n\nEmbedding tables: one for tokens and one for positions.\nAttention layers: query, key, value, and output projections.\nMLP layers: two linear layers plus their biases.\nLayerNorms: scale (gamma) and shift (beta) values.\nFinal projection: maps hidden states back to vocab size for logits.\n\nTogether, these add up to hundreds of millions of numbers, even for GPT-2 Small.\n\n\nFlat Memory Design in llm.c\nInstead of allocating each parameter separately, llm.c stores all parameters in one contiguous block of memory. Each layer is given a slice of this big array.\nThis has two benefits:\n\nSimplicity: You only need one malloc (or cudaMalloc) for all parameters.\nPerformance: Contiguous memory access is faster on both CPU and GPU.\n\nTo keep track of where each layer’s weights live inside the block, the code uses offsets.\n\n\nExample in Code\nIn train_gpt2.c, parameters are packed into a single array:\nfloat* params = (float*)mallocCheck(config.num_parameters * sizeof(float));\nLater, helper functions compute pointers into this array for each sub-module. For example, the token embedding weights are just the first slice:\nfloat* token_embedding_table = params;\nThen the program moves forward, assigning chunks to positional embeddings, attention weights, and so on.\n\n\nShapes of the Tensors\nEven though parameters are stored in 1D memory, they conceptually form 2D or 3D tensors. For example:\n\n\n\n\n\n\n\n\nParameter\nShape\nPurpose\n\n\n\n\nToken embeddings\n[vocab_size, channels]\nMaps token IDs to vectors.\n\n\nPositional embeddings\n[max_seq_len, channels]\nAdds position info.\n\n\nAttention weights (Q, K, V, O)\n[channels, channels]\nProject hidden states.\n\n\nMLP layers\n[channels, 4×channels] and [4×channels, channels]\nExpand and contract hidden states.\n\n\nLayerNorm scale/shift\n[channels]\nNormalize and rescale features.\n\n\n\nWhen you look at the code, remember: these shapes are “virtual.” They’re just views into slices of the big 1D array.\n\n\nWhy This Layout Works Well\nPyTorch or TensorFlow manage parameter tensors with lots of abstractions. llm.c strips this away: you see the raw memory, the exact number of parameters, and the order they’re laid out in. This makes it clear how large the model really is and why it uses so much RAM or VRAM.\nIt also means you can easily save and load checkpoints by writing or reading the flat array directly to disk. No need for complicated serialization formats.\n\n\nWhy It Matters\nUnderstanding parameter layout helps you:\n\nSee how the model’s size explodes as you increase layers, heads, or channels.\nDebug memory issues by checking how big each slice is.\nRealize how much of training is just linear algebra on big arrays of floats.\n\nThis perspective is powerful because it demystifies deep learning: at its core, GPT-2 is just multiplying slices of one giant float array again and again.\n\n\nTry It Yourself\n\nPrint parameter count: Add a line in the code to print config.num_parameters. Compare it with the table for GPT-2 Small/Medium/Large.\nInspect a slice: Print the first 10 numbers of the embedding table. They’ll look random (from initialization).\nChange precision: Modify the code to allocate half (FP16) instead of float. How much memory do you save?\nCheckpoint peek: Save a checkpoint, then open it in a hex viewer. It’s just raw floats-proof that parameters are stored flat.\nParameter scaling: Double the number of layers and see how num_parameters changes. Can you predict the increase?\n\n\n\nThe Takeaway\nIn llm.c, parameters are not hidden inside classes or objects. They live in one flat block of memory, sliced up by convention into embeddings, attention matrices, MLP weights, and norms. This design makes the relationship between model architecture and memory crystal clear-and reminds you that even a billion-parameter transformer is “just” a giant array of numbers.\n\n\n\n23. Embedding Tables: Token + Positional\nBefore a transformer can reason about text, it first needs to turn tokens into vectors. In llm.c, this job is handled by the embedding tables: one for tokens, one for positions. These tables are the very first layer of GPT-2, and they transform plain integer IDs into continuous values that the neural network can process.\n\nToken Embedding Table\nWhen you feed in a batch of token IDs, the model looks up their corresponding vectors in the token embedding table.\n\nShape: [vocab_size, channels]\n\nvocab_size ≈ 50,257 (for GPT-2)\nchannels = hidden size (768 for GPT-2 Small)\n\nEach row corresponds to one token in the vocabulary.\nEach row is a dense vector of size channels.\n\nSo if your input batch has size (B, T), looking up embeddings gives you a tensor of shape (B, T, channels).\nIn the code, this is implemented as an array slice from the flat parameter block:\nfloat* token_embedding_table = params;  // first slice of parameters\nAt runtime, token IDs index directly into this table.\n\n\nPositional Embedding Table\nTransformers don’t inherently know about word order. That’s what positional embeddings are for.\n\nShape: [max_seq_len, channels]\n\nmax_seq_len = 1024 in GPT-2 Small\nSame channel dimension as token embeddings\n\nEach position (0, 1, 2, …, 1023) has its own vector.\n\nDuring training, when the model sees token i at position j, it takes the token embedding vector and adds the positional embedding vector for j. This gives the model both word identity and word position.\nIn llm.c, positional embeddings immediately follow the token embeddings in the flat parameter array.\n\n\nAdding Them Together\nThe embedding layer’s forward pass is simple:\nembedding_out[token, pos] = token_embedding[token] + positional_embedding[pos]\nThis results in a (B, T, channels) tensor that becomes the input to the first transformer block.\n\n\nWhy This Matters\nEmbeddings are the bridge between discrete tokens and continuous math. Without them, the model couldn’t use linear algebra to learn patterns. By adding positional embeddings, GPT-2 knows the difference between:\n\n“dog bites man” → dog comes first, man comes last\n“man bites dog” → same tokens, but swapped positions change the meaning\n\nThis small step is essential: order and identity must both be captured before attention can begin.\n\n\nTry It Yourself\n\nInspect shapes: Print the sizes of the token and positional embedding tables during initialization. Confirm they match [vocab_size, channels] and [max_seq_len, channels].\nLook at first rows: Print the first 5 vectors of the token embedding table. They should look like small random floats from initialization.\nChange max_seq_len: Double max_seq_len in the config. How does this change the size of the positional table? Does training still work?\nOverwrite embeddings: Try setting the token embedding table to all zeros. What happens to training loss?\nSampling experiment: After training a few steps, decode outputs without adding positional embeddings. Do the results become nonsensical or repetitive?\n\n\n\nThe Takeaway\nThe embedding tables are the foundation of GPT-2. Token embeddings give meaning to symbols, while positional embeddings give structure to sequences. In llm.c, they are just two slices of the flat parameter array, added together at the very start of the forward pass-but without them, the transformer would be blind to both words and order.\n\n\n\n24. Attention Stack: QKV Projections and Geometry\nAfter embeddings, the real magic of transformers begins: the attention mechanism. In GPT-2, every transformer block contains an attention stack. This is where the model learns how each token relates to others in the sequence-whether it’s paying attention to the previous word, the beginning of a sentence, or even punctuation marks far away.\n\nWhat Attention Does\nAttention lets the model answer the question:\n\n“Given the current word, which other words in the context should I care about, and how much?”\n\nInstead of treating words independently, the model uses attention to build connections across the sequence.\n\n\nThe Q, K, V Projections\nEach attention block starts with three linear projections:\n\n\n\n\n\n\n\n\nName\nShape\nPurpose\n\n\n\n\nQ (Query)\n[channels, channels]\nRepresents what each token is asking about.\n\n\nK (Key)\n[channels, channels]\nRepresents how each token can be recognized.\n\n\nV (Value)\n[channels, channels]\nRepresents the actual information to pass along.\n\n\n\nHere’s the flow:\n\nEach input vector (from embeddings or previous block) is multiplied by these three matrices to produce Q, K, and V vectors.\nAttention scores are computed by comparing Qs with Ks.\nThese scores are used to weight the Vs, mixing information from other tokens into the current one.\n\n\n\nGeometry of Attention\n\nQ and K define a similarity score: how well does this token match another one?\nV carries the actual features (like meaning, grammar cues).\nThe result is a weighted sum: tokens borrow information from others based on attention scores.\n\nIn equations:\nscores = Q × K^T / sqrt(d_k)\nweights = softmax(scores + mask)\noutput  = weights × V\nThe division by sqrt(d_k) normalizes scores so they don’t blow up as dimensions grow.\n\n\nMulti-Head Attention\nGPT-2 doesn’t use just one attention projection-it uses many in parallel, called heads. Each head learns to focus on different types of relationships:\n\nOne head might track subject–verb agreement.\nAnother might watch punctuation and quotes.\nAnother might connect pronouns to their referents.\n\nFor GPT-2 Small:\n\n12 heads per layer\nEach head works on a reduced dimension (channels / num_heads)\nOutputs are concatenated and projected back to channels\n\nThis setup is what gives transformers their flexibility.\n\n\nImplementation in llm.c\nIn the parameter array, each transformer block has slices for Q, K, V, and output projection (O). During forward pass:\n\nMultiply input by Q, K, V matrices.\nReshape into heads.\nCompute attention scores (masked to prevent looking forward).\nApply softmax.\nMultiply by V to get weighted values.\nConcatenate heads and apply the O projection.\n\nAll of this is done with plain matrix multiplications and softmax calls-no magic beyond linear algebra.\n\n\nWhy It Matters\nAttention is the beating heart of GPT-2. It’s how the model captures dependencies across text, from short-term grammar to long-range coherence. Without QKV, embeddings would stay isolated, and the model could never build context-aware representations.\n\n\nTry It Yourself\n\nPrint shapes: Log the shapes of Q, K, V matrices in one layer. Confirm they match [channels, channels].\nVisualize scores: After a forward pass, print the attention weights for one head. Do they concentrate on recent tokens or spread across the sequence?\nReduce heads: Change num_heads from 12 to 4. What happens to validation loss?\nBreak symmetry: Initialize all Q, K, V matrices with zeros. Does training loss decrease at all?\nMask experiment: Disable the causal mask (allow looking ahead). Does the model “cheat” by predicting future tokens perfectly?\n\n\n\nThe Takeaway\nThe attention stack is where tokens stop being isolated and start talking to each other. Q, K, and V projections turn context into weighted relationships, and multi-head attention lets the model juggle many types of dependencies at once. In llm.c, this is implemented with straightforward linear algebra, making the most powerful idea in modern NLP visible and accessible.\n\n\n\n25. MLP Block: Linear Layers + Activation\nAfter attention mixes information across tokens, GPT-2 applies a second transformation inside each block: the MLP (Multi-Layer Perceptron). This part doesn’t look at other tokens-it processes each position independently. But it’s just as important because it gives the model extra capacity to transform and refine the hidden features before passing them to the next layer.\n\nWhat the MLP Looks Like\nEvery transformer block contains an MLP with two linear layers and a nonlinear activation in between:\nhidden = Linear1(x)\nhidden = GELU(hidden)\nout    = Linear2(hidden)\nThis structure expands the feature dimension and then compresses it back down, which lets the network learn richer representations.\n\n\nShapes of the Layers\nIf the hidden size (channels) is d_model, the MLP works as follows:\n\n\n\n\n\n\n\n\nStep\nShape\nPurpose\n\n\n\n\nInput\n[B, T, d_model]\nOutput of attention for each token.\n\n\nLinear1\n[d_model, 4 × d_model]\nExpands features 4× wider.\n\n\nGELU\nelementwise\nIntroduces nonlinearity.\n\n\nLinear2\n[4 × d_model, d_model]\nProjects back to original size.\n\n\nOutput\n[B, T, d_model]\nSame shape as input, ready for residual add.\n\n\n\nFor GPT-2 Small (d_model = 768), Linear1 expands to 3072 channels, then Linear2 reduces back to 768.\n\n\nActivation: GELU\nThe activation function in GPT-2 is GELU (Gaussian Error Linear Unit). It’s smoother than ReLU, giving the model a more nuanced way of handling values around zero. In code, GELU looks like:\nfloat gelu(float x) {\n    return 0.5f * x * (1.0f + tanhf(0.79788456f * (x + 0.044715f * x * x * x)));\n}\nThis formula may look complicated, but the idea is simple: it smoothly squashes negative values toward zero and keeps positive values flowing through.\n\n\nWhy Expand and Shrink?\nThe expansion to 4 × d_model may seem wasteful, but it’s deliberate:\n\nExpanding gives the model more capacity to represent patterns at each token.\nShrinking keeps the overall parameter count manageable.\nTogether, they act like a bottleneck layer that forces the model to transform information more effectively.\n\nThis “expand → activate → shrink” design is one of the main reasons transformers scale so well.\n\n\nImplementation in llm.c\nJust like attention, the MLP parameters live in the flat array of floats. Each block stores two weight matrices and two bias vectors. During forward pass:\n\nMultiply input by Linear1 weights, add bias.\nApply GELU elementwise.\nMultiply by Linear2 weights, add bias.\nPass result through residual connection.\n\nBecause each position is processed independently, the MLP is easy to parallelize across tokens.\n\n\nWhy It Matters\nThe MLP is the nonlinear refiner of transformer blocks. Attention spreads information, but MLPs transform it in-place, giving the model more expressive power. Without the MLP, the network would be mostly linear, limiting its ability to capture complex patterns in text.\n\n\nTry It Yourself\n\nPrint shapes: Log the dimensions of Linear1 and Linear2 weights in one block. Do they match [768, 3072] and [3072, 768] for GPT-2 Small?\nSwap activation: Replace GELU with ReLU in the code. Does training still work? How does validation loss compare?\nReduce expansion: Change expansion from 4× to 2× ([768, 1536]). What effect does this have on parameter count and performance?\nZero out MLP: Set MLP weights to zero. Does the model still learn anything, or does performance collapse?\nCompare speed: Measure training step time with and without the MLP enabled. How much slower is it?\n\n\n\nThe Takeaway\nThe MLP block in GPT-2 is a simple two-layer network with GELU activation, applied independently to each token. It expands, activates, and compresses features, giving the model nonlinear power to reshape hidden states. In llm.c, it’s implemented with basic matrix multiplications and a smooth GELU function, proving that even small building blocks can have a big impact on the model’s ability to learn language.\n\n\n\n26. LayerNorm: Theory and Implementation (doc/layernorm)\nDeep neural networks often suffer from unstable training if activations drift too high or too low. To stabilize this, GPT-2 uses Layer Normalization (LayerNorm) inside every transformer block. In llm.c, LayerNorm is implemented directly in C, and there’s even a detailed explanation in the repo’s doc/layernorm file to help learners understand how it works.\n\nThe Idea of Normalization\nWhen you pass vectors through many layers, their values can become unbalanced-some features dominate while others shrink. Normalization fixes this by:\n\nCentering: subtracting the mean of the vector.\nScaling: dividing by the standard deviation.\n\nThis makes every feature vector have mean 0 and variance 1, improving stability.\n\n\nWhy “Layer” Norm?\nThere are different kinds of normalization (BatchNorm, InstanceNorm, etc.). LayerNorm is special because:\n\nIt normalizes across the features of a single token (the “layer”), not across the batch.\nThis makes it independent of batch size, which is important for NLP where batch sizes can vary.\n\nSo if a hidden vector has 768 channels, LayerNorm computes the mean and variance over those 768 numbers for each token.\n\n\nTrainable Parameters\nLayerNorm isn’t just normalization-it also has two trainable vectors:\n\nγ (gamma): scales each feature after normalization.\nβ (beta): shifts each feature after normalization.\n\nThese allow the network to “undo” normalization when necessary, giving it flexibility.\n\n\nFormula\nFor each input vector x of size d:\nmean = (1/d) * Σ x_i\nvar  = (1/d) * Σ (x_i - mean)^2\nx_norm = (x - mean) / sqrt(var + eps)\ny = γ * x_norm + β\nWhere eps is a tiny constant (like 1e-5) to avoid dividing by zero.\n\n\nImplementation in llm.c\nIn the code, LayerNorm is implemented as a simple function that loops over features, computes mean and variance, and applies the formula above. It’s not hidden inside a framework-it’s right there in C, so you can step through it line by line.\nFor example, the forward pass looks like this (simplified):\nvoid layernorm_forward(float* out, float* inp, float* weight, float* bias, int N) {\n    float mean = 0.0f, var = 0.0f;\n    for (int i = 0; i &lt; N; i++) mean += inp[i];\n    mean /= N;\n    for (int i = 0; i &lt; N; i++) var += (inp[i] - mean) * (inp[i] - mean);\n    var /= N;\n    float inv_std = 1.0f / sqrtf(var + 1e-5f);\n    for (int i = 0; i &lt; N; i++) {\n        out[i] = (inp[i] - mean) * inv_std * weight[i] + bias[i];\n    }\n}\nThis is the kind of clear, low-level implementation that makes llm.c educational.\n\n\nWhere It Fits in GPT-2\nEach transformer block contains two LayerNorms:\n\nOne before attention.\nOne before the MLP.\n\nGPT-2 uses Pre-LN architecture: inputs are normalized before each sublayer. This makes training more stable and gradients flow better.\n\n\nWhy It Matters\nLayerNorm may look like a small detail, but without it, GPT-2 would fail to train reliably. It smooths out the flow of activations so attention and MLP layers can do their job. In practice, this is one of the critical “glue” components that makes deep transformers trainable at scale.\n\n\nTry It Yourself\n\nPrint statistics: After applying LayerNorm, print the mean and variance of the output. Do they stay close to 0 and 1?\nRemove γ and β: Force gamma to 1 and beta to 0. Does the model still train? Compare losses.\nDisable normalization: Comment out LayerNorm and train. How unstable does training become?\nCompare positions: Try switching to Post-LN (apply normalization after attention/MLP). Does this change convergence speed?\nVary epsilon: Change 1e-5 to 1e-2 or 1e-8. How sensitive is training?\n\n\n\nThe Takeaway\nLayerNorm is the quiet stabilizer of GPT-2. It makes sure each token’s features stay balanced, while γ and β keep flexibility. In llm.c, it’s implemented directly with clear C code, letting you see exactly how normalization is calculated. It’s a small but indispensable piece of the transformer puzzle.\n\n\n\n27. Residual Connections: Keeping the Signal Flowing\nTransformers like GPT-2 don’t just stack layers on top of each other blindly. They use residual connections-a trick that allows the input of a layer to be added back to its output. This simple addition helps signals flow through the network without vanishing or exploding, and it makes training deep models possible.\n\nThe Basic Idea\nImagine you have a function F(x) representing some transformation (like attention or an MLP). Instead of just computing:\ny = F(x)\nthe transformer does:\ny = F(x) + x\nThis means the layer learns only the difference it needs to add to the input, instead of replacing it entirely.\n\n\nWhy This Helps\nResiduals solve two big problems in deep networks:\n\nGradient flow: During backpropagation, gradients can get smaller and smaller as they pass through many layers. Adding the input back ensures gradients always have a path straight through.\nInformation preservation: Even if F(x) distorts the signal, the original x is still there. This prevents the model from “forgetting” important information.\nFaster training: The network doesn’t have to re-learn identity mappings-it can just pass them through the skip connection.\n\n\n\nImplementation in llm.c\nResiduals in llm.c are implemented as a straightforward elementwise addition:\nvoid residual_forward(float* out, float* inp1, float* inp2, int N) {\n    for (int i = 0; i &lt; N; i++) {\n        out[i] = inp1[i] + inp2[i];\n    }\n}\nHere:\n\ninp1 is the output of the layer (like attention).\ninp2 is the original input.\nout is the combined result.\n\nThis is done for every token position and feature channel.\n\n\nWhere Residuals Are Used\nIn GPT-2, every transformer block has two residuals:\n\nAttention residual: Adds the input of the attention layer to its output.\nMLP residual: Adds the input of the MLP to its output.\n\nSo the data flowing through the network always carries both the new transformation and the original signal.\n\n\nWhy It Matters\nWithout residual connections, stacking 12–48 transformer blocks would be nearly impossible to train. Gradients would vanish, and the model would either stop learning or take forever to converge. Residuals let deep transformers scale smoothly.\nThey also add an intuitive interpretation: each block is like a “refinement step” rather than a full rewrite of the representation.\n\n\nTry It Yourself\n\nRemove residuals: Comment out the addition in the code. Does training collapse?\nScale residuals: Multiply the input by 0.5 before adding. Does this slow convergence?\nCheck loss curves: Compare training with and without residuals for the first 500 steps.\nInspect outputs: Print the norms of inp1, inp2, and out. Are the scales balanced?\nDeeper models: Increase the number of layers from 12 to 24. Does the importance of residuals become more obvious?\n\n\n\nThe Takeaway\nResidual connections are the “lifeline” of deep transformers. By simply adding inputs back into outputs, they make it possible to train very deep networks without losing gradients or information. In llm.c, the implementation is as simple as looping over arrays and adding them-but the effect is profound: residuals are what let GPT-2 go deep and still work.\n\n\n\n28. Attention Masking: Enforcing Causality\nOne of the defining traits of GPT-2 is that it’s a causal language model. That means it predicts the next token given all the tokens before it, but never cheats by looking ahead. To enforce this, GPT-2 applies an attention mask inside every attention layer.\n\nWhy a Mask Is Needed\nWithout a mask, attention is free to connect any token to any other, including future ones. For example:\n\nInput: “The cat sat on the”\nTarget: “mat”\n\nIf the model could peek at “mat” while computing attention, the task would be trivial-it could just copy the next word. That would break the training objective.\nThe mask forces the model to only use tokens at or before the current position when making predictions.\n\n\nHow the Mask Works\nWhen computing attention scores (Q × K^T / sqrt(d_k)), the result is a matrix of size [T, T] where each row corresponds to one token attending to all others.\nThe mask modifies this matrix:\n\nAllowed positions (past and present): keep scores as is.\nDisallowed positions (future): set scores to -inf.\n\nAfter applying softmax, those -inf entries become zero probability, effectively blocking attention to the future.\n\n\nImplementation in llm.c\nThe causal mask is applied during the attention forward pass. The code uses a loop to zero out invalid positions:\nfor (int t = 0; t &lt; T; t++) {\n    for (int u = t + 1; u &lt; T; u++) {\n        scores[t][u] = -1e9; // block future positions\n    }\n}\nHere T is the sequence length. This ensures that token t can only attend to itself and earlier tokens.\n\n\nVisualizing the Mask\nThink of the mask as a triangular matrix:\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n✓\n\n\n\n\n\n1\n✓\n✓\n\n\n\n\n2\n✓\n✓\n✓\n\n\n\n3\n✓\n✓\n✓\n✓\n\n\n\nEach row shows which past tokens a given position can look at. Future positions remain blank.\n\n\nWhy It Matters\nThe mask is what makes GPT-2 a predictive model instead of a bidirectional encoder like BERT. Without it, the model could “cheat” and the training objective would no longer match how it’s used at inference time (generating text step by step).\nThis small detail-just filling part of a matrix with -inf-is critical to making autoregressive text generation possible.\n\n\nTry It Yourself\n\nDisable the mask: Comment out the masking code. Watch validation loss drop unrealistically, then notice that text generation produces garbage.\nReverse the mask: Block the past and allow the future. Does the model still train? What does it predict?\nPartial mask: Only allow attention to the previous 5 tokens (a sliding window). How does this affect learning long-range structure?\nPrint scores: Before and after masking, log a row of attention scores. Notice how future positions become huge negatives.\nVisualize: Write a small script to plot the attention mask as a matrix. It should look strictly lower-triangular.\n\n\n\nThe Takeaway\nAttention masking is a simple but essential trick. By filling future positions with -inf before softmax, GPT-2 ensures that each token can only attend to its past. In llm.c, this is implemented with just a couple of loops-but it’s what turns a generic transformer into a true causal language model.\n\n\n\n29. Output Head: From Hidden States to Vocabulary\nAfter tokens pass through embeddings, attention, MLPs, LayerNorm, and residuals, we end up with hidden states for every position in the sequence. But GPT-2’s final job is not to output vectors-it must predict the next token from the vocabulary. This is handled by the output head, the last stage of the model.\n\nWhat the Output Head Does\nThe output head maps hidden states of shape (B, T, channels) into logits of shape (B, T, vocab_size). Each logit represents the model’s “raw score” for how likely a particular token is at the next step.\nThe pipeline looks like this:\nhidden states → Linear projection → Logits → Softmax → Probabilities\n\nLogits: real numbers, one per token in the vocabulary.\nSoftmax: converts logits into probabilities that sum to 1.\nPredicted token: the token with the highest probability (or sampled from the distribution).\n\n\n\nTied Weights with Embeddings\nIn GPT-2, the token embedding table and the output head share weights. This means the same matrix is used both for:\n\nMapping tokens to vectors at the start (embedding lookup).\nMapping vectors back to tokens at the end (output head).\n\nMathematically, this improves efficiency and helps align input and output representations.\nIn llm.c, this is done by simply pointing both embedding and output head to the same parameter slice.\n// token embedding table\nfloat* token_embedding_table = params;\n// output head reuses the same memory\nfloat* output_head = token_embedding_table;\nWhen the model projects hidden states back to vocab space, it does a matrix multiply with this shared matrix.\n\n\nShapes in Action\nFor GPT-2 Small:\n\nHidden states: [B, T, 768]\nOutput projection (embedding transpose): [768, 50257]\nLogits: [B, T, 50257]\n\nThat’s more than 50k scores per position, one for each token in the vocabulary.\n\n\nWhy Weight Tying Helps\n\nMemory efficiency: You don’t need a separate giant matrix for the output head.\nBetter learning: The same vectors that represent tokens going in also represent them going out, which reinforces consistency.\nSimpler code: Just reuse the same parameter slice.\n\nThis trick is why GPT-2 can scale vocab sizes without blowing up parameter counts too much.\n\n\nWhy It Matters\nThe output head is where everything comes together. For each position, the model collapses its hidden representation into a distribution over possible next tokens. This is how GPT-2 generates text one step at a time. Without this step, you’d only have abstract hidden states-useful internally, but not something you can read.\n\n\nTry It Yourself\n\nPrint logits: After a forward pass, print the logits for the last token. Do they look like random floats at initialization?\nCheck probability sum: Apply softmax to logits and verify the probabilities sum to 1.\nUntie weights: Make the output head its own matrix instead of reusing embeddings. Does training still work? How does the parameter count change?\nTop-k sampling: Modify sampling to keep only the top 5 logits before softmax. What kind of text does this produce?\nGreedy vs random: Compare greedy decoding (argmax) vs random sampling from probabilities. Which one gives more interesting outputs?\n\n\n\nThe Takeaway\nThe output head is the final bridge between hidden vectors and actual words. By reusing the token embedding matrix, GPT-2 projects hidden states back into vocabulary space and produces logits for every possible token. In llm.c, this step is just another matrix multiplication-but it’s the one that turns internal math into real text predictions.\n\n\n\n30. Loss Function: Cross-Entropy over Vocabulary\nTraining GPT-2 means teaching it to predict the next token in a sequence. To measure how well it’s doing, we need a loss function that compares the model’s predicted probabilities with the true token IDs. In llm.c, this is done with the cross-entropy loss-a standard choice for classification tasks.\n\nFrom Logits to Probabilities\nAfter the output head, we have logits of shape (B, T, vocab_size). These are raw scores. To turn them into probabilities:\nprobs = softmax(logits)\nSoftmax ensures two things:\n\nAll values are between 0 and 1.\nThey sum to 1 across the vocabulary.\n\nSo for each position, you get a probability distribution over all possible next tokens.\n\n\nCross-Entropy Definition\nCross-entropy compares the predicted distribution p with the true distribution q. For language modeling:\n\nq is a one-hot vector (all zeros, except 1 at the true token index).\np is the probability vector from softmax.\n\nThe formula for one token:\nloss = -log(p[true_token])\nFor a batch, you average across all tokens in all sequences.\n\n\nImplementation in llm.c\nIn C, this boils down to:\nfloat loss = 0.0f;\nfor (int b = 0; b &lt; B; b++) {\n    for (int t = 0; t &lt; T; t++) {\n        int target = targets[b*T + t];\n        float logit_max = -1e9;\n        for (int v = 0; v &lt; vocab_size; v++) {\n            if (logits[b*T*vocab_size + t*vocab_size + v] &gt; logit_max) {\n                logit_max = logits[b*T*vocab_size + t*vocab_size + v];\n            }\n        }\n        // compute softmax denominator\n        float sum = 0.0f;\n        for (int v = 0; v &lt; vocab_size; v++) {\n            sum += expf(logits[b*T*vocab_size + t*vocab_size + v] - logit_max);\n        }\n        float logprob = logits[b*T*vocab_size + t*vocab_size + target] - logit_max - logf(sum);\n        loss += -logprob;\n    }\n}\nloss /= (B * T);\nThis snippet shows how llm.c explicitly computes softmax and cross-entropy in loops. No black boxes-just raw math.\n\n\nIntuition\n\nIf the model assigns high probability to the correct token → loss is small.\nIf the model assigns low probability to the correct token → loss is large.\nMinimizing loss means pushing probability mass toward the right answers.\n\n\n\nWhy Cross-Entropy Works for Language\nLanguage modeling is essentially a huge multi-class classification problem: at each step, which word comes next? Cross-entropy is perfect here because it directly penalizes wrong predictions proportional to how confident the model was.\n\n\nWhy It Matters\nThe loss function is the only signal the model gets about how well it’s doing. Everything else-parameter updates, weight tuning, learning dynamics-flows from this single number. A well-implemented cross-entropy ensures training is stable and meaningful.\n\n\nTry It Yourself\n\nCheck values: Print the loss after the first few steps. It should be close to log(vocab_size) (≈10.8 for 50k vocab) before training.\nOverfit tiny batch: Train on just one sequence. Does the loss go near 0 after enough steps?\nChange target: Replace the true token with a random one. Does the loss increase immediately?\nCompare vocab sizes: Train with a smaller vocabulary (e.g., 100 tokens). Does initial loss drop to log(100) ≈ 4.6?\nInspect probabilities: For one token, print the top 5 predicted probabilities. Does the true token climb to the top as training progresses?\n\n\n\nThe Takeaway\nThe cross-entropy loss is the compass guiding GPT-2 during training. It turns raw logits into probabilities and measures how well the model predicts the correct next token. In llm.c, it’s implemented with explicit loops and math, letting you see exactly how probabilities and losses are computed. Without this step, the model would have no way to learn from its mistakes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html#chapter-4.-cpu-inference-forward-only",
    "href": "books/en-US/book.html#chapter-4.-cpu-inference-forward-only",
    "title": "The Book",
    "section": "Chapter 4. CPU Inference (Forward Only)",
    "text": "Chapter 4. CPU Inference (Forward Only)\n\n31. Forward Pass Walkthrough\nWhen we talk about the forward pass in GPT-2, we mean the process of turning an input sentence (like “The cat sat on the”) into predictions for the next word. In simple terms, it’s how the model “thinks” before giving an answer. In train_gpt2.c, this happens inside the function gpt2_forward. Let’s walk through it slowly, step by step, so you can see how numbers flow through the model and transform along the way.\n\n1. From Words to Numbers\nComputers don’t understand words like cat or sat. They only understand numbers. Before the forward pass starts, text is already tokenized into IDs (integers). For example:\n\"The cat sat\" → [464, 3290, 616]\nEach number is a token ID. The model doesn’t yet know what “464” means in plain English-it just knows it’s a number that points into a table.\n\n\n2. Embedding: Giving Words Meaning\nThe first real step in the forward pass is embedding lookup. Imagine we have a huge dictionary, but instead of definitions in English, each word ID points to a long vector of numbers (say, 768 numbers for GPT-2 small).\n\nWord embeddings (wte): Each token ID becomes a vector that captures the meaning of the word.\nPosition embeddings (wpe): Each token also gets a vector for its position: first word, second word, third word, etc.\n\nThe model adds these two vectors together. This way, it knows not just what the word is, but also where it is in the sentence.\nFor example:\n\n\n\n\n\n\n\n\n\n\nToken\nWord\nWord Embedding (shortened)\nPosition Embedding (shortened)\nCombined Vector\n\n\n\n\n464\n“The”\n[0.2, -0.5, 0.1, …]\n[0.0, 0.1, -0.3, …]\n[0.2, -0.4, -0.2, …]\n\n\n3290\n“cat”\n[0.9, -0.2, 0.4, …]\n[0.1, -0.1, -0.2, …]\n[1.0, -0.3, 0.2, …]\n\n\n\nNow every token is a vector with both meaning and position built in.\n\n\n3. Transformer Layers: The Thinking Steps\nGPT-2 has multiple identical layers stacked on top of each other. Each layer has two big parts: attention and MLP (feed-forward network).\nAttention (looking around):\n\nEach word asks: “Which other words should I pay attention to right now?”\nFor “sat,” attention might focus heavily on “cat,” because those words are related.\nThe code computes queries, keys, and values for every word, then does dot-products, softmax, and weighted sums to mix information.\n\nMLP (processing deeply):\n\nAfter attention, each token passes through a mini neural network (two matrix multiplications with a nonlinear GELU function in between).\nThis helps each word refine its understanding, even if it doesn’t directly attend to another word.\n\nBoth blocks have residual connections: the input is added back to the output, like keeping the original notes while adding new insights. This prevents information loss.\n\n\n4. Normalization: Keeping Numbers Stable\nAt many points, the model normalizes vectors so they don’t explode in size or shrink too small. This is called LayerNorm. It ensures training is stable, like making sure your cooking pot doesn’t boil over or dry out.\n\n\n5. The Final Prediction Layer\nAfter all layers, the model produces a final vector for each position. Then:\n\nIt multiplies those vectors by the embedding table again (but transposed).\nThis gives logits: raw scores for each word in the vocabulary (about 50k options).\n\nExample: for the last token “on the,” the logits might be:\n\n\n\nWord\nLogit\nProbability (after softmax)\n\n\n\n\n“mat”\n7.2\n0.85\n\n\n“dog”\n5.1\n0.10\n\n\n“car”\n3.0\n0.05\n\n\n\nThe highest probability is “mat.”\n\n\n6. Softmax: Turning Scores into Probabilities\nThe logits are big numbers, but they don’t mean much until we apply softmax. Softmax makes them into probabilities that sum to 1. This way, we can interpret them as chances: “There’s an 85% chance the next word is mat.”\n\n\n7. Cross-Entropy Loss: Measuring Mistakes\nIf we’re training, we also give the model the correct next word. The model checks how much probability it gave to that word. If it gave it high probability, the loss is low. If it gave it low probability, the loss is high.\n\nCorrect: “mat” (probability 0.85 → loss ≈ 0.16, small).\nWrong: “car” (probability 0.05 → loss ≈ 3.0, large).\n\nThis loss is averaged across all tokens, and it’s the signal that tells the backward pass how to update the model.\n\n\n8. Why It Matters\nThe forward pass is the part of GPT-2 that generates predictions. Without it, the model can’t “think” or make sense of input. It’s like the brain processing sensory input before deciding what to do. In train_gpt2.c, the forward pass is written with plain C loops, which makes the math crystal clear instead of hidden inside deep learning libraries.\n\n\n9. Try It Yourself\n\nPrint embeddings: Modify the code to print the vector for the first token. See how it’s just numbers, but those numbers are the “meaning” of the word.\nInspect probabilities: After the forward pass, print the softmax probabilities for one position. They should sum to 1.0.\nChange sequence length: Increase T from 64 to 128. Notice how validation slows down, because attention compares all tokens with all others (T² scaling).\nBaseline loss: Before training, measure the loss. It should be around log(vocab_size) (≈10.8 for GPT-2 small). That’s the loss of random guessing.\nMask experiment: Temporarily remove the causal mask in attention. The model will “cheat” by looking ahead, and loss will drop unrealistically.\n\n\n\nThe Takeaway\nThe forward pass is like the thought process of GPT-2. Input words become vectors, vectors mix through attention and MLPs, everything gets normalized, and finally the model produces probabilities for the next word. It’s a carefully choreographed dance of math operations, all coded in plain C loops in train_gpt2.c. Once you understand this flow, you can follow exactly how GPT-2 turns raw tokens into intelligent predictions.\n\n\n\n32. Token and Positional Embedding Lookup\nBefore GPT-2 can do anything intelligent with text, it needs to turn raw numbers (token IDs) into vectors that capture meaning and context. This is the role of embeddings. In train_gpt2.c, this step is handled by the function encoder_forward. Let’s take a closer look at how it works and why it matters.\n\nTokens Are Just Numbers\nSuppose you type:\n\"The cat sat on the mat.\"\nAfter tokenization, this sentence might look like:\n[464, 3290, 616, 319, 262, 1142, 13]\nThese are just IDs. The model doesn’t inherently know that 3290 means “cat.” It only knows it needs to use these numbers to fetch vectors from a table.\n\n\nThe Embedding Tables\nThe model has two important tables stored in memory:\n\nWord Token Embeddings (wte)\n\nSize: (V, C) where V is vocab size (~50,000 for GPT-2 small) and C is channels (768).\nEach row corresponds to a token ID.\nExample: row 3290 might be [0.12, -0.45, 0.88, …].\n\nPositional Embeddings (wpe)\n\nSize: (maxT, C) where maxT is the maximum sequence length (e.g. 1024).\nEach row corresponds to a position index: 0 for the first token, 1 for the second, etc.\nExample: position 2 might be [0.07, 0.31, -0.22, …].\n\n\nBoth tables are filled with trainable values. At the start, they’re random. As training progresses, the optimizer updates them so they encode useful patterns.\n\n\nAdding Them Together\nFor each token at position t:\n\nLook up its word vector from wte.\nLook up its position vector from wpe.\nAdd them elementwise.\n\nThis gives a final vector of size C that represents what the token is and where it is.\nExample with simplified numbers:\n\n\n\n\n\n\n\n\n\n\nToken ID\nWord\nWord Embedding\nPosition\nCombined\n\n\n\n\n464\nThe\n[0.1, -0.2, 0.3]\n[0.2, 0.0, -0.1]\n[0.3, -0.2, 0.2]\n\n\n3290\ncat\n[0.4, 0.5, -0.3]\n[0.0, 0.1, 0.2]\n[0.4, 0.6, -0.1]\n\n\n\nNow the vector doesn’t just mean “cat,” it means “cat at position 1.”\n\n\nWhy Position Matters\nWithout positions, the model would treat:\n\n“The cat sat”\n“Sat cat the”\n\nas identical, because they use the same tokens. But word order is essential in language. By adding positional embeddings, GPT-2 knows the difference between “dog bites man” and “man bites dog.”\n\n\nInside the Code\nThe embedding lookup is written explicitly with loops in C:\nfor (int b = 0; b &lt; B; b++) {\n    for (int t = 0; t &lt; T; t++) {\n        float* out_bt = out + b * T * C + t * C;\n        int ix = inp[b * T + t];\n        float* wte_ix = wte + ix * C;\n        float* wpe_t = wpe + t * C;\n        for (int i = 0; i &lt; C; i++) {\n            out_bt[i] = wte_ix[i] + wpe_t[i];\n        }\n    }\n}\nWhat’s happening here:\n\nLoop over batches (b) and sequence positions (t).\nFind the token ID ix.\nFetch its embedding wte_ix.\nFetch its position embedding wpe_t.\nAdd them element by element.\n\nThe result, out_bt, is the vector for this token at this position.\n\n\nAnalogy\nThink of it like name tags at a conference:\n\nThe word embedding is your name: “Alice.”\nThe position embedding is your table number: “Table 7.”\nTogether, they tell the conference staff who you are and where you are seated.\n\nWithout the table number, they might know who you are but not where to find you. Without your name, they just know there’s someone at Table 7 but not who. Both are needed for proper context.\n\n\nWhy It Matters\nEmbeddings are the foundation of the whole model. If this step is wrong, everything else collapses. They transform meaningless IDs into rich vectors that carry semantic and positional information. This is the entry point where language starts becoming something a neural network can reason about.\n\n\nTry It Yourself\n\nPrint a token embedding: Modify the code to print out wte_ix for a specific token ID like “cat.” You’ll see a vector of floats, the learned representation.\nPrint a position embedding: Do the same for wpe_t at position 0, 1, 2… Notice how positions have unique but consistent patterns.\nCheck the sum: Verify that out_bt[i] = wte_ix[i] + wpe_t[i]. This is literally how word and position are fused.\nShuffle words: Try feeding “cat sat” vs. “sat cat.” The embeddings will differ because the position vectors change, even though the words are the same.\nObserve growth during training: After some training steps, dump the embeddings again. You’ll notice they stop being random and start showing structure.\n\n\n\nThe Takeaway\nThe embedding lookup is the very first step of the forward pass. It takes raw numbers and makes them meaningful by combining token identity and position. This prepares the input for the deeper transformer layers. Even though the C code looks simple-a few nested loops-it’s doing the crucial work of giving words a mathematical shape the model can understand.\n\n\n\n33. Attention: Matmuls, Masking, and Softmax on CPU\nThe attention mechanism is the heart of GPT-2. It’s where each word in the input sequence decides which other words to look at when forming its representation. In train_gpt2.c, this happens inside the attention_forward function, which implements multi-head self-attention using plain C loops and matrix multiplications. Let’s break it down carefully, step by step, so even an absolute beginner can follow the flow.\n\nThe Big Idea of Attention\nImagine you’re reading:\n\n“The cat sat on the mat.”\n\nWhen the model is trying to understand the word sat, it doesn’t just look at sat by itself. It wants to consider other words like cat (the subject) and mat (likely the object). Attention gives each token a way to “consult” earlier tokens and decide how important they are.\nThis is done mathematically by projecting each token into three roles: Query (Q), Key (K), and Value (V).\n\nQuery (Q): “What am I looking for?”\nKey (K): “What do I offer?”\nValue (V): “What information do I carry?”\n\n\n\nStep 1: Creating Q, K, and V\nFor every input vector of size C (e.g., 768), the code performs three separate linear projections (matrix multiplications). These produce Q, K, and V vectors of smaller size, divided among attention heads.\nIn the code:\nmatmul_forward(acts.q, acts.ln1, params.wq, params.bq, B, T, C, C);\nmatmul_forward(acts.k, acts.ln1, params.wk, params.bk, B, T, C, C);\nmatmul_forward(acts.v, acts.ln1, params.wv, params.bv, B, T, C, C);\nHere:\n\nacts.ln1 is the normalized input from the previous step.\nparams.wq, params.wk, params.wv are the weight matrices.\nThe output shapes are (B, T, C).\n\nSo each token now has three new representations: Q, K, and V.\n\n\nStep 2: Computing Attention Scores\nFor each token at position t, we want to know how much it should pay attention to every earlier token (including itself). This is done with a dot product between its Query and all Keys.\nMathematically:\nscore[t][u] = (Q[t] ⋅ K[u]) / sqrt(dk)\n\nt = current token.\nu = another token at or before t.\nsqrt(dk) is a scaling factor (dk = size of each head) to keep values stable.\n\nIn the code, these dot products are done explicitly in loops.\n\n\nStep 3: Applying the Causal Mask\nGPT-2 is an autoregressive model, meaning it only predicts the future from the past, not the other way around. To enforce this, the attention matrix is masked:\n\nToken at position t can only look at positions ≤ t.\nAnything beyond t is set to a very negative value (-1e9), which becomes effectively zero after softmax.\n\nThis ensures, for example, that when predicting the 3rd word, the model doesn’t cheat by looking at the 4th.\n\n\nStep 4: Turning Scores into Probabilities\nThe scores are raw numbers that can be large and unstable. To convert them into meaningful weights, the code applies softmax:\nattention_weights[t][u] = exp(score[t][u]) / Σ exp(score[t][v])\nThis makes all weights positive and ensures they sum to 1. Now each token has a probability distribution over earlier tokens.\nExample for the word sat:\n\n\n\nAttended Token\nRaw Score\nAfter Softmax\n\n\n\n\nThe\n1.2\n0.10\n\n\ncat\n3.4\n0.80\n\n\nsat (itself)\n0.7\n0.10\n\n\non\nmasked\n0.00\n\n\n\nClearly, sat focuses most strongly on cat.\n\n\nStep 5: Weighted Sum of Values\nOnce the attention weights are computed, the model uses them to take a weighted sum of the Value vectors:\noutput[t] = Σ attention_weights[t][u] * V[u]\nThis produces a new representation for each token that blends in information from others.\nFor sat, its new vector will be mostly influenced by cat, but also a little by The and itself.\n\n\nStep 6: Multi-Head Attention\nIn practice, attention is split into multiple heads (12 for GPT-2 small). Each head works on smaller chunks of the vector (C/heads).\n\nHead 1 might focus on subject–verb relationships.\nHead 2 might track distances (like “how far back was this token?”).\nHead 3 might specialize in punctuation.\n\nAfter all heads compute their outputs, the results are concatenated and projected back into size C with another matrix multiply.\n\n\nStep 7: Residual Connection\nFinally, the output of the attention block is added back to the original input (residual connection). This keeps the original signal flowing, even if the attention introduces distortions.\nresidual_forward(acts.residual2, acts.ln1, acts.att, B*T, C);\nThis ensures information isn’t lost and gradients flow smoothly during training.\n\n\nWhy It Matters\nAttention is the mechanism that lets GPT-2 capture relationships between words. Without it, the model would treat each token independently, losing context. By explicitly computing “who should I look at?” for every token, GPT-2 learns patterns like subject–verb agreement, long-distance dependencies, and even stylistic nuances.\n\n\nTry It Yourself\n\nInspect the attention mask: Print out the scores before and after masking. Notice how future tokens are set to huge negative values.\nVisualize weights: Run attention on a short sentence and plot the weights. You’ll see which words attend to which.\nChange sequence length: Try increasing T and observe how computation grows quadratically (T²). Attention is expensive!\nExperiment with heads: Force the model to use only 1 head instead of 12. See how this limits the diversity of patterns it can capture.\nCheck sum of weights: For one token, verify that all attention weights add up to 1.0 after softmax.\n\n\n\nThe Takeaway\nAttention is what makes transformers powerful. It allows each word to dynamically decide which other words matter for understanding its role in a sentence. In train_gpt2.c, this process is spelled out with explicit loops and matrix multiplications, so you can follow every step of the math. Understanding this section gives you the key to why GPT-2-and all modern LLMs-work so well.\n\n\n\n34. MLP: GEMMs and Activation Functions\nAfter the attention block lets tokens “talk to each other,” GPT-2 applies a second kind of transformation called the MLP block (multi-layer perceptron). Unlike attention, which mixes information between tokens, the MLP processes each token independently, enriching its internal representation. Even though it looks simpler than attention, the MLP is essential for capturing complex relationships in language.\n\nWhat the MLP Does\nEvery token’s vector (size C, e.g., 768 for GPT-2 small) goes through:\n\nLinear expansion: project from size C to size 4C (3072 in GPT-2 small).\nNonlinear activation: apply the GELU function, which adds flexibility.\nLinear projection back: reduce size from 4C back to C.\nResidual connection: add the input vector back to the output, keeping the original signal intact.\n\nThis allows the model to not only share information between tokens (via attention) but also refine how each token represents itself.\n\n\nStep 1: Expanding with a Matrix Multiply\nThe first step is to expand each token’s vector from 768 to 3072 dimensions. This is done with a general matrix multiply (GEMM):\nmatmul_forward(acts.mlp_in, acts.ln2, params.wfc, params.bfc, B, T, C, 4*C);\n\nacts.ln2: the normalized input from the previous residual.\nparams.wfc: the weight matrix of size (C, 4C).\nparams.bfc: bias vector of size (4C).\nacts.mlp_in: the result, shape (B, T, 4C).\n\nThink of it like stretching a rubber band-suddenly, the token has much more room to express richer features.\n\n\nStep 2: GELU Activation\nAfter expansion, each number passes through GELU (Gaussian Error Linear Unit).\nThe formula:\nGELU(x) = 0.5 * x * (1 + tanh(√(2/π) * (x + 0.044715 * x^3)))\nThis looks complicated, but the key idea is:\n\nFor small negative numbers, output ≈ 0 (ignore weak signals).\nFor large positive numbers, output ≈ x (pass strong signals).\nFor numbers in between, it smoothly blends.\n\nUnlike ReLU, which just chops off negatives, GELU lets small signals through in a probabilistic way. This makes it better for language, where even small hints matter.\nAnalogy: Imagine you’re grading homework. If an answer is completely wrong, you give 0 points (ReLU style). If it’s perfect, you give full credit. But if it’s partially right, you give partial credit. GELU behaves like that-soft, nuanced grading.\n\n\nStep 3: Projecting Back Down\nOnce the token vector has been expanded and passed through GELU, it’s projected back to the original size C:\nmatmul_forward(acts.mlp_out, acts.mlp_in_gelu, params.wproj, params.bproj, B, T, 4*C, C);\n\nparams.wproj: the projection weights, size (4C, C).\nparams.bproj: bias, size (C).\nacts.mlp_out: result, shape (B, T, C).\n\nNow each token has gone through a non-linear “thinking step,” mixing and reshaping features.\n\n\nStep 4: Residual Connection\nJust like with attention, the MLP output is added back to the input:\nresidual_forward(acts.residual3, acts.residual2, acts.mlp_out, B*T, C);\nThis means the token keeps its old representation while adding the new refinements. If the MLP makes a mistake early in training, the residual ensures the token doesn’t lose all its meaning.\n\n\nInside the Code: Simplicity\nEven though MLPs in deep learning libraries like PyTorch are one-liners (nn.Linear + nn.GELU + nn.Linear), here in C you see every step spelled out:\n\nFirst GEMM expands to 4C.\nLoop applies GELU element by element.\nSecond GEMM projects back to C.\nResidual adds input and output.\n\nIt’s like watching a magician reveal the trick instead of just seeing the final illusion.\n\n\nWhy the Expansion Matters\nYou might ask: why expand to 4C and then shrink back? Why not just keep the size the same?\nThe expansion allows the model to capture more complicated combinations of features. By spreading information out, applying a nonlinear transformation, and then compressing it again, the model can discover patterns that wouldn’t fit in the smaller space.\nThink of it like brainstorming on a huge whiteboard. You spread out all your ideas (4C), reorganize them, and then condense the best ones into a neat summary (C).\n\n\nExample Walkthrough\nLet’s say we’re processing the token “cat” in the sentence The cat sat.\n\nInput vector (size 768): [0.12, -0.08, 0.33, …]\nAfter first matrix multiply: expanded to [1.2, -0.9, 0.5, …] (size 3072).\nAfter GELU: [1.1, -0.0, 0.4, …] (smooth nonlinearity).\nAfter projection: back to [0.15, -0.02, 0.27, …] (size 768).\nAdd back original input: [0.27, -0.10, 0.60, …].\n\nNow “cat” has been enriched with new internal features that help the model predict what comes next.\n\n\nWhy It Matters\nThe MLP is the part of GPT-2 that lets each token refine itself. Attention gives context from neighbors, but the MLP deepens the representation of the token itself. Without it, the model would lack the ability to detect fine-grained patterns.\n\n\nTry It Yourself\n\nPrint intermediate sizes: Add debug prints to see how token vectors grow to 4C and shrink back to C.\nSwap activation: Replace GELU with ReLU in the code and train. Compare losses-you’ll notice GPT-2 prefers GELU.\nDisable residual: Temporarily remove the residual add. Watch how the model struggles to learn, because it can’t preserve information.\nVisualize values: Track how many values are near 0 before and after GELU. You’ll see GELU softly zeroes out weak signals.\nSmaller expansion: Try changing 4C to 2C in the code. You’ll save memory but lose accuracy, since the MLP has less expressive power.\n\n\n\nThe Takeaway\nThe MLP block is a token’s personal deep thinker. It stretches the representation wide, filters it through GELU, compresses it again, and then adds it back to the original. While attention handles the conversation between words, the MLP ensures each word processes and refines its own role. Together, they create the layered reasoning ability that makes GPT-2 so powerful.\n\n\n\n35. LayerNorm on CPU (Step-by-Step)\nOne of the most important but often overlooked ingredients in GPT-2 is Layer Normalization, or LayerNorm for short. While attention and MLPs are the big stars, LayerNorm is like the stage crew keeping everything running smoothly behind the scenes. It ensures the numbers flowing through the network stay stable and balanced, preventing explosions or collapses that could make training impossible. In train_gpt2.c, LayerNorm is implemented with explicit loops so you can see every calculation. Let’s walk through it carefully.\n\nWhy Do We Need Normalization?\nImagine a classroom where every student talks at different volumes. Some whisper, some shout. If you try to listen to all of them at once, the loud voices drown out the quiet ones.\nNeural networks face a similar problem. The outputs of layers can have wildly different scales. If one dimension of a vector is much larger than the others, it dominates. Training becomes unstable, and gradients may vanish or explode.\nLayerNorm fixes this by ensuring that, for each token at each layer, the vector has:\n\nMean = 0 (centered around zero)\nVariance = 1 (consistent spread of values)\n\nAfter that, trainable parameters scale and shift the result so the model can still learn flexible transformations.\n\n\nThe Math Behind LayerNorm\nFor a given token vector x of size C (e.g., 768):\n\nCompute mean:\nμ = (1/C) * Σ x[i]\nCompute variance:\nσ² = (1/C) * Σ (x[i] - μ)²\nNormalize:\nx_norm[i] = (x[i] - μ) / sqrt(σ² + ε)\nwhere ε is a tiny constant (like 1e-5) to avoid division by zero.\nScale and shift with trainable weights g (gamma) and b (beta):\ny[i] = g[i] * x_norm[i] + b[i]\n\nSo the final output has controlled statistics, but still enough flexibility for the model to adjust.\n\n\nThe Code in train_gpt2.c\nHere’s a simplified version from the repository:\nvoid layernorm_forward(float* out, float* inp, float* weight, float* bias, int B, int T, int C) {\n    for (int b = 0; b &lt; B; b++) {\n        for (int t = 0; t &lt; T; t++) {\n            float* x = inp + b*T*C + t*C;\n            float* o = out + b*T*C + t*C;\n\n            // mean\n            float mean = 0.0f;\n            for (int i = 0; i &lt; C; i++) mean += x[i];\n            mean /= C;\n\n            // variance\n            float var = 0.0f;\n            for (int i = 0; i &lt; C; i++) {\n                float diff = x[i] - mean;\n                var += diff * diff;\n            }\n            var /= C;\n\n            // normalize, scale, shift\n            for (int i = 0; i &lt; C; i++) {\n                float norm = (x[i] - mean) / sqrtf(var + 1e-5f);\n                o[i] = norm * weight[i] + bias[i];\n            }\n        }\n    }\n}\nNotice the structure:\n\nOuter loops over batch B and sequence length T.\nInner loops compute mean, variance, and then apply normalization per token vector of length C.\nweight and bias are the learnable gamma and beta.\n\nThis is exactly what LayerNorm means: normalize each layer’s inputs per token.\n\n\nExample Walkthrough\nSuppose we have a single token vector (C=4) = [2.0, -1.0, 3.0, 0.0].\n\nMean: (2 - 1 + 3 + 0)/4 = 1.0\nVariance: ((2-1)² + (-1-1)² + (3-1)² + (0-1)²)/4 = (1 + 4 + 4 + 1)/4 = 2.5\nNormalize: subtract mean and divide by sqrt(2.5):\n[ (2-1)/1.58, (-1-1)/1.58, (3-1)/1.58, (0-1)/1.58 ]\n= [0.63, -1.26, 1.26, -0.63]\nScale and shift (say weight=[1,1,1,1], bias=[0,0,0,0]):\n[0.63, -1.26, 1.26, -0.63]\n\nNow the vector has mean 0, variance 1, and is ready for the next layer.\n\n\nAnalogy\nThink of LayerNorm like a baking recipe. If one ingredient is way too strong (like adding five times too much salt), the whole dish is ruined. LayerNorm tastes the mixture, balances all the flavors, and then lets you adjust the seasoning with learnable gamma (scale) and beta (shift).\n\n\nWhy It Matters\nWithout LayerNorm, the model would quickly become unstable:\n\nSome tokens would dominate, while others fade.\nGradients could explode, making loss jump wildly.\nTraining would be inconsistent between batches.\n\nWith LayerNorm, each layer works with clean, normalized inputs. This allows deeper stacks of attention and MLP blocks to learn reliably.\n\n\nTry It Yourself\n\nPrint statistics: Add debug code to check the mean and variance before and after LayerNorm. Before: mean ≠ 0, variance ≠ 1. After: mean ≈ 0, variance ≈ 1.\nRemove LayerNorm: Comment out LayerNorm in the code. Watch training collapse-loss will not decrease properly.\nChange epsilon: Try making ε = 1e-1 or ε = 1e-12. See how too large or too small values can break stability.\nObserve gamma and beta: Initialize gamma=1, beta=0. During training, watch how these parameters drift, fine-tuning normalization.\nExperiment with batch norm: Replace LayerNorm with BatchNorm (not typical for transformers). You’ll see it doesn’t work well, because transformers process variable-length sequences where per-batch statistics vary too much.\n\n\n\nThe Takeaway\nLayerNorm is the quiet but critical stabilizer in GPT-2. It ensures every token vector is balanced, centered, and scaled before moving into attention or MLP. In train_gpt2.c, you see exactly how it works: compute mean, compute variance, normalize, then scale and shift. Even though it’s just a few lines of C code, it’s one of the main reasons deep transformers can stack dozens of layers without breaking.\n\n\n\n36. Residual Adds and Signal Flow\nOnce embeddings, attention, and MLP blocks are computed, there’s still one piece left to keep the whole network stable and effective: residual connections. In train_gpt2.c, these appear in functions like residual_forward, where outputs of a layer are added back to their inputs. This simple-looking step is one of the key reasons GPT-2 and other deep transformer models can stack many layers without collapsing.\n\nThe Core Idea\nA residual connection says:\noutput = input + transformation(input)\nInstead of replacing the old representation with the new one, the model adds them together. That way, the original signal always survives, even if the new transformation is noisy or imperfect.\nThink of it like taking lecture notes. Each time the teacher explains more, you don’t throw away your old notes. You add new details next to them. That way, you preserve everything learned so far, while layering new insights on top.\n\n\nWhy Residuals Are Crucial\n\nPreventing information loss: If you only applied transformations, some features might vanish forever. Adding the input back ensures no information is lost.\nHelping gradients flow: During backpropagation, gradients must travel backward through many layers. Without shortcuts, they can vanish or explode. Residuals create direct paths for gradients, making learning stable.\nImproving training speed: With residuals, deeper networks converge faster because the model can “skip” bad transformations while still using the identity mapping.\n\n\n\nThe Code in train_gpt2.c\nHere’s the implementation of residual addition:\nvoid residual_forward(float* out, float* inp1, float* inp2, int N) {\n    for (int i = 0; i &lt; N; i++) {\n        out[i] = inp1[i] + inp2[i];\n    }\n}\nIt’s deceptively simple:\n\ninp1 is the original input.\ninp2 is the new transformation (from attention or MLP).\nout stores the sum.\nN is the total number of floats (B * T * C).\n\nEven though it’s just a single line inside a loop, this is what makes stacking 12+ transformer blocks possible.\n\n\nExample Walkthrough\nSuppose we’re processing the token “cat.”\n\nInput vector (simplified, size=3): [0.5, -0.3, 0.7]\nAfter attention block: [0.2, 0.1, -0.4]\n\nResidual addition:\n[0.5 + 0.2, -0.3 + 0.1, 0.7 - 0.4] = [0.7, -0.2, 0.3]\nNow the representation of “cat” contains both the original signal and the contextual information from attention.\nLater, after the MLP:\n\nInput again: [0.7, -0.2, 0.3]\nMLP output: [0.1, -0.5, 0.4]\nResidual: [0.8, -0.7, 0.7]\n\nStep by step, the vector grows richer without losing its foundation.\n\n\nAnalogy\nResidual connections are like building layers in Photoshop. Each new layer adds adjustments, but you always keep the original photo underneath. If a new adjustment is bad, you can still see the original. This makes the final composition stronger and safer to experiment with.\n\n\nResiduals Across the Model\nIn GPT-2’s forward pass, residuals appear in two main places inside each transformer block:\n\nAfter attention:\nx = x + Attention(x)\nAfter MLP:\nx = x + MLP(x)\n\nTogether with LayerNorm before each block, these form the backbone of the transformer architecture:\nx → LayerNorm → Attention → Residual Add → LayerNorm → MLP → Residual Add\n\n\nWhy It Matters\nWithout residual connections, GPT-2 would struggle to train past a few layers. Deeper stacks would lose track of the original signal, gradients would vanish, and performance would stall. Residuals are the glue that holds the whole architecture together, enabling models with billions of parameters to train effectively.\n\n\nTry It Yourself\n\nRemove residuals: Temporarily comment out the residual_forward calls. Training will quickly fail-the loss won’t decrease properly.\nPrint before/after: Inspect a token’s vector before and after residual add. Notice how the numbers change smoothly rather than being overwritten.\nExperiment with scaling: Try replacing out[i] = inp1[i] + inp2[i]; with out[i] = inp1[i] + 0.1 * inp2[i];. This reduces the impact of the new transformation-sometimes used in advanced architectures.\nCompare to skip-less RNNs: Research how older recurrent networks without residuals had trouble scaling deep. You’ll see why residuals are a game changer.\nChain of signals: Track how a single token’s vector evolves across all 12 layers. You’ll notice it keeps its core identity while absorbing new context.\n\n\n\nThe Takeaway\nResidual connections may look like a simple addition, but they’re the key to deep learning’s success in transformers. They preserve information, stabilize training, and allow GPT-2 to stack many layers without falling apart. In train_gpt2.c, this idea is laid bare: a few lines of C code implementing one of the most powerful tricks in modern neural networks.\n\n\n\n37. Cross-Entropy Loss on CPU\nAfter embeddings, attention, MLPs, LayerNorm, and residuals have done their job, the model produces logits-raw scores for every word in the vocabulary at every position in the sequence. But logits alone don’t tell us if the model is “good” or “bad” at predicting the right word. To measure performance and guide learning, GPT-2 uses the cross-entropy loss function. In train_gpt2.c, this is implemented in the function crossentropy_forward.\n\nWhat Are Logits?\nAt the final stage of the forward pass, each token position has a vector of length V (vocabulary size, ~50k). For example, the model might produce these logits for a tiny vocabulary of 3 words:\nlogits = [5.2, 1.1, -2.7]\nLogits are just numbers-bigger means “more likely,” smaller means “less likely”-but they aren’t probabilities yet.\n\n\nStep 1: Softmax – Turning Scores Into Probabilities\nTo compare predictions with the true target, we first convert logits into probabilities. The tool for this is the softmax function:\np[i] = exp(logits[i]) / Σ exp(logits[j])\nSoftmax has two important effects:\n\nIt makes all values positive.\nIt normalizes them so they sum to 1, forming a probability distribution.\n\nExample:\n\nLogits: [5.2, 1.1, -2.7]\nSubtract max (5.2) for stability → [0.0, -4.1, -7.9]\nExponentiate → [1.0, 0.017, 0.0004]\nNormalize → [0.98, 0.017, 0.0004]\n\nNow the model is saying:\n\nWord 0: 98% chance\nWord 1: 1.7% chance\nWord 2: 0.04% chance\n\n\n\nStep 2: Cross-Entropy – Measuring Mistakes\nCross-entropy compares the predicted probability for the correct word against the ideal case (probability = 1).\nFormula:\nloss = -log(probability_of_correct_word)\n\nIf the model assigns high probability to the correct word, loss is small.\nIf the model assigns low probability, loss is large.\n\nExample:\n\nCorrect word = Word 0, probability = 0.98 → loss = -log(0.98) ≈ 0.02 (excellent).\nCorrect word = Word 1, probability = 0.017 → loss = -log(0.017) ≈ 4.1 (bad).\n\n\n\nStep 3: Averaging Over the Batch\nIn practice, we don’t train on just one word, but a batch of sequences. The code loops over every token in every batch, collects their losses, and averages them.\nFrom train_gpt2.c:\nfloat loss = 0.0f;\nfor (int i = 0; i &lt; B*T; i++) {\n    int target = targets[i];\n    float logit_max = -1e9;\n    for (int j = 0; j &lt; Vp; j++) {\n        if (logits[i*Vp + j] &gt; logit_max) logit_max = logits[i*Vp + j];\n    }\n    float sum = 0.0f;\n    for (int j = 0; j &lt; Vp; j++) {\n        sum += expf(logits[i*Vp + j] - logit_max);\n    }\n    float log_sum = logf(sum);\n    float correct_logit = logits[i*Vp + target];\n    loss += (log_sum + logit_max - correct_logit);\n}\nloss /= (B*T);\nWhat’s happening here:\n\nFor each token, find the max logit (logit_max) to improve numerical stability.\nCompute softmax denominator (sum).\nCalculate log probability of the correct token.\nAccumulate losses across all tokens.\nDivide by total tokens (B*T) to get the average.\n\n\n\nNumerical Stability Tricks\nWithout subtracting logit_max, exp(logits) can overflow. For example, exp(1000) is infinite. By subtracting the max, the largest logit becomes 0, so its exponential is 1, and all others are ≤ 1. This keeps numbers manageable while preserving the probability ratios.\n\n\nExample With a Sentence\nSentence: The cat sat on the mat.\nSuppose the model predicts probabilities for the last token:\n\n“mat”: 0.85\n“dog”: 0.10\n“car”: 0.05\n\nCorrect word = “mat.”\nLoss = -log(0.85) ≈ 0.16.\nIf instead the model guessed “dog” with 0.10, loss = -log(0.10) ≈ 2.3. Higher penalty for being wrong.\n\n\nAnalogy\nCross-entropy is like grading multiple-choice exams. If the student picks the right answer confidently (high probability), they lose almost no points. If they’re hesitant or wrong, they lose more points. Over many questions (tokens), you calculate their average score-the training loss.\n\n\nWhy It Matters\nCross-entropy loss is the guiding signal for the entire training process. It tells the optimizer:\n\n“Increase probability for the right words.”\n“Decrease probability for the wrong words.”\n\nWithout it, GPT-2 would have no way of knowing whether its predictions are improving.\n\n\nTry It Yourself\n\nCheck baseline loss: Before training, print the loss. It should be close to log(vocab_size) (~10.8 for GPT-2 small), which corresponds to random guessing.\nInspect softmax sums: For one token, sum all probabilities. It should equal ~1.0.\nForce the wrong answer: Temporarily change the target to an incorrect word. Watch how the loss shoots up.\nObserve loss during training: Print loss every step. It should steadily decrease as the model learns.\nCompare with accuracy: Track how often the model’s top prediction matches the target. Loss and accuracy will move together, but loss is smoother and more informative.\n\n\n\nThe Takeaway\nCross-entropy loss turns raw model scores into a clear training signal. It penalizes wrong predictions, rewards confident correct ones, and ensures the optimizer knows exactly how to adjust weights. In train_gpt2.c, you see this implemented explicitly, without any library shortcuts-just loops, exponentials, and logs. Understanding this section is key to understanding how GPT-2 learns from its mistakes.\n\n\n\n38. Putting It All Together: The gpt2_forward Function\nUp to this point, we’ve explored the forward pass piece by piece - embeddings, attention, feed-forward layers, layer normalization, residual connections, and finally the loss. But a model doesn’t live as disconnected pieces; they all come together in a single function that drives inference: gpt2_forward. This function is where the code actually executes the story we’ve been telling. Let’s walk through it carefully so you can see how every building block plugs into the whole picture.\n\nThe role of gpt2_forward\nThink of gpt2_forward as the director of the play. The actors (embeddings, attention, MLP, layernorm, etc.) already know their roles. The director calls them on stage in the right order and makes sure they hand the script off smoothly to the next actor. In our case:\n\nTokens come in as integers (word IDs).\nThey’re turned into embeddings (token + position).\nEach transformer block processes the sequence through attention, MLP, layernorm, and residuals.\nThe final hidden states are mapped back into vocabulary space.\nIf labels are provided, a loss is computed.\n\n\n\nCode skeleton\nHere’s a simplified excerpt of the real function from train_gpt2.c (slightly shortened for readability):\nvoid gpt2_forward(GPT2 *model, int *tokens, int *labels, int B, int T) {\n    // Step 1: Embedding lookup\n    embedding_forward(model-&gt;token_embedding, tokens, B, T);\n    embedding_forward(model-&gt;position_embedding, positions, B, T);\n\n    // Step 2: Transformer blocks\n    for (int l = 0; l &lt; model-&gt;config-&gt;n_layer; l++) {\n        attention_forward(&model-&gt;blocks[l].attn, ...);\n        mlp_forward(&model-&gt;blocks[l].mlp, ...);\n        layernorm_forward(&model-&gt;blocks[l].ln, ...);\n        residual_forward(...);\n    }\n\n    // Step 3: Final normalization + logits\n    layernorm_forward(model-&gt;final_ln, ...);\n    matmul_forward(model-&gt;lm_head, ...); // project to vocab\n\n    // Step 4: Loss (optional)\n    if (labels != NULL) {\n        crossentropy_forward(...);\n    }\n}\nDon’t worry if this looks intimidating - we’ll decode each part in plain language.\n\n\nStep 1: Embedding lookup\nBefore the model can reason about words, it has to map token IDs into continuous vectors. That’s where embedding tables come in:\n\ntoken_embedding converts each integer token ID into a dense vector of size C (the channel dimension).\nposition_embedding does the same for positions (0, 1, 2, …, T-1).\nThese two are added together, giving each token both a meaning (word identity) and a place in the sentence.\n\n\n\nStep 2: Transformer blocks\nEach block is like a mini-pipeline that processes the sequence and passes it forward. Inside the loop:\n\nAttention: compares tokens with each other, weighted by learned Q/K/V projections.\nMLP: expands each token vector, applies a nonlinear GELU activation, then projects back down.\nLayerNorm: normalizes values for stable training and inference.\nResidual: adds the input of the block back to its output to keep information flowing.\n\nThis loop runs n_layer times - for GPT-2 124M, that’s 12 blocks.\n\n\nStep 3: Final normalization and logits\nAfter the last block, the sequence of token representations goes through a final layer normalization. Then, a large matrix multiplication (lm_head) projects each token’s hidden state into the size of the vocabulary (≈50,000 for GPT-2). The result is a tensor of shape (B, T, vocab_size) containing the raw prediction scores for each next token.\n\n\nStep 4: Optional loss computation\nIf you pass labels (the correct next tokens) into gpt2_forward, the function calls crossentropy_forward. This compares the predicted scores with the true tokens and outputs a single number: the loss. The loss tells you “how wrong” the model was, which is critical during training. But if you’re only doing inference, you don’t need this step.\n\n\nHow the pieces connect\nHere’s a table that maps our earlier sections to the parts of gpt2_forward:\n\n\n\n\n\n\n\n\nCode Step\nConcept\nSection Covered Earlier\n\n\n\n\nEmbeddings\ntoken + positional vectors\n32\n\n\nAttention\nQKV projections, masking, softmax\n33\n\n\nMLP\nfeed-forward expansion and compression\n34\n\n\nLayerNorm\nnormalization for stability\n35\n\n\nResidual\nskip connections for signal flow\n36\n\n\nCrossEntropy\ncomparing predictions with labels\n37\n\n\n\nSo gpt2_forward is really just a neat orchestration of everything you’ve already learned.\n\n\nWhy it matters\nUnderstanding gpt2_forward gives you the complete mental picture of inference. It shows how embeddings, attention, MLP, normalization, and residuals work together in code to turn a batch of tokens into predictions. Without this integration step, the model would just be a collection of disconnected parts.\n\n\nTry it yourself\n\nPrint shapes: Add printf statements inside gpt2_forward to print tensor shapes after embeddings, after each block, and after logits. This helps you see the data flow.\nUse a single block: Change the loop to run only 1 transformer block instead of all 12. Watch how the outputs degrade - the model loses depth of reasoning.\nDisable position embeddings: Comment out the line that adds position_embedding. Try running inference. You’ll notice the model becomes worse at handling word order.\nLoss vs no loss: Call gpt2_forward with and without labels. Compare the difference - with labels you get a scalar loss, without labels you just get logits.\nSmaller vocab: Try using a toy tokenizer with a small vocabulary and rerun the projection step. You’ll see the logits shrink to (B, T, tiny_vocab_size).\n\n\n\nThe takeaway\ngpt2_forward is where GPT-2 inference really happens. It ties together every concept - embeddings, attention, feed-forward layers, normalization, residuals, and the final projection into vocabulary space. Once you understand this function, you don’t just know the pieces of GPT-2, you know how they actually work together to produce predictions. It’s the “main stage” of inference, and mastering it means you can confidently say you understand how a transformer runs forward on CPU.\n\n\n\n39. OpenMP Pragmas for Parallel Loops\nCPU training in train_gpt2.c is intentionally “plain C,” but it still squeezes out a lot of speed by adding a few OpenMP pragmas (#pragma omp …) in the hottest loops. OpenMP lets the compiler split a loop’s iterations across multiple CPU cores-no threads to create by hand, no locks to manage. If you compile without OpenMP support, these pragmas are simply ignored and the code still runs (just slower).\nBelow we’ll (1) show exactly where OpenMP is used, (2) explain why those loops are good candidates, and (3) offer practical tips to get solid speedups on your machine.\n\nOpenMP in this file: where it appears and why\n\n\n\n\n\n\n\n\n\nLocation / Function\nPragma used\nWhat’s parallelized\nWhy it’s a great fit\n\n\n\n\nmatmul_forward_naive\n#pragma omp parallel for collapse(2)\nOuter loops over b (batch) and t (time)\nEach (b,t) row computes an independent output vector; no write conflicts. Large, regular work = easy scaling.\n\n\nmatmul_forward (tiled)\n#pragma omp parallel for\nCollapsed B*T loop in tiles of LOOP_UNROLL\nHeaviest compute in the model; tiling + per-thread tiles keep caches warm.\n\n\nmatmul_backward (part 1)\n#pragma omp parallel for collapse(2)\nBackprop into inp over (b,t)\nEach (b,t) reads weights and dout, writes a private slice of dinp → no overlap.\n\n\nmatmul_backward (part 2)\n#pragma omp parallel for\nBackprop into weight/bias over o (output channel)\nEach thread owns one output channel’s gradient row, avoiding atomics.\n\n\nsoftmax_forward\n#pragma omp parallel for collapse(2)\nOver (b,t) positions\nEach softmax is independent; perfect “embarrassingly parallel” loop.\n\n\nattention_forward\n#pragma omp parallel for collapse(3)\nOver (b, t, h) = batch, time, head\nPer (b,t,h) head’s work is independent; big 3-D grid parallelizes extremely well.\n\n\n\nA few key patterns to notice:\n\nCollapse clauses (collapse(2) / collapse(3)) fuse nested loops into one big iteration space so the scheduler can distribute more, smaller chunks-great for load-balancing when B, T, or NH are modest.\nParallelizing along independent dimensions avoids race conditions. For example, in matmul_backward the pass that writes dinp[b,t,:] is parallelized over (b,t) so no two threads update the same memory.\nOwn-your-row strategy: when accumulating dweight, the loop goes over o (output channels) so each thread writes its own gradient row dweight[o,:]. No atomics needed.\n\n\n\nQuick refresher: what OpenMP is doing\nA typical pattern looks like:\n#pragma omp parallel for collapse(2)\nfor (int b = 0; b &lt; B; b++) {\n    for (int t = 0; t &lt; T; t++) {\n        // compute outputs for (b, t) independently\n    }\n}\nWhen compiled with OpenMP, the compiler creates a team of threads and divides the iteration space (B*T in this example) among them. Each thread executes its assigned iterations; when the loop finishes, threads sync at an implicit barrier.\nBecause each (b,t) (or (b,t,h)) writes to a disjoint slice of the output arrays, there’s no need for locks or atomics. This is why these loops scale cleanly across cores.\n\n\nEnabling OpenMP, safely\n\nThe source guards the OpenMP header with:\n#ifdef OMP\n#include &lt;omp.h&gt;\n#endif\nso you can define OMP in your build and add your compiler switch. Example (GCC/Clang):\n-D OMP -fopenmp\nIf you forget -fopenmp (or your platform’s equivalent), the pragmas are ignored and the program runs single-threaded.\nYou can control threads at runtime:\nexport OMP_NUM_THREADS=8\nA good rule of thumb is to start with the number of physical cores on your CPU.\n\n\n\nWhy these loops benefit the most\n\nMatrix multiplies dominate runtime. matmul_forward/matmul_backward consume the bulk of CPU time. Parallelizing them yields the largest end-to-end speedups.\nSoftmax is independent per position. Each (b,t) softmax computes a max, then exponentials and a sum-no cross-talk between positions.\nAttention splits across batch/time/head. The triple loop over (b,t,h) has lots of work per iteration (Q·K, softmax, weighted sum), making thread overhead negligible compared to useful compute.\nMinimal synchronization and no atomics. By choosing iteration spaces that own exclusive output slices, we avoid costly synchronization.\n\n\n\nPractical tips for better scaling\n\nSet OMP_NUM_THREADS to your CPU. Too many threads can hurt (oversubscription). Start with physical cores, then experiment.\nPin threads (optional, advanced). Some OpenMP runtimes support OMP_PROC_BIND=close to improve cache locality.\nMind memory bandwidth. On wide CPUs, GEMMs may become bandwidth-bound. Bigger B/T improves arithmetic intensity; tiny batches underutilize cores.\nWarm caches with tiling. The “tiled” matmul_forward keeps small accumulators in registers and reuses loaded weights across LOOP_UNROLL inner iterations.\nAvoid hidden sharing. If you add new parallel loops, ensure each thread writes to unique memory regions. If you must accumulate to the same place, restructure (like “own-your-row”) or use per-thread scratch buffers then reduce.\n\n\n\nMicro-walkthrough: why collapse helps\nConsider softmax_forward:\n#pragma omp parallel for collapse(2)\nfor (int b = 0; b &lt; B; b++) {\n    for (int t = 0; t &lt; T; t++) {\n        // 1) find max over V\n        // 2) exp/log-sum\n        // 3) normalize first V entries; zero out padded [V..Vp)\n    }\n}\nIf B=4, T=64, that’s 256 independent softmaxes. With collapse(2), OpenMP sees a single loop of 256 iterations to distribute evenly; without collapse, it might chunk by b first (only 4 big chunks), which can load-imbalance.\n\n\nCommon pitfalls (and how this code avoids them)\n\nRace conditions: Two threads writing the same out[i]. Avoided by design: each parallel loop writes distinct slices (e.g., per (b,t) or per o).\nFalse sharing: Threads write adjacent memory locations on the same cache line. It’s minimized by the large, contiguous slices per thread (entire rows/tiles), but if you extend the code with fine-grained parallelism, keep this in mind.\nTiny loops: Overhead can exceed work. The file parallelizes only large, hot loops (GEMMs, attention, softmax), not small scalar ops.\n\n\n\nTry it yourself\n\nChange thread count: Run with OMP_NUM_THREADS=1,2,4,8,… and log step time. Plot speedup vs. threads.\nToggle a pragma: Comment out #pragma omp in matmul_forward only. Measure the slowdown; you’ll see where most time goes.\nExperiment with collapse: Remove collapse(2) in softmax_forward. On small B, you’ll likely see worse scaling.\nPer-layer profiling: Print elapsed time around matmul_forward, attention_forward, and softmax_forward to see which benefits most on your CPU.\nSchedule policy (advanced): Try #pragma omp parallel for schedule(static) vs. dynamic on a heavy loop to see if it changes load balance (defaults are usually fine here).\n\n\n\nThe takeaway\nA handful of well-placed OpenMP pragmas deliver big wins on CPU by parallelizing the most expensive loops (GEMMs, attention, softmax) across cores-without complicating the code. The design ensures each thread works on independent slices, so there’s no locking, no atomics, and very little overhead. If you compile with OpenMP enabled, you get fast, multi-core training; if not, you still have a clean, readable reference implementation.\n\n\n\n40. CPU Memory Footprint and Performance\nWhen you train GPT-2 on your CPU using train_gpt2.c, two big questions usually pop up almost immediately: how much memory is this going to take? and how fast will it run? Let’s walk through both of these in a beginner-friendly way, so you understand not just what happens in the code, but why it behaves the way it does.\n\nMemory: where does it all go?\nImagine training GPT-2 is like cooking a big meal in a small kitchen. You need space for ingredients, bowls for mixing, and counter space for preparing. Memory on your CPU is that kitchen. GPT-2 needs several “bowls” to hold different parts of the computation:\n\nParameters (the weights of the model). These are the “fixed recipe” - the actual numbers the network learns. They come from the checkpoint file you load at the start. For GPT-2 124M, this is about 124 million floating-point numbers. Each one takes 4 bytes, so just the weights are around 500 MB.\nOptimizer state (AdamW). Training doesn’t just adjust weights blindly; it keeps track of two extra moving averages for each weight, called m and v. That means for every single parameter, you store three numbers: the weight, m, and v. So memory for optimizer state is often double the size of the weights themselves. For GPT-2 124M, that’s about 1 GB more.\nGradients. Every time we run a backward pass, we store how much each weight should change. That’s another buffer roughly the same size as the weights - another 500 MB.\nActivations (intermediate results). This is the sneaky one. Every forward pass produces temporary tensors like embeddings, attention maps, and feed-forward outputs. Their size depends on batch size (B) and sequence length (T). If B=4 and T=64, activations are a few hundred MB. If B=32 and T=1024, they can balloon to many gigabytes.\n\nHere’s a rough mental budget for GPT-2 124M with a small setup (B=4, T=64):\n\nParameters: ~500 MB\nOptimizer state: ~1 GB\nGradients: ~500 MB\nActivations: ~200–300 MB Total: ~2–2.5 GB\n\nEven for the “tiny” GPT-2, you already need a couple gigabytes of RAM to train. On a laptop, this can quickly push you to the limit.\n\n\nPerformance: where does time go?\nNow let’s talk speed. When you run train_gpt2.c on CPU, you’ll see lines like:\nstep 1: train loss 5.191576 (took 1927.230000 ms)\nThat “took X ms” tells you how long one step took. Why is it slow? Three main reasons:\n\nMatrix multiplications (matmuls). These are the heart of neural networks. Every attention head and every MLP layer does them. On CPU, most of your step time is spent here. That’s why the code uses OpenMP pragmas (#pragma omp) to parallelize loops across cores.\nAttention softmax. Attention compares every token in a sequence with every other token. If your sequence length is 1024, that’s over a million comparisons per head per layer. On CPU, this quadratic growth is painful.\nMemory bandwidth. CPUs can only move numbers from RAM to cores so fast. Even if you had infinite FLOPs, you’d still be slowed down by how quickly you can fetch and store these huge tensors.\n\n\n\nA simple experiment\nYou can see these effects yourself:\n\nChange batch size (B). Run with B=1, then with B=8. Notice how memory usage and step time scale up.\nChange sequence length (T). Try T=16, then T=256. You’ll see attention costs grow dramatically.\nChange threads. Set OMP_NUM_THREADS=1 versus OMP_NUM_THREADS=8. With more threads, you’ll often see speedups, but only up to the number of physical cores your CPU has.\n\n\n\nWhy this matters\nFor beginners, CPU runs are perfect for learning:\n\nYou can debug with small batches and short sequences.\nYou can step into functions with a debugger and watch tensors being created.\nYou don’t need a GPU just to understand how training works.\n\nBut when it comes to serious training - larger GPT-2 models or even long sequences - CPU is simply too slow. What takes seconds on GPU may take minutes on CPU. That’s why in practice, people use CPUs for learning and testing, and GPUs for large-scale training.\n\n\nThe takeaway\nTraining GPT-2 on CPU is like practicing piano on a small keyboard. It’s slower, limited, and you can’t play the biggest pieces, but it’s great for learning the fundamentals. Memory usage comes from weights, optimizer state, gradients, and activations, and performance is dominated by matmuls and attention. Once you understand where the resources go, you can adjust batch size, sequence length, and threads to find the sweet spot for your machine.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html#chapter-5.-training-loop-cpu-path",
    "href": "books/en-US/book.html#chapter-5.-training-loop-cpu-path",
    "title": "The Book",
    "section": "Chapter 5. Training Loop (CPU Path)",
    "text": "Chapter 5. Training Loop (CPU Path)\n\n41. Backward Pass Walkthrough\nUp until now, we’ve spent all our time looking at the forward pass. That’s the part of the model that takes tokens, pushes them through embeddings, attention, feed-forward layers, and finally produces logits or a loss. For inference, forward pass alone is enough. But if you want to train a model, forward is only half the story.\nTraining means adjusting the weights of the model so that its predictions become better over time. To do this, we need a way to figure out how wrong each weight was and in what direction it should move to reduce the loss. That’s the job of the backward pass.\nThe backward pass is also called backpropagation. It’s the algorithm that moves information in reverse through the network: from the loss, back through the final logits, through every transformer block, down to the embeddings. Along the way, it calculates gradients - small numbers that tell us how much each weight contributed to the error.\n\nThe big idea: chain rule in action\nAt the heart of backpropagation is something very familiar from calculus: the chain rule. If the output of the network depends on many functions stacked together (embedding → attention → MLP → … → loss), then the derivative of the loss with respect to an early parameter is a product of partial derivatives through the entire chain.\nInstead of writing long formulas, the code in train_gpt2.c simply calls each layer’s backward function in reverse order. The gradient flows backward, step by step, and each layer computes its own contribution using local rules.\nThink of it like a relay race, but run backwards: the loss hands a “blame baton” to the output head, which hands it back to the last transformer block, and so on, until it reaches the very first embedding table.\n\n\nWalking through gpt2_backward\nHere’s a simplified sketch of how the backward function looks in the code (names shortened for readability):\nvoid gpt2_backward(GPT2 *model, int *tokens, int *labels, int B, int T) {\n    // Step 1: loss gradient\n    crossentropy_backward(...);\n\n    // Step 2: final projection (lm_head)\n    matmul_backward(model-&gt;lm_head, ...);\n\n    // Step 3: transformer blocks in reverse\n    for (int l = model-&gt;config-&gt;n_layer - 1; l &gt;= 0; l--) {\n        residual_backward(...);\n        layernorm_backward(&model-&gt;blocks[l].ln, ...);\n        mlp_backward(&model-&gt;blocks[l].mlp, ...);\n        attention_backward(&model-&gt;blocks[l].attn, ...);\n    }\n\n    // Step 4: embeddings\n    embedding_backward(model-&gt;token_embedding, tokens, ...);\n    embedding_backward(model-&gt;position_embedding, positions, ...);\n}\nLet’s unpack this line by line.\n\n\nStep 1: Starting from the loss\nThe journey begins with the loss function. In training, the most common loss is cross-entropy. Its backward function compares the predicted probabilities with the true labels and produces a gradient for the logits.\n\nIf the model predicted “cat” with high confidence and the true label was “dog,” the gradient will push the logits away from “cat” and toward “dog.”\nThis gradient is the starting signal that propagates backward through the entire network.\n\n\n\nStep 2: Back through the output head\nAfter the loss, the next stop is the final linear projection (lm_head). This is just a big matrix multiply that turns hidden states into vocabulary logits. Its backward function computes two things:\n\nThe gradient with respect to the weights of lm_head.\nThe gradient with respect to the hidden states that fed into it.\n\nThis hidden-state gradient is then passed back to the last transformer block.\n\n\nStep 3: Transformer blocks in reverse\nHere comes the heavy lifting. Each block has multiple components, and their backward functions are called in the exact opposite order of the forward pass.\n\nResidual backward: the skip connection splits the gradient into two paths - one flowing back into the transformed output, one flowing back into the original input.\nLayerNorm backward: computes gradients with respect to its scale (gamma) and shift (beta), and also passes gradients back to the normalized input.\nMLP backward: applies the chain rule to the two linear layers and the GELU activation. The code reuses temporary values from the forward pass (like activations) to make this efficient.\nAttention backward: this is the trickiest. It computes gradients for Q, K, and V projections, as well as for the softmaxed attention weights. It has to apply the causal mask again to ensure no illegal gradient flows.\n\nThis loop continues until all transformer blocks have been processed.\n\n\nStep 4: Back to embeddings\nFinally, the gradient reaches the embedding tables. This is where the model first looked up vectors for tokens and positions. Now it calculates how much each embedding contributed to the error. These gradients are added into the embedding matrices, telling the optimizer how to update them.\n\n\nWhy this matters\nThe backward pass is what makes learning possible. Without it, the model would forever output the same predictions, never improving. By flowing “blame” backwards, each parameter learns how to nudge itself so that the next forward pass is a little bit better.\nEven though the code looks like a lot of function calls, the principle is simple: start from the loss, step backward through each layer, apply the chain rule locally, and collect gradients.\n\n\nTry it yourself\n\nPrint gradient norms: Add a printf to see the average gradient magnitude at each layer. Notice how they change - sometimes exploding, sometimes vanishing.\nFreeze a layer: Comment out mlp_backward for one block and see how the model fails to update properly.\nInspect embeddings: After training a few steps, dump a few rows of the token embedding matrix. You’ll see the numbers changing because of gradient updates.\nTiny dataset experiment: Train on a very small dataset (like a 10-word corpus) and watch how the backward pass quickly pushes embeddings to memorize it.\nCheck symmetry: Compare the order of calls in gpt2_forward with gpt2_backward. They’re exact opposites - forward builds, backward unbuilds.\n\n\n\nThe takeaway\nBackpropagation is the learning engine of neural networks. In llm.c, the backward pass is written out explicitly, showing how gradients flow from the loss, through the output head, back through every transformer block, and finally into embeddings. Once you understand this flow, you can see how training stitches forward and backward together to slowly shape a random model into a working language model.\n\n\n\n42. Skeleton of Training Loop\nThe backward pass gave us gradients, but gradients by themselves don’t train a model. Training requires a loop: a cycle that repeatedly runs forward, backward, and update steps over and over until the model improves. This cycle is called the training loop, and it is the heartbeat of every deep learning program. In train_gpt2.c, the loop is written explicitly in C, which means you can see every piece instead of it being hidden away in a framework.\n\nThe basic rhythm\nEvery training step follows the same rhythm:\n\nGet a batch of data (input tokens and their labels).\nRun the forward pass to compute predictions and loss.\nRun the backward pass to compute gradients.\nUpdate weights using an optimizer like AdamW.\nLog progress and, occasionally, validate.\n\nThis rhythm repeats thousands or millions of times. With each repetition, the weights shift slightly, nudging the model toward lower loss and better predictions.\n\n\nHow the loop looks in code\nHere’s a simplified sketch from train_gpt2.c (with some details omitted for clarity):\nfor (int step = 0; step &lt; max_steps; step++) {\n    // 1. Load batch of tokens and labels\n    dataloader_next_batch(&train_loader, tokens, labels, B, T);\n\n    // 2. Forward pass\n    gpt2_forward(&model, tokens, labels, B, T);\n\n    // 3. Zero gradients\n    gpt2_zero_grad(&model);\n\n    // 4. Backward pass\n    gpt2_backward(&model, tokens, labels, B, T);\n\n    // 5. Optimizer step (AdamW)\n    adamw_update(&opt, &model, learning_rate);\n\n    // 6. Logging and validation\n    if (step % log_interval == 0) { print_loss(step, model.loss); }\n    if (step % val_interval == 0) { run_validation(...); }\n}\nThis loop captures the full training lifecycle: data, forward, backward, update, and monitoring.\n\n\nStep 1: Batching data\nThe dataloader feeds the loop with small chunks of tokens. Instead of sending the whole dataset at once, it breaks it down into batches of size B (number of sequences per batch) and length T (number of tokens per sequence).\n\nExample: if B=4 and T=128, each batch is 512 tokens long.\nEach sequence has a matching set of labels, which are simply the same tokens shifted one position ahead (so the model always predicts the next word).\n\nThis batching keeps memory use manageable and helps the model see many small samples instead of a few giant ones.\n\n\nStep 2: Forward pass\nThe forward pass computes predictions for all tokens in the batch and calculates the loss. This is the “evaluation” step - how well did the model do on this batch? The result is stored in model.loss.\n\n\nStep 3: Zeroing gradients\nBefore calculating new gradients, the old ones must be cleared out. If you skip this step, gradients from previous batches would accumulate and corrupt the update. In frameworks like PyTorch you’d call optimizer.zero_grad(). Here it’s a plain C function:\ngpt2_zero_grad(&model);\nIt walks through all parameters and resets their gradient buffers to zero.\n\n\nStep 4: Backward pass\nNow the backward function is called. It pushes gradients back through the network, computing how each weight influenced the error. At this point, every parameter has an associated gradient stored in memory.\n\n\nStep 5: Optimizer update\nWith gradients ready, the optimizer (AdamW in this code) updates each parameter:\nnew_weight = old_weight - learning_rate * gradient (with AdamW tweaks)\nThis step is what actually changes the model. Without it, the model would never learn - the forward and backward passes would just repeat the same results forever.\n\n\nStep 6: Logging and validation\nEvery few steps, the loop prints out useful numbers: current step, loss, time taken, and sometimes throughput (tokens per second). This feedback is important to check whether training is actually working.\nEvery few hundred or thousand steps, the loop also runs a validation pass on held-out data. This tells you whether the model is just memorizing training data or genuinely learning patterns that generalize.\n\n\nWhy the training loop matters\nThe training loop is deceptively simple, but it is the engine room of machine learning. Every improvement in model performance happens because this loop runs many times. By writing it explicitly in C, llm.c exposes details that high-level frameworks usually hide: zeroing gradients, passing pointers to arrays, calling backward and optimizer functions directly.\nThis makes it a perfect learning tool. You can see clearly:\n\nWhere the data comes in,\nWhere predictions are made,\nWhere gradients are calculated,\nAnd where learning actually happens.\n\n\n\nTry it yourself\n\nPrint the loss curve: Add a printf inside the loop and write the loss to a file. Plot it - you should see it decrease over time.\nChange batch size: Set B=1 vs. B=8. Notice how the loop becomes noisier with smaller batches but smoother with larger ones.\nSkip backward: Comment out gpt2_backward and optimizer update. Run the loop. You’ll see the loss never decreases - a clear demonstration that forward alone doesn’t train.\nExperiment with steps: Try max_steps=10 vs. max_steps=1000. Short runs show no improvement; longer runs start to reduce the loss.\nSlow it down: Insert a sleep(1); inside the loop. This makes the rhythm visible step by step, so you can literally watch the model “breathe” as it trains.\n\n\n\nThe takeaway\nThe skeleton of the training loop is the core cycle of learning. It feeds data into the model, computes predictions, finds errors, sends them backward, updates weights, and logs progress. Everything else - optimizers, schedulers, distributed training, mixed precision - is just an enhancement of this basic loop. If you understand how this loop works in llm.c, you understand the beating heart of deep learning training.\n\n\n\n43. AdamW Implementation in C\nTraining a neural network is about adjusting millions of parameters so that the model gradually becomes better at predicting text. The function gpt2_update in train_gpt2.c is responsible for this adjustment. It implements the AdamW optimizer, one of the most widely used algorithms in deep learning. Let’s walk through both the theory and the actual implementation.\n\nFrom Gradient Descent to AdamW\nThe most basic optimizer is gradient descent:\nnew_param = old_param - learning_rate * gradient\nThis approach works, but it has weaknesses. The step size (learning rate) must be tuned carefully: too small and training is slow, too large and training diverges. Moreover, all parameters use the same step size, even though some may need gentler updates.\nAdamW improves this by keeping track of moving averages of gradients and scaling updates adaptively. It also introduces weight decay, which prevents parameters from growing too large and helps regularize the model.\n\n\nHow AdamW Works\nAdamW combines several techniques into a single update rule. First, it uses momentum: instead of relying only on the current gradient, it averages recent gradients. This smooths noisy updates. Second, it maintains a running estimate of the squared gradient, which scales down steps in directions where gradients are consistently large. These are sometimes called the first and second moments.\nSince both running averages start at zero, the algorithm applies bias correction during the first few steps. Without this, the early updates would be too small. Finally, AdamW applies weight decay directly in the update, shrinking parameter values slightly each step.\nPutting it together, each parameter update looks like this:\nm_t = β1 * m_(t-1) + (1 - β1) * g_t\nv_t = β2 * v_(t-1) + (1 - β2) * g_t²\nm̂_t = m_t / (1 - β1^t)\nv̂_t = v_t / (1 - β2^t)\n\nnew_param = old_param - lr * ( m̂_t / (sqrt(v̂_t) + ε) + λ * old_param )\nHere m is momentum, v is variance, lr is learning rate, ε is a small constant for stability, and λ is the weight decay factor.\n\n\nThe Implementation in train_gpt2.c\nvoid gpt2_update(GPT2 *model, float learning_rate, float beta1, float beta2,\n                 float eps, float weight_decay, int t) {\n    if (model-&gt;m_memory == NULL) {\n        model-&gt;m_memory = (float*)calloc(model-&gt;num_parameters, sizeof(float));\n        model-&gt;v_memory = (float*)calloc(model-&gt;num_parameters, sizeof(float));\n    }\n\n    for (size_t i = 0; i &lt; model-&gt;num_parameters; i++) {\n        float param = model-&gt;params_memory[i];\n        float grad = model-&gt;grads_memory[i];\n\n        float m = beta1 * model-&gt;m_memory[i] + (1.0f - beta1) * grad;\n        float v = beta2 * model-&gt;v_memory[i] + (1.0f - beta2) * grad * grad;\n\n        float m_hat = m / (1.0f - powf(beta1, t));\n        float v_hat = v / (1.0f - powf(beta2, t));\n\n        model-&gt;m_memory[i] = m;\n        model-&gt;v_memory[i] = v;\n        model-&gt;params_memory[i] -= learning_rate *\n            (m_hat / (sqrtf(v_hat) + eps) + weight_decay * param);\n    }\n}\nThe first if block allocates memory for the moving averages m and v the first time the optimizer runs. Then, for each parameter, the code computes the new averages, applies bias correction, and finally updates the parameter with the AdamW formula.\n\n\nExample Walkthrough\nSuppose we have a parameter w = 0.5 with gradient g = 0.2 on the first training step. Using β1 = 0.9 and β2 = 0.999:\n\nMomentum:\nm = 0.9 * 0 + 0.1 * 0.2 = 0.02\nVariance:\nv = 0.999 * 0 + 0.001 * 0.04 = 0.00004\nBias correction:\nm̂ = 0.02 / (1 - 0.9) = 0.2\nv̂ = 0.00004 / (1 - 0.999) = 0.04\nFinal update (lr = 0.001, weight_decay = 0.01):\nupdate = 0.001 * (0.2 / sqrt(0.04) + 0.01 * 0.5)\n       = 0.001 * (1.0 + 0.005)\n       = 0.001005\n\nSo the parameter becomes w = 0.498995.\n\n\nIntuition\nThink of a ball rolling down a slope. The gradient is the slope itself. Momentum makes the ball keep rolling even if the slope flattens briefly. The variance term makes the ball slow down on rocky ground where the slope changes rapidly. Bias correction ensures the ball doesn’t move too timidly at the start. Weight decay adds friction so the ball doesn’t roll out of control.\n\n\nWhy It Matters\nOptimizers are the difference between a model that trains smoothly and one that diverges or gets stuck. AdamW became popular because it combines stability with efficiency. It automatically adapts to each parameter’s scale, reduces the need for manual learning rate tuning, and includes weight decay in a principled way. For GPT-style models with hundreds of millions of parameters, these qualities make training feasible.\n\n\nTry It Yourself\n\nChange the learning rate from 0.001 to 0.01 in the code and see how quickly the model diverges.\nSet weight_decay = 0 and compare validation loss after a few epochs. The model might overfit more quickly.\nPrint out the first 10 values of m_memory and v_memory during training to watch how they evolve over steps.\nReplace AdamW with plain SGD (just param -= lr * grad) and compare training speed and stability.\nExperiment with β1 = 0 (no momentum) or β2 = 0 (no variance smoothing) and see how noisy updates become.\n\n\n\nThe Takeaway\nAdamW provides a balance of speed, stability, and generalization. In practice, it allows models like GPT-2 to train much more reliably than with vanilla gradient descent. The C implementation in llm.c demonstrates that beneath the math, it’s just a simple loop applying a few arithmetic operations for each parameter.\n\n\n\n44. Gradient Accumulation and Micro-Batching\nModern language models are enormous, and so are the batches of text we would like to feed them during training. But real hardware has limits: a single GPU or CPU may not have enough memory to process a large batch in one go. To solve this, training code often uses gradient accumulation and micro-batching. Both ideas allow us to simulate training with larger batches without requiring more memory than our hardware can provide.\n\nWhat Problem Are We Solving?\nWhen you process a batch of data, you run forward and backward passes to calculate gradients. If your batch size is very large, you get smoother gradients (less noisy), which often helps the model converge better. But large batches may not fit in memory.\nImagine trying to train with a batch of 1024 sequences on a GPU that can only handle 128 sequences at once. Without tricks, you would be forced to use the smaller batch size and give up the benefits of larger batches. Gradient accumulation fixes this problem by letting you split the big batch into smaller micro-batches, process them one at a time, and accumulate the results as if you had processed the big batch all at once.\n\n\nHow It Works in Practice\nLet’s say we want an effective batch size of 1024, but our hardware only supports 128. We split the big batch into 8 micro-batches of 128 each:\n\nRun forward + backward on micro-batch 1, store the gradients.\nRun forward + backward on micro-batch 2, add its gradients to the stored ones.\nRepeat until all 8 micro-batches are processed.\nOnce gradients for all 8 are accumulated, perform the optimizer update.\n\nThe important part is step 4: we only update the parameters once per effective batch, not after each micro-batch. This preserves the effect of training with a large batch.\n\n\nPseudocode Example\nHere’s how this might look in simplified pseudocode:\nint accumulation_steps = 8;\nfor (int step = 0; step &lt; total_steps; step++) {\n    zero_grad(&model);\n    for (int i = 0; i &lt; accumulation_steps; i++) {\n        dataloader_next_batch(&train_loader, tokens, labels, B, T);\n        forward(&model, tokens, labels, B, T);\n        backward(&model, tokens, labels, B, T);\n        // do NOT call optimizer update yet\n    }\n    adamw_update(&model, lr, beta1, beta2, eps, weight_decay, step+1);\n}\nNotice how the optimizer only runs once per outer loop iteration, even though gradients were accumulated across multiple micro-batches.\n\n\nWhy Gradient Accumulation Helps\n\nMemory efficiency: You can train with larger effective batch sizes without needing more hardware.\nTraining stability: Larger batches reduce the variance of gradients, making training less noisy.\nFlexibility: You can scale effective batch size up or down depending on your needs without changing hardware.\n\n\n\nMicro-Batching vs. Accumulation\nMicro-batching refers to the act of splitting a batch into smaller parts. Gradient accumulation is what you do after micro-batching: sum up the gradients across those parts. Together, they allow you to simulate training with any batch size you want, within memory constraints.\n\n\nWhy It Matters\nThe quality of training often depends on batch size. If you can’t fit a large batch directly, gradient accumulation ensures you still reap the benefits. It’s one of those “engineering hacks” that makes training state-of-the-art models possible on limited resources.\n\n\nTry It Yourself\n\nRun training with batch size = 16 and no accumulation. Watch how noisy the loss curve looks.\nNow set micro-batch size = 4 and accumulation_steps = 4. This simulates batch size = 16, but in smaller chunks. Compare the loss curve.\nIncrease accumulation_steps to simulate batch size = 32. Observe if training becomes smoother.\nExperiment with turning accumulation off and on while keeping the same effective batch size. Notice how optimizer updates per epoch differ.\nPrint out how many times the optimizer is called. With accumulation, it should be fewer than the number of micro-batches.\n\n\n\nThe Takeaway\nGradient accumulation and micro-batching are techniques that let you train with large effective batch sizes while staying within the limits of your hardware. They preserve the benefits of large batches-stability and smoother gradients-without demanding extra memory. In llm.c, the simplicity of the training loop means you can clearly see where accumulation fits: gradients are summed across micro-batches, and only then does the optimizer step in. This is a small adjustment in code but a huge enabler in practice.\n\n\n\n45. Logging and Progress Reporting\nEvery training loop needs a way to show what’s happening under the hood. Without logs, you wouldn’t know if the model is improving, if the code is running efficiently, or if something has silently gone wrong. In train_gpt2.c, logging is intentionally minimal but highly informative: each training step prints the step number, the current training loss, and how long that step took to run.\n\nThe Real Code for Logging\nHere’s the relevant snippet from train_gpt2.c:\n// do a training step\nclock_gettime(CLOCK_MONOTONIC, &start);\ndataloader_next_batch(&train_loader);\ngpt2_forward(&model, train_loader.inputs, train_loader.targets, B, T);\ngpt2_zero_grad(&model);\ngpt2_backward(&model);\ngpt2_update(&model, 1e-4f, 0.9f, 0.999f, 1e-8f, 0.0f, step+1);\nclock_gettime(CLOCK_MONOTONIC, &end);\n\ndouble time_elapsed_s = (end.tv_sec - start.tv_sec) +\n                        (end.tv_nsec - start.tv_nsec) / 1e9;\nprintf(\"step %d: train loss %f (took %f ms)\\n\",\n       step, model.mean_loss, time_elapsed_s * 1000);\nThis small block accomplishes two things:\n\nIt measures how long the training step took using clock_gettime.\nIt reports the step number, the loss, and the elapsed time in milliseconds.\n\nThe output looks like this when training:\nstep 0: train loss 4.677779 (took 1987.546 ms)\nstep 1: train loss 5.191576 (took 1927.230 ms)\nstep 2: train loss 4.438685 (took 1902.987 ms)\n\n\nUnderstanding What’s Reported\n\nStep number (step) Tells you where you are in training. Since deep learning often runs for thousands of steps, this acts like a progress bar.\nTraining loss (model.mean_loss) Shows how well the model is fitting the training batch. A lower value generally means better predictions. Watching this number decrease over time is the main signal that learning is happening.\nStep duration (time_elapsed_s * 1000) Measures performance. If one step takes 2000 ms, then 5000 steps would take about 3 hours. Monitoring this helps you estimate total training time and spot performance regressions (e.g., if a new change suddenly doubles the step time).\n\n\n\nWhy It Matters\nLogs are your window into the training process. If the loss goes down smoothly, training is healthy. If it suddenly spikes or stays flat, something is wrong-maybe the learning rate is too high, or the model has run out of capacity. Timing information also matters: you need to know whether the code is running efficiently or wasting cycles.\n\n\nTry It Yourself\n\nChange the learning rate from 1e-4 to 1e-2 and watch how the loss behaves. If it jumps or becomes unstable, you’ll see it directly in the logs.\nAdd validation logging by running the model on a held-out dataset every 100 steps and printing val_loss. Compare it to train_loss.\nRecord the log output to a file with:\n./train_gpt2 &gt; log.txt\nThen plot train_loss over steps in Python or Excel to visualize the curve.\nAdd throughput reporting: divide the batch size times sequence length (B*T) by the step time to print tokens per second. This gives a clearer sense of efficiency.\nTry disabling clock_gettime and only print loss. Notice how much harder it becomes to judge performance without timing information.\n\n\n\nThe Takeaway\nEven the simplest logs can tell you a lot. With just a single line-step, loss, and duration-you know how fast training is, whether it’s converging, and how long it will take. In larger frameworks, this kind of information is often hidden behind dashboards and monitoring tools, but the core idea is the same: training is only useful if you can see and interpret its progress.\n\n\n\n46. Validation Runs in the Training Loop\nWhen you train a model, it is not enough to look only at how well it does on the training data. The real test is whether the model has learned patterns that apply to new, unseen data. This is where validation comes in. Validation is like a quiz the model takes from time to time during training. It does not count toward learning-it is just a check to see how much the model has really understood.\nIn train_gpt2.c, validation is built right into the training loop. Every so often, instead of updating weights, the program pauses and runs the model on a set of tokens it has never trained on. It then prints out the average validation loss. This number tells you if the model is actually generalizing, not just memorizing.\n\nHow the validation code looks\nHere is the actual block of code that handles validation:\nif (step % 10 == 0) {\n    float val_loss = 0.0f;\n    dataloader_reset(&val_loader);\n    for (int i = 0; i &lt; val_num_batches; i++) {\n        dataloader_next_batch(&val_loader);\n        gpt2_forward(&model, val_loader.inputs, val_loader.targets, B, T);\n        val_loss += model.mean_loss;\n    }\n    val_loss /= val_num_batches;\n    printf(\"val loss %f\\n\", val_loss);\n}\nAt first glance, this might look like just a few lines of C code. But behind it are several important ideas about how machine learning models are tested while they learn. Let’s go through this step by step.\n\n\nStep-by-step explanation\nThe first line checks whether it is time to run validation:\nif (step % 10 == 0) {\nThis means that validation happens every 10 steps. The % operator is “modulo,” which returns the remainder of a division. If the step number is divisible by 10 (like 0, 10, 20, 30), then the block runs. By spacing it out this way, validation does not slow training too much but still gives you regular updates.\nNext, the code sets up a place to store the running total of the validation loss:\nfloat val_loss = 0.0f;\nThen it resets the validation dataloader:\ndataloader_reset(&val_loader);\nThis makes sure the validation dataset starts from the beginning each time. That way, the results are consistent-you’re always checking the model on the same set of text, rather than starting from a random place.\nNow comes the loop over validation batches:\nfor (int i = 0; i &lt; val_num_batches; i++) {\n    dataloader_next_batch(&val_loader);\n    gpt2_forward(&model, val_loader.inputs, val_loader.targets, B, T);\n    val_loss += model.mean_loss;\n}\nHere’s what’s happening inside:\n\ndataloader_next_batch fetches the next chunk of tokens and labels from the validation set.\ngpt2_forward runs the model forward on those tokens, predicting the next word for each one, and computes the loss against the true labels.\nThe loss from that batch is added to val_loss.\n\nNotice that there is no call to gpt2_zero_grad, no gpt2_backward, and no gpt2_update. That is because validation does not train the model. It only measures performance.\nFinally, the program averages the loss across the number of batches:\nval_loss /= val_num_batches;\nAnd prints the result:\nprintf(\"val loss %f\\n\", val_loss);\nThis gives you a single number that summarizes how well the model is performing on unseen data at this point in training.\n\n\nHow to read validation loss\nImagine you are training and see logs like this:\nstep 0: train loss 4.677779 (took 1987.546 ms)\nval loss 4.901234\nstep 1: train loss 5.191576 (took 1927.230 ms)\nstep 2: train loss 4.438685 (took 1902.987 ms)\n...\nstep 10: train loss 3.912342 (took 1890.321 ms)\nval loss 4.100321\nThe training loss is printed every step, while the validation loss appears every 10 steps. If both numbers are going down, that is a sign the model is genuinely learning. If training loss drops but validation loss stays the same or starts going up, the model is probably memorizing the training set-this is called overfitting.\n\n\nWhy validation is important\nWithout validation, you could be tricked into thinking the model is improving just because the training loss is going down. But that might only mean it has memorized the training data. Validation checks prevent this by showing you whether the model can handle data it has not seen before. It is like a student practicing with old exam papers (training) versus being tested with new problems (validation).\n\n\nSmall details that matter\nThe code averages validation loss over val_num_batches, which is set earlier to 5. That means it only checks 5 batches, not the entire validation dataset. This is a shortcut-it makes validation much faster, at the cost of some accuracy in the measurement. But for training feedback, this is usually enough.\nThe batch size B and sequence length T for validation are the same as training. This keeps the loss comparable between training and validation.\n\n\nTry it yourself\nYou can experiment with the validation process to understand it better. Here are some ideas:\n\nChange the frequency from every 10 steps to every 5 or even every step. You’ll see more validation updates, but training will slow down.\nIncrease val_num_batches to 20. The validation loss will become less noisy, but each check will take longer.\nComment out the validation block and train again. Notice how you lose a sense of whether the model is really generalizing.\nSave validation loss values to a file and plot them. Compare the curve against the training loss curve. You’ll see how they move together or diverge.\nTry using a very small validation dataset. Watch how the loss jumps around more compared to a larger, more stable dataset.\n\n\n\nThe takeaway\nValidation runs are short forward-only tests that give you confidence the model is learning patterns that apply to new text. They are easy to implement-a few lines of code in train_gpt2.c-but they are one of the most important tools for monitoring training. By checking validation loss regularly, you make sure your model is not just memorizing but actually becoming better at language modeling.\n\n\n\n47. Checkpointing Parameters and Optimizer State\nTraining a model can take hours, days, or even weeks. If you stop the program halfway-whether by accident (a crash, a power cut) or on purpose (pausing to save compute)-you don’t want to start over from scratch. Checkpointing solves this problem by saving the model’s parameters and optimizer state to disk so you can resume training later.\n\nWhat a checkpoint contains\nA checkpoint is like a “save game” for machine learning. At a minimum, it needs:\n\nModel parameters – the actual weights of the neural network, stored as floating-point numbers in memory. These define what the model has learned so far.\nOptimizer state – for AdamW, this includes the running averages of gradients (m_memory) and squared gradients (v_memory). Without these, the optimizer would lose its “memory” of past updates, which could destabilize resumed training.\nStep counter – the number of steps completed so far. This matters for bias correction in AdamW and for scheduling the learning rate.\n\nTogether, these three things capture the full training state.\n\n\nSaving a checkpoint\nAlthough train_gpt2.c is kept minimal and does not include full checkpointing code, the idea is straightforward. You allocate a file, write all parameters, optimizer buffers, and metadata, then close the file. In pseudocode, it looks like this:\nFILE *f = fopen(\"checkpoint.bin\", \"wb\");\nfwrite(model.params_memory, sizeof(float), model.num_parameters, f);\nfwrite(model.m_memory, sizeof(float), model.num_parameters, f);\nfwrite(model.v_memory, sizeof(float), model.num_parameters, f);\nfwrite(&step, sizeof(int), 1, f);\nfclose(f);\nThis is a binary dump of the model and optimizer. Later, you can load the file back with fread calls into the same memory locations.\n\n\nLoading a checkpoint\nLoading is the reverse:\nFILE *f = fopen(\"checkpoint.bin\", \"rb\");\nfread(model.params_memory, sizeof(float), model.num_parameters, f);\nfread(model.m_memory, sizeof(float), model.num_parameters, f);\nfread(model.v_memory, sizeof(float), model.num_parameters, f);\nfread(&step, sizeof(int), 1, f);\nfclose(f);\nOnce loaded, training can continue exactly where it left off.\n\n\nWhy optimizer state matters\nIt might seem enough to save only the model’s parameters. But AdamW depends on moving averages of past gradients. If you throw those away and restart with only the parameters, the optimizer will behave differently. Learning may suddenly become unstable, or the effective learning rate may feel wrong. That’s why saving both the parameters and optimizer state gives the most faithful restart.\n\n\nWhy checkpointing is essential\nTraining is rarely smooth. Servers reboot, experiments are interrupted, bugs are found. Without checkpoints, any interruption means wasted compute and lost progress. With checkpoints, you can pause and resume at will. They also let you archive important moments in training-for example, saving the model when validation loss is lowest, not just at the end.\n\n\nTry it yourself\n\nWrite a small function that saves the model’s parameters after every 100 steps. Then kill the program midway and reload from the saved file. Confirm that resumed training picks up where it left off.\nTry saving only parameters but not optimizer state. Resume training and compare loss curves. You’ll see that the run diverges from the original.\nSave checkpoints at multiple steps and later reload them to compare model generations (does the model produce more fluent text after 10 steps, 100 steps, 1000 steps?).\nIntentionally corrupt part of a checkpoint file (flip a few bytes) and try reloading. This helps you understand why consistency checks or checksums are often added in real systems.\nStore checkpoints in a versioned way (e.g., checkpoint_step100.bin, checkpoint_step200.bin) so you can roll back if a later training phase degrades performance.\n\n\n\nThe takeaway\nCheckpointing is what makes long-running training practical. By saving parameters, optimizer state, and the step counter, you preserve not just what the model knows but how it is learning. In real projects, checkpoints are the bridge between experiments and production: they let you stop, resume, compare, and deploy models without ever starting from scratch. Even though llm.c does not fully implement it, the concept is simple and invaluable.\n\n\n\n48. Reproducibility and Small Divergences\nWhen training deep learning models, two runs that look identical on the surface can still behave differently. One run might converge quickly, another might take longer, and sometimes losses diverge even though you used the same dataset and code. This happens because of the way randomness and numerical precision interact during training. Reproducibility is about controlling these factors so that results are consistent and meaningful.\n\nSources of randomness\nThere are several places where randomness sneaks into training:\n\nData order: if batches are shuffled differently, the model sees tokens in a new sequence. Early steps can influence the trajectory of training.\nWeight initialization: initial parameters are usually set randomly. Different seeds lead to slightly different starting points.\nDropout and sampling: while train_gpt2.c is minimal and doesn’t include dropout layers, many neural networks do. Dropout randomly disables activations during training.\nFloating-point arithmetic: on CPUs and GPUs, the order of summations or parallel reductions can cause tiny rounding differences. Over many steps, these small changes accumulate.\n\n\n\nHow llm.c handles reproducibility\nThe repository includes functions like manual_seed and random_f32 in llmc/rand.h. These are simple random number generators that can be seeded with a fixed value. For example:\nmanual_seed(1337);  \nIf you call this before training, the random number generator starts from the same state every run. That means weight initialization and sampling will be reproducible.\nThe dataloaders also have reproducibility options. When you initialize a DataLoader, you can decide whether it shuffles batches or not. Keeping this consistent ensures the model sees the same data order each run.\n\n\nWhy small divergences happen anyway\nEven with fixed seeds, you might notice that two runs are not perfectly identical. On CPUs, differences often come from OpenMP parallel loops-threads may sum numbers in a different order, producing slightly different results. On GPUs, parallelism and library implementations (like cuBLAS or cuDNN) can do the same.\nThese differences are usually very small, but deep learning systems are chaotic: tiny changes in the early steps can grow into visible differences later. This doesn’t mean the code is wrong-it just means floating-point math has limits.\n\n\nWhy reproducibility matters\nReproducibility isn’t just about peace of mind. It has real uses:\n\nDebugging: if a bug appears, you want to reproduce the exact same run to diagnose it.\nComparisons: when testing new optimizers, schedulers, or architectures, you want fair comparisons on identical conditions.\nScience: reproducible results are essential for research papers and benchmarks.\n\nAt the same time, absolute bit-for-bit reproducibility is often unrealistic in large parallel systems. Instead, the goal is practical reproducibility: ensuring that runs are similar enough to reach the same conclusions.\n\n\nExample experiment\nSuppose you seed training with manual_seed(1337) and use the same dataset. You might get a loss curve like this:\nRun A: step 1000 → val loss 3.42  \nRun B: step 1000 → val loss 3.43  \nThe numbers are not identical, but they are close. The important part is that the model’s learning trajectory is stable and results are comparable.\nIf you remove the seed and allow full randomness, you might get:\nRun A: step 1000 → val loss 3.42  \nRun B: step 1000 → val loss 3.89  \nBoth are valid, but harder to compare.\n\n\nTry it yourself\n\nRun training twice without setting a seed. Compare how training loss and validation loss differ at step 500.\nSet a fixed seed with manual_seed(42) before building the model. Run training twice and compare again. You should see closer numbers.\nEnable OpenMP with multiple threads and then run with a single thread. Notice how results differ slightly due to floating-point summation order.\nSave two checkpoints from runs with different seeds. Use the model to generate text and compare outputs. You’ll see different wording, but both grammatically plausible.\nIncrease the dataset size and check if differences between runs shrink. With more data, randomness matters less.\n\n\n\nThe takeaway\nReproducibility in training is about controlling randomness where possible and accepting small divergences where not. In llm.c, reproducibility is made clear through simple seeding functions and deterministic dataloader options. Perfect bit-level reproducibility isn’t the point-the goal is to ensure results are stable, comparable, and scientifically sound, even if tiny numerical differences creep in.\n\n\n\n49. Command-Line Flags and Defaults\nWhen you run a training program, you often want to change certain settings without editing the source code. For example, you might want to try a different batch size, adjust the learning rate, or train for more steps. Command-line flags make this possible. In train_gpt2.c, defaults are set inside the program, but it can also be compiled to accept arguments, giving you flexibility while keeping the code minimal.\n\nWhy flags exist\nDeep learning experiments are highly sensitive to hyperparameters-values like learning rate, batch size, sequence length, or number of steps. If every change required modifying source code, recompiling, and rerunning, experimentation would be slow and error-prone. Flags allow you to configure these parameters quickly at runtime.\nIn many large frameworks (like PyTorch or TensorFlow), command-line arguments are parsed with helper libraries. In llm.c, the philosophy is simplicity: flags are either defined in code as constants, or you can extend main with standard C argument parsing to override defaults.\n\n\nDefaults in train_gpt2.c\nLooking at the code, here are the main defaults hardcoded in the main function:\n\nBatch size (B):\nint B = 4; // number of sequences per batch\nSequence length (T):\nint T = 64; // tokens per sequence\nValidation batches:\nint val_num_batches = 5;\nTraining steps:\nfor (int step = 0; step &lt;= 40; step++) {\nBy default, only 40 steps are run in this example.\nOptimizer hyperparameters (inside gpt2_update call):\ngpt2_update(&model, 1e-4f, 0.9f, 0.999f, 1e-8f, 0.0f, step+1);\nHere the learning rate is 1e-4, beta values for AdamW are 0.9 and 0.999, epsilon is 1e-8, and weight decay is 0.0.\n\nThese defaults are chosen to make the reference training loop run quickly and predictably, especially on small datasets like Tiny Shakespeare or Tiny Stories.\n\n\nHow to add flags\nIf you want flexibility, you can extend main with argument parsing:\nint main(int argc, char argv) {\n    int B = 4;\n    int T = 64;\n    int max_steps = 40;\n    if (argc &gt; 1) B = atoi(argv[1]);\n    if (argc &gt; 2) T = atoi(argv[2]);\n    if (argc &gt; 3) max_steps = atoi(argv[3]);\n    ...\n}\nNow you can run:\n./train_gpt2 8 128 100\nThis sets batch size to 8, sequence length to 128, and steps to 100, without changing source code.\n\n\nWhy it matters\nCommand-line flags make experimentation far more efficient. You can try multiple configurations in one day without recompiling or editing the file repeatedly. This is especially useful when running jobs on clusters where you want scripts that launch many experiments automatically with different parameters.\nDefaults are equally important: they give you a safe, predictable starting point. Beginners can run the code without thinking about flags, while advanced users can override values as needed.\n\n\nTry it yourself\n\nKeep the default batch size of 4 and sequence length of 64. Run training and note the time per step.\nChange batch size to 8 by editing the code. Observe how training speed changes and how memory usage increases.\nModify the loop to train for 200 steps instead of 40. Watch how loss decreases further.\nAdd argument parsing to accept learning rate as a flag. Experiment with 1e-3 vs. 1e-5 and see how quickly training diverges or stalls.\nCreate a shell script that runs training multiple times with different values for B and T. Compare results.\n\n\n\nThe takeaway\nCommand-line flags and defaults balance simplicity with flexibility. Defaults make the code runnable out of the box, while flags let you scale experiments without constantly editing the source. In train_gpt2.c, this design keeps the training loop minimal but still adaptable, encouraging both clarity and experimentation.\n\n\n\n50. Example CPU Training Logs and Outputs\nOne of the best ways to understand what a training loop is doing is by reading its logs. Logs are the program’s way of telling you how training is progressing: what the loss is, how fast it’s running, and whether validation checks are improving. In train_gpt2.c, logging is deliberately minimal so you can easily see the essentials without being overwhelmed.\n\nWhat the logs look like\nHere’s a snippet of output from running the CPU training loop on Tiny Shakespeare:\ntrain dataset num_batches: 1192\nval dataset num_batches: 128\n[GPT-2]\nmax_seq_len: 1024\nvocab_size: 50257\npadded_vocab_size: 50304\nnum_layers: 12\nnum_heads: 12\nchannels: 768\nnum_parameters: 124475904\nnum_activations: 73347840\nval loss 5.325529\nstep 0: train loss 4.677779 (took 1987.546000 ms)\nstep 1: train loss 5.191576 (took 1927.230000 ms)\nstep 2: train loss 4.438685 (took 1902.987000 ms)\n...\nEach part of this output has meaning:\n\nDataset sizes: how many training and validation batches are available.\nModel config: confirmation that the GPT-2 model was loaded correctly (sequence length, vocab size, number of layers, etc.).\nValidation loss: an average measure of how well the model is doing on unseen data.\nTraining step logs: for each step, you see the training loss and how long the step took in milliseconds.\n\n\n\nUnderstanding loss values\nLoss is the number that tells us how far the model’s predictions are from the correct answers. Lower is better.\n\nA loss around 5.3 means the model is essentially guessing.\nAs training progresses, you want to see this number slowly decrease.\nIf the number gets stuck, or goes up, it can indicate problems with the learning rate, dataset, or implementation.\n\nThink of it like a report card: at the beginning, the model is failing every test, but as it practices (trains), the grades (loss values) improve.\n\n\nSpeed measurements\nThe “took … ms” part shows how long each step took. On CPU, this is usually slow, sometimes a couple of seconds per step. On GPU, the same step might only take tens of milliseconds.\nTiming logs are useful because they help you:\n\nEstimate how long full training will take.\nCompare performance between machines.\nSpot problems if training suddenly slows down.\n\n\n\nOccasional validation checks\nEvery few steps, the code switches to validation data and prints a val loss. This is crucial: training loss always goes down if the model memorizes the training set, but validation loss tells you if it is actually learning patterns that generalize.\nIf training loss goes down but validation loss stays high, that’s a sign of overfitting.\n\n\nGenerated samples\nAt certain steps, the code also prints generated text like this:\ngenerating:\n\n The King had not\n that the Duke of Northumberland and the Duke of\n...\n\nEven though the text might look strange at first, it’s a powerful sign that the model is learning. At the beginning, output is pure gibberish, but as training continues, you start to see recognizable words and patterns.\n\n\nWhy it matters\nLogs are your window into the training process. Without them, training would be a black box-you’d wait hours and have no idea if it was working. By watching loss curves, step times, and sample outputs, you can make informed adjustments and gain confidence that the model is on the right track.\n\n\nTry it yourself\n\nRun the training loop as-is and save the console output. Mark how loss changes between step 0 and step 40.\nIncrease the number of steps to 200 and compare how the losses evolve.\nChange the batch size from 4 to 8 and note both the training speed and the loss behavior.\nEdit the code to print validation loss every step instead of every 10 steps. Does the trend look smoother?\nSave the generated samples at steps 20 and 40. Compare how the quality changes.\n\n\n\nThe takeaway\nTraining logs are like a diary of the model’s progress. They show you how quickly the model is learning, how well it generalizes, and how fast the computation runs. By reading and interpreting logs carefully, you can guide experiments, detect problems early, and appreciate the progress that’s happening inside the model.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html#chapter-6.-testing-and-profiling",
    "href": "books/en-US/book.html#chapter-6.-testing-and-profiling",
    "title": "The Book",
    "section": "Chapter 6. Testing and Profiling",
    "text": "Chapter 6. Testing and Profiling\n\n51. Debug State Structs and Their Role\nWhen building and training a model as complex as GPT-2, you need ways to peek inside and check whether the values being passed around make sense. This is where debug state structs come in. In llm.c, the code is written in plain C, without the rich debugging utilities of frameworks like PyTorch. That means the developers had to create their own mechanism to store, inspect, and compare intermediate values.\nA struct in C is just a container that groups related variables together. For debugging, you can think of a struct as a little notebook where the program writes down numbers as it computes them. These numbers might include:\n\nThe raw embeddings for tokens.\nThe attention scores before and after softmax.\nThe outputs of each MLP block.\nThe predicted probabilities for the next token.\n\nBy saving these into a structured format, the program can later compare them to the outputs of a trusted reference implementation (usually PyTorch).\n\nHow it works in practice\nInside llm.c, there are places where arrays of floats—like hidden states or logits—are copied into a debug state struct. Once stored, these values can be printed, dumped to a file, or checked against “golden” results from PyTorch.\nImagine you’re testing a tiny batch of input tokens. The forward pass runs as usual, but at specific checkpoints (say, right after attention or after the final linear projection), the program writes those arrays into a struct. Later, when running a PyTorch model with the same inputs and weights, the two outputs can be compared element by element.\nThis is essential for catching subtle errors:\n\nA misplaced transpose in matrix multiplication.\nForgetting to apply a mask in attention.\nA floating-point precision mismatch in softmax.\n\nWithout the struct, you’d only know the model loss looks “off.” With the struct, you know exactly which step went wrong.\n\n\nWhy it matters\nDebug structs bridge the gap between C and Python ecosystems. PyTorch has a decade of battle-tested layers, so it’s the gold standard for correctness. By saving intermediate activations in C and comparing them against PyTorch, developers ensure that every layer behaves identically. This builds confidence that the llm.c codebase isn’t just “roughly correct,” but precisely reproduces GPT-2’s math.\nFor anyone modifying the code—for example, writing a new activation function or experimenting with quantization—the debug structs act like a safety net. You can quickly see if your change accidentally altered the outputs in a way that breaks parity with the original model.\n\n\nTry it yourself\n\nTrace a forward pass: Run the CPU version with a tiny batch and enable debug dumps. Look at the embeddings, attention scores, and final logits.\nCross-check with PyTorch: Run the same input through Hugging Face GPT-2. Print the same tensors. Compare a few entries by hand—do they match closely?\nIntroduce a bug: Change the scaling factor in attention (e.g., remove the 1/√d term). Run again and see how quickly the mismatch shows up in the debug struct.\nExtend the struct: Add a new field for an intermediate step you care about, like LayerNorm outputs. Print it during debugging to see how normalization changes the activations.\n\n\n\nThe takeaway\nDebug state structs are the microscope of llm.c. They allow you to pause the flow of numbers, record them, and compare them against a known-good model. Without them, development would feel like working blindfolded. With them, you can track down errors precisely, ensure parity with PyTorch, and confidently extend the system knowing you have a reliable safety net.\n\n\n\n52. test_gpt2.c: CPU vs PyTorch\nTesting is one of the most important parts of the llm.c project. The file test_gpt2.c exists specifically to check whether the C implementation of GPT-2 produces the same outputs as PyTorch, which is the trusted reference. Without this file, you would only know if the final training loss looked reasonable. With it, you can verify that every part of the forward pass matches.\nAt its core, test_gpt2.c runs a very controlled experiment: it loads a GPT-2 model checkpoint (exported from PyTorch), prepares a small batch of input tokens, executes a forward pass in C, and compares the outputs against the corresponding tensors from PyTorch. If everything matches within a tight numerical tolerance, you know the C code is correct. If not, you have a clear signal that something is wrong.\n\nHow the test works\n\nLoad the checkpoint The test begins by reading a binary checkpoint file, such as gpt2_124M.bin, which contains the weights of the GPT-2 model. These weights were originally trained in PyTorch and then exported into a binary format that llm.c can understand.\nPrepare the inputs The test uses a known sequence of token IDs—sometimes from a dataset like Tiny Shakespeare, sometimes just a few hand-picked tokens. This ensures the same inputs can be run through both PyTorch and C implementations.\nRun the C forward pass The function gpt2_forward is executed on the CPU. All embeddings, attention layers, MLPs, and final logits are computed exactly as they would be during real inference or training.\nCompare with PyTorch For each major tensor (e.g., hidden states, attention outputs, final logits), the values are compared against saved outputs from PyTorch. The comparison usually allows for very small differences, since floating-point math can vary slightly between libraries. A tolerance like 1e-5 is common.\nReport mismatches If any element deviates beyond the allowed tolerance, the test reports the difference. Developers can then investigate where the divergence started, often by adding more debug dumps of intermediate states.\n\n\n\nWhy this test is crucial\nC code is low-level and unforgiving. A single indexing mistake, a wrong stride, or a missing scaling factor in attention can make outputs diverge wildly. Since GPT-2 has millions of parameters, such errors are almost impossible to spot by hand. By tying the implementation back to PyTorch, test_gpt2.c provides a ground truth check.\nThis also ensures scientific reproducibility. If someone else downloads llm.c and runs test_gpt2.c, they should see the same level of agreement with PyTorch. That way, they can trust that training runs, losses, and model outputs are not artifacts of a broken implementation.\n\n\nExample in action\nImagine you’ve just modified the attention code to optimize the matrix multiplication. You recompile and run test_gpt2.c. If you see an error like:\nMismatch at position [0, 12, 42]: C=0.1234, Torch=0.1235\nyou know the two match within tolerance—everything is fine. But if you see:\nMismatch at position [0, 12, 42]: C=0.3456, Torch=0.1235\nthat’s a red flag. It means the optimization introduced a bug. Without the test, you might not notice until much later when training fails to converge.\n\n\nWhy it matters\ntest_gpt2.c is the guarantee that the C implementation is not just “close enough,” but faithful. It ensures that improvements, optimizations, or even rewrites don’t silently corrupt the model’s behavior. It’s a direct bridge between the experimental world of C internals and the well-established baseline of PyTorch.\n\n\nTry it yourself\n\nRun make test_gpt2 in the repository. Observe whether the outputs match PyTorch.\nDeliberately change one line of code in gpt2_forward—for example, remove the attention scaling factor. Run the test again and see how quickly it fails.\nAdd your own print statements to show which tensors are being compared. Watch how the numbers line up almost exactly.\nTry running with different checkpoints (e.g., 124M vs 355M) to see if parity holds across scales.\n\n\n\nThe takeaway\ntest_gpt2.c is not just another file in the repository—it’s the truth meter. It reassures you that the complicated layers of GPT-2 have been implemented correctly in C and remain consistent with PyTorch. This confidence is what allows further work—whether training, profiling, or extending the model—to proceed on a solid foundation.\n\n\n\n53. Matching Outputs Within Tolerances\nOnce you have a test like test_gpt2.c set up, the next challenge is figuring out how close the outputs need to be for the test to pass. Computers don’t always produce bit-for-bit identical results when doing floating-point math. The order of operations, the precision of instructions, and even the type of hardware (CPU vs GPU) can cause tiny differences.\nIf you demanded exact matches, most tests would fail even when the implementation is correct. That’s why llm.c uses tolerance-based comparison. Instead of asking “are these numbers exactly equal?”, the code asks “are these numbers close enough?”\n\nAbsolute vs relative tolerance\nThere are two common ways to define “close enough”:\n\nAbsolute tolerance: check that the difference between two numbers is smaller than a threshold. For example,\n|0.123456 - 0.123455| = 0.000001 &lt; 1e-5\nThis works well for values near zero.\nRelative tolerance: check that the difference is small relative to the size of the numbers. For example,\n|1000.0 - 1000.1| / 1000.0 = 0.0001\nEven though the absolute difference is 0.1, that’s tiny compared to the scale of 1000.\n\nIn practice, the code often combines both. It passes if the difference is smaller than either the absolute tolerance or the relative tolerance.\n\n\nWhy tolerances are necessary\nImagine you run the forward pass on CPU in C and in PyTorch. PyTorch might use fused kernels or higher-precision accumulations. If you compare the final logits, you may see values like:\n\nPyTorch: -3.4521234\nC: -3.4521255\n\nThe difference is just 0.0000021. For practical purposes, they’re the same. Without tolerance, this tiny difference would fail the test. With tolerance, you can safely say both implementations agree.\n\n\nExample from debugging\nSuppose you compare the probabilities after softmax. You might get:\n\nPyTorch: 0.3333333, 0.3333333, 0.3333333\nC: 0.3333334, 0.3333333, 0.3333333\n\nHere, the first value differs in the last decimal place. The tolerance rule says that’s fine, since the absolute error is smaller than 1e-7.\nBut if you saw something like:\n\nPyTorch: 0.3333333, 0.3333333, 0.3333333\nC: 0.5000000, 0.2500000, 0.2500000\n\nthe mismatch is huge—no tolerance rule would allow it. That’s a clear bug.\n\n\nWhy it matters\nMatching within tolerance isn’t just a technical detail; it’s about trust. It lets you say with confidence that the implementation is mathematically faithful to the reference. You don’t waste time chasing harmless decimal noise, but you also don’t miss real mistakes.\nThis approach is also what makes cross-platform development possible. The same llm.c code can be run on Linux, macOS, or even inside different compilers, and as long as results fall within tolerance, you know the model behavior is preserved.\n\n\nTry it yourself\n\nRun test_gpt2.c and look at the output logs. Notice how many decimal places match.\nChange the tolerance threshold in the code from 1e-5 to something stricter like 1e-8. See if the test starts failing due to harmless floating-point noise.\nAdd a deliberate bug—for example, skip dividing by √d in the attention code—and rerun. The mismatches will be far larger than the tolerance, proving the bug is real.\nCompare CPU results with PyTorch, then recompile with different compiler flags (like -O0 vs -O3) and check if results still fall within tolerance.\n\n\n\nThe takeaway\nTolerance-based testing is what allows llm.c to be both rigorous and realistic. It ensures that differences are only flagged when they matter, while ignoring the harmless quirks of floating-point math. This makes the test suite a reliable tool for catching true errors without overwhelming you with false alarms.\n\n\n\n54. Profiling with profile_gpt2.c\nAfter verifying that the outputs match PyTorch, the next big question is: how fast is the code running? Correctness is essential, but performance is what makes a minimal C implementation like llm.c worthwhile. That’s where profile_gpt2.c comes in. It is a small program that runs controlled forward passes through GPT-2 and measures the time they take, helping you understand where the bottlenecks are.\n\nWhat profiling means\nProfiling is the act of measuring the performance of a program, not just its correctness. Instead of asking “does this number match PyTorch?”, profiling asks:\n\nHow many milliseconds does one forward pass take?\nWhich part of the model consumes the most time?\nDoes using OpenMP threads actually speed things up?\nHow does batch size affect runtime?\n\nBy answering these, you can make informed decisions about optimization.\n\n\nHow profile_gpt2.c works\nThe profiling program is structured like a simplified inference loop.\n\nModel setup It loads a GPT-2 checkpoint (e.g., gpt2_124M.bin) into memory and allocates space for activations.\nDummy input Instead of using real text, it creates random token IDs. That way, the cost measured comes purely from the computation, not from data loading or tokenization.\nTiming with clock functions Before and after each forward pass, it records timestamps with clock_gettime(CLOCK_MONOTONIC, &start) and &end. The difference gives the runtime in seconds, which is usually converted into milliseconds.\nLooping for stability A single run can be noisy due to background processes on your computer. To smooth things out, profile_gpt2.c runs the forward pass multiple times and averages the results.\nReporting results Finally, it prints the average time per forward pass. Sometimes it also estimates FLOPs (floating-point operations per second), giving you a rough idea of efficiency compared to the theoretical peak of your CPU.\n\n\n\nWhat you can learn from profiling\nRunning profile_gpt2.c on a CPU gives insights like:\n\nThe attention blocks dominate runtime, because they involve large matrix multiplications.\nIncreasing batch size makes the runtime longer, but not proportionally—sometimes bigger batches use hardware more efficiently.\nOpenMP can speed things up when there are multiple CPU cores available, but scaling may flatten out after a certain number of threads.\n\nThis helps decide where to spend effort. For example, if LayerNorm takes 2% of the runtime but attention takes 70%, you know optimization should focus on the attention code.\n\n\nWhy it matters\nProfiling isn’t just about numbers. It’s about guiding your development choices. Without profiling, you might spend weeks hand-optimizing LayerNorm, only to discover it barely affects overall runtime. With profiling, you see immediately where the slowdowns are and can focus on the real bottlenecks.\nIt also provides baseline performance numbers. If you change something in the implementation, re-running profile_gpt2.c will tell you if it sped things up or slowed them down. This feedback loop is essential for optimization work.\n\n\nTry it yourself\n\nCompile and run profile_gpt2.c with a small model checkpoint. Note the reported runtime per forward pass.\nChange the batch size B and sequence length T, then re-run. Watch how runtime scales with larger inputs.\nSet OMP_NUM_THREADS=1 to disable threading and compare it against OMP_NUM_THREADS=4 or higher. How much faster is it with multiple cores?\nModify the code to time individual layers (embeddings, attention, MLP). This gives even more precise insight into which parts dominate computation.\n\n\n\nThe takeaway\nprofile_gpt2.c turns performance from a guess into hard data. It tells you exactly how long a forward pass takes, how much threading helps, and where the real bottlenecks are. With it, you can track progress as you optimize the code, ensuring that your changes make the model not only correct but also efficient.\n\n\n\n55. Measuring FLOPs and CPU Performance\nWhen talking about performance in deep learning, it’s not enough to say “this run took 200 milliseconds.” To really understand efficiency, we need a measure that’s independent of hardware and input size. That’s where FLOPs come in: floating-point operations. A FLOP is a single numerical calculation involving real numbers—like an addition, multiplication, or division.\nBy counting how many FLOPs a model requires and comparing it to how many the computer can perform per second, you can evaluate how close your implementation is to the theoretical maximum speed of your CPU.\n\nWhat FLOPs mean in practice\nEvery layer of GPT-2—embeddings, attention, feed-forward—can be broken down into a sequence of multiplications and additions. For example:\n\nA matrix multiplication between two matrices of size M × K and K × N requires 2 × M × K × N FLOPs.\nA softmax across a vector of size d requires about 3d FLOPs (exponentials, sums, and divisions).\nAn MLP block with hidden size h and intermediate size 4h requires multiple matrix multiplications, adding up to billions of FLOPs per training step.\n\nWhen you sum these across all layers, even a small GPT-2 model like 124M parameters involves several gigaFLOPs (billions of operations) for one forward pass.\n\n\nHow profile_gpt2.c estimates FLOPs\nThe profiling code doesn’t literally count every multiplication. Instead, it uses formulas derived from matrix dimensions:\n\nThe configuration struct (model.config) gives the number of layers, heads, embedding size, and sequence length.\nFor each block (attention + MLP), the code applies standard FLOPs formulas for matrix multiplication and softmax.\nThese counts are added up to estimate the total FLOPs for a forward pass.\nDividing the total FLOPs by the measured runtime (in seconds) gives FLOPs/second, also known as throughput.\n\nFor example, if a forward pass takes 0.1 seconds and involves 20 billion FLOPs, then throughput is about 200 GFLOPs/s.\n\n\nWhy this is useful\n\nCompare hardware: You can test the same model on a laptop CPU and a server CPU, then compare FLOPs/s to see how much faster the server is.\nCompare implementations: If you modify attention to use a different algorithm, the FLOPs count won’t change, but if runtime decreases, throughput increases—showing the optimization worked.\nKnow your limits: CPUs often achieve only a fraction of their theoretical peak FLOPs due to memory bottlenecks. Profiling shows how close you’re getting in practice.\n\n\n\nExample\nLet’s say:\n\nForward pass FLOPs: 15 billion\nRuntime: 0.2 seconds\nThroughput: 75 GFLOPs/s\n\nIf your CPU’s datasheet says the peak is 200 GFLOPs/s, then you’re at about 37% efficiency. That gap might be due to memory latency, cache misses, or lack of vectorization.\n\n\nTry it yourself\n\nRun profile_gpt2.c and note the reported FLOPs and runtime.\nChange the sequence length T and observe how FLOPs scale linearly with it.\nIncrease the number of layers in the model configuration—watch FLOPs rise accordingly.\nCompare your measured FLOPs/s with the theoretical maximum listed for your CPU. How close are you?\n\n\n\nThe takeaway\nFLOPs turn performance from “this feels fast” into hard numbers you can compare across runs, machines, and implementations. By knowing both the operation count and the achieved throughput, you gain a clear picture of how efficient the llm.c code really is and where further optimizations might pay off.\n\n\n\n56. Capturing Memory Usage on CPU\nWhile FLOPs tell us how much raw computation a model needs, performance isn’t just about speed—it’s also about memory usage. Modern language models are enormous, and on CPUs with limited RAM, memory can become the true bottleneck. That’s why llm.c also emphasizes monitoring how much memory is used during inference and training.\n\nWhat consumes memory in GPT-2\nThere are several key components that take up space:\n\nParameters (weights): GPT-2 124M has about 124 million parameters. Each is stored as a 32-bit float (4 bytes). That alone is roughly 500 MB.\nGradients: During training, gradients for each parameter are stored. That doubles the memory usage.\nOptimizer states: AdamW requires two additional memory slots per parameter (m and v), which doubles it again. With parameters, gradients, and optimizer states combined, training can require 4× the parameter size in memory.\nActivations: These are the intermediate outputs of each layer (attention scores, MLP results, normalized states). For backpropagation, activations from the forward pass must be kept until gradients are computed. Depending on batch size and sequence length, activations can rival parameter memory.\n\n\n\nMeasuring memory in practice\nOn CPU, memory usage can be inspected in several ways:\n\nOperating system tools: top, htop, or Activity Monitor show total memory used by the program.\nManual accounting in code: llm.c knows how many parameters, gradients, and optimizer states exist. By multiplying their counts by 4 bytes, it can estimate usage precisely.\nInstrumentation during profiling: you can add checkpoints that print memory usage at different stages of the forward or backward pass.\n\nFor example:\n\nParameters only: ~500 MB.\nParameters + gradients: ~1 GB.\nParameters + gradients + optimizer: ~2 GB.\nAdding activations: 2.5–3 GB, depending on batch size and sequence length.\n\n\n\nWhy memory matters on CPU\nOn a CPU, you don’t just care about “can it fit in RAM?” You also care about cache efficiency. Modern CPUs have multiple levels of cache (L1, L2, L3), which are much faster than main RAM. If activations or weights don’t fit well in cache, performance can suffer even if you technically have enough RAM.\nMemory footprint also limits experiment flexibility. For example, increasing sequence length from 64 to 1024 multiplies activation storage by 16. A run that fits at T=64 may crash or swap at T=1024.\n\n\nExample scenario\nSuppose you run GPT-2 124M with:\n\nBatch size B=4\nSequence length T=64\n\nThis might use ~2.5 GB of memory for training. If you raise T to 512, suddenly activations balloon, and total usage may exceed 10 GB. On a laptop with 8 GB RAM, this simply won’t work.\nBy monitoring memory carefully, you can avoid mysterious crashes and plan runs realistically.\n\n\nTry it yourself\n\nRun training with a small batch (B=2, T=64) and check memory usage with htop.\nIncrease T step by step (128, 256, 512) and record the growth. Watch how activations dominate beyond a certain length.\nCalculate parameter memory manually: num_params × 4 bytes. Compare it to what the OS reports. The difference comes from activations and optimizer states.\nModify the code to print memory allocations explicitly when arrays are created. This gives an internal log of usage at each step.\n\n\n\nThe takeaway\nMemory is the silent partner of FLOPs: you need both to train and run models efficiently. Profiling without tracking memory is incomplete—you might have a model that’s fast but impossible to run on your machine. By capturing and understanding memory usage, you gain the ability to scale responsibly, balance batch size and sequence length, and keep your experiments stable.\n\n\n\n57. Reproducing Known Loss Curves (CPU-only)\nOnce the model is correct and its performance is measured, the next important step is to check whether it learns in the way we expect. In deep learning, we usually monitor this with a loss curve—a graph that shows how the training loss decreases over time as the model sees more data.\nFor GPT-2 and other language models, the standard loss function is cross-entropy, which measures how well the predicted probability distribution over tokens matches the actual next token in the dataset. If the implementation is right, the loss should fall in a predictable way when trained on text like Tiny Shakespeare or Tiny Stories.\n\nWhat a “known” loss curve looks like\nThe community has already run countless GPT-2 experiments in PyTorch, so we know roughly what the curve should look like:\n\nAt the very beginning, the loss is high (around 5–6) because the model is basically guessing.\nAfter a few hundred steps, the loss starts dropping steadily.\nFor Tiny Shakespeare, loss often goes down toward ~2.0 with a small GPT-2 model (124M parameters).\nThe exact numbers can vary, but the shape—a downward trend with small fluctuations—is consistent.\n\nIf the C implementation produces a similar curve, that’s a strong sign the forward pass, backward pass, and optimizer are all working correctly.\n\n\nHow reproduction is tested in practice\n\nTrain with a small dataset The test usually uses Tiny Shakespeare or Tiny Stories since they are small enough to run quickly on CPU.\nLog the loss per step Each training step prints something like:\nstep 0: train loss 4.87\nstep 10: train loss 4.12\nstep 20: train loss 3.75\n...\nPlot the curve Save the loss values and make a simple plot with step on the x-axis and loss on the y-axis.\nCompare against PyTorch If you train the same model in PyTorch with the same hyperparameters, the loss curve should look almost identical. Small differences are normal due to random seeds or floating-point math.\n\n\n\nWhy this is important\nReproducing a known loss curve is more than just a sanity check. It tells you:\n\nThe math is right: gradients, optimizer updates, and scheduler logic are functioning.\nThe data pipeline is correct: tokens are being fed in properly and batches are consistent.\nNothing is silently broken: without this, a bug might go unnoticed until much later in training.\n\nThis is especially important on CPU, because training is slower and you may only run a few hundred steps. If the curve starts to dip in the expected way, you know you’re on the right track.\n\n\nExample scenario\nSuppose you train GPT-2 124M with B=4 and T=64 on Tiny Shakespeare. The loss starts around 4.9, and by step 200 it falls to around 3.2. If PyTorch shows a similar trajectory, then your implementation is validated.\nBut if your loss stays flat, say at 5.0 for hundreds of steps, that’s a red flag. It could mean gradients are not flowing, the optimizer isn’t updating weights, or your data loader is feeding the same batch repeatedly.\n\n\nTry it yourself\n\nTrain for 200 steps on Tiny Shakespeare with the C code and save the printed losses.\nTrain the same setup in PyTorch. Plot both curves together and compare.\nIntentionally break something—for example, comment out the optimizer update step—and observe how the loss no longer decreases.\nExperiment with learning rates. Too high may cause the curve to bounce up and down, while too low will make it drop very slowly.\n\n\n\nThe takeaway\nReproducing known loss curves is the ultimate integration test. It proves that the entire pipeline—data, model, training loop, optimizer—works together in harmony. When your loss curve matches the reference, you can trust that your C implementation of GPT-2 is not only correct in theory but also effective in practice.\n\n\n\n58. Debugging Numerical Stability (NaNs, Infs)\nEven if the model produces correct outputs most of the time, there’s a hidden danger in deep learning: numerical instability. This happens when floating-point numbers inside the computation blow up to infinity (Inf) or collapse into “not a number” (NaN). When this occurs, training usually grinds to a halt—loss becomes undefined, gradients explode, and parameters no longer update meaningfully.\n\nWhy NaNs and Infs happen\nNeural networks involve many multiplications, exponentials, and divisions. On paper, all of these are fine. But computers store numbers with limited precision (32-bit floats in this case). When values get too large or too small, they can no longer be represented correctly.\nCommon sources include:\n\nSoftmax overflow: computing exp(x) on large positive numbers leads to Inf.\nDivision by very small numbers: for example, dividing by sqrt(v_hat) + eps in AdamW can produce instability if eps is too small.\nExploding gradients: during backpropagation, errors compound across many layers, producing extremely large values.\nImproper initialization or learning rates: weights that are too large or step sizes that are too aggressive can push activations outside a stable range.\n\n\n\nHow to detect instability in llm.c\nBecause the code is written in C without automatic checks, NaNs and Infs can spread silently unless you look for them. Some useful strategies include:\n\nAssertions: insert assert(!isnan(value) && !isinf(value)); inside loops to catch bad values immediately.\nDebug prints: log sample values from activations or gradients each step to see if they drift toward extremely large numbers.\nCheck the loss: if the loss suddenly becomes nan or inf, that’s a strong signal something went wrong upstream.\nSmall runs: testing on tiny sequences and batches makes it easier to inspect values directly.\n\n\n\nHow to fix instability\nSeveral practical techniques help keep numbers stable:\n\nAdd an epsilon: in divisions or square roots, add a small constant (like 1e-8) to prevent division by zero.\nRescale before softmax: subtract the maximum value in the vector before computing exponentials. This keeps values in a safe range.\nGradient clipping: cap gradients so they cannot exceed a certain norm. This stops runaway updates.\nAdjust learning rate: if training diverges, lowering the learning rate often restores stability.\nCheck data: corrupted inputs or unexpected tokens can inject extreme values into the model.\n\n\n\nExample scenario\nSuppose you’re training GPT-2 on Tiny Shakespeare. The first few steps look fine:\nstep 0: train loss 4.95\nstep 1: train loss 4.72\nstep 2: train loss nan\nThis sudden jump to nan suggests instability. Checking the gradients reveals extremely large values in the attention weights. The fix might be lowering the learning rate from 1e-4 to 5e-5 or enabling gradient clipping.\n\n\nTry it yourself\n\nTrain with a very high learning rate (1e-2) and watch how quickly NaNs appear.\nAdd a debug check inside gpt2_forward that prints when any activation exceeds 1e6. Run a few steps and observe if values explode.\nModify the softmax code to omit subtracting the max. Compare stability before and after.\nAdd a gradient clipping routine and measure whether it prevents loss from diverging.\n\n\n\nThe takeaway\nNumerical stability is the difference between a model that trains smoothly and one that collapses after a few steps. By anticipating where NaNs and Infs can arise, adding checks, and applying stabilizing tricks, you make llm.c robust. This ensures that experiments are reliable and that debugging focuses on real algorithmic issues rather than avoidable numerical traps.\n\n\n\n59. From Unit Test to Full Training Readiness\nUnit tests are the first line of defense: they check whether small, isolated parts of the code—like embeddings, attention, or softmax—produce the correct outputs. But passing unit tests isn’t the same as being ready for full training. The transition from “this layer works” to “the whole system learns correctly over thousands of steps” involves additional challenges.\n\nThe gap between unit tests and training\n\nUnit tests check correctness: for example, verifying that gpt2_forward produces the same logits as PyTorch on a single batch.\nTraining readiness checks robustness: making sure the model can run repeatedly for thousands of steps without crashing, diverging, or leaking memory.\n\nThink of it like testing a car. Unit tests are like checking the brakes, headlights, and steering individually. Training readiness is taking the car on a 500-mile road trip and making sure nothing overheats, rattles loose, or fails under stress.\n\n\nWhat needs to be validated for training readiness\n\nLoss curve behavior Run the training loop for several hundred steps. The training loss should steadily decrease, matching known curves from PyTorch. If it stagnates or spikes, something is wrong in gradients or the optimizer.\nValidation runs Regularly measure validation loss during training. If it decreases at first and then stabilizes, that shows the model is generalizing. If it decreases too quickly and then shoots up, that suggests overfitting.\nMemory stability Training uses more memory than inference because of gradients and optimizer states. A memory leak—forgetting to free arrays or reallocating without release—will cause the program to crash after many steps.\nOptimizer state updates Check that AdamW accumulates m and v correctly over many iterations. If bias correction is missing, loss curves will diverge from expected baselines.\nReproducibility With the same random seed, two runs should produce nearly identical loss curves. Small differences are normal, but major deviations suggest nondeterministic bugs.\n\n\n\nWhy it matters\nWithout this step, you might believe your implementation is complete after unit tests, only to discover that training silently fails at step 500. Training readiness ensures the system is not only mathematically correct in small pieces but also practically usable for long-running experiments.\nThis is also where confidence in deploying the code comes from. Passing training readiness means others can clone the repository, run the scripts, and expect stable training without mysterious crashes.\n\n\nExample scenario\nYou run test_gpt2.c and all outputs match PyTorch within tolerance—great. Then you launch training for 5,000 steps. After step 600, the loss becomes nan. Investigation reveals that gpt2_update wasn’t applying bias correction properly, so the optimizer went unstable. That’s a bug you’d never catch with a one-batch unit test, but training readiness exposes it.\n\n\nTry it yourself\n\nRun training for 1,000 steps on Tiny Shakespeare and log the loss every 10 steps. Check that it decreases smoothly.\nAdd validation runs every 100 steps. Watch for the classic gap between train loss (lower) and validation loss (slightly higher).\nUse htop or similar tools to monitor memory usage during training. Confirm that it stays steady rather than creeping upward.\nRun the same training twice with the same seed. Compare the two loss curves—are they nearly identical?\n\n\n\nThe takeaway\nUnit tests prove the pieces are correct. Training readiness proves the whole system works under real conditions. Both are necessary. Together, they give you the confidence that llm.c isn’t just a collection of working parts, but a functioning engine capable of training GPT-2 models end to end.\n\n\n\n60. Limitations of CPU Testing\nTesting GPT-2 on CPU is invaluable for verifying correctness, but it comes with clear limits. Understanding these limitations helps you interpret the results properly and know when it’s time to move on to GPU-based experiments.\n\nSpeed constraints\nThe most obvious limitation is speed. CPUs are optimized for general-purpose tasks, not the massive parallelism that neural networks demand. A single forward and backward pass on GPT-2 124M can take seconds or even minutes on CPU, while a GPU might handle it in milliseconds. This makes:\n\nFull-scale training impractical: training GPT-2 124M to convergence could take weeks or months on CPU.\nExperiment cycles slower: testing new optimizations or debugging is slowed because each run takes longer.\n\nFor this reason, CPU testing is best suited for small-scale sanity checks, not full training runs.\n\n\nMemory overhead\nCPU memory is typically more abundant than GPU VRAM, but slower. The bottleneck often isn’t “do we have enough RAM?” but “how quickly can we move data in and out of memory?” As sequence length T grows, the activations balloon, and cache efficiency drops. This makes even medium-sized runs sluggish.\n\n\nLimited realism\nAlthough CPU runs confirm that the math is correct, they don’t always reflect the realities of GPU execution. For example:\n\nCUDA kernels have different numerical characteristics (fused operations, different rounding).\nGPU memory layouts can expose bugs that CPU arrays hide.\nParallel execution may create timing or synchronization issues that never appear on CPU.\n\nSo while CPU parity with PyTorch is necessary, it isn’t sufficient. You must repeat testing once CUDA code is introduced.\n\n\nLoss of scale insights\nA CPU test can prove correctness for a few batches, but it doesn’t tell you how the code scales under heavy load. On GPU, you learn about kernel efficiency, memory throughput, and distributed training. CPU tests simply don’t expose those concerns.\n\n\nWhy it matters\nCPU testing is the foundation: it proves the algorithm is implemented correctly, step by step, without relying on specialized hardware. But if you stop there, you’ll miss the bigger picture of performance and scalability. CPU results should be treated as a green light to proceed, not the final word on readiness.\n\n\nExample scenario\nSuppose you run 500 training steps on Tiny Shakespeare. The loss curve drops exactly as expected—success. But training on CPU is so slow that finishing an epoch takes several hours. This validates correctness, but makes it obvious that GPUs are required for meaningful experiments.\n\n\nTry it yourself\n\nTrain GPT-2 124M on CPU for 100 steps and record the time per step. Extrapolate how long it would take to run 100k steps.\nIncrease sequence length from 64 to 512 and observe how memory access times affect throughput.\nCompare your CPU loss curve with a GPU run from PyTorch. Notice they align in shape but differ dramatically in speed.\nUse profiling tools (perf, valgrind, or gprof) to see which CPU functions dominate runtime.\n\n\n\nThe takeaway\nCPU testing is the safe laboratory where you validate correctness, catch numerical errors, and reproduce known loss curves. But its limitations—slow speed, reduced realism, and lack of scaling insights—mean it’s only a first step. Once CPU testing passes, the journey continues with GPU testing, profiling, and multi-device scaling.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html#chapter-7.-cuda-training-train_gpt2.cu",
    "href": "books/en-US/book.html#chapter-7.-cuda-training-train_gpt2.cu",
    "title": "The Book",
    "section": "Chapter 7. CUDA Training (train_gpt2.cu)",
    "text": "Chapter 7. CUDA Training (train_gpt2.cu)\n\n61. CUDA Architecture Overview (streams, kernels)\nWhen the CPU version of llm.c runs, it executes instructions one after another on your processor cores. This is fine for small models or debugging, but deep learning workloads—especially Transformers like GPT-2—demand an enormous number of floating-point operations. To handle that, llm.c also includes CUDA versions of the training loop that shift computation to NVIDIA GPUs.\nAt a high level, CUDA is NVIDIA’s programming model that lets developers write code to run directly on the GPU. Unlike CPUs, which might have a few cores optimized for general-purpose tasks, GPUs contain thousands of simpler cores designed to process large batches of data in parallel. CUDA provides the tools to organize work so that those cores can stay busy.\n\nKernels: Small Programs That Run on the GPU\nIn CUDA, a kernel is a function that runs on the GPU. When you launch a kernel, you don’t call it once like a normal C function—you launch thousands of copies at the same time. Each copy handles a different piece of the data. For example, if you want to multiply two vectors of a million elements, you can launch a million GPU threads, each multiplying one pair of numbers.\nIn llm.c, kernels are used for operations that can be expressed in terms of lots of small, independent tasks. Examples include:\n\nApplying the GeLU activation function elementwise to a big tensor.\nAdding residual connections across every dimension.\nNormalizing values in LayerNorm.\n\nFor bigger, structured operations like matrix multiplications (GEMMs), the CUDA code often relies on specialized libraries such as cuBLAS or cuBLASLt, which are highly tuned for NVIDIA GPUs.\n\n\nStreams: Overlapping Work\nA GPU has the ability to handle multiple tasks at once. CUDA introduces the idea of streams, which are sequences of operations that run in order relative to each other, but can overlap with operations in other streams. This means:\n\nWhile one kernel is executing, another can start transferring data between CPU and GPU.\nComputation and communication can overlap, reducing idle time.\n\nIn the training loop of llm.c, streams let you schedule batches of work so that data preparation and model computation can proceed side by side. This is crucial for keeping the GPU saturated with useful work instead of waiting on the CPU.\n\n\nThe Memory Hierarchy\nCUDA programming is also shaped by the GPU memory hierarchy:\n\nRegisters: Fastest, private to each thread.\nShared Memory: Small chunks of memory shared among threads in a block; much faster than global memory.\nGlobal Memory: Large, but slower. This is where tensors like weights, activations, and gradients usually live.\nHost Memory (CPU RAM): Separate from GPU memory; transferring between them can be slow and should be minimized.\n\nFor example, in the attention kernel, partial results might be stored in registers or shared memory while processing a block of the sequence, before writing the final result back to global memory.\n\n\nHow This Fits Into llm.c\nIn train_gpt2.cu, most of the heavy lifting is done by calls into cuBLAS/cuBLASLt and cuDNN for matrix multiplications and attention. But understanding the CUDA model—kernels, streams, and memory—helps explain:\n\nWhy we batch operations the way we do.\nWhy minimizing data transfers between CPU and GPU is so important.\nHow GPU kernels map naturally to the kinds of tensor operations GPT-2 requires.\n\n\n\nWhy It Matters\nWithout CUDA, training GPT-2 would be painfully slow, even on a powerful CPU. CUDA gives access to thousands of cores working in parallel, but it also requires careful programming to avoid bottlenecks. Knowing about kernels, streams, and memory hierarchy is the foundation for understanding later sections where we dive into matrix multiplication, attention, and optimization strategies.\n\n\nTry It Yourself\n\nWrite a simple CUDA kernel that adds two arrays elementwise. Compare its performance to a CPU loop.\nModify the kernel to use shared memory and see if it improves performance for larger arrays.\nCreate two CUDA streams: one for computing a kernel, and another for copying data. Measure whether the operations overlap in time.\nUse nvprof or nsys to profile a CUDA program and observe how kernels and memory transfers appear on the timeline.\nThink about how you would split a big matrix multiplication across thousands of threads—each thread computing one row, one column, or one element? What are the tradeoffs?\n\n\n\nThe Takeaway\nCUDA is not just about writing code for GPUs—it’s about rethinking computation as thousands of small tasks that can run side by side. Kernels handle the per-thread work, streams let you schedule and overlap operations, and the memory hierarchy dictates how to organize data for maximum speed. All of these ideas come together in llm.c’s CUDA implementation, making training feasible for models like GPT-2.\n\n\n\n62. Matrix Multiplication via cuBLAS/cuBLASLt\nMatrix multiplication—often called GEMM (General Matrix-Matrix Multiply)—is the beating heart of deep learning. In GPT-2, most of the computation comes from multiplying large matrices: projecting embeddings into query, key, and value vectors, applying attention weights, and processing the MLP feed-forward layers. On the CPU, we saw this done with nested loops and mild optimizations. On the GPU, however, we need far more efficient approaches. That’s where cuBLAS and cuBLASLt come in.\n\nWhy Matrix Multiplication Is So Central\nAlmost every step of a Transformer involves multiplying two big matrices:\n\nEmbedding lookup can be seen as a matrix multiply between one-hot token vectors and the embedding table.\nThe attention mechanism computes dot products between queries and keys, followed by a weighted sum of values.\nThe MLP applies two fully connected layers, each of which is essentially a GEMM.\n\nIf you profile GPT-2, you’ll find that GEMM operations dominate the runtime. That’s why NVIDIA’s libraries devote enormous effort to making these multiplications as fast as possible.\n\n\ncuBLAS: The Classic Workhorse\ncuBLAS is NVIDIA’s GPU-accelerated version of the BLAS (Basic Linear Algebra Subprograms) library. It provides highly optimized implementations of GEMM and related routines. Under the hood, cuBLAS:\n\nSplits large matrices into tiles that fit into the GPU’s shared memory.\nSchedules thousands of threads to compute different tiles in parallel.\nUses fused multiply-add (FMA) instructions for high throughput.\nAdapts to different GPU architectures to exploit Tensor Cores where available.\n\nA typical call looks like this:\ncublasHandle_t handle;\ncublasCreate(&handle);\n\nfloat alpha = 1.0f, beta = 0.0f;\ncublasSgemm(handle,\n    CUBLAS_OP_N, CUBLAS_OP_N,\n    m, n, k,\n    &alpha,\n    A, m,\n    B, k,\n    &beta,\n    C, m);\nHere A and B are the input matrices, and C is the result. The function handles all the low-level scheduling.\n\n\ncuBLASLt: The Flexible Successor\nWhile cuBLAS is powerful, it’s somewhat rigid. cuBLASLt (Lightweight cuBLAS) is a newer API that adds:\n\nBetter support for mixed precision (e.g., FP16 or BF16 inputs with FP32 accumulation).\nMore control over algorithm selection, so developers can tune for performance or memory usage.\nFeatures like epilogues, which let you fuse additional operations (e.g., bias addition, activation functions) directly into the GEMM, reducing memory transfers.\n\nIn practice, cuBLASLt often outperforms cuBLAS because it can exploit Tensor Cores more aggressively and fuse multiple steps into a single kernel call.\n\n\nPrecision and Tensor Cores\nOn modern NVIDIA GPUs (Volta, Turing, Ampere, Hopper), Tensor Cores accelerate matrix multiplications dramatically when using FP16, BF16, or TF32. These special hardware units can perform matrix-multiply-and-accumulate on small blocks of numbers in a single instruction.\nFor example:\n\nOn CPUs, multiplying two 16×16 matrices is done with many scalar multiplications.\nOn GPUs with Tensor Cores, the entire block can be computed in one fused operation.\n\nIn GPT-2 training, using FP16 with cuBLASLt enables much higher throughput, while keeping master weights in FP32 to preserve numerical stability.\n\n\nPractical Example in llm.c\nIn train_gpt2.cu, most of the calls to perform linear layers—such as projecting input activations into query, key, and value matrices—are implemented with cuBLAS/cuBLASLt. For instance:\n\nInputs (B, T, C) are multiplied by a weight matrix (C, 3C) to produce (B, T, 3C).\nLater, the output of attention (B, T, C) is multiplied by another projection matrix (C, C).\n\nInstead of writing custom kernels for each case, the code defers to cuBLAS/cuBLASLt, ensuring maximum performance across GPU architectures.\n\n\nWhy It Matters\nMatrix multiplications are so frequent and heavy that their performance directly determines how fast you can train GPT-2. By leaning on cuBLAS/cuBLASLt, llm.c avoids reinventing the wheel and gets near-peak GPU efficiency. This makes the code clean, maintainable, and scalable to larger models.\n\n\nTry It Yourself\n\nWrite a small CUDA program that multiplies two matrices using naive kernels, and compare its performance to cuBLAS.\nExperiment with FP32 versus FP16 inputs and observe the speedup when Tensor Cores are enabled.\nEnable cuBLASLt’s epilogues to fuse bias addition into GEMM, and measure memory savings.\nProfile GPT-2 training with nvprof or nsys to see how much time is spent in GEMM calls.\nTry scaling up matrix sizes to simulate bigger models and note how performance grows relative to CPU implementations.\n\n\n\nThe Takeaway\nMatrix multiplication is the computational engine of GPT-2, and on GPUs it’s powered by cuBLAS and cuBLASLt. These libraries harness the GPU’s architecture—tiling, Tensor Cores, mixed precision—to squeeze out maximum efficiency. Understanding how they work gives insight into why the GPU version of llm.c runs so much faster than the CPU version, and sets the stage for attention kernels and other CUDA-accelerated components.\n\n\n\n63. Attention Kernels: cuDNN FlashAttention\nThe attention mechanism is at the core of every Transformer. It allows the model to weigh different parts of the input sequence when producing an output. For GPT-2, this means that when generating the next token, the model doesn’t just look at the last word—it considers the entire sequence of words before it, adjusting how much each past token contributes. But attention is expensive. Naively, it scales quadratically with sequence length: for a sequence of 1024 tokens, you need to compute a 1024×1024 attention matrix. That’s more than a million entries, and each must be computed, normalized, and multiplied back into the value vectors.\nOn CPUs, we saw how this is implemented step by step: queries, keys, and values are projected with matrix multiplications, dot products between queries and keys are computed, softmax is applied, and the result is multiplied by values. On GPUs, we want to do the same thing, but much faster. That’s where cuDNN’s FlashAttention comes into play.\n\nWhat Is FlashAttention?\nFlashAttention is an algorithm that rethinks how attention is computed. Instead of materializing the full attention matrix in memory, it computes softmax and the weighted sum in a streaming fashion. This reduces memory usage and improves cache efficiency.\nNormally, attention involves these steps:\n\nCompute scores = Q × Kᵀ (queries times keys transpose).\nApply softmax over scores to get attention weights.\nMultiply weights × V (values) to get the output.\n\nThe problem: step 1 produces a huge score matrix of size (sequence_length × sequence_length). Storing and processing this full matrix becomes the bottleneck.\nFlashAttention avoids storing the full matrix by computing attention block by block. It processes tiles of queries and keys, applies the softmax incrementally, and accumulates results directly into the output. This drastically cuts memory bandwidth requirements, which is critical for GPUs.\n\n\ncuDNN FlashAttention in Practice\nIn llm.c’s CUDA training code, when USE_CUDNN is enabled, the code can take advantage of cuDNN’s implementation of FlashAttention. This means:\n\nThe library handles the tiling and streaming automatically.\nIt can leverage Tensor Cores for mixed-precision computation (FP16/BF16 inputs with FP32 accumulation).\nIt reduces memory use, which allows training longer sequences without running out of GPU memory.\n\nFrom a developer’s point of view, enabling cuDNN FlashAttention usually involves passing specific descriptors and flags to cuDNN routines rather than writing custom kernels. Instead of manually managing loops and softmax stability tricks, you hand over the responsibility to cuDNN, which has a heavily optimized kernel.\n\n\nWhy This Is a Game-Changer\nThe quadratic cost of attention has long been a bottleneck in scaling Transformers. With FlashAttention, the bottleneck shifts. The computation is still O(N²), but because memory is handled so much more efficiently, the GPU spends less time waiting on memory loads and more time doing actual math. This means:\n\nTraining can be faster even at the same sequence length.\nYou can push to larger sequence lengths (e.g., 2K or 4K tokens) without running out of GPU memory.\nEnergy efficiency improves because you avoid redundant reads/writes to global memory.\n\n\n\nExample: Why Memory Access Matters\nLet’s imagine a toy example with 4 tokens. A naive implementation might build a 4×4 attention matrix, compute softmax, and multiply by values. That’s fine for 4 tokens, but with 1024 tokens, you’d be juggling matrices of a million entries. Even if each entry is just 2 bytes (FP16), that’s megabytes of temporary storage per step. On a real GPU, constantly moving that in and out of global memory slows everything down.\nFlashAttention says: instead of storing the whole million entries, compute them in chunks, normalize them on the fly, and immediately use them to update the output. This way, only small temporary blocks live in memory, and global memory pressure drops dramatically.\n\n\nHow It Shows Up in GPT-2 Training\nWhen GPT-2 processes a batch of sequences, each block of the model applies attention. In the CUDA version of llm.c, these attention calls can be routed through cuDNN FlashAttention. Practically, this means that the inner loop of training—the part that would otherwise grind on those giant attention matrices—becomes leaner and faster.\nThis matters even more as models grow. For GPT-2 124M (12 layers, 12 heads, 1024 sequence length), attention is already expensive. For GPT-2 1.5B or LLaMA-style models with longer contexts, FlashAttention can be the difference between feasible training and “out of memory” errors.\n\n\nWhy It Matters\nAttention is the defining operation of Transformers, but it’s also their Achilles heel. FlashAttention addresses the biggest inefficiency—memory bandwidth—without changing the model’s outputs. By using cuDNN’s optimized kernels, llm.c ensures it runs close to hardware peak performance while still producing correct results. For anyone learning about deep learning systems, this is a perfect example of how algorithmic innovations (streaming softmax) and hardware-level optimizations (Tensor Cores, tiling) combine to make state-of-the-art training practical.\n\n\nTry It Yourself\n\nRun GPT-2 training in llm.c with USE_CUDNN=0 and then with USE_CUDNN=1. Compare training speed and GPU memory usage.\nWrite a naive CUDA kernel that builds the full attention matrix, then benchmark it against cuDNN FlashAttention.\nVary sequence lengths (128, 512, 1024, 2048) and see how performance diverges between naive and FlashAttention implementations.\nExamine how mixed precision interacts with FlashAttention—try FP16 versus BF16.\nExplore the FlashAttention paper and compare its algorithmic explanation with what you see in practice using llm.c.\n\n\n\nThe Takeaway\nAttention is expensive, but it doesn’t have to be crippling. FlashAttention shows that clever algorithm design plus hardware-aware implementation can shrink the memory bottleneck dramatically. By leaning on cuDNN’s implementation, llm.c can train GPT-2 models more efficiently, and learners get a real-world view of how deep learning libraries squeeze performance out of GPUs.\n\n\n\n64. Mixed Precision: FP16/BF16 with Master FP32 Weights\nTraining large models like GPT-2 involves multiplying and adding enormous amounts of numbers—billions of operations in every training step. GPUs can do this very quickly, but the type of numbers you use matters a lot. Traditionally, training is done in 32-bit floating point (FP32), which gives good precision but is heavy on memory and compute. Modern GPUs offer special hardware—Tensor Cores—that run much faster when using reduced precision, such as FP16 (half-precision floating point) or BF16 (bfloat16). This technique is called mixed precision training.\n\nWhy Mixed Precision Helps\nUsing FP16 or BF16 has two main benefits:\n\nSpeed: GPUs can perform more FP16/BF16 operations per clock cycle than FP32. For example, NVIDIA Tensor Cores are specifically designed to accelerate half-precision math, often delivering 2× or more throughput.\nMemory: FP16/BF16 values take half the storage of FP32. That means you can fit larger batches or longer sequences into the same GPU memory, which is critical for scaling models.\n\nBut reduced precision comes with a tradeoff: it’s easier for numbers to underflow (become zero) or overflow (become infinity), which can destabilize training.\n\n\nMaster Weights in FP32\nThe trick used in llm.c (and also in PyTorch and TensorFlow) is to keep a master copy of weights in FP32. Here’s the process:\n\nDuring the forward pass, weights are cast to FP16/BF16 so the GPU can run the math on Tensor Cores.\nThe gradients are computed in reduced precision as well.\nWhen it’s time to update parameters, the optimizer applies updates to the FP32 master copy.\nThe updated master weights are cast back to FP16/BF16 for the next forward pass.\n\nThis way, you get the speed and memory savings of mixed precision without fully losing the stability of FP32.\n\n\nFP16 vs. BF16\nBoth FP16 and BF16 use 16 bits, but they split the bits differently:\n\n\n\n\n\n\n\n\n\n\nFormat\nExponent Bits\nMantissa Bits\nRange\nPrecision\n\n\n\n\nFP16\n5\n10\nSmaller\nHigher precision for small numbers\n\n\nBF16\n8\n7\nWider\nRougher precision, better range\n\n\n\n\nFP16 has better precision near zero but a narrower range, so it’s more prone to overflow.\nBF16 has the same exponent size as FP32, giving it a much wider range but less precision.\n\nModern NVIDIA GPUs (Ampere, Hopper) support both, but BF16 is often preferred for stability in very large models.\n\n\nExample in Practice\nImagine training GPT-2 with a sequence length of 1024 and batch size of 32. With FP32, the activations might take ~12 GB of GPU memory. Switching to FP16 halves that to ~6 GB, leaving room for larger models or more sequences.\nIn llm.c, enabling mixed precision means the forward pass can look something like this:\n\nCast embeddings, weights, and activations to FP16/BF16.\nRun matrix multiplications on Tensor Cores (very fast).\nCompute gradients in reduced precision.\nConvert gradients back to FP32 for stable updates.\n\nThis flow is invisible to the high-level code but handled internally in CUDA/cuBLAS/cuDNN calls.\n\n\nCommon Challenges\nMixed precision introduces new wrinkles:\n\nLoss scaling: small gradients may underflow to zero in FP16. The solution is to multiply the loss by a large factor during backpropagation, then divide gradients back later. This preserves information.\nDebugging: NaNs and Infs become more common when switching to FP16. Careful monitoring is required to catch these early.\nPerformance tuning: Not all operations benefit equally from FP16. For example, reductions (like summing a large array) may lose too much precision unless done in FP32.\n\n\n\nWhy It Matters\nMixed precision is one of the key reasons modern Transformers can be trained efficiently on today’s hardware. Without it, many models would require double the GPU memory and much more time to train. By combining FP16/BF16 for speed and memory efficiency with FP32 master weights for stability, llm.c mirrors the strategy used in production frameworks. This shows how even a minimalist codebase can teach the cutting-edge tricks that power real-world large-scale training.\n\n\nTry It Yourself\n\nTrain GPT-2 in llm.c with FP32 only, then repeat with FP16. Compare memory usage (nvidia-smi) and runtime per step.\nExperiment with FP16 vs. BF16 if your GPU supports both. Observe whether one is more stable.\nIntentionally remove the FP32 master weights (update parameters in FP16 only) and see how quickly training diverges.\nPlot validation loss curves with FP32, FP16, and BF16 runs to see if the model quality differs.\nTry scaling the batch size up with FP16 and note how much bigger a model you can fit into the same GPU.\n\n\n\nThe Takeaway\nMixed precision combines the best of both worlds: the speed and memory efficiency of FP16/BF16 with the stability of FP32. This technique has become a standard in deep learning, and llm.c demonstrates it in a clear, accessible way. It’s not just a neat optimization—it’s what makes training large language models on modern GPUs feasible at all.\n\n\n\n65. Loss Scaling in Mixed Precision Training\nWhen training in mixed precision (FP16 or BF16), one of the biggest challenges is numerical underflow. Gradients can become so small that they round down to zero when represented in 16-bit format. If that happens too often, the optimizer stops receiving meaningful updates, and training can stagnate or collapse. To address this, frameworks introduce a technique called loss scaling.\n\nThe Core Idea\nLoss scaling works by multiplying the loss value by a constant factor (called the scale factor) before starting backpropagation. Since gradients are proportional to the loss, this also multiplies all gradients by the same factor, making them larger and less likely to underflow when stored in FP16.\nAt the end of backpropagation, the gradients are divided by the same scale factor, restoring their correct values before the optimizer step.\nMathematically:\n\nscaled_loss = loss × scale\nCompute gradients of scaled_loss → produces scaled_gradients\ntrue_gradients = scaled_gradients ÷ scale\n\nThe optimizer then uses true_gradients to update the weights.\n\n\nStatic vs. Dynamic Scaling\nThere are two common approaches:\n\nStatic scaling: Use a fixed scale factor throughout training. For example, always multiply the loss by 1024. This is simple but risky; if the scale is too high, gradients may overflow to infinity. If it’s too low, underflow still happens.\nDynamic scaling: Adjust the scale factor on the fly. If overflows (NaNs or Infs) are detected, the scale factor is reduced. If training proceeds smoothly, the scale factor is gradually increased. This balances stability and efficiency.\n\nIn practice, dynamic scaling is the standard. Libraries like PyTorch’s GradScaler automatically handle this logic, so users don’t have to tweak values manually.\n\n\nHow It Appears in llm.c\nThe minimal design of llm.c doesn’t yet include automatic loss scaling, but the idea fits neatly into its training loop. Before calling gpt2_backward, you would scale the loss. After gradients are computed, you would unscale them before gpt2_update. Conceptually:\nfloat scale = 1024.0f; // example scale factor\nfloat scaled_loss = model.mean_loss * scale;\ngpt2_backward_scaled(&model, scaled_loss); // backprop with scaled loss\nunscale_gradients(&model, scale); // divide gradients by scale\ngpt2_update(&model, lr, beta1, beta2, eps, weight_decay, step);\nThis is not yet in the repository, but it’s how one could extend llm.c to support stable FP16 training.\n\n\nWhy It Matters\nWithout loss scaling, mixed precision can fail silently. Training might appear to run, but the gradients may be effectively zero for many parameters. This wastes GPU time and produces poor results. With loss scaling, FP16/BF16 training becomes both fast and reliable, combining the hardware speedups with numerical stability.\n\n\nExample Scenario\nSuppose you are training GPT-2 with FP16 and notice that the validation loss barely decreases after several hundred steps. One possible reason is gradient underflow. By enabling loss scaling with a scale factor of 512 or 1024, you might suddenly see the loss curve behave normally again, matching the FP32 baseline.\n\n\nTry It Yourself\n\nTrain with FP16 but without loss scaling. Monitor whether the loss decreases meaningfully.\nAdd a static scale factor (like 512) and rerun. Observe improvements in stability.\nImplement a simple dynamic scaler: start with 128, double it if no NaNs appear for 100 steps, halve it if NaNs are detected.\nCompare training curves (FP32 vs. FP16 with and without scaling) to see the effect.\nExperiment with very large scale factors to trigger overflow intentionally, then watch how dynamic scaling recovers.\n\n\n\nThe Takeaway\nLoss scaling is the hidden ingredient that makes mixed precision training practical. By rescaling the loss and gradients, we protect tiny numbers from disappearing in FP16 while still enjoying the massive performance and memory benefits. Even in a minimal codebase like llm.c, understanding loss scaling bridges the gap between a model that trains poorly and one that matches FP32 performance at half the cost.\n\n\n\n66. Activation Checkpointing and Memory Tradeoffs\nTraining deep networks like GPT-2 involves storing a large number of activations—the intermediate outputs produced at every layer during the forward pass. These activations are needed later in the backward pass to compute gradients. The problem is that they take up a huge amount of GPU memory. For a 12-layer GPT-2 with long sequences and large batch sizes, activations can consume more memory than the model weights themselves.\n\nWhy Activations Matter\nLet’s say you have a batch size of 8, sequence length of 1024, hidden size of 768, and 12 layers. Each layer produces an activation tensor of shape (batch_size, sequence_length, hidden_size), or 8 × 1024 × 768. That’s about 6.3 million numbers per layer. Multiply by 12 layers, and you have ~75 million numbers. At FP16, that’s around 150 MB per forward pass just for storing activations, and this grows with larger models.\nIf you scale up to GPT-2 Medium or GPT-2 XL, this number balloons quickly into gigabytes, which may not fit in GPU memory.\n\n\nThe Idea of Checkpointing\nActivation checkpointing offers a tradeoff: instead of storing all activations, you only keep a small subset (the checkpoints) and recompute the rest during the backward pass.\n\nDuring forward pass: save only checkpoints (for example, the activations at the end of each transformer block).\nDuring backward pass: when gradients for a layer are needed, recompute the missing activations by running part of the forward pass again.\n\nThis saves memory at the cost of extra computation.\n\n\nHow It Works in GPT-2\nA GPT-2 block has multiple steps: embedding lookup, attention, MLP, layer norm, residual connections. Normally, you’d store every output tensor. With checkpointing, you might only store the input to each block and discard the intermediate results. When backpropagation reaches that block, you rerun the forward pass locally to regenerate those results, then compute gradients.\nThis reduces memory usage almost linearly with the number of discarded activations, at the cost of roughly 30–40% more compute.\n\n\nIn llm.c Context\nllm.c doesn’t yet include activation checkpointing in its minimal implementation, but it’s a natural extension. In CUDA, this might be implemented by wrapping blocks of code with a “checkpoint” function that decides whether to save or discard activations. In PyTorch, the equivalent is torch.utils.checkpoint.\nIf you train with longer sequences (e.g., 2048 tokens instead of 1024), checkpointing could mean the difference between fitting in memory or running into out-of-memory (OOM) errors.\n\n\nWhy It Matters\nModern GPUs have enormous compute capacity, but memory remains the bottleneck. Checkpointing shifts the tradeoff: you spend a bit more compute (re-running some forward passes) in exchange for freeing up gigabytes of memory. This lets you:\n\nTrain larger models on the same hardware.\nUse longer sequence lengths for better context handling.\nIncrease batch size for more stable gradients.\n\nIn practice, this technique is used in nearly every large-scale Transformer training run today.\n\n\nExample Analogy\nThink of it like studying for an exam. You could take detailed notes on every page of the textbook (storing all activations), but your notebook would get huge. Alternatively, you could only mark the chapter headings (checkpoints) and re-read sections when you need them during review (recomputation). It takes more time, but saves notebook space.\n\n\nTry It Yourself\n\nRun training on a GPU with long sequences until you hit an out-of-memory error.\nImplement a simple checkpointing scheme where you only store activations every other layer.\nMeasure how much memory usage decreases (using nvidia-smi) and how much runtime increases per step.\nExperiment with different checkpointing frequencies—every layer, every 2 layers, every 4 layers—and find the balance between memory savings and compute overhead.\nCompare validation loss curves to confirm that checkpointing does not affect training quality (only runtime).\n\n\n\nThe Takeaway\nActivation checkpointing is a clever strategy to bend the memory limits of GPUs. By discarding and recomputing activations on demand, you can fit models or sequence lengths that would otherwise be impossible. The tradeoff is extra computation, but with today’s hardware, compute is usually cheaper than memory. This technique is one of the quiet enablers behind scaling Transformers to billions of parameters.\n\n\n\n67. GPU Memory Planning: Parameters, Gradients, States\nWhen training a GPT-2 model on GPU, one of the most important practical challenges is managing memory. Every tensor—the model’s parameters, gradients, optimizer state, and activations—must fit into limited GPU RAM. Unlike CPUs, where you can often rely on swap space or large RAM pools, GPU memory is tight and unforgiving. If you exceed the limit, the program crashes with an out-of-memory error.\n\nBreaking Down What Takes Memory\n\nParameters (Weights) These are the trainable values of the model: embeddings, attention projections, MLP weights, and so on. For GPT-2 124M, there are about 124 million parameters.\n\nIn FP32, that’s roughly 500 MB.\nIn FP16/BF16, it’s about 250 MB. Larger GPT-2 models (355M, 774M, 1.5B) scale this up proportionally.\n\nGradients For each parameter, the backward pass produces a gradient tensor of the same size. If parameters take 500 MB, gradients also take ~500 MB in FP32. Mixed precision can halve this.\nOptimizer States Optimizers like AdamW don’t just store gradients—they also track moving averages (m and v). Each adds another full-sized tensor. With AdamW, you often end up with 3× the parameter size: weights + m + v.\nActivations During the forward pass, every layer’s intermediate outputs must be stored for the backward pass. This is often the largest single consumer of memory. For a 12-layer GPT-2 with sequence length 1024 and batch size 8, activations can easily exceed several GB. Checkpointing (as discussed earlier) helps reduce this.\n\n\n\nA Simple Calculation Example\nFor GPT-2 124M in FP32:\n\nParameters: ~500 MB\nGradients: ~500 MB\nAdamW states: ~1 GB (two copies)\nActivations: 2–4 GB (depends on batch size and sequence length)\n\nTotal: ~3–6 GB, which fits on most modern GPUs.\nFor GPT-2 774M in FP32:\n\nParameters: ~3 GB\nGradients: ~3 GB\nAdamW states: ~6 GB\nActivations: 8–12 GB\n\nTotal: ~20+ GB—too large for many GPUs unless you use tricks like FP16 and checkpointing.\n\n\nStrategies for Memory Efficiency\n\nMixed Precision Using FP16/BF16 cuts parameter, gradient, and optimizer sizes in half. Instead of 20 GB, you may get by with ~10 GB.\nActivation Checkpointing Store fewer activations and recompute them during backpropagation. This often saves multiple GB.\nGradient Accumulation Instead of training with a huge batch at once, split it into smaller micro-batches and accumulate gradients across them. This reduces activation memory requirements.\nParameter Sharding (Advanced) In multi-GPU setups, parameters and optimizer states can be split across devices (e.g., ZeRO optimizer in DeepSpeed). While not in llm.c, it’s a common technique at scale.\n\n\n\nIn llm.c\nThe GPT2 struct organizes memory into fields like params_memory, grads_memory, m_memory, and v_memory. These are allocated as flat arrays, making it easy to calculate their size. This minimal design highlights the reality: for every parameter, there’s at least one matching gradient and potentially two optimizer state values.\nThis structure mirrors how full frameworks like PyTorch allocate memory, but llm.c exposes it transparently so you can see exactly what’s taking up space.\n\n\nWhy It Matters\nWhen training models, memory is usually the first limit you hit, not compute. Even if your GPU is powerful enough to handle the math, if you run out of memory, you can’t proceed. Understanding how parameters, gradients, optimizer states, and activations interact helps you design training runs that actually fit.\n\n\nTry It Yourself\n\nCalculate memory usage for GPT-2 124M, 355M, and 774M using FP32 vs FP16. Compare your numbers to your GPU’s memory size.\nRun llm.c with increasing batch sizes until you hit an out-of-memory error. Record the exact point where it breaks.\nEnable mixed precision and checkpointing to see how much further you can push sequence length or batch size.\nWrite a script to print the sizes of params_memory, grads_memory, and optimizer states in llm.c. Compare this to nvidia-smi output during training.\nExperiment with reducing optimizer states (e.g., try SGD instead of AdamW) to see the memory difference.\n\n\n\nThe Takeaway\nTraining Transformers is not just about writing the forward and backward passes—it’s about planning memory carefully. Parameters, gradients, optimizer states, and activations all compete for limited GPU RAM. By understanding these categories and using techniques like mixed precision and checkpointing, you can fit bigger models or longer contexts on the same hardware. This balance between memory and compute is at the heart of scaling modern deep learning.\n\n\n\n68. Kernel Launch Configurations and Occupancy\nWhen writing CUDA code for training models like GPT-2, one of the most important yet subtle factors in performance is how kernels are launched. A kernel is just a function that runs on the GPU, but the way you configure it—the number of threads, blocks, and how work is divided—can make the difference between a GPU that runs at 10% efficiency and one that’s near peak utilization.\n\nThreads, Blocks, and Grids\nCUDA organizes computation hierarchically:\n\nThread: the smallest unit of execution. Each thread runs the kernel code once.\nBlock: a collection of threads that share fast, on-chip memory (called shared memory).\nGrid: a collection of blocks. Together, the grid represents the entire kernel launch.\n\nWhen you launch a kernel, you decide:\nmy_kernel&lt;&lt;&lt;num_blocks, threads_per_block&gt;&gt;&gt;(...);\nFor example:\nint threads = 256;\nint blocks = (N + threads - 1) / threads;\nmy_kernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(...);\nHere, N might be the total number of elements to process. The division ensures that all elements get covered.\n\n\nWhat Is Occupancy?\nOccupancy refers to how many threads are active on a GPU relative to the maximum possible. Higher occupancy usually means the GPU is better utilized, but it’s not the only factor—memory access patterns and instruction throughput also matter.\nEach GPU has a fixed number of Streaming Multiprocessors (SMs), and each SM can support only a certain number of threads and blocks at once. If your kernel launch doesn’t provide enough threads, the GPU will be underutilized. If you launch too many, they may compete for shared memory and registers, leading to inefficiency.\n\n\nExample: A Simple Vector Add\nSuppose you want to add two arrays of size N = 1e6.\n\nIf you use threads_per_block = 32, you’ll have many tiny blocks. This wastes parallelism because modern GPUs are designed to run hundreds of threads per SM.\nIf you use threads_per_block = 1024, you may hit the hardware limit but run very large blocks that restrict scheduling flexibility.\nA good balance might be 256 or 512 threads per block, which lets the GPU overlap computation and memory access effectively.\n\n\n\nIn Transformer Training\nFor GPT-2 in llm.c, most heavy lifting is done by:\n\nMatrix multiplications (handled by cuBLAS/cuBLASLt). These libraries pick kernel launch parameters automatically.\nAttention and normalization kernels (custom or cuDNN). When written by hand, launch configuration becomes crucial.\n\nFor example, in a softmax kernel over a sequence of 1024 tokens:\n\nYou might launch one block per sequence row, with 1024 threads per block (one per token).\nAlternatively, you might launch multiple blocks per row, each handling a tile of tokens, if shared memory limits require it.\n\nChoosing wisely can double or triple performance.\n\n\nBalancing Factors\nWhen configuring kernels, you balance:\n\nOccupancy: Are enough threads active to use the GPU fully?\nMemory Coalescing: Do threads access memory in aligned, sequential chunks (which is fast) or scattered patterns (which is slow)?\nShared Memory and Registers: Each block has limited resources. If your kernel uses too much shared memory, fewer blocks can fit per SM, reducing occupancy.\nArithmetic Intensity: If the kernel does a lot of math per memory load, occupancy matters less; if it’s memory-bound, occupancy matters more.\n\n\n\nWhy It Matters\nIn GPU training, kernel launch decisions directly control how efficiently hardware is used. Two kernels implementing the same math can differ by 5–10× in runtime purely because of launch configuration. cuBLAS and cuDNN automate much of this for matrix-heavy ops, but understanding it is crucial when writing custom kernels.\n\n\nTry It Yourself\n\nWrite a simple CUDA kernel for vector addition with different threads_per_block values (32, 128, 256, 512, 1024). Measure runtime and see which is fastest.\nUse nvprof or nsys to inspect occupancy of kernels during GPT-2 training. Note which kernels run at &lt;50% occupancy.\nExperiment with a softmax kernel: launch one block per row vs. multiple blocks per row. Compare performance and memory use.\nExplore how shared memory allocation per block affects occupancy by artificially increasing shared memory usage.\nCompare cuBLAS GEMM (matrix multiply) performance to a naive CUDA implementation and observe how kernel configuration explains the speed difference.\n\n\n\nThe Takeaway\nKernel launch configuration is the hidden lever of GPU performance. By adjusting how threads and blocks are assigned, you control how much of the GPU is kept busy, how well memory bandwidth is used, and how smoothly computations flow. For models like GPT-2, libraries handle most kernels, but knowing what’s happening under the hood is key to writing or debugging efficient CUDA code.\n\n\n\n69. CUDA Error Handling and Debugging\nWhen writing or running CUDA code, one of the most frustrating parts is that errors often don’t show up immediately. Unlike CPU code, where an invalid pointer or division by zero can crash right away, CUDA launches kernels asynchronously. This means the host code (running on the CPU) queues up GPU work and moves on, while the GPU processes it in the background. If something goes wrong inside the kernel, the error might not be visible until later—sometimes only after you try to synchronize.\n\nCommon Error Sources\n\nOut-of-bounds memory access A kernel thread tries to read or write past the end of an array. This can silently produce incorrect results or crash the program.\nInvalid memory alignment Some CUDA operations require pointers to be aligned. Misaligned access can degrade performance or trigger errors.\nIllegal instruction or unsupported hardware feature Using Tensor Cores on an older GPU, or using instructions not supported by your GPU’s compute capability, can fail.\nOut of memory (OOM) Allocating more GPU memory than available causes runtime errors. Unlike CPU memory, GPUs cannot “swap” to disk.\nRace conditions Threads within a block or across blocks accessing the same memory location without synchronization can corrupt results.\n\n\n\nHow CUDA Reports Errors\nEvery CUDA runtime API call returns an error code. For example:\ncudaError_t err = cudaMalloc(&ptr, size);\nif (err != cudaSuccess) {\n    printf(\"Error: %s\\n\", cudaGetErrorString(err));\n}\nSimilarly, after launching a kernel, you should check:\nmy_kernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(...);\ncudaError_t err = cudaGetLastError();\nif (err != cudaSuccess) {\n    printf(\"Kernel launch failed: %s\\n\", cudaGetErrorString(err));\n}\ncudaDeviceSynchronize(); // forces the GPU to finish and report errors\nThis pattern ensures that if something fails, you see it quickly instead of later.\n\n\nDebugging Tools\n\ncuda-gdb A GPU-aware debugger. Lets you step through CUDA kernels much like gdb on CPU code.\ncuda-memcheck Detects out-of-bounds accesses, race conditions, and misaligned memory operations. Essential when kernels produce “mysterious” wrong outputs.\nNsight Systems / Nsight Compute Profiling tools that show kernel timelines, occupancy, memory throughput, and errors.\nSanity checks in code Often, simply inserting assertions (assert(i &lt; N)) or zero-initializing memory can catch problems earlier.\n\n\n\nDebugging in llm.c\nIn llm.c, most of the CUDA-heavy lifting is handled by cuBLAS and cuDNN. But when experimenting with custom kernels (e.g., softmax, masking, or layernorm), debugging becomes crucial. A small indexing mistake could make training diverge or crash with nan losses. By adding cudaGetLastError() checks after every kernel launch, you can catch issues right where they happen.\n\n\nExample: A Softmax Bug\nImagine a kernel computing softmax across 1024 tokens per row. If one thread index accidentally runs past 1024, it may read garbage memory. Without error checking, you might just see “loss is NaN” 100 steps later. With cuda-memcheck, you’d immediately see:\nInvalid global read of size 4\n    at softmax.cu:42\n    by thread (1025,0,0) in block (1,0,0)\nNow you know exactly where to fix the bug.\n\n\nWhy It Matters\nTraining large models is expensive. A single bug in a CUDA kernel can waste hours of GPU time, produce invalid gradients, or silently corrupt weights. Robust error handling and debugging practices save not only frustration but also significant cost.\n\n\nTry It Yourself\n\nWrite a CUDA kernel with an intentional bug (e.g., forget to check array bounds). Run it with and without cuda-memcheck to see the difference.\nAdd cudaGetLastError() after every kernel in a simple project and watch how it pinpoints issues earlier.\nExperiment with Nsight Systems: run GPT-2 training and inspect kernel launches, checking for errors or unexpected stalls.\nTrain with bad initialization (e.g., NaNs in inputs) and see how error checking reports failures.\nIntroduce a race condition by having two threads update the same memory without __syncthreads(). Debug using cuda-memcheck.\n\n\n\nThe Takeaway\nCUDA’s asynchronous nature makes error handling less straightforward than CPU programming. But with the right tools—error codes, synchronization, cuda-memcheck, and debuggers—you can systematically catch and fix problems. In llm.c, this discipline ensures that CUDA kernels not only run fast but also run correctly, which is just as important when training large-scale models.\n\n\n\n70. dev/cuda/: From Simple Kernels to High Performance\nInside the llm.c repository, there is a folder called dev/cuda/. At first glance it may look like a side experiment, but it’s actually one of the most instructive parts of the project. The main training files (train_gpt2.cu, train_gpt2_fp32.cu) rely heavily on cuBLAS and cuDNN—optimized libraries that already deliver near-peak performance. But if you want to understand how CUDA really works under the hood, you have to look at how kernels are written from scratch, and that’s exactly what this folder shows.\n\nWhy This Folder Exists\nThe goal of dev/cuda/ is not to replace cuBLAS or cuDNN. Instead, it acts as a sandbox for:\n\nBuilding intuition about how GPU kernels are structured.\nExperimenting with small-scale implementations of operations like vector addition, matrix multiplication, or normalization.\nComparing naive CUDA implementations to highly optimized library calls.\nTeaching developers how memory layout, thread synchronization, and shared memory affect performance.\n\nIt’s a bridge: start simple with “hello world” style kernels, then step closer to the performance tricks used by NVIDIA’s professional libraries.\n\n\nA Journey from Naive to Optimized\n\nSimple Elementwise Kernels The first step is usually a kernel where each thread processes one element. For example, adding two vectors C[i] = A[i] + B[i]. This teaches indexing, memory coalescing, and the idea of grids and blocks.\nReduction Kernels Next, you move to slightly harder tasks like summing an array. Now you need thread cooperation and synchronization (__syncthreads()), plus shared memory usage.\nMatrix Multiplication (GEMM) A naive kernel might have each thread compute one output element by looping over the input dimension. It works, but is slow because it reloads data from global memory repeatedly. The optimized version uses tiling: load a tile of the matrix into shared memory, let threads reuse it many times, and then move to the next tile. This can speed up performance by 10× or more.\nAdvanced Optimizations Later examples may add warp-level primitives, vectorized loads, and Tensor Core usage. These bring performance closer to cuBLAS.\n\n\n\nEducational Value\nSeeing these steps side by side makes the performance story very tangible:\n\nA naive GEMM kernel might achieve 1% of cuBLAS speed.\nA tiled shared-memory GEMM can jump to 30–40%.\nWith careful warp scheduling, it can reach 60–70%.\ncuBLAS goes further with hand-tuned assembly and Tensor Cores, pushing 90–95%.\n\nThis teaches that optimization is not magic—it’s a sequence of logical improvements, each shaving off inefficiencies.\n\n\nWhy It Matters for GPT-2 Training\nEven if you never plan to reimplement matrix multiplication yourself, understanding what happens in dev/cuda/ helps explain why the main training loop in train_gpt2.cu is so fast. You see why cuBLAS/cuDNN kernels are black boxes of efficiency: because writing your own at that level is extremely hard.\nBut this also means you’re better prepared to write custom kernels when you need them. For example, maybe you want to test a new activation function or a different attention mechanism. By borrowing patterns from the experimental kernels, you can build your own, test them, and compare to baselines.\n\n\nExample: Vector Add Kernel\nHere’s a simple kernel you might find in this folder:\n__global__ void vector_add(const float* A, const float* B, float* C, int N) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i &lt; N) {\n        C[i] = A[i] + B[i];\n    }\n}\nIt’s trivial compared to GPT-2’s attention, but this is where everyone starts. From here you scale up to 2D indexing for matrices, then to tiled shared-memory patterns.\n\n\nTry It Yourself\n\nRun a kernel from dev/cuda/ that does naive matrix multiplication. Compare its runtime to cuBLAS for the same dimensions.\nModify the naive GEMM to use tiling with shared memory. Measure how performance improves.\nInspect PTX (the intermediate assembly) generated by NVCC for a simple kernel. Observe how memory loads are translated.\nAdd timing code around kernels to see how much performance scales with different block sizes.\nImplement a new custom kernel (e.g., ReLU activation) and compare its speed to applying ReLU via cuDNN.\n\n\n\nThe Takeaway\nThe dev/cuda/ folder is not about production training. It’s about learning and experimenting. It starts with the simplest CUDA kernels and builds up to performance-conscious designs. This progression mirrors how professional libraries achieve their speed. By studying and experimenting here, you gain a deeper appreciation of what it takes to make GPUs run at full tilt—and you gain the skills to write your own kernels when the libraries don’t provide what you need.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html#chapter-8.-multi-gpu-and-multi-node-training",
    "href": "books/en-US/book.html#chapter-8.-multi-gpu-and-multi-node-training",
    "title": "The Book",
    "section": "Chapter 8. Multi-GPU and Multi-node training",
    "text": "Chapter 8. Multi-GPU and Multi-node training\n\n71. Data Parallelism in llm.c\nWhen you want to train a large model, a single GPU often isn’t enough. Either the model doesn’t fit in memory, or the training takes too long. One of the simplest and most widely used ways to scale training across multiple GPUs is data parallelism. The idea is conceptually simple: instead of giving all the training data to one GPU, you split it into smaller batches, send each GPU a piece, let them process it independently, and then combine their results.\n\nThe Core Idea\nImagine you have a batch of 128 sequences and 4 GPUs. In data parallelism:\n\nGPU 0 sees sequences 0–31\nGPU 1 sees sequences 32–63\nGPU 2 sees sequences 64–95\nGPU 3 sees sequences 96–127\n\nEach GPU runs the forward pass, computes the loss, and calculates gradients for its slice. At the end of the step, the gradients are averaged across GPUs, ensuring that all models stay synchronized. Every GPU holds a full copy of the model parameters, so they are always consistent after gradient averaging.\n\n\nIn llm.c\nThe llm.c repository keeps things minimal, so there isn’t a full-fledged DeepSpeed or PyTorch DDP implementation. But the same principle applies:\n\nEach GPU gets a copy of the GPT-2 model.\nThe batch is split across devices.\nAfter the backward pass, gradients from all GPUs must be synchronized.\n\nThis synchronization is usually done using NCCL all-reduce (covered in the next section), but the design remains data parallel at heart.\n\n\nWhy Data Parallelism Works\nThe forward and backward passes are embarrassingly parallel across different data samples. A token in sequence A doesn’t need to know about a token in sequence B when computing gradients. As long as all GPUs agree on parameter updates after each step, splitting the batch is perfectly valid.\n\n\nExample Walkthrough\nLet’s say we’re training GPT-2 on TinyStories with batch size B = 32 and sequence length T = 64.\n\nOn a single GPU, the forward pass computes embeddings, attention, MLP, and loss for all 32 sequences.\nWith 2 GPUs, we set B = 16 on each. Each GPU processes 16 sequences in parallel.\nAfter backpropagation, both GPUs hold gradients for their half of the batch. Before applying the optimizer, the gradients are averaged so that the weight update is equivalent to training with the full batch of 32.\n\nFrom the model’s perspective, it’s as if nothing changed—it just sees gradients from the whole batch.\n\n\nMemory and Speed Benefits\n\nMemory: Each GPU stores only the activations for its local batch. This reduces per-GPU memory use, making it possible to train with larger global batches.\nSpeed: Training steps finish faster because multiple GPUs share the work. For example, doubling the number of GPUs often cuts training time per step nearly in half, though communication overhead prevents perfect scaling.\n\n\n\nLimitations\n\nCommunication Overhead Synchronizing gradients across GPUs can become expensive, especially with large models or when running across multiple nodes.\nI/O Bottlenecks Feeding data to multiple GPUs fast enough requires efficient dataloaders and prefetching.\nOptimizer State Replication With AdamW, each GPU also needs to store optimizer states (m and v). This means memory scales with the number of GPUs instead of shrinking.\n\n\n\nWhy It Matters\nData parallelism is the workhorse of deep learning scaling. It’s conceptually easy to understand, straightforward to implement, and works well even for large models. In practice, nearly all large-scale GPT training begins with data parallelism, often enhanced by techniques like gradient accumulation or mixed precision.\n\n\nTry It Yourself\n\nTrain GPT-2 in llm.c on a single GPU, then split the batch across two GPUs using CUDA_VISIBLE_DEVICES. Compare throughput and loss curves.\nExperiment with increasing global batch size while keeping per-GPU batch size fixed. Notice how validation loss behaves.\nSimulate gradient averaging by writing a simple script that averages arrays from two processes. Connect this idea back to how NCCL all-reduce works.\nMeasure the difference in memory usage per GPU when training with 1 vs 2 GPUs.\nRun a small experiment with different numbers of GPUs (1, 2, 4) and plot how training time per step changes.\n\n\n\nThe Takeaway\nData parallelism splits the workload across GPUs by dividing the batch. Each GPU trains a full model replica on part of the data, then synchronizes gradients so that updates are consistent. It’s simple but powerful, forming the foundation of scaling strategies in llm.c and in most deep learning frameworks. Without it, training GPT-2 and larger models on modern datasets would be impractical.\n\n\n\n72. MPI Process Model and GPU Affinity\nWhen you scale training beyond a single GPU, you need a way to manage multiple processes and devices. In the llm.c codebase, the minimalist approach relies on MPI (Message Passing Interface), a library that has been around for decades in high-performance computing. MPI provides a simple abstraction: you launch multiple processes, each assigned a rank (an ID number), and they can communicate with each other by sending and receiving messages.\nIn distributed deep learning, MPI typically works alongside NCCL (NVIDIA Collective Communications Library). MPI handles process management—spawning workers, assigning GPUs, setting up environment variables—while NCCL handles the actual gradient synchronization.\n\nMPI Processes and Ranks\nSuppose you want to train on 4 GPUs. MPI will start 4 processes. Each process:\n\nLoads the same GPT-2 model code.\nInitializes CUDA on one GPU.\nReads a shard of the training data or the same dataset, depending on setup.\n\nEach process gets a rank:\n\nRank 0 → GPU 0\nRank 1 → GPU 1\nRank 2 → GPU 2\nRank 3 → GPU 3\n\nRanks are important because they determine roles. For example, rank 0 often acts as the “master,” printing logs or handling checkpoints, while the others focus purely on computation.\n\n\nGPU Affinity\nIf you don’t explicitly map processes to GPUs, they can all try to use the same device. That leads to oversubscription—multiple processes fighting for one GPU while the others sit idle. To prevent this, you set GPU affinity.\nThe environment variable CUDA_VISIBLE_DEVICES is the simplest way to do this. For example:\n# Run with 4 GPUs\nmpirun -np 4 \\\n  -x CUDA_VISIBLE_DEVICES=0,1,2,3 \\\n  ./train_gpt2_mpi\nMPI automatically assigns process 0 to GPU 0, process 1 to GPU 1, and so on. Inside the code, you can confirm this by calling cudaSetDevice(rank).\nOn multi-node clusters, GPU affinity also needs to consider network topology. You want each process close to its GPU and ideally aligned with the node’s network card for faster NCCL communication.\n\n\nSynchronization and Communication\nAfter each forward and backward pass, each MPI process has its local gradients. These must be averaged across processes to keep the model weights consistent. MPI itself offers collective operations like MPI_Allreduce, but in practice, llm.c uses NCCL for GPU-to-GPU communication because it is faster and topology-aware. MPI sets up the group, NCCL does the heavy lifting.\n\n\nExample Workflow\n\nLaunch: mpirun -np 4 ./train_gpt2 starts 4 processes.\nInitialization: Each process determines its rank and sets its GPU with cudaSetDevice(rank).\nTraining loop:\n\nForward pass on each process’s GPU.\nBackward pass to compute gradients.\nGradients averaged with NCCL All-Reduce.\n\nUpdate: Every process updates its copy of the weights.\nSync: At the next step, all model replicas are identical.\n\n\n\nDebugging GPU Affinity Issues\nIf you accidentally misconfigure GPU affinity, symptoms include:\n\nTwo processes trying to use the same GPU → out-of-memory errors.\nGPUs left idle because no process is assigned.\nSlowdowns because processes are spread inefficiently across sockets or PCIe lanes.\n\nA quick way to debug is to print the rank and the GPU ID at startup:\nint rank;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nint device;\ncudaGetDevice(&device);\nprintf(\"Process %d is using GPU %d\\n\", rank, device);\n\n\nWhy It Matters\nMPI and GPU affinity might feel like low-level plumbing, but they are critical for scaling. If you don’t get this right, training may run at a fraction of expected speed or crash outright. For small setups (2–4 GPUs), it may feel like overkill, but for larger clusters with 8, 16, or 64 GPUs, careful mapping is the difference between success and wasted compute time.\n\n\nTry It Yourself\n\nTrain GPT-2 with mpirun -np 2 and verify each process prints a different GPU ID.\nIntentionally misconfigure CUDA_VISIBLE_DEVICES so both processes map to GPU 0, then observe the OOM error.\nOn a multi-GPU machine, experiment with running processes pinned to different GPUs. Measure training throughput.\nUse nvidia-smi topo -m to view the PCIe topology of your GPUs. Try to align MPI ranks with nearby GPUs for better performance.\nPrint the time spent in all-reduce with different mappings to see how GPU affinity affects communication overhead.\n\n\n\nThe Takeaway\nMPI is the backbone for managing multiple processes in distributed training, and GPU affinity ensures each process has exclusive access to the right device. Together, they lay the groundwork for efficient multi-GPU training in llm.c. Get these details right, and scaling is smooth; get them wrong, and you run into memory crashes, idle GPUs, or bottlenecked communication.\n\n\n\n73. NCCL All-Reduce for Gradient Sync\nOnce each GPU finishes its forward and backward pass, it has a set of gradients that reflect only its portion of the training data. To keep the model parameters consistent across all GPUs, these gradients must be synchronized. The standard way to do this in modern deep learning systems is with All-Reduce, and NVIDIA’s NCCL (NVIDIA Collective Communications Library, pronounced “Nickel”) provides the optimized implementation.\n\nWhat All-Reduce Does\nAll-Reduce is a collective communication operation. Every process (GPU) starts with its own local buffer of values—here, the gradients—and the operation combines them (using a reduction, usually summation) and distributes the result back to all processes.\nMathematically, if GPU 0 has g0, GPU 1 has g1, GPU 2 has g2, and GPU 3 has g3, then after All-Reduce, each GPU has the same result:\ng_all = (g0 + g1 + g2 + g3) / 4\nThe division by 4 is optional—it depends whether you want a sum or an average—but averaging is common for gradient updates.\nThis ensures that every GPU applies the same weight update and stays synchronized with the others.\n\n\nWhy NCCL?\nWhile MPI provides an All-Reduce primitive, NCCL is specifically optimized for GPUs. It knows about PCIe, NVLink, NVSwitch, and Infiniband topologies and arranges communication to maximize bandwidth and minimize latency. Some of its key strategies include:\n\nRing All-Reduce: GPUs are arranged in a ring. Each GPU sends its data to the next while receiving from the previous, accumulating partial sums as the data flows. This scales well with many GPUs.\nTree All-Reduce: Organizes communication as a tree, reducing depth (latency) at the cost of bandwidth.\nHybrid schemes: NCCL dynamically chooses strategies depending on GPU count and topology.\n\nBy exploiting topology awareness, NCCL can saturate available communication channels.\n\n\nExample in llm.c Training\nIn the CPU-only training loop, gradients are updated directly without communication. In the multi-GPU CUDA path, after backpropagation (gpt2_backward), each GPU has its local gradients in memory. At this point:\nncclAllReduce(model.grads_memory,\n              model.grads_memory,\n              model.num_parameters,\n              ncclFloat32,  // or half precision\n              ncclSum,\n              comm, stream);\nAfter this call, model.grads_memory on every GPU contains the summed gradients across all GPUs. Dividing by the number of GPUs turns it into the average.\n\n\nWhy Gradient Sync Matters\nWithout gradient synchronization, each GPU would drift apart, updating weights independently. This would be equivalent to training multiple smaller models rather than one unified model. Synchronization makes sure all replicas behave like a single large-batch training job.\n\n\nMemory and Performance Considerations\n\nBandwidth-bound: Gradient synchronization often dominates runtime as model size grows. For GPT-2 774M, gradients alone can be several GB per step.\nOverlapping Communication with Compute: Advanced systems overlap gradient exchange with backward computation. While later layers are computing gradients, earlier layers are already being synchronized.\nPrecision: Gradients can be synchronized in FP16/BF16 to cut communication bandwidth in half. This is called gradient compression.\n\n\n\nAnalogy\nThink of four chefs cooking the same dish in separate kitchens. Each chef tastes their own version and suggests adjustments (gradients). If they don’t talk, their recipes diverge. With All-Reduce, the chefs share notes, average their adjustments, and apply the same changes—so all four kitchens end up cooking the same dish.\n\n\nTry It Yourself\n\nRun training on 2 GPUs with and without gradient synchronization (by commenting out All-Reduce). Watch how quickly the models diverge in loss.\nUse NCCL’s NCCL_DEBUG=INFO environment variable to print communication patterns. Observe the chosen ring/tree strategies.\nExperiment with FP32 vs FP16 gradient synchronization and measure bandwidth savings.\nProfile training with nsys or nvprof to see how much time is spent in All-Reduce.\nScale from 2 GPUs to 4 or 8 and measure how synchronization overhead grows.\n\n\n\nThe Takeaway\nNCCL All-Reduce is the backbone of multi-GPU training in llm.c. It ensures that gradients computed on separate GPUs are combined into a single, consistent update. By leveraging topology-aware algorithms like ring and tree reductions, NCCL keeps synchronization efficient even as models and GPU counts scale up. Without it, distributed training would produce inconsistent, drifting models rather than a unified one.\n\n\n\n74. Building and Running Multi-GPU Trainers\nGetting multiple GPUs to cooperate isn’t automatic—you need to set up the environment, initialize communication, and make sure each process knows which GPU to use. In llm.c, the design is intentionally minimalist, but it still has to integrate with MPI (Message Passing Interface) and NCCL to allow training across several GPUs.\n\nStep 1: MPI Launch\nMulti-GPU training starts with MPI. You don’t run the program once; you launch it with mpirun or mpiexec, which spawns one process per GPU. For example:\nmpirun -np 4 ./train_gpt2_cu\nHere, -np 4 starts four processes. Each process will attach itself to one GPU.\nMPI provides:\n\nRank: a unique ID for each process (0, 1, 2, 3).\nWorld size: the total number of processes (here, 4).\n\nEach process knows who it is and how many peers it has.\n\n\nStep 2: GPU Assignment\nOnce MPI assigns ranks, each process must select a GPU. This is often done with:\ncudaSetDevice(rank);\nSo process 0 gets GPU 0, process 1 gets GPU 1, and so on. Without this step, processes might all pile onto the same GPU, leading to chaos.\n\n\nStep 3: NCCL Communicator\nNext, the code creates an NCCL communicator. Think of it as a “conference call” between all GPUs. NCCL sets up the communication paths (rings, trees) across devices. A typical setup looks like:\nncclCommInitRank(&comm, world_size, nccl_id, rank);\nHere:\n\nworld_size is the number of GPUs.\nnccl_id is a shared identifier obtained via MPI (all processes must use the same one).\nrank is the local ID.\n\nNow the GPUs can talk to each other.\n\n\nStep 4: Training Loop Integration\nOnce communication is established, the training loop doesn’t look dramatically different. Each GPU:\n\nLoads its own batch of data (so the dataset is divided across GPUs).\nRuns the forward pass.\nRuns the backward pass.\nCalls NCCL All-Reduce to sync gradients.\nUpdates parameters.\n\nThe only new ingredient is step 4. Without it, each GPU would wander off with its own gradients.\n\n\nExample Command\nSuppose you have 2 GPUs on your machine. You can train with:\nmpirun -np 2 ./train_gpt2_cu -batch_size 8 -seq_len 128\nEach GPU trains on 8 sequences of 128 tokens. Combined, it’s like training with batch size 16, but split across GPUs.\n\n\nCommon Pitfalls\n\nForgetting to set device by rank: all processes fight for GPU 0.\nMismatched NCCL IDs: communicator fails to initialize.\nMPI vs NCCL versions: some builds are picky, and you may need to recompile with matching CUDA/NCCL.\nNetworking issues: on multi-node setups, firewalls or missing InfiniBand drivers can block communication.\n\n\n\nWhy It Matters\nBuilding a multi-GPU trainer is the gateway to scaling. A single GPU may take weeks to train a large model, but spreading the work across 4, 8, or 16 GPUs cuts the time dramatically. The simplicity of llm.c shows that distributed training doesn’t require a massive framework—just careful use of MPI and NCCL.\n\n\nTry It Yourself\n\nLaunch training with 1 GPU and then 2 GPUs, keeping the global batch size the same. Compare training speed.\nLaunch with 2 GPUs but forget All-Reduce. Notice how validation loss behaves differently on each GPU.\nUse NCCL_DEBUG=INFO to see how NCCL sets up communication.\nTry deliberately mismatching ranks and devices—observe the crash to understand why assignment matters.\nMeasure GPU utilization with nvidia-smi during training to confirm both GPUs are working.\n\n\n\nThe Takeaway\nMulti-GPU trainers in llm.c are built around three pillars: MPI to manage processes, NCCL to synchronize gradients, and CUDA to run the math. Once these are in place, the training loop remains familiar, but the computation spreads across GPUs seamlessly. This design keeps the code minimal while still unlocking significant scaling power.\n\n\n\n75. Multi-Node Bootstrapping with MPI\nSo far, running across multiple GPUs on a single machine is relatively straightforward: every process talks through shared memory or high-speed interconnects like NVLink. Things become more interesting when training has to scale across multiple machines (often called “nodes”)—for example, when you want to run on 2 servers, each with 4 GPUs, to make a total of 8.\n\nThe MPI World\nMPI was designed for this. When you run:\nmpirun -np 8 -hostfile myhosts ./train_gpt2_cu\n\n-np 8 says you want 8 processes.\n-hostfile myhosts lists the machines (and how many processes to run on each).\n\nMPI then launches processes across nodes and assigns each one a rank. From the program’s perspective, it doesn’t matter if two ranks are on the same machine or different machines—they all see a global communicator of size 8.\n\n\nSetting Up NCCL Across Nodes\nNCCL doesn’t know how to find other machines by itself. It relies on MPI to exchange a unique NCCL ID. The typical flow is:\n\nRank 0 creates a new NCCL ID.\nRank 0 broadcasts the ID to all other ranks using MPI.\nEach process calls ncclCommInitRank with the shared ID, total world size, and its own rank.\n\nThis ensures all GPUs, even across different machines, join the same “conference call.”\n\n\nNetworking Considerations\nWhen scaling across nodes, networking becomes critical:\n\nEthernet vs InfiniBand: Standard Ethernet works but can be slow. High-performance clusters use InfiniBand for much higher bandwidth and lower latency.\nFirewall rules: NCCL needs open ports to connect nodes. Firewalls or strict security settings can block communication.\nEnvironment variables: Variables like NCCL_SOCKET_IFNAME (to pick the right network interface) often need to be set. For example:\nexport NCCL_SOCKET_IFNAME=eth0\n\n\n\nExample Hostfile\nA simple myhosts file could look like this:\nnode1 slots=4\nnode2 slots=4\nThis says node1 and node2 each have 4 GPUs. MPI will launch 4 processes on each, totaling 8.\n\n\nSynchronization Across Nodes\nBecause communication now spans machines, synchronization overhead becomes more visible. Gradient All-Reduce has to move data not only between GPUs in one server but also across the network. Efficient scaling depends on:\n\nLarge enough batch sizes (so compute time outweighs communication).\nOverlapping communication with computation (advanced optimization).\nFast interconnects between machines.\n\n\n\nWhy It Matters\nTraining large models rarely happens on a single machine. Multi-node training is how researchers and companies scale models to billions of parameters. By showing how to bootstrap MPI and NCCL across nodes, llm.c demonstrates the foundation of distributed AI training systems, but in a minimal and transparent way.\n\n\nTry It Yourself\n\nPrepare two machines with CUDA and NCCL installed, connected via the same network.\nWrite a hostfile listing both machines, then launch with mpirun.\nSet NCCL_DEBUG=INFO to watch how NCCL connects across nodes.\nCompare throughput between single-node and two-node runs with the same number of GPUs.\nExperiment with environment variables like NCCL_SOCKET_IFNAME or NCCL_IB_DISABLE=1 to see how network choices affect speed.\n\n\n\nThe Takeaway\nBootstrapping multi-node training is about extending the same principles as single-node multi-GPU training, but with networking in the mix. MPI handles process management, NCCL sets up communication, and CUDA runs the math. With just a few lines of setup, llm.c can stretch from one GPU on your laptop to dozens of GPUs spread across multiple servers.\n\n\n\n76. SLURM and PMIx Caveats\nOn many research clusters or supercomputers, you don’t launch jobs manually with mpirun and a hostfile. Instead, you interact with a job scheduler, most commonly SLURM. SLURM takes care of allocating resources, starting processes across nodes, and enforcing quotas. While this saves you from manually managing hostfiles, it introduces its own set of details that you need to understand.\n\nSLURM Basics\nIn SLURM, you typically request GPUs and nodes using a script or command like:\nsalloc -N 2 -G 8 --time=01:00:00\n\n-N 2 asks for 2 nodes.\n-G 8 requests 8 GPUs (across those nodes).\n--time=01:00:00 sets a one-hour time limit.\n\nOnce the job starts, SLURM sets environment variables such as:\n\nSLURM_NTASKS: total number of tasks (processes).\nSLURM_PROCID: the rank of the current process.\nSLURM_NODEID: which node this process is running on.\n\nMPI implementations (OpenMPI, MPICH) and NCCL can use these to bootstrap communication automatically.\n\n\nPMIx Integration\nModern SLURM often works with PMIx (Process Management Interface for Exascale). PMIx allows MPI and other runtimes to query process information directly from SLURM without relying on older launchers. In practice, this means:\n\nYou might not use mpirun at all. Instead, SLURM provides srun.\nFor example:\nsrun -n 8 ./train_gpt2_cu\nHere -n 8 launches 8 tasks across your allocated nodes. SLURM/PMIx handles the rank assignments.\n\n\n\nCommon Pitfalls\n\nMPI version mismatch If your cluster has multiple MPI libraries installed, you may accidentally compile with one and run with another. Always confirm that the mpicc and mpirun you’re using match the library your job is linking against.\nEnvironment variable propagation NCCL relies on environment variables like NCCL_DEBUG, NCCL_SOCKET_IFNAME, and NCCL_IB_HCA. Sometimes SLURM doesn’t forward these to all nodes unless you configure it to. Using --export=ALL or adding exports in your job script can fix this.\nGPU visibility SLURM manages GPU allocation via CUDA_VISIBLE_DEVICES. Each process only “sees” the GPUs it was assigned. If your code assumes a global view of all GPUs, it may break. In llm.c, the mapping between rank and GPU ID needs to respect this.\nNetwork fabric mismatches On big clusters, you may have multiple network fabrics (Ethernet, InfiniBand). If NCCL picks the wrong one, performance plummets. Explicitly setting NCCL_SOCKET_IFNAME or NCCL_IB_DISABLE can solve this.\n\n\n\nWhy It Matters\nLearning to run across nodes with SLURM is essential if you want to scale training beyond a single server. While local mpirun commands work for development, almost all serious training runs—whether academic or industrial—happen under SLURM or a similar workload manager. Understanding the quirks of SLURM and PMIx ensures that your code scales smoothly without mysterious hangs or slowdowns.\n\n\nTry It Yourself\n\nWrite a small SLURM job script that requests 2 GPUs for 10 minutes and runs a dummy llm.c training loop.\nUse srun to launch the program and print out the SLURM_PROCID and SLURM_NODEID for each process.\nSet NCCL_DEBUG=INFO in your job script and observe how NCCL initializes communication.\nExperiment with srun --ntasks-per-node to control how many processes land on each node.\nIntentionally misconfigure CUDA_VISIBLE_DEVICES to see how it affects rank-to-GPU mapping.\n\n\n\nThe Takeaway\nSLURM and PMIx streamline distributed training on large clusters, but they add another layer of complexity. The principles remain the same—MPI ranks, NCCL communicators, and CUDA kernels—but the scheduler decides how processes are placed and how environments are set up. With a bit of practice, these tools allow llm.c to move from simple multi-GPU experiments to scalable cluster-wide training runs.\n\n\n\n77. Debugging Multi-GPU Hangs and Stalls\nWhen training on multiple GPUs, one of the most frustrating experiences is a job that simply hangs — no errors, no crashes, just frozen processes. In distributed deep learning, hangs are almost always related to synchronization mismatches. Every GPU worker is supposed to “meet up” at communication points (like gradient all-reduce), and if even one process gets lost, the whole group stalls.\n\nCommon Causes of Hangs\n\nMismatched Collective Calls If one rank calls ncclAllReduce while another rank skips it or calls ncclBroadcast, the system deadlocks. All GPUs wait forever because they’re not speaking the same “language” at that step.\nUneven Batch Sizes If the training data isn’t perfectly divisible across GPUs, one process might run out of data earlier than others. The code tries to sync gradients, but some ranks never reach that point.\nCUDA Errors Silently Ignored A kernel launch failure on one GPU can prevent it from reaching synchronization. If error checks are missing, you won’t see the failure until the program is stuck.\nNetworking Issues NCCL depends on reliable network connections. If one node has a bad InfiniBand card, firewall rule, or misconfigured interface, communication halts.\n\n\n\nDebugging Strategies\n\nEnable NCCL Debugging Set:\nexport NCCL_DEBUG=INFO\nexport NCCL_DEBUG_SUBSYS=ALL\nThis produces logs showing when each rank enters and leaves collective operations. By comparing ranks, you can see who got stuck.\nCheck CUDA Errors Always wrap CUDA calls with error checks, or run with:\ncuda-memcheck ./train_gpt2_cu\nThis detects invalid memory access or kernel failures that might lead to stalls.\nSimplify the Setup Start with 2 GPUs on a single node. If it works, increase to 4 GPUs, then expand to multiple nodes. This isolates whether the bug is in GPU logic or network communication.\nTimeouts and Watchdogs NCCL provides environment variables like NCCL_TIMEOUT that can help detect when a collective is stalled. Although it won’t fix the hang, it prevents wasting hours waiting for nothing.\n\n\n\nWhy It Matters\nIn multi-GPU training, hangs are not rare — they are part of the debugging journey. Understanding that hangs usually mean “one rank is out of sync” helps you approach the problem methodically. By checking logs, validating batch sizes, and carefully testing collective calls, you avoid endless frustration and wasted GPU hours.\n\n\nTry It Yourself\n\nRun a 2-GPU training job and intentionally misconfigure the code so only one rank calls gpt2_backward. Observe how the system hangs.\nEnable NCCL_DEBUG=INFO and compare logs between the two ranks.\nModify the dataloader so that one GPU gets fewer batches than the other. Watch how training stalls at the first gradient sync.\nExperiment with cuda-memcheck to catch silent CUDA errors in a simple kernel.\nPractice scaling up from 1 node to 2 nodes to see where hangs are more likely to appear.\n\n\n\nThe Takeaway\nHangs in distributed training almost always trace back to mismatched synchronization, unbalanced workloads, or hidden errors. By using NCCL’s debug tools, adding error checks, and testing systematically, you can turn mysterious freezes into solvable problems. Multi-GPU training isn’t just about raw speed — it’s about learning to keep many moving parts in lockstep.\n\n\n\n78. Scaling Stories: GPT-2 124M → 774M → 1.6B\nOne of the most exciting parts of llm.c is that it doesn’t just stop at toy models. The same code that trains a small GPT-2 model on Tiny Shakespeare can be scaled up to much larger models, like GPT-2 774M and even 1.6B. But scaling isn’t just about making the numbers bigger — it changes almost everything about how you train: memory requirements, communication costs, optimizer stability, and even your workflow.\n\nStarting Small: GPT-2 124M\nThe 124M parameter model is the “hello world” of GPT-2 training. It fits comfortably on a single modern GPU, and you can even run a trimmed-down version on CPU. At this size:\n\nBatch sizes can stay small (e.g., 4–8).\nMemory requirements are modest — a few gigabytes of VRAM.\nTraining speed is relatively fast, so you can iterate quickly.\nPurpose: sanity checks, debugging kernels, verifying correctness.\n\nThink of 124M as the training wheels stage: you’re learning to balance, not yet racing.\n\n\nMoving to GPT-2 774M\nAt ~774M parameters, the picture changes:\n\nA single GPU can still fit the model, but training speed slows dramatically.\nGradient synchronization across multiple GPUs becomes essential to get reasonable throughput.\nCommunication costs start to matter: an all-reduce of hundreds of megabytes per step stresses both PCIe and network bandwidth.\nStability becomes more sensitive: learning rates and warmup schedules need more careful tuning.\n\nHere, training is less about “does the code run?” and more about “does the system scale?” This size is often used in academic replications of GPT-2 because it’s large enough to be interesting but not impossibly expensive.\n\n\nGPT-2 1.6B: Scaling to the Edge\nAt 1.6B parameters, the model is too large for a single GPU to train efficiently. You need:\n\nMulti-GPU setups with NCCL all-reduce to share gradient updates.\nMulti-node training on clusters when even 8 GPUs aren’t enough.\nCareful optimizer tuning — without proper settings for AdamW and schedulers, the model may diverge.\nMemory tricks like mixed precision (FP16/BF16) and gradient checkpointing to fit activations in memory.\n\nTraining GPT-2 1.6B is a significant engineering challenge, but it proves that llm.c is not just a toy project — it’s a minimal yet real implementation that can push to billion-parameter scale.\n\n\nScaling Lessons\nAs you climb from 124M to 774M to 1.6B, several lessons emerge:\n\nDebug small, scale big — always test on 124M before attempting larger models.\nCommunication dominates — at 774M and beyond, time spent moving gradients often exceeds compute time.\nHyperparameters evolve — a learning rate that works for 124M may explode the loss at 1.6B.\nInfrastructure matters — GPUs, interconnects, and schedulers become as important as code.\n\n\n\nWhy It Matters\nScaling stories show that deep learning isn’t just about writing a clever algorithm — it’s about making that algorithm work under increasingly heavy loads. Each jump in size uncovers new bottlenecks and new engineering challenges. By following this path, you gain intuition for how large models are really trained in practice.\n\n\nTry It Yourself\n\nTrain GPT-2 124M on Tiny Shakespeare until the loss stabilizes. Record how long each step takes.\nAttempt the same experiment on OpenWebText with 124M — watch how dataset size now becomes the limiting factor.\nScale up to GPT-2 355M or 774M if you have access to multiple GPUs. Measure how much time is spent in NCCL all-reduce compared to compute.\nIf you have cluster access, try running 774M with srun or mpirun across nodes.\nStudy published training logs for GPT-2 1.6B and compare them to your own — how does scaling change the shape of the loss curve?\n\n\n\nThe Takeaway\nScaling isn’t just “bigger models need bigger GPUs.” Each increase in model size reshapes the training process, introducing new bottlenecks and requiring new techniques. llm.c is valuable precisely because it makes these transitions transparent: you can start with a tiny model and gradually experience the real engineering hurdles of training state-of-the-art language models.\n\n\n\n79. NCCL Tuning and Overlap Opportunities\nOnce your training runs extend beyond a single GPU, communication overhead becomes a central challenge. Every training step requires gradients to be exchanged among GPUs so that the optimizer updates stay synchronized. This is where NCCL (NVIDIA Collective Communications Library) comes in. NCCL provides efficient implementations of collective operations like all-reduce, all-gather, and broadcast. But simply using NCCL isn’t enough: how you tune it, and how you overlap communication with computation, can make the difference between sluggish training and near-linear scaling.\n\nHow NCCL Works in Training\nIn llm.c, when multiple GPUs train together, each GPU computes its local gradients during backpropagation. At the end of the backward pass, NCCL’s all-reduce combines gradients across GPUs so that every GPU ends up with the same values. Only then can the optimizer step forward.\nWithout NCCL, you’d have to write custom point-to-point code with cudaMemcpyPeer or MPI, which would be both slower and harder to maintain. NCCL ensures the communication pattern is efficient for the underlying hardware — PCIe, NVLink, or InfiniBand.\n\n\nKey NCCL Tuning Parameters\n\nNCCL_DEBUG Setting NCCL_DEBUG=INFO helps you understand what NCCL is doing. For performance tuning, logs are essential.\nNCCL_SOCKET_IFNAME On multi-node clusters, this decides which network interface NCCL binds to. Using the wrong interface (like Ethernet instead of InfiniBand) can slow training by orders of magnitude.\nNCCL_ALGO Determines how collectives are executed:\n\nRing: good for large message sizes, stable performance.\nTree: faster for small messages, less latency. Some training runs benefit from experimenting with both.\n\nNCCL_IB_DISABLE Useful if you want to force NCCL to avoid InfiniBand and stick with TCP/IP, usually for debugging network issues.\n\n\n\nOverlapping Communication with Computation\nThe backward pass doesn’t need to wait until all gradients are computed before starting to communicate. In fact, gradients for earlier layers can start their all-reduce while later layers are still computing gradients. This is called communication-computation overlap.\nFor example:\n\nWithout overlap: compute gradients for all layers → run NCCL all-reduce for all gradients → update parameters.\nWith overlap: while gradients for higher layers are still being computed, start the all-reduce for earlier layers.\n\nThis reduces idle time and often leads to substantial throughput gains. Some frameworks (like PyTorch’s DistributedDataParallel) implement this automatically. In a low-level system like llm.c, this would require careful kernel launch ordering and stream management.\n\n\nPractical Example\nImagine you’re training GPT-2 774M across 8 GPUs. Each backward pass produces ~3 GB of gradients. If you wait until all gradients are ready before syncing, the all-reduce might take 200 ms. If your compute step also takes 200 ms, then half of your training time is spent idle. With overlap, you can hide much of that communication inside compute time, potentially cutting step time almost in half.\n\n\nWhy It Matters\nAs model sizes increase, communication costs can rival or exceed computation. Without tuning, GPUs spend more time waiting for data to arrive than actually training the model. By understanding NCCL and applying overlap techniques, you unlock the ability to scale efficiently to dozens or even hundreds of GPUs.\n\n\nTry It Yourself\n\nRun a multi-GPU training job with NCCL_DEBUG=INFO enabled and watch the communication patterns.\nChange NCCL_ALGO between Ring and Tree and measure the effect on step times.\nExperiment with setting CUDA_LAUNCH_BLOCKING=1 to remove overlap, then remove it again to see how communication and computation interleave.\nIf you have a cluster, try forcing NCCL to use Ethernet instead of InfiniBand and compare bandwidth.\nProfile a multi-GPU run using nvprof or Nsight Systems and check whether NCCL collectives overlap with kernel execution.\n\n\n\nThe Takeaway\nEfficient distributed training is not only about having more GPUs — it’s about keeping them busy. NCCL provides the communication backbone, but how you configure and overlap its operations determines whether you get close to linear scaling or waste resources. Mastering these details transforms multi-GPU training from “just working” into truly efficient large-scale computation.\n\n\n\n80. Common Multi-GPU Errors and Fixes\nWhen running llm.c across multiple GPUs, errors can range from confusing hangs to cryptic NCCL messages. These problems are normal in distributed training, but they can eat up hours unless you recognize the patterns. The good news is that most errors fall into a handful of common categories, and once you learn the typical causes, they’re easier to diagnose and fix.\n\nError Type 1: Process Hangs with No Output\nSymptom: Training starts but then freezes. No error message, no crash, just silence. Cause: Usually, one or more ranks are out of sync. This could mean:\n\nDifferent batch sizes on each rank (one rank has fewer tokens left).\nA mismatch in collective calls — for example, one GPU calls all_reduce while another skips it.\nA CUDA error in one process that prevents it from reaching synchronization. Fix:\nCheck that dataloaders feed the same number of steps to every rank.\nAdd error checking to CUDA calls.\nEnable NCCL_DEBUG=INFO to trace which rank got stuck.\n\n\n\nError Type 2: NCCL WARN Net: no interface found\nSymptom: NCCL reports it can’t find a network interface, or training is extremely slow. Cause: NCCL can’t discover the correct interface to use for inter-node communication. By default, it may try Ethernet instead of InfiniBand. Fix:\n\nSet NCCL_SOCKET_IFNAME to the right interface, e.g.:\nexport NCCL_SOCKET_IFNAME=ib0\nConfirm with your sysadmin which network interfaces are available for high-performance GPU communication.\n\n\n\nError Type 3: CUDA_ERROR_OUT_OF_MEMORY\nSymptom: Processes crash when allocating model weights or during the backward pass. Cause:\n\nModel too large for the available GPU memory.\nBatch size too high.\nMemory fragmentation from repeated allocations. Fix:\nReduce batch size B or sequence length T.\nTry mixed precision (FP16/BF16) if supported.\nRestart processes to clear memory fragmentation.\n\n\n\nError Type 4: unhandled system error, NCCL version mismatch\nSymptom: One process logs NCCL version 2.17 and another logs 2.14. Training fails. Cause: Different NCCL libraries are being used across nodes. This happens when software environments aren’t identical. Fix:\n\nUse the same container or module environment on all nodes.\nConfirm NCCL versions with ldd or conda list.\n\n\n\nError Type 5: Validation Loss Diverges on Multi-GPU but Not on Single GPU\nSymptom: Loss values explode only when running across multiple GPUs. Cause: Gradient synchronization may be broken — for example, only a subset of parameters are being all-reduced. Another possibility is using a different effective learning rate because of batch size scaling. Fix:\n\nConfirm that all parameters participate in gradient synchronization.\nScale the learning rate properly: if you double the global batch size by using more GPUs, you may need to adjust the learning rate.\n\n\n\nWhy It Matters\nMulti-GPU training is powerful but unforgiving: even tiny mismatches in environment, data, or synchronization can cause errors. Instead of treating these as random mysteries, it’s helpful to recognize the patterns. Each error message or hang has a likely cause, and learning to map symptoms to fixes will make distributed training much smoother.\n\n\nTry It Yourself\n\nIntentionally reduce the dataset size so one rank runs out of data early — observe the hang.\nLaunch a multi-node run without setting NCCL_SOCKET_IFNAME and watch how performance collapses.\nIncrease the batch size until you trigger CUDA_ERROR_OUT_OF_MEMORY, then reduce it step by step to see the limit.\nExperiment with mismatched NCCL versions across nodes if you have access to multiple environments, then fix it by standardizing.\nRun a small model with different batch sizes per rank and study how the validation loss diverges.\n\n\n\nThe Takeaway\nMost multi-GPU errors aren’t mysterious once you understand what’s happening under the hood. They usually boil down to synchronization mismatches, network misconfigurations, memory limits, or environment inconsistencies. With the right debugging tools and a systematic mindset, you can fix these problems quickly and keep your training runs moving forward.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html#chapter-9.-extending-the-codebase",
    "href": "books/en-US/book.html#chapter-9.-extending-the-codebase",
    "title": "The Book",
    "section": "Chapter 9. Extending the codebase",
    "text": "Chapter 9. Extending the codebase\n\n81. The dev/cuda Library for Custom Kernels\nSo far, most of the CUDA logic in llm.c has relied on NVIDIA’s optimized libraries like cuBLAS and cuDNN. These libraries are extremely powerful and efficient, but sometimes you want more control: maybe you’re experimenting with a new attention mechanism, or maybe you want to fuse multiple operations into a single kernel to reduce memory traffic. That’s where the dev/cuda directory comes in. It’s the playground for custom kernels.\n\nWhat Lives in dev/cuda\nIf you look at the repository structure, you’ll notice a folder named dev/cuda. This is not part of the minimal training path — you can train GPT-2 models without ever touching it. Instead, it contains experimental kernels that showcase how to move from simple CUDA examples toward more advanced, production-level implementations.\nInside, you’ll typically find:\n\nHello World kernels: basic examples like elementwise addition to get familiar with CUDA.\nFused operations: simple prototypes for combining steps like bias addition + activation.\nBenchmark code: small programs that measure kernel performance compared to cuBLAS/cuDNN.\n\nThese files are not polished production code. They’re meant to be read, modified, and played with — like lab notebooks for CUDA development.\n\n\nWhy Custom Kernels Matter\nLibraries like cuBLAS are designed to cover a wide range of use cases, but they don’t always hit the sweet spot for your specific workload. Writing custom kernels allows you to:\n\nFuse operations: Instead of launching separate kernels for bias addition, activation, and dropout, you can do all three in one kernel, saving time on memory reads/writes.\nExperiment with new algorithms: If you invent a new type of attention or normalization, you can’t rely on cuDNN — you need to implement it yourself.\nLearn how GPUs actually work: Reading and writing custom kernels teaches you about thread blocks, memory hierarchy, and warp scheduling, all of which deepen your understanding of GPU programming.\n\n\n\nExample: A Simple Elementwise Kernel\nHere’s a very small kernel from a toy example you might find in dev/cuda:\n__global__ void add_kernel(const float* a, const float* b, float* out, int N) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i &lt; N) {\n        out[i] = a[i] + b[i];\n    }\n}\nThis kernel adds two arrays a and b elementwise. While trivial compared to attention mechanisms, it illustrates the GPU execution model:\n\nEach thread handles one index i.\nThreads are grouped into blocks, and blocks form a grid.\nMemory access is explicit: you control exactly how out is written.\n\nScaling this up to real workloads means adding more complexity — shared memory, warp shuffles, half-precision math — but the principles remain the same.\n\n\nWhy It Matters\nThe dev/cuda directory is not just for fun experiments. It’s a bridge between “using GPU libraries” and “designing GPU algorithms.” By learning to write kernels here, you gain the freedom to customize and optimize beyond what standard libraries provide. That skill becomes essential if you want to innovate in model architectures or squeeze the last bit of performance out of your hardware.\n\n\nTry It Yourself\n\nOpen one of the example .cu files in dev/cuda and compile it with nvcc.\nModify the elementwise kernel so it performs out[i] = a[i] * b[i] instead of addition.\nBenchmark your kernel against the equivalent cuBLAS call (e.g., cublasSaxpy) and compare performance.\nWrite a fused kernel that does bias addition followed by ReLU activation in one pass.\nUse nvprof or Nsight Systems to measure how many kernel launches occur in a forward pass and imagine how custom fusion might reduce them.\n\n\n\nThe Takeaway\nThe dev/cuda library is your sandbox for learning and experimenting with CUDA. It’s not required for running GPT-2, but it’s where you build the skills to go beyond libraries and design your own GPU operations. Whether you’re optimizing for speed or testing new research ideas, this directory is where theory meets practice in GPU programming.\n\n\n\n82. Adding New Dataset Pipelines (dev/data/*)\nTraining a language model is not just about having a clever model — it’s equally about the data you feed it. In llm.c, datasets are handled in a very lightweight way compared to frameworks like PyTorch. Instead of complicated abstractions, the project keeps things simple: tokenize your text once, save it as a .bin file, and then stream batches of tokens into the model.\nThe dev/data/ directory is where this happens. It contains scripts and utilities for preparing different datasets, from tiny toy corpora like Tiny Shakespeare to larger collections like TinyStories or OpenWebText subsets. Understanding how this directory works is the key to plugging in your own datasets.\n\nHow Data Pipelines Work in llm.c\nAt a high level, the pipeline follows three steps:\n\nDownload or provide raw text data. For example, tiny_shakespeare.txt is just a plain text file with plays concatenated.\nTokenize the data once using the GPT-2 tokenizer. The tokenizer converts text into integers according to gpt2_tokenizer.bin.\nWrite the tokens to a binary file (.bin). This is a flat array of integers stored as 32-bit values, which makes it fast to memory-map and stream during training.\n\nOnce the .bin files exist, dataloader_init can open them, divide them into training and validation splits, and generate batches of shape (B, T) for the model.\n\n\nWhat’s Inside dev/data/\nThe folder contains small scripts like:\n\ndownload_starter_pack.sh — downloads Tiny Shakespeare and TinyStories.\nTokenization scripts — often small Python snippets that run the GPT-2 tokenizer over raw text.\nPrebuilt .bin files — these are used in the quickstart so you don’t need to regenerate them yourself.\n\nThe design choice here is minimalism: instead of a heavy dataset framework, you get plain files and short scripts. You can read and understand everything in a few minutes.\n\n\nAdding Your Own Dataset\nSuppose you want to train on your company’s support chat logs or a new dataset you’ve found. The process looks like this:\n\nPrepare raw text in a simple format (one text file is fine).\nRun the tokenizer from llm.c on it:\npython dev/data/tokenizer.py --input my_corpus.txt --output my_corpus.bin\nThis produces a binary token file.\nDrop the file into dev/data/. You might name it my_corpus_train.bin and my_corpus_val.bin.\nPoint the dataloader at it in your training code:\ndataloader_init(&train_loader, \"dev/data/my_corpus_train.bin\", B, T, 0, 1, 1);\ndataloader_init(&val_loader, \"dev/data/my_corpus_val.bin\", B, T, 0, 1, 0);\n\nThat’s it — you now have a new dataset pipeline integrated with the same training loop.\n\n\nWhy It Matters\nMany frameworks hide data preprocessing behind layers of abstractions. llm.c takes the opposite approach: it makes the process transparent. You see exactly how text becomes tokens, how those tokens become batches, and how the model consumes them. This transparency makes it easier to debug, extend, and customize. Adding a new dataset is no longer a mystery — it’s just a matter of writing one file and updating a path.\n\n\nTry It Yourself\n\nExplore the dev/data/ directory and read through the provided scripts.\nTokenize a new small dataset of your choice (a novel, a set of Wikipedia pages, or your own text).\nTrain a 124M model on your new dataset and observe the loss curve.\nCompare validation loss between Tiny Shakespeare and your dataset — how does the model behave differently?\nTry increasing sequence length T to see how batching interacts with longer documents.\n\n\n\nThe Takeaway\nThe dev/data folder is where you connect language models to the real world. It shows how raw text becomes training-ready binary files with almost no overhead. By learning to add your own pipelines, you gain the ability to train llm.c on any dataset — from classic literature to domain-specific corpora — while keeping the workflow fast and understandable.\n\n\n\n83. Adding a New Optimizer to the Codebase\nSo far, llm.c has focused on AdamW, which is the workhorse optimizer for training transformer models. But deep learning is a fast-moving field: new optimizers appear, older ones sometimes resurface, and certain workloads benefit from alternatives. The simplicity of llm.c makes it a great environment to learn how to implement and experiment with optimizers.\n\nWhere Optimizers Live in llm.c\nIn the CPU training path, the optimizer logic is implemented directly in C in the function gpt2_update. This function iterates through every parameter, applies AdamW’s moment updates, applies bias correction, and then modifies the parameter values in place.\nBecause the parameters, gradients, and optimizer states are all stored as contiguous arrays in memory (params_memory, grads_memory, m_memory, v_memory), adding a new optimizer usually means:\n\nAllocating any new state arrays you need.\nDefining the update rule in the training loop.\nAdding function calls for your new optimizer, similar to gpt2_update.\n\n\n\nExample: Implementing SGD with Momentum\nStochastic Gradient Descent (SGD) with momentum is much simpler than AdamW. The update rule looks like this:\n// SGD with momentum update\nvoid gpt2_update_sgd(GPT2 *model, float learning_rate, float momentum) {\n    if (model-&gt;m_memory == NULL) {\n        model-&gt;m_memory = (float*)calloc(model-&gt;num_parameters, sizeof(float));\n    }\n\n    for (size_t i = 0; i &lt; model-&gt;num_parameters; i++) {\n        float grad = model-&gt;grads_memory[i];\n        float m = momentum * model-&gt;m_memory[i] + grad;\n        model-&gt;m_memory[i] = m;\n        model-&gt;params_memory[i] -= learning_rate * m;\n    }\n}\nHere, m_memory stores the velocity (the exponentially decayed average of past gradients). There’s no second moment estimate like in AdamW, so it’s leaner in both code and memory usage.\n\n\nComparing Optimizers\nAdding new optimizers lets you experiment and compare behaviors:\n\n\n\n\n\n\n\n\n\nOptimizer\nStrengths\nWeaknesses\nMemory Needs\n\n\n\n\nSGD\nSimple, stable, fewer hyperparameters\nSlow convergence on large models\nLow\n\n\nSGD + Momentum\nFaster convergence, smooths updates\nStill less adaptive than Adam\nLow\n\n\nAdam\nAdapts learning rates per parameter\nCan overfit small datasets\nMedium\n\n\nAdamW\nSame as Adam + correct weight decay\nMore complex\nMedium\n\n\nAdagrad/RMSProp\nGood for sparse features\nNot always stable for transformers\nMedium\n\n\n\nIn llm.c, each optimizer is just a loop over parameters with a few math operations. That makes it the perfect playground to see how different optimizers actually behave in practice.\n\n\nWhy It Matters\nOptimizers control how your model learns. While architectures like GPT-2 get a lot of attention, small changes in optimization can make the difference between a model that converges smoothly and one that diverges. By adding your own optimizers, you get a clearer understanding of this often “black box” part of deep learning.\n\n\nTry It Yourself\n\nImplement the SGD with momentum function shown above and swap it into the training loop instead of AdamW.\nRun training on Tiny Shakespeare and compare how many steps it takes for the loss to reach 2.0.\nModify the code to implement RMSProp (similar to Adam, but without momentum on the first moment).\nBenchmark memory usage: notice how AdamW allocates both m_memory and v_memory, while SGD only uses one.\nTry running AdamW with a very small dataset versus SGD — does one overfit faster?\n\n\n\nThe Takeaway\nOptimizers are just math on arrays. By writing and testing new ones inside llm.c, you’ll demystify how learning actually happens at the parameter level. This makes it easier to appreciate why AdamW became the default for transformers, and also gives you the tools to explore alternatives in a clean, transparent environment.\n\n\n\n84. Adding a New Scheduler (cosine, step, etc.)\nTraining isn’t only about choosing an optimizer; the way you adjust the learning rate over time is just as important. A scheduler tells the optimizer how fast to learn at each step. Without a scheduler, you’d use a fixed learning rate, which often works poorly for large models. In llm.c, schedulers are kept intentionally simple so you can see exactly how they influence training.\n\nWhere Schedulers Fit\nIf you look at the training loop, every step calls the optimizer like this:\ngpt2_update(&model, lr, beta1, beta2, eps, weight_decay, step+1);\nThe lr here doesn’t have to be constant. Instead, a scheduler function can compute it based on the step number. In llm.c, this logic lives in schedulers.h and related helpers.\n\n\nCommon Schedulers You Can Add\n\nStep Decay Reduce the learning rate by a fixed factor every N steps.\nfloat step_decay(int step, float base_lr, int decay_every, float decay_factor) {\n    int k = step / decay_every;\n    return base_lr * powf(decay_factor, k);\n}\nCosine Decay Smoothly decrease the learning rate following a cosine curve.\nfloat cosine_decay(int step, int max_steps, float base_lr) {\n    float progress = (float)step / max_steps;\n    return base_lr * 0.5f * (1.0f + cosf(M_PI * progress));\n}\nLinear Warmup + Cosine Decay Start with a gradual increase (warmup) to avoid instability, then switch to cosine decay. This is the most common choice for transformers.\n\n\n\nExample: Cosine with Warmup\nHere’s how you might implement cosine with warmup in llm.c:\nfloat lr_scheduler(int step, int warmup_steps, int max_steps, float base_lr) {\n    if (step &lt; warmup_steps) {\n        return base_lr * (float)(step + 1) / warmup_steps;\n    } else {\n        float progress = (float)(step - warmup_steps) / (max_steps - warmup_steps);\n        return base_lr * 0.5f * (1.0f + cosf(M_PI * progress));\n    }\n}\nThis means:\n\nSteps 0–warmup_steps: linearly scale from 0 → base_lr.\nAfter warmup: smoothly decay the learning rate using cosine.\n\n\n\nWhy It Matters\nSchedulers help stabilize training. At the beginning, gradients can be very noisy, so warming up slowly prevents divergence. At the end, lowering the learning rate helps the model converge instead of bouncing around minima. Without schedulers, you’d often need more tuning to get the same results.\n\n\nTry It Yourself\n\nTrain with a constant learning rate on Tiny Shakespeare and record the loss curve.\nSwitch to a step decay scheduler and see if convergence improves.\nImplement cosine decay with warmup and compare against constant LR — which reaches a lower validation loss?\nExperiment with different warmup lengths (e.g., 10 steps vs 100 steps) and watch how training stability changes.\nTry running the same experiment on TinyStories and see if dataset size affects which scheduler works best.\n\n\n\nThe Takeaway\nSchedulers are small pieces of code with big impact. They don’t change the model or the optimizer, but they control the pace of learning. By adding new schedulers to llm.c, you get a hands-on way to see why modern training recipes almost always combine warmup with a smooth decay schedule.\n\n\n\n85. Alternative Attention Mechanisms\nTransformers became famous because of the self-attention mechanism, but “attention” is not a single fixed formula. Researchers have explored many alternatives that trade off memory use, speed, and accuracy. In llm.c, the default is scaled dot-product attention, but nothing prevents you from experimenting with new approaches.\n\nThe Default: Scaled Dot-Product Attention\nThe standard attention formula looks like this:\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n\\]\n\nQ = queries\nK = keys\nV = values\n\\(d_k\\) = key dimension (used for scaling)\n\nIn llm.c, this is implemented using matrix multiplications and masking to enforce causality. It’s correct and faithful to GPT-2, but has quadratic cost in sequence length \\(T\\).\n\n\nVariants You Could Add\n\nSparse Attention Instead of attending to every token, restrict attention to a local window or a set of important positions.\n\nGood for long sequences.\nSaves compute and memory.\nExample: “sliding window” attention where each token only looks back 128 steps.\n\nLinformer / Low-Rank Attention Approximate \\(QK^T\\) using low-rank projections.\n\nReduces memory from \\(O(T^2)\\) to \\(O(T)\\).\nWorks well when redundancy exists in sequences.\n\nPerformer (Linear Attention) Replace the softmax with kernel approximations so attention becomes linear in sequence length.\n\nTrades exactness for scalability.\nAllows much longer sequences on the same hardware.\n\nALiBi (Attention with Linear Biases) Adds simple position-dependent biases instead of full positional embeddings.\n\nExtremely efficient.\nHelps extrapolate to longer sequences than seen in training.\n\n\n\n\nHow to Experiment in llm.c\nThe attention implementation lives inside the attention_forward and attention_backward routines in train_gpt2.c (and their CUDA equivalents). To try an alternative:\n\nReplace the part where \\(QK^T\\) is computed with your chosen method.\nKeep the interface the same: given inputs Q, K, V, return outputs shaped (B, T, C).\nRun unit tests (test_gpt2.c) against the baseline to make sure the outputs stay reasonable.\n\n\n\nWhy It Matters\nAttention is often the bottleneck in transformers. Quadratic time and memory usage limit how long your sequences can be. By experimenting with alternatives, you not only improve efficiency but also learn how new research ideas are implemented in practice. Many of today’s “efficient transformers” came from simple tweaks to this block.\n\n\nTry It Yourself\n\nModify attention so each token only attends to the last 16 tokens (a toy form of sparse attention). Train on Tiny Shakespeare and compare speed vs. accuracy.\nImplement ALiBi by adding linear position-dependent bias terms and see if your model generalizes better to longer text.\nBenchmark the GPU memory footprint of standard attention vs. your custom version using Nsight Systems or nvidia-smi.\nTry removing the scaling factor \\(1/\\sqrt{d_k}\\) — does training become unstable?\nReplace softmax with a simple ReLU and see how the model behaves (hint: it usually diverges, but it teaches why softmax is important).\n\n\n\nThe Takeaway\nThe attention block is where much of the magic happens in transformers, but it’s also the biggest bottleneck. By experimenting with alternatives in llm.c, you’ll gain a deeper understanding of why the standard formula works, what its weaknesses are, and how new ideas from research can be tested directly in code.\n\n\n\n86. Profiling and Testing New Kernels\nWhen you start adding custom CUDA kernels or experimenting with new attention mechanisms, the next big question is: how do you know if they’re correct and efficient? That’s where profiling and testing come in. llm.c keeps this process minimal but transparent so you can see exactly how to validate both correctness and performance.\n\nCorrectness First: Testing Against a Reference\nAny new kernel you write should be compared against a known-good implementation. In llm.c, PyTorch usually serves as the “oracle.” For example:\n\nGenerate random input tensors in both llm.c and PyTorch.\nRun your custom kernel in llm.c.\nRun the equivalent operation in PyTorch.\nCompare the outputs within a small tolerance (e.g., differences less than 1e-5).\n\nThis ensures your kernel doesn’t silently compute the wrong thing. Without this step, you might train for hours before realizing your model is learning nonsense.\n\n\nPerformance: Profiling the Kernels\nOnce correctness is established, the next step is performance. NVIDIA provides several tools:\n\nnvprof (older): still widely used, easy to launch.\nNsight Systems / Nsight Compute (modern): more detailed, lets you see kernel timings, memory transfers, occupancy, and more.\n\nIn practice:\n\nRun your training loop with profiling enabled.\nIdentify which kernels take the most time.\nCheck if your custom kernel is faster than the baseline (e.g., cuBLAS or cuDNN).\n\n\n\nCommon Metrics to Watch\n\nKernel time (how long each launch takes).\nOccupancy (how many CUDA cores are active relative to maximum).\nMemory throughput (are you saturating memory bandwidth?).\nLaunch count (do you call your kernel too many times instead of fusing operations?).\n\nEven a correct kernel can be slower than a library implementation if it doesn’t use the GPU efficiently.\n\n\nExample Workflow\nSuppose you write a fused bias + ReLU kernel. You can test it like this:\n\nGenerate a random tensor in C and in PyTorch.\nApply your fused kernel vs. PyTorch’s separate + and ReLU ops.\nCompare results for correctness.\nProfile both approaches: is your kernel faster? Did it reduce kernel launch overhead?\n\n\n\nWhy It Matters\nCustom kernels are fun to write, but without testing and profiling they’re just guesswork. Many research ideas look promising in theory but fall apart in practice because they run slower or break correctness. By learning to systematically test and profile, you can separate ideas that are genuinely useful from those that are just experiments.\n\n\nTry It Yourself\n\nWrite a simple fused kernel for bias + ReLU. Compare it against PyTorch’s x + bias followed by relu(x).\nUse nvprof to check how many kernel launches happen in each version.\nRun Nsight Systems and look at the timeline: do you see your fused kernel overlapping better with other GPU activity?\nTry scaling sequence length T to very large values — does your kernel still perform well?\nRecord memory usage before and after your kernel runs. Is there a difference compared to the unfused version?\n\n\n\nThe Takeaway\nProfiling and testing turn kernel hacking from random tinkering into real engineering. With a reference for correctness and tools for performance measurement, you can iterate confidently, knowing when your changes are truly improvements. This is how llm.c bridges the gap between a learning project and real GPU systems work.\n\n\n\n87. Using PyTorch Reference as Oracle\nOne of the guiding principles of llm.c is to stay small, readable, and minimal — but that doesn’t mean you’re left without a safety net. When implementing something as mathematically dense as a transformer, how do you know your C or CUDA code is doing the right thing? The answer is to use PyTorch as a reference implementation, often called an “oracle.”\n\nWhat Does “Oracle” Mean Here?\nAn oracle is simply a trusted system you compare against. PyTorch is trusted because:\n\nIt’s widely used in production and research.\nIts operators (matrix multiply, attention, layernorm, etc.) have been thoroughly tested.\nIt gives you both CPU and GPU implementations with stable numerical behavior.\n\nIf your llm.c forward or backward pass matches PyTorch’s within a small error tolerance, you can be confident that your implementation is correct.\n\n\nHow the Comparison Works\nThe workflow usually looks like this:\n\nSet up the same model in both PyTorch and llm.c with identical weights.\nFeed the same inputs to both models.\nCompare outputs — logits, losses, or gradients.\nAllow for tiny differences due to floating point arithmetic, usually within 1e-5 to 1e-6.\n\nFor example, test_gpt2.c in the repository runs a forward pass in C and compares the logits to those produced by a PyTorch GPT-2 checkpoint.\n\n\nExample\nSuppose you’re testing the embedding lookup. In PyTorch you might write:\nimport torch\nfrom transformers import GPT2Model, GPT2Tokenizer\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2Model.from_pretrained(\"gpt2\")\ntokens = torch.tensor([[50256, 200]])  # EOT and an example token\nout = model.wte(tokens)  # word token embeddings\nprint(out.detach().numpy())\nIn llm.c, you’d run the same two tokens through the embedding lookup and compare the resulting vectors element by element. If they match, your embedding implementation is correct.\n\n\nWhy PyTorch is the Perfect Oracle\n\nTransparency: it’s easy to extract weights from PyTorch checkpoints (.bin files).\nFlexibility: you can test individual layers, not just the whole model.\nDebuggability: if something goes wrong, you can isolate which layer diverges first.\n\nThis last point is crucial — instead of training for days only to find your loss curve diverges, you can catch mismatches immediately at the layer level.\n\n\nWhy It Matters\nDeep learning models are fragile. Even a tiny mistake in normalization, masking, or gradient flow can ruin training. By anchoring your work to PyTorch, you avoid “trusting your gut” and instead rely on a battle-tested baseline. This practice isn’t unique to llm.c — many professional frameworks (Megatron-LM, DeepSpeed) also validate against PyTorch during development.\n\n\nTry It Yourself\n\nExtract GPT-2 weights from Hugging Face Transformers and load them into llm.c.\nRun a forward pass in both PyTorch and llm.c with the same input tokens. Compare outputs numerically.\nFocus on a single block: check whether the attention output matches PyTorch’s.\nModify a kernel (e.g., change softmax to ReLU) and watch how quickly the outputs diverge from PyTorch’s.\nUse PyTorch to verify gradients by calling .backward() and comparing with gpt2_backward in llm.c.\n\n\n\nThe Takeaway\nPyTorch is your map and compass when navigating the dense jungle of transformer internals. By treating it as an oracle, you can move confidently from one layer to the next, knowing that your small, hand-written code matches the behavior of a full-featured deep learning framework. This practice turns llm.c into more than a toy project — it becomes a faithful, verifiable reimplementation of GPT-2.\n\n\n\n88. Exploring Beyond GPT-2: LLaMA Example\nWhile llm.c focuses on GPT-2 for clarity, the same framework can extend to newer, larger, and more modern models such as LLaMA. LLaMA, released by Meta, uses many of the same building blocks as GPT-2 — embeddings, attention layers, MLPs, normalization, and residual streams — but with tweaks that improve efficiency and scaling. Looking at LLaMA through the lens of llm.c helps you see how language model designs evolve while still sharing the same DNA.\n\nWhat Stays the Same\n\nToken embeddings: both GPT-2 and LLaMA use lookup tables to turn token IDs into dense vectors.\nTransformer blocks: the fundamental loop of attention → MLP → residuals is unchanged.\nAutoregressive training: predict the next token given all previous ones, using causal masking.\n\nThis means much of the code in llm.c — dataloaders, embeddings, forward loops — would work with LLaMA almost unchanged.\n\n\nWhat’s Different\n\nNormalization\n\nGPT-2 uses LayerNorm before each block output.\nLLaMA uses RMSNorm, which normalizes using only the root mean square of activations (no mean subtraction).\nThis reduces compute slightly and improves stability.\n\nPositional Encoding\n\nGPT-2 has learned positional embeddings.\nLLaMA uses Rotary Position Embeddings (RoPE), which rotate queries and keys in attention space to encode positions.\nRoPE scales better to longer contexts.\n\nVocabulary\n\nGPT-2’s vocab size is 50,257.\nLLaMA uses a different tokenizer (SentencePiece/BPE) with a larger vocabulary, closer to 32k for LLaMA-2.\n\nModel Scale\n\nGPT-2 tops out at 1.6B parameters.\nLLaMA-2 and LLaMA-3 scale from 7B up to 70B+. This makes distributed training mandatory, with mixed precision and checkpointing as standard.\n\n\n\n\nAdapting llm.c to LLaMA\nIf you wanted to modify llm.c to approximate LLaMA, the main tasks would be:\n\nReplace LayerNorm with an RMSNorm implementation.\nAdd RoPE into the attention mechanism. This means modifying the step where Q and K vectors are built, applying a rotation based on token positions.\nSwap out the GPT-2 tokenizer with a SentencePiece tokenizer trained on the desired vocabulary.\n\nThe rest of the pipeline — optimizer, schedulers, dataloaders, multi-GPU support — would remain valid.\n\n\nWhy It Matters\nBy studying LLaMA in the context of GPT-2, you see that modern LLMs aren’t completely alien. They’re evolutionary improvements on the same transformer backbone. Recognizing these small architectural changes (RMSNorm, RoPE, scaling) helps demystify why newer models outperform older ones, and it shows you exactly what you’d need to tweak in llm.c to explore beyond GPT-2.\n\n\nTry It Yourself\n\nImplement RMSNorm in C by adapting the LayerNorm code in llm.c.\nAdd a simplified version of RoPE to the attention kernel and run it on Tiny Shakespeare.\nSwap the GPT-2 tokenizer for a SentencePiece model and train a small LLaMA-like model on your own dataset.\nCompare training stability between LayerNorm and RMSNorm — does the loss curve look different?\nStudy the memory use of GPT-2 vs. a small LLaMA-style variant and see how scaling behaves.\n\n\n\nThe Takeaway\nExploring LLaMA through llm.c shows how flexible the codebase really is. With only a few targeted changes — normalization, positional encoding, tokenizer — you can shift from replicating GPT-2 to experimenting with the building blocks of modern LLMs. This makes llm.c not just a study tool for one model, but a foundation for understanding the entire lineage of transformers.\n\n\n\n89. Porting Playbook: C → Go/Rust/Metal\nThe llm.c codebase is written in plain C for maximum readability and minimal dependencies. But in practice, many developers want to experiment with other languages or platforms — for example, writing a Go or Rust version for better tooling, or targeting Apple’s Metal API for GPU acceleration on Macs. Porting is not just a copy-paste exercise; it requires careful thinking about how low-level memory, math operations, and parallelism map across ecosystems.\n\nWhy Port at All?\n\nGo: strong concurrency model (goroutines, channels), good for building training services or distributed experiments.\nRust: memory safety and performance without garbage collection, ideal for writing reliable numerical kernels.\nMetal (Apple): GPU acceleration on macOS/iOS, a must if you want to train or run models efficiently on Apple Silicon.\n\nEach ecosystem has strengths that make llm.c’s concepts more approachable or more production-ready.\n\n\nMapping the Core Components\nLet’s look at how the key pieces of llm.c translate:\n\n\n\n\n\n\n\n\n\n\nComponent\nC (llm.c)\nGo Equivalent\nRust Equivalent\nMetal Equivalent\n\n\n\n\nMemory allocation\nmalloc, calloc\nmake, slices, unsafe.Pointer\nVec&lt;T&gt;, Box, unsafe if needed\nBuffers allocated on GPU\n\n\nMath kernels\nmanual loops, OpenMP\nloops or cgo bindings to BLAS\nloops with iterators, Rayon for CPU\nMetal compute shaders\n\n\nTokenizer\nfread binary file\nstandard file I/O, encoding/json\nserde, binary read\nPreprocessing on CPU, feed to GPU\n\n\nTraining loop\nfor-loops, structs\ngoroutines for dataloader + trainer\nasync tasks, channels\nCPU driver, GPU kernels\n\n\nParallelism\n#pragma omp\ngoroutines + sync primitives\nRayon or explicit threads\nWarp/thread groups in Metal\n\n\n\n\n\nExample: LayerNorm in Rust\nHere’s a small Rust sketch of how a forward pass for LayerNorm might look:\nfn layernorm_forward(out: &mut [f32], inp: &[f32], weight: &[f32], bias: &[f32], c: usize) {\n    let mean: f32 = inp.iter().sum::&lt;f32&gt;() / c as f32;\n    let var: f32 = inp.iter().map(|x| (x - mean).powi(2)).sum::&lt;f32&gt;() / c as f32;\n    let rstd = 1.0 / (var + 1e-5).sqrt();\n\n    for i in 0..c {\n        let norm = (inp[i] - mean) * rstd;\n        out[i] = norm * weight[i] + bias[i];\n    }\n}\nThis looks strikingly similar to the C code, but benefits from Rust’s type safety and the absence of manual memory management bugs.\n\n\nExample: Attention Kernel in Metal\nMetal would handle attention differently — you’d write a compute shader in .metal language:\nkernel void attention_forward(\n    device float* out [[buffer(0)]],\n    device float* qkv [[buffer(1)]],\n    uint id [[thread_position_in_grid]]\n) {\n    // compute a dot product between query and key vectors\n    // accumulate into out, using threadgroup memory for efficiency\n}\nThis isn’t a line-for-line port, but it shows how the concept — multiply queries with keys, apply softmax, weight values — remains the same while the implementation moves into GPU-land.\n\n\nChallenges You’ll Face\n\nNumerical libraries: C often leans on BLAS/LAPACK or just hand-written loops. In Go and Rust, you’ll either bind to these libraries or reimplement them.\nPerformance portability: getting code that runs fast both on CPU and GPU isn’t trivial. What works in C with OpenMP won’t directly translate.\nTokenizer compatibility: making sure tokenization matches byte-for-byte is essential. One mismatch can ruin training reproducibility.\n\n\n\nWhy It Matters\nPorting llm.c forces you to understand what each piece of the code is doing — you can’t just rely on torch.nn.LayerNorm or torch.nn.MultiheadAttention. This makes it an excellent exercise for truly learning transformers, while also giving you practical implementations in different environments.\n\n\nTry It Yourself\n\nReimplement one kernel (like LayerNorm or matmul) in Go or Rust. Test it against the C version.\nWrite a minimal Metal kernel that adds two vectors, then extend it to matrix multiplication.\nBuild a Rust tokenizer reader that loads gpt2_tokenizer.bin and decodes IDs back into text.\nCompare training speed: C with OpenMP vs. Rust with Rayon vs. Go with goroutines.\nTry porting just the forward pass first — inference is easier than training because you don’t need backprop.\n\n\n\nThe Takeaway\nThe llm.c design is portable by nature — it doesn’t hide behind opaque frameworks. Porting it to Go, Rust, or Metal is not just about performance or language preference. It’s about proving to yourself that the transformer algorithm is universal, and you can implement it anywhere once you truly understand it.\n\n\n\n90. Keeping the Repo Minimal and Clean\nOne of the defining features of llm.c is its minimalism. The repository avoids the sprawling complexity of large frameworks and instead sticks to a small, readable core. This design choice isn’t an accident—it’s a philosophy. By keeping the codebase small and clean, contributors can focus on understanding the fundamentals of transformers rather than navigating thousands of lines of boilerplate.\n\nThe Philosophy of Minimalism\n\nReadability over performance: While production frameworks like PyTorch or TensorFlow optimize aggressively, llm.c intentionally trades some performance for clarity. A loop in plain C is easier to study than a chain of optimized CUDA calls hidden behind macros.\nPortability: A smaller codebase can be ported more easily to new environments (Go, Rust, Metal) without pulling in dozens of dependencies.\nLearning-first design: Every line of code has a clear purpose. There are no abstractions “just in case.”\n\nThis philosophy turns llm.c into both a working training framework and an educational resource.\n\n\nHow Minimalism Is Enforced\n\nFlat structure: The repository avoids deep directory hierarchies. Most files live directly under llmc/ or dev/.\nNo external libraries unless critical: You’ll see OpenMP, cuBLAS, or cuDNN for performance, but not sprawling dependency chains.\nOne feature, one file: Tokenization, dataloading, schedulers, and samplers each get their own small C file. This prevents “god files” where too much is lumped together.\nConsistent naming: Functions and structs use clear, descriptive names (e.g., dataloader_next_batch, tokenizer_decode) so readers don’t get lost.\n\n\n\nPractical Examples\nIf you open train_gpt2.c, you’ll find that it builds the model, initializes dataloaders, and runs the training loop. It doesn’t try to handle every possible model configuration, dataset format, or distributed scenario. Those belong in specialized files or external tools.\nLikewise, llmc/utils.h only defines the absolute essentials: safe file I/O wrappers (fopenCheck, freadCheck) and memory allocators. It’s not bloated with generic helpers unrelated to training GPT-2.\n\n\nWhy It Matters\nA minimal repo lowers the barrier to entry. Beginners can trace execution from main() all the way to the optimizer update without detours. Researchers can fork the code and modify it without worrying about breaking dozens of interconnected modules. Even advanced developers benefit because the simplicity forces clarity in reasoning about algorithms.\n\n\nTry It Yourself\n\nPick any file in the repo and count how many lines it has. Most are under a few hundred. Compare this with a similar file in PyTorch or TensorFlow.\nTry adding a new feature—say, a different activation function. Notice how easy it is to slot it in because the structure is clean.\nExplore what happens if you make the repo “heavier.” Add too many helpers, abstractions, or configs. Does it make the code harder to read?\nPractice explaining the training loop to a friend. If the repo is simple, you should be able to walk them through without glossing over details.\n\n\n\nThe Takeaway\nKeeping llm.c minimal is not just about saving lines of code. It’s about preserving clarity, ensuring reproducibility, and making the repository a place where anyone—from curious learners to experienced engineers—can open a file and understand what’s happening. The simplicity is the point, and that’s what makes llm.c a rare and valuable resource in a world of bloated ML frameworks.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  },
  {
    "objectID": "books/en-US/book.html#chapter-10.-reproduction-community-and-roadmap",
    "href": "books/en-US/book.html#chapter-10.-reproduction-community-and-roadmap",
    "title": "The Book",
    "section": "Chapter 10. Reproduction, community and roadmap",
    "text": "Chapter 10. Reproduction, community and roadmap\n\n91. Reproducing GPT-2 124M on Single Node\nThe first major milestone for anyone exploring llm.c is to reproduce the training of GPT-2 124M, the smallest version of the GPT-2 family. This model has around 124 million parameters, which is large enough to be interesting, but still small enough to train on a single modern GPU—or even slowly on CPU for demonstration purposes.\n\nWhy Start with 124M?\nGPT-2 comes in multiple sizes: 124M, 355M, 774M, and 1.6B parameters. Training the largest requires clusters of GPUs and serious compute budgets. The 124M version, however, fits comfortably on consumer-grade GPUs like an NVIDIA 3090, and can even be run on a laptop CPU if you’re patient. It’s the “hello world” of transformer reproduction: small, approachable, and still real.\n\n\nWhat the Training Setup Looks Like\nTraining GPT-2 124M involves a few key steps:\n\nModel Configuration The config for 124M is baked into llm.c with 12 layers, 12 heads, hidden dimension of 768, and max sequence length of 1024.\nGPT2Config config = {\n    .max_seq_len = 1024,\n    .vocab_size = 50257,\n    .num_layers = 12,\n    .num_heads = 12,\n    .channels = 768\n};\nDataset You can train on small datasets like Tiny Shakespeare or Tiny Stories for quick runs. For more realistic reproduction, you need something closer to OpenWebText. The data is tokenized with the GPT-2 tokenizer (gpt2_tokenizer.bin) and stored in .bin files.\nBatch Size and Sequence Length A common setting is B = 8, T = 1024, meaning 8 sequences, each of length 1024 tokens. Adjust these based on available memory.\nOptimizer AdamW with a learning rate around 3e-4 is the default. Warmup and cosine decay scheduling can be enabled to match published GPT-2 training curves.\n\n\n\nWhat to Expect in Practice\nOn CPU, training a single step may take several seconds. On a single GPU with CUDA, each step may take under 100 milliseconds. With 124M parameters, training from scratch on a dataset the size of OpenWebText still takes days, but you can reproduce key dynamics (loss curve, sample generations) on smaller datasets in hours.\nAs an example, here’s the type of log you might see:\nstep 0: train loss 6.9321 (took 421.5 ms)\nstep 100: train loss 4.2137 (took 95.2 ms)\nstep 200: train loss 3.8914 (took 94.8 ms)\nval loss 3.7725\nEven within a few hundred steps, the model begins to generate text resembling English rather than pure noise.\n\n\nWhy It Matters\nReproducing GPT-2 124M is a confidence check. If your setup is correct, your loss curves should match those in the original OpenAI paper or the PyTorch reference implementation. This validates that llm.c is a faithful reproduction, not just a toy. It also teaches you how much compute and data go into even the smallest GPT-2 model, building intuition about scaling laws.\n\n\nTry It Yourself\n\nTrain GPT-2 124M for 1000 steps on Tiny Shakespeare. Watch how the generated text improves.\nChange the batch size B from 8 to 4. What happens to the speed and the stability of training?\nRun training on CPU vs. GPU. Compare how long each step takes.\nTrack both training and validation loss. Notice how they diverge when the model begins to overfit.\n\n\n\nThe Takeaway\nThe GPT-2 124M run is more than just a demo—it’s your gateway into real LLM training. You see how data, model size, optimizer, and hardware come together. Once you’ve mastered this reproduction, you’re ready to push toward larger models and more complex setups. It’s the foundation on which everything else in llm.c builds.\n\n\n\n92. Reproducing GPT-2 355M (Constraints and Tricks)\nOnce GPT-2 124M has been successfully reproduced, the next logical step is scaling up to GPT-2 355M. This version is roughly three times larger, with about 355 million parameters. It introduces new challenges that don’t appear at the smaller scale: memory pressure, training stability, and compute cost.\n\nModel Configuration\nThe 355M model still uses 1024 tokens as the maximum sequence length and the same GPT-2 tokenizer. The difference is in the depth and width of the network:\n\nLayers: 24 transformer blocks instead of 12\nHidden dimension (channels): 1024 instead of 768\nHeads: 16 instead of 12\n\nThe total parameter count rises from ~124M to ~355M. That means not just three times more math per step, but also more memory needed for parameters, gradients, and optimizer state.\n\n\nThe Compute Challenge\nWith 124M, a single GPU with 8–12 GB of VRAM is enough. For 355M, you need at least 16 GB to run comfortably with sequence length 1024 and batch size of 8. On smaller GPUs, you’ll quickly hit “CUDA out of memory” errors.\nOne trick is to reduce batch size (B) or sequence length (T). For example, instead of training with (B=8, T=1024), you might use (B=4, T=512). This halves the memory footprint but still lets you test scaling dynamics.\nAnother approach is to use gradient accumulation: simulate a larger batch size by running multiple small steps and accumulating gradients before updating.\n\n\nTraining Stability\nLarger models are more sensitive to hyperparameters. The AdamW optimizer still works, but the learning rate schedule becomes more important. Many practitioners use:\n\nLearning rate: ~3e-4 peak\nWarmup steps: a few thousand\nCosine decay: to taper the learning rate gradually\n\nIf you skip warmup, the larger model may diverge early (loss exploding instead of decreasing).\n\n\nTricks for Feasibility\n\nMixed Precision Training (FP16 or BF16): Cuts memory use nearly in half. Supported in CUDA paths of llm.c.\nActivation Checkpointing: Save memory by recomputing activations during backpropagation. Slower, but lets you fit bigger models.\nSmaller Dataset Runs: Train on Tiny Shakespeare or Tiny Stories to sanity-check the setup, then scale to OpenWebText-like data.\n\n\n\nExample Logs\nRunning GPT-2 355M for a short demo might look like:\nstep 0: train loss 7.1032 (took 321.8 ms)\nstep 50: train loss 5.4231 (took 310.4 ms)\nstep 100: train loss 4.8217 (took 311.0 ms)\nval loss 4.7322\nThe loss drops more slowly than with 124M because the model has more capacity to learn, but also needs more data to generalize.\n\n\nWhy It Matters\n355M is the first step into “medium-sized” LLMs. You start to feel the bottlenecks that dominate larger models: VRAM limits, training speed, and hyperparameter tuning. Solving these prepares you for the 774M and 1.6B experiments, where such problems become even more pronounced.\n\n\nTry It Yourself\n\nTrain GPT-2 355M with batch size 4 and sequence length 512. Record how long each step takes.\nExperiment with warmup steps: run once with warmup=0, and once with warmup=2000. Compare stability.\nEnable mixed precision if you have a CUDA-capable GPU. Measure memory usage before and after.\nTry training on Tiny Shakespeare vs. Tiny Stories. Does the model overfit faster on the smaller dataset?\n\n\n\nThe Takeaway\nReproducing GPT-2 355M is all about learning how to stretch limited resources. You’ll discover memory-saving tricks, the importance of learning rate schedules, and the role of data scale. It’s a practical exercise in resource management—just like the real challenges faced when training today’s billion-parameter models.\n\n\n\n93. Reproducing GPT-2 774M (Scaling Up)\nThe 774M parameter version of GPT-2 is often called “GPT-2 Medium.” This is the point where training transitions from a personal experiment to a small-scale research project. It’s about six times larger than the 124M baseline, and roughly twice the size of 355M. Running it requires careful planning of hardware, memory, and software tricks.\n\nModel Configuration\nFor 774M, the architecture expands again:\n\nLayers (transformer blocks): 36\nHidden size (channels): 1280\nAttention heads: 20\nMaximum sequence length: 1024 (unchanged)\n\nThis jump in size increases both the parameter storage and the number of activations that must be kept during training. Optimizer states (AdamW’s m and v vectors) alone consume several gigabytes.\n\n\nHardware Requirements\nRunning GPT-2 774M from scratch generally requires GPUs with 24 GB VRAM or more (e.g., NVIDIA RTX 3090/4090, A100, or H100). With smaller cards, you’ll almost certainly hit memory errors unless you aggressively reduce batch size and use techniques like activation checkpointing.\nOn CPUs, training is technically possible but far too slow to be practical—steps that take milliseconds on GPUs might take many seconds or even minutes.\n\n\nPractical Constraints\n\nBatch Size: In practice, you may need to lower B to 2 or even 1 with sequence length 1024.\nGradient Accumulation: A must-have to simulate larger batch sizes and stabilize training.\nMixed Precision: FP16 or BF16 reduces memory by about half, without hurting convergence much.\nCheckpointing: Recomputes intermediate results instead of storing them, trading time for memory.\n\n\n\nTraining Dynamics\nThe loss curve for GPT-2 774M drops more steadily and requires much more data to reach its potential. If you only train on Tiny Shakespeare or Tiny Stories, it will quickly overfit: the model is too large for such a small dataset. For meaningful reproduction, you need a dataset similar in scale to OpenWebText.\nTraining logs for a sanity check run might look like:\nstep 0: train loss 8.0123 (took 512.4 ms)\nstep 50: train loss 5.7892 (took 490.7 ms)\nstep 100: train loss 5.1428 (took 495.1 ms)\nval loss 5.0039\nNotice that losses start higher (due to the larger random initialization space) but decrease predictably once training gets underway.\n\n\nWhy It Matters\nThe 774M model is the sweet spot where scaling laws become obvious. Compared to 124M and 355M, it generalizes better, generates more fluent text, and demonstrates the benefits of parameter growth. But it also shows why infrastructure matters: without careful management, it’s nearly impossible to train this model on consumer-grade hardware.\nThis is also where distributed training (covered in Chapter 8) becomes relevant, because one GPU is often not enough for efficient scaling.\n\n\nTry It Yourself\n\nTrain GPT-2 774M with (B=1, T=1024) and gradient accumulation over 8 steps. Watch how it simulates a batch size of 8.\nCompare training with FP32 vs. mixed precision. Measure both memory use and speed.\nRun a short fine-tuning experiment on Tiny Stories. Observe how quickly the model memorizes the dataset.\nPlot the training and validation loss curves. Does the larger model overfit faster or slower than 355M?\n\n\n\nThe Takeaway\nReproducing GPT-2 774M is about scaling into the territory of real research workloads. You face serious memory constraints, dataset requirements, and compute costs. But if you succeed, you’ll see firsthand why the machine learning community kept pushing toward billion-parameter models: larger networks unlock noticeably stronger capabilities, even with the same architecture.\n\n\n\n94. Reproducing GPT-2 1.6B on 8×H100 (24h Run)\nThe largest GPT-2 model, with 1.6 billion parameters, represents the upper bound of the original GPT-2 family. Training this model from scratch is not something you can casually attempt on a single workstation. It demands cluster-scale resources, distributed training software, and careful tuning to keep everything stable. In this section, we’ll walk through what makes the 1.6B model special, what infrastructure is required, and how a full reproduction might look.\n\nModel Configuration\nThe jump from 774M to 1.6B doubles the parameter count and makes the network both deeper and wider:\n\nLayers (transformer blocks): 48\nHidden size (channels): 1600\nAttention heads: 25\nSequence length: still 1024\n\nWith these dimensions, every forward and backward pass requires massive amounts of memory and compute. Just storing the parameters in FP32 takes around 6.4 GB. Once you add gradients, optimizer states (AdamW’s m and v), and activations, the memory footprint easily exceeds 100 GB.\n\n\nHardware Setup\nTo reproduce this model realistically, you need access to high-end accelerators such as NVIDIA A100s or H100s. A common baseline is 8 GPUs with 80 GB each. With this setup, it is possible to train GPT-2 1.6B in under 24 hours, assuming efficient utilization.\nWithout multi-GPU, training is impractical. Even if you could somehow fit the model on one GPU, the runtime would be weeks or months.\n\n\nDistributed Training\nThe main strategy is data parallelism: each GPU processes a different mini-batch of data, and gradients are averaged across all devices with NCCL all-reduce. The code paths in llm.c support this via MPI integration, so you can scale from single-GPU to multi-node setups.\nThe training loop looks nearly identical to smaller models, but behind the scenes, every parameter update is coordinated across devices.\n\n\nTraining Dynamics\nThe loss curve for 1.6B smooths out compared to smaller models. With enough data, the model continues to improve where 774M starts to plateau. This was one of the key insights from the original GPT-2 paper: scaling laws hold, and performance improves predictably with size, data, and compute.\nLogs from a distributed run might look like this:\n[rank 0] step 0: train loss 8.5029 (took 312.6 ms)\n[rank 0] step 50: train loss 6.3121 (took 308.2 ms)\n[rank 0] step 100: train loss 5.7210 (took 309.4 ms)\n[rank 0] val loss 5.5347\nNotice the speed: each step still takes only a few hundred milliseconds despite the massive size, thanks to parallelism across multiple H100s.\n\n\nWhy It Matters\nReproducing GPT-2 1.6B is less about training a useful model today and more about understanding the scaling challenges of large language models. This exercise demonstrates how compute, memory, and distributed infrastructure become the limiting factors as models grow. It also shows why modern research labs design entire pipelines around multi-GPU and multi-node scaling.\n\n\nTry It Yourself\n\nSimulate a multi-GPU run with fewer resources by reducing the model size but using the same parallel training setup. For example, train GPT-2 124M across 2 GPUs to practice the workflow.\nExperiment with gradient accumulation to mimic large global batch sizes even on smaller clusters.\nTry enabling and disabling mixed precision. Watch how memory use drops dramatically with FP16/BF16.\nPlot validation loss curves for 124M, 355M, 774M, and 1.6B side by side. Notice how the larger models sustain improvements longer.\n\n\n\nThe Takeaway\nThe GPT-2 1.6B reproduction is the capstone project of llm.c. It forces you to combine everything you’ve learned: data pipelines, optimizers, schedulers, distributed training, and system-level debugging. While few people will actually train 1.6B themselves, understanding what it takes provides a window into the engineering behind state-of-the-art LLMs and prepares you to engage with even larger modern models.\n\n\n\n95. CPU-only Fine-Tune Demo (Tiny Shakespeare)\nNot everyone has access to powerful GPUs or large compute clusters. One of the strengths of llm.c is that it provides a clean, minimal CPU-only path that lets you run real experiments—even if they are small and slow. A practical way to explore this is by fine-tuning GPT-2 on a small dataset like Tiny Shakespeare, which has only about 1 MB of text.\n\nWhy Tiny Shakespeare?\nTiny Shakespeare is a classic toy dataset in machine learning. It’s small enough to fit in memory, yet it contains a rich variety of words, characters, and structures. Fine-tuning GPT-2 on this dataset allows the model to mimic Shakespearean style in just a few thousand steps. It’s not about building a state-of-the-art model—it’s about seeing the training process work end-to-end on modest hardware.\n\n\nSetup\nThe fine-tuning process uses the same train_gpt2.c CPU path, but with fewer steps, smaller batch sizes, and lower sequence lengths to keep things fast. A typical setup looks like this:\nint B = 4;    // batch size\nint T = 64;   // sequence length\nint steps = 1000;  // training iterations\nThe dataset is tokenized once using gpt2_tokenizer.bin and stored in binary .bin files:\ndev/data/tinyshakespeare/tiny_shakespeare_train.bin\ndev/data/tinyshakespeare/tiny_shakespeare_val.bin\nThese files are only a few megabytes, making them perfect for quick experiments.\n\n\nTraining Dynamics\nWhen you fine-tune on Tiny Shakespeare, the training logs may look like this:\nstep 0: train loss 6.9312 (took 1220.4 ms)\nstep 50: train loss 4.3217 (took 1175.1 ms)\nstep 100: train loss 3.7120 (took 1169.4 ms)\nval loss 3.5894\nWithin a few hundred steps, the loss drops rapidly. By the time you’ve run 1000 steps, the model starts producing text that looks recognizably Shakespearean—complete with archaic words, unusual punctuation, and rhythmic patterns.\n\n\nExample Output\nHere’s a short sample from a fine-tuned run:\ngenerating:\n\nROMEO: But hark, what light through yonder window breaks?\nJULIET: Ay me! the time is near, and I must away.\nROMEO: Fear not, sweet love, for night shall bring us peace.\n\nIt isn’t perfect, but it captures the “feel” of Shakespeare, which is remarkable given the tiny dataset and limited compute.\n\n\nWhy It Matters\nThe CPU-only Tiny Shakespeare demo proves that LLMs are not just for massive data centers. With a minimal setup, you can watch the model learn, generate text, and overfit to a dataset. This hands-on practice builds intuition about what training does, how loss curves behave, and why scaling up matters.\n\n\nTry It Yourself\n\nChange the sequence length from 64 to 128. How does training speed and loss change?\nReduce the number of training steps to 200. Do you still see Shakespeare-like text in generation?\nIncrease the batch size to 8. Does the loss curve become smoother or noisier?\nFine-tune again but initialize from scratch (random weights). Compare the results to fine-tuning from pretrained GPT-2 124M.\n\n\n\nThe Takeaway\nFine-tuning GPT-2 on Tiny Shakespeare with CPU-only training is a simple yet powerful demonstration. You don’t need GPUs to understand the mechanics of transformers. Even a modest laptop can teach you how training works, why overfitting happens, and how LLMs adapt to new domains. It’s a reminder that the best way to learn machine learning is by rolling up your sleeves and running experiments—even small ones.\n\n\n\n96. Cost and Time Estimation for Runs\nOne of the most eye-opening parts of working with large language models is realizing how expensive and time-consuming training can be. While llm.c makes it possible to run models of all sizes with simple, minimal C code, the hardware requirements grow quickly as you scale from GPT-2 124M to GPT-2 1.6B. Understanding cost and time estimation helps set realistic expectations, whether you’re running on a laptop CPU, a single GPU, or a rented cluster of accelerators.\n\nKey Factors Affecting Training Time\nSeveral components determine how long training takes and how much it costs:\n\nModel size (parameters): Larger models mean more multiplications per forward/backward pass, and more memory for parameters, gradients, and optimizer states.\nBatch size and sequence length: Increasing either multiplies the amount of work per step.\nDataset size: Bigger datasets require more steps to complete one epoch.\nHardware speed: CPUs are far slower than GPUs; high-end GPUs like H100s can be 100× faster than CPUs for this workload.\nParallelism: Multi-GPU or multi-node setups let you divide the work, reducing time per step.\n\n\n\nRough Time Estimates\nHere’s a simplified view of how long it might take to train GPT-2 models depending on hardware and setup. These are very approximate, assuming full training on a dataset like OpenWebText:\n\n\n\n\n\n\n\n\n\n\nModel\nParameters\nHardware\nTime per Step\nTotal Training Time\n\n\n\n\nGPT-2 124M\n~124M\nLaptop CPU\n1–2 s\nMonths\n\n\nGPT-2 124M\n~124M\nSingle RTX 3090\n~100 ms\n~2–4 weeks\n\n\nGPT-2 355M\n~355M\nSingle RTX 3090\n~300 ms\n~6–8 weeks\n\n\nGPT-2 774M\n~774M\n2× A100 40GB\n~200 ms\n~4–6 weeks\n\n\nGPT-2 1.6B\n~1.6B\n8× H100 80GB\n~300 ms\n~24 hours\n\n\n\nThese numbers show why scaling matters. The larger models don’t just need more compute per step—they also need more data to reach their potential. That means the total cost balloons unless you have a cluster of top-tier GPUs.\n\n\nCost in Cloud Environments\nIf you run these experiments on cloud providers like AWS, GCP, or Azure, costs can add up quickly. For example:\n\nAn NVIDIA A100 40GB instance costs around $2–3 per hour (spot pricing can be cheaper).\nTraining GPT-2 124M for a week might cost $500–1,000.\nTraining GPT-2 1.6B for 24 hours on 8× H100s might cost $5,000–10,000, depending on the provider.\n\nThis is why many researchers test code paths on small datasets and small models first, then only scale up when absolutely necessary.\n\n\nWhy It Matters\nEstimating cost and time prevents frustration and wasted money. It teaches you to prototype at small scale (CPU or 124M runs), validate your setup, and only then scale up to medium (355M, 774M) or large (1.6B) models. It also gives you a realistic appreciation for the engineering and budget that went into OpenAI’s original GPT-2 training runs.\n\n\nTry It Yourself\n\nTime how long a single training step takes on your hardware for GPT-2 124M. Multiply by 1000 to estimate training time for 1000 steps.\nReduce the batch size by half. Does time per step decrease linearly, or not?\nRun a short fine-tune (e.g., 200 steps) and measure the electricity cost if you’re on a home machine.\nUse a cloud GPU for one hour. Compare the cost and speed to your local CPU.\n\n\n\nThe Takeaway\nTraining large language models is a balancing act between ambition, hardware, and budget. llm.c gives you the tools to explore everything from toy demos to billion-parameter reproductions, but you’ll quickly see why the field has shifted toward big labs and shared infrastructure. With careful planning, though, you can still learn a tremendous amount by running smaller experiments and scaling them up thoughtfully.\n\n\n\n97. Hyperparameter Sweeps (sweep.sh)\nGetting a model like GPT-2 to train well isn’t just about writing the code or having enough compute—it’s also about finding the right hyperparameters. These include the learning rate, batch size, weight decay, dropout rate, and scheduler configuration. A setting that works well for one dataset or model size might completely fail for another. That’s where hyperparameter sweeps come in: systematically trying different configurations to see which ones give the best results.\n\nThe Role of sweep.sh\nIn llm.c, there’s a simple shell script called sweep.sh designed to automate hyperparameter testing. It’s not a complex experiment management system like Ray Tune or Optuna; instead, it’s lightweight, transparent, and easy to adapt. The script usually looks like a loop over different learning rates or batch sizes, running the training executable with each setting, and logging the output.\nA very simplified version might look like this:\n#!/bin/bash\nfor lr in 1e-3 5e-4 1e-4\ndo\n    echo \"Running with learning rate $lr\"\n    ./train_gpt2 -lr $lr -epochs 5 &gt; logs/lr_$lr.txt\ndone\nThis way, you can launch multiple experiments with a single command and later compare validation losses to decide which hyperparameters are best.\n\n\nWhy Sweeps Are Important\nTraining a transformer is highly sensitive to hyperparameters. For example:\n\nIf the learning rate is too high, loss might explode.\nIf it’s too low, training will be painfully slow.\nToo much weight decay can hurt performance, while too little can cause overfitting.\nThe number of warmup steps can make the difference between stable convergence and failure in the first few hundred iterations.\n\nInstead of guessing, sweeps let you see patterns. For instance, you might discover that 3e-4 is optimal for GPT-2 124M, but GPT-2 355M prefers 2e-4.\n\n\nExample of a Sweep in Practice\nSuppose you want to test three learning rates and two batch sizes. You can write a nested loop:\nfor lr in 3e-4 2e-4 1e-4\ndo\n    for B in 4 8\n    do\n        echo \"Running with lr=$lr and batch_size=$B\"\n        ./train_gpt2 -lr $lr -B $B -steps 500 &gt; logs/lr_${lr}_B${B}.txt\n    done\ndone\nAfterward, you could open the logs and compare validation loss at the end of each run. This gives you data-driven evidence about what works best.\n\n\nWhy It Matters\nHyperparameter sweeps are a cornerstone of practical machine learning. Even though llm.c is a minimalist project, the ability to quickly test and compare runs is essential. It transforms training from guesswork into an empirical process. You don’t just hope your model will converge—you verify it across multiple settings and pick the winner.\n\n\nTry It Yourself\n\nRun a sweep over learning rates [1e-3, 3e-4, 1e-4] for GPT-2 124M on Tiny Shakespeare. Which one converges fastest?\nTry sweeping over sequence length (T=32, 64, 128). How does it affect speed and loss?\nCompare runs with and without weight decay. Which generalizes better to the validation set?\nExtend the sweep to test different schedulers (cosine vs. step decay).\n\n\n\nThe Takeaway\nHyperparameter sweeps are the “experimentation muscle” of training LLMs. They teach you that no single setting works everywhere and that systematic testing is far more effective than intuition alone. With a simple script like sweep.sh, you can explore dozens of setups in a reproducible way and build confidence that your model is training as well as it can.\n\n\n\n98. Validating Evaluation and Loss Curves\nTraining logs—loss values printed after each step—are just numbers. To really understand whether your model is learning, you need to validate those numbers, plot them, and compare them across runs. This process of analyzing evaluation and loss curves is one of the most important skills in machine learning. It’s how you know whether your model is converging, overfitting, or failing entirely.\n\nTraining Loss vs. Validation Loss\nThere are two kinds of loss curves to pay attention to:\n\nTraining loss: computed on the batches the model actually sees during training.\nValidation loss: computed on a held-out dataset (like *_val.bin in llm.c).\n\nTraining loss almost always goes down steadily. Validation loss is the real test: if it decreases alongside training loss, the model is learning useful patterns. If it stalls or increases while training loss keeps dropping, the model is overfitting.\n\n\nPlotting Loss Curves\nEven though llm.c is a pure C project, you can redirect its training logs to a file and then use tools like Python + matplotlib to visualize. For example:\n./train_gpt2 &gt; logs/run1.txt\nThen parse logs/run1.txt in Python:\nimport re, matplotlib.pyplot as plt\n\nsteps, train_loss = [], []\nfor line in open(\"logs/run1.txt\"):\n    match = re.match(r\"step (\\d+): train loss ([0-9.]+)\", line)\n    if match:\n        steps.append(int(match.group(1)))\n        train_loss.append(float(match.group(2)))\n\nplt.plot(steps, train_loss, label=\"train loss\")\nplt.legend()\nplt.show()\nAdding validation loss to the same plot makes it even more useful: you’ll see both curves and their relationship over time.\n\n\nWhat a Healthy Curve Looks Like\n\nEarly phase: Both training and validation loss drop quickly.\nMiddle phase: Training loss continues downward, validation loss drops more slowly.\nLate phase: Training loss may keep decreasing, but validation loss stabilizes or begins to rise. That’s the point of overfitting.\n\nFor example, a good curve might look like this:\nstep 0: train loss 6.92, val loss 6.85\nstep 100: train loss 4.31, val loss 4.52\nstep 200: train loss 3.78, val loss 4.01\nstep 500: train loss 2.91, val loss 3.95\nTraining loss keeps going down, but validation loss plateaus around step 500. That’s a signal to stop training or adjust hyperparameters.\n\n\nWhy It Matters\nLoss curves are your window into model behavior. They reveal whether your model is underfitting (loss too high), overfitting (gap between train and val too large), or training stably (both losses decreasing together). Without them, you’re flying blind—just staring at numbers without context.\n\n\nTry It Yourself\n\nTrain GPT-2 124M for 500 steps on Tiny Shakespeare and plot both training and validation loss.\nReduce the dataset size by half. Watch how validation loss worsens earlier due to overfitting.\nRun two experiments with different learning rates. Plot both curves and compare stability.\nExtend plotting to multiple runs (e.g., GPT-2 124M vs. 355M) to see scaling effects.\n\n\n\nThe Takeaway\nValidating evaluation and loss curves turns raw logs into insight. It helps you decide when to stop training, how to tune hyperparameters, and whether scaling up is worth it. In llm.c, even though the project is minimal, capturing and plotting these curves is the single most effective way to understand what’s happening inside your model.\n\n\n\n99. Future Work: Kernel Library, Less cuDNN Dependence\nThe CUDA path in llm.c already uses cuBLAS and cuDNN, NVIDIA’s high-performance math libraries, to handle the heavy lifting of matrix multiplications and attention operations. These libraries are battle-tested and extremely fast, but they also act as a “black box”: you call into them, they do the work, and you get results without seeing what’s inside. While this is convenient, it limits flexibility and makes it harder to experiment with novel optimizations.\nThat’s why one of the most exciting areas of future work is building a lightweight custom kernel library for llm.c. This would mean replacing parts of cuDNN with hand-written CUDA kernels for operations like attention, normalization, and activation functions.\n\nWhy Reduce cuDNN Dependence?\n\nTransparency: With custom kernels, you see exactly how operations are implemented, which is great for learning and debugging.\nFlexibility: You can experiment with new ideas (e.g., alternative attention mechanisms, sparsity tricks) without waiting for cuDNN support.\nPortability: cuDNN is NVIDIA-specific. A custom kernel library could make it easier to port llm.c to other backends, like AMD GPUs (HIP) or even Metal for Apple silicon.\nPerformance Tuning: For small to medium models, hand-tuned kernels can sometimes outperform generic library calls because they’re tailored to the workload.\n\n\n\nWhat a Kernel Library Might Include\nA first version of a kernel library for llm.c might implement:\n\nMatrix multiplication (the workhorse of transformers) using tiling and shared memory.\nSoftmax kernels with numerical stability built in.\nLayerNorm kernels for both forward and backward passes.\nAttention kernels that integrate matmul + masking + softmax in one fused operation.\nActivation functions like GELU or ReLU in fused forms.\n\nFor example, a very simplified CUDA kernel for vector addition might look like this:\n__global__ void vec_add(float* a, float* b, float* c, int N) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i &lt; N) {\n        c[i] = a[i] + b[i];\n    }\n}\nWhile trivial, this illustrates the idea: instead of calling a library, you write the math yourself and control how threads are launched and memory is accessed.\n\n\nThe Path Forward\nDeveloping a kernel library is a long-term effort. It requires profiling, benchmarking, and iterative tuning. At first, custom kernels may be slower than cuDNN, but over time, they can evolve into a compact, educational library of building blocks for transformer training.\n\n\nWhy It Matters\nReducing dependence on cuDNN isn’t just about performance—it’s about control and portability. By having your own kernels, you gain the freedom to run llm.c on more platforms, test new research ideas, and understand exactly what’s happening inside the GPU. For a minimal, educational project like this one, that’s a natural next step.\n\n\nTry It Yourself\n\nWrite a simple CUDA kernel for vector addition, like the one above, and call it from a C program. Compare its performance to cublasSaxpy.\nReplace one small piece of llm.c (e.g., softmax) with a custom kernel. Check if the outputs match cuDNN.\nBenchmark a custom kernel against cuDNN on a small input size. Does cuDNN still dominate?\nTry porting your kernel to HIP (for AMD) or Metal (for Apple GPUs).\n\n\n\nThe Takeaway\nBuilding a kernel library is about moving from consumer of black-box libraries to creator of transparent, flexible tools. It’s a lot of work, but it transforms llm.c into not just a project for using LLMs, but also a platform for learning the deep internals of GPU programming. By reducing cuDNN dependence, you open the door to true end-to-end control of model training.\n\n\n\n100. Community, GitHub Discussions, and Suggested Learning Path\nLarge language models are complicated, but one of the goals of llm.c is to make them accessible. The code is clean, minimal, and approachable—but learning doesn’t stop at reading code. The broader community around llm.c plays a huge role in helping people understand, experiment, and grow. This section highlights where to connect with others, how to contribute, and how to build your own learning journey.\n\nGitHub as the Hub\nThe central place for llm.c discussions is the GitHub repository. There, you’ll find:\n\nIssues: where users ask questions, report bugs, or propose improvements.\nDiscussions: an open forum for sharing results, asking “how do I…?” questions, and comparing training logs.\nPull Requests: contributions ranging from bug fixes to new features, often with valuable code reviews.\n\nEven just browsing issues and discussions can be an educational experience. Many questions you might have—about CUDA errors, dataset preparation, or optimizer quirks—have already been asked and answered.\n\n\nThe Value of Community\nWorking alone on language models can feel overwhelming. By engaging with the community, you:\n\nSee how others run experiments with different datasets and hardware.\nLearn troubleshooting strategies for common problems (e.g., out-of-memory errors).\nGet inspiration for extensions—like custom kernels, new optimizers, or non-GPT architectures.\nFind collaborators for experiments that go beyond what one person can do.\n\n\n\nSuggested Learning Path\nBecause llm.c is minimal, it works well as a self-study tool. Here’s a suggested path to build up your knowledge:\n\nStart Small\n\nTrain GPT-2 124M on Tiny Shakespeare using CPU-only mode.\nInspect training logs and watch how loss decreases.\nGenerate text and see how quickly the model memorizes.\n\nStep Into CUDA\n\nSwitch to train_gpt2.cu and train with GPU acceleration.\nTry mixed precision (FP16/BF16) and observe memory savings.\n\nScale Up\n\nAttempt GPT-2 355M or 774M on your hardware (or cloud GPUs).\nLearn how to use gradient accumulation and checkpointing.\n\nExperiment\n\nModify the training loop: try new schedulers, tweak optimizer hyperparameters.\nAdd your own datasets (e.g., your personal text corpus).\n\nExplore Internals\n\nStep through forward and backward passes in train_gpt2.c.\nWrite small experiments to isolate key concepts (e.g., LayerNorm).\n\nJoin the Discussion\n\nShare your results on GitHub Discussions.\nContribute improvements, even small ones—like documentation fixes.\n\n\n\n\nWhy It Matters\nThe journey of learning LLM internals isn’t just about reading code—it’s about active practice, asking questions, and comparing experiences with others. Community provides the feedback loop that accelerates learning and keeps motivation alive.\n\n\nTry It Yourself\n\nClone the llm.c repository and explore open issues. Can you answer one for someone else?\nRun a training experiment and share your loss curve in GitHub Discussions.\nContribute a small improvement (like a new dataset script) as a pull request.\nCreate your own “learning log” to track experiments, much like a public notebook.\n\n\n\nThe Takeaway\nllm.c isn’t just a codebase—it’s an invitation to join a learning community. By engaging with GitHub, trying experiments, and sharing your results, you move from passive reader to active participant. That’s where the deepest understanding comes from: learning together, not alone.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Book</span>"
    ]
  }
]