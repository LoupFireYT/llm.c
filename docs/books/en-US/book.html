<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.23">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>The Book – The Little Book of llm.c</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../index.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-1fe81d0376b2c50856e68e651e390326.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-27c261d06b905028a18691de25d09dde.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../books/en-US/book.html"><span class="chapter-title">The Book</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">The Little Book of llm.c</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Content</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../books/en-US/book.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">The Book</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#chapter-1.-orientation" id="toc-chapter-1.-orientation" class="nav-link active" data-scroll-target="#chapter-1.-orientation">Chapter 1. Orientation</a>
  <ul class="collapse">
  <li><a href="#what-llm.c-is" id="toc-what-llm.c-is" class="nav-link" data-scroll-target="#what-llm.c-is">1. What <em>llm.c</em> Is</a></li>
  <li><a href="#repository-tour" id="toc-repository-tour" class="nav-link" data-scroll-target="#repository-tour">2. Repository Tour</a></li>
  <li><a href="#makefile-targets-flags" id="toc-makefile-targets-flags" class="nav-link" data-scroll-target="#makefile-targets-flags">3. Makefile Targets &amp; Flags</a></li>
  <li><a href="#quickstart-cpu-reference-path-train_gpt2.c" id="toc-quickstart-cpu-reference-path-train_gpt2.c" class="nav-link" data-scroll-target="#quickstart-cpu-reference-path-train_gpt2.c">4. Quickstart: CPU Reference Path (<code>train_gpt2.c</code>)</a></li>
  <li><a href="#quickstart-1-gpu-legacy-path-train_gpt2_fp32.cu" id="toc-quickstart-1-gpu-legacy-path-train_gpt2_fp32.cu" class="nav-link" data-scroll-target="#quickstart-1-gpu-legacy-path-train_gpt2_fp32.cu">5. Quickstart: 1-GPU Legacy Path (<code>train_gpt2_fp32.cu</code>)</a></li>
  <li><a href="#quickstart-modern-cuda-path-train_gpt2.cu" id="toc-quickstart-modern-cuda-path-train_gpt2.cu" class="nav-link" data-scroll-target="#quickstart-modern-cuda-path-train_gpt2.cu">6. Quickstart: Modern CUDA Path (<code>train_gpt2.cu</code>)</a></li>
  <li><a href="#starter-artifacts-data-prep-devdownload_starter_pack.sh-devdata" id="toc-starter-artifacts-data-prep-devdownload_starter_pack.sh-devdata" class="nav-link" data-scroll-target="#starter-artifacts-data-prep-devdownload_starter_pack.sh-devdata">7. Starter Artifacts &amp; Data Prep (<code>dev/download_starter_pack.sh</code>, <code>dev/data/</code>)</a></li>
  <li><a href="#debugging-tips-ide-stepping--g" id="toc-debugging-tips-ide-stepping--g" class="nav-link" data-scroll-target="#debugging-tips-ide-stepping--g">8. Debugging Tips &amp; IDE Stepping (<code>-g</code>)</a></li>
  <li><a href="#project-constraints-readability-contract" id="toc-project-constraints-readability-contract" class="nav-link" data-scroll-target="#project-constraints-readability-contract">9. Project Constraints &amp; Readability Contract</a></li>
  <li><a href="#community-discussions-and-learning-path" id="toc-community-discussions-and-learning-path" class="nav-link" data-scroll-target="#community-discussions-and-learning-path">10. Community, Discussions, and Learning Path</a></li>
  </ul></li>
  <li><a href="#chapter-2.-data-tokenization-and-loaders" id="toc-chapter-2.-data-tokenization-and-loaders" class="nav-link" data-scroll-target="#chapter-2.-data-tokenization-and-loaders">Chapter 2. Data, Tokenization, and Loaders</a>
  <ul class="collapse">
  <li><a href="#gpt-2-tokenizer-artifacts-gpt2_tokenizer.bin" id="toc-gpt-2-tokenizer-artifacts-gpt2_tokenizer.bin" class="nav-link" data-scroll-target="#gpt-2-tokenizer-artifacts-gpt2_tokenizer.bin">11. GPT-2 Tokenizer Artifacts (<code>gpt2_tokenizer.bin</code>)</a></li>
  <li><a href="#binary-dataset-format-train.bin-and-val.bin" id="toc-binary-dataset-format-train.bin-and-val.bin" class="nav-link" data-scroll-target="#binary-dataset-format-train.bin-and-val.bin">12. Binary Dataset Format (<code>train.bin</code> and <code>val.bin</code>)</a></li>
  <li><a href="#dataset-scripts-in-devdata" id="toc-dataset-scripts-in-devdata" class="nav-link" data-scroll-target="#dataset-scripts-in-devdata">13. Dataset Scripts in <code>dev/data/</code></a></li>
  <li><a href="#dataloader-design-batching-strides-epochs" id="toc-dataloader-design-batching-strides-epochs" class="nav-link" data-scroll-target="#dataloader-design-batching-strides-epochs">14. DataLoader Design (Batching, Strides, Epochs)</a></li>
  <li><a href="#evalloader-and-validation-workflow" id="toc-evalloader-and-validation-workflow" class="nav-link" data-scroll-target="#evalloader-and-validation-workflow">15. EvalLoader and Validation Workflow</a></li>
  <li><a href="#sequence-length-and-memory-budgeting" id="toc-sequence-length-and-memory-budgeting" class="nav-link" data-scroll-target="#sequence-length-and-memory-budgeting">16. Sequence Length and Memory Budgeting</a></li>
  <li><a href="#reproducibility-and-seeding-across-runs" id="toc-reproducibility-and-seeding-across-runs" class="nav-link" data-scroll-target="#reproducibility-and-seeding-across-runs">17. Reproducibility and Seeding Across Runs</a></li>
  <li><a href="#error-surfaces-from-bad-data-bounds-asserts" id="toc-error-surfaces-from-bad-data-bounds-asserts" class="nav-link" data-scroll-target="#error-surfaces-from-bad-data-bounds-asserts">18. Error Surfaces from Bad Data (Bounds, Asserts)</a></li>
  <li><a href="#tokenization-edge-cases-unks-eos-bos" id="toc-tokenization-edge-cases-unks-eos-bos" class="nav-link" data-scroll-target="#tokenization-edge-cases-unks-eos-bos">19. Tokenization Edge Cases (UNKs, EOS, BOS)</a></li>
  <li><a href="#data-hygiene-and-logging" id="toc-data-hygiene-and-logging" class="nav-link" data-scroll-target="#data-hygiene-and-logging">20. Data Hygiene and Logging</a></li>
  </ul></li>
  <li><a href="#chapter-3.-model-definition-and-weights" id="toc-chapter-3.-model-definition-and-weights" class="nav-link" data-scroll-target="#chapter-3.-model-definition-and-weights">Chapter 3. Model Definition and Weights</a>
  <ul class="collapse">
  <li><a href="#gpt-2-config-vocab-layers-heads-channels" id="toc-gpt-2-config-vocab-layers-heads-channels" class="nav-link" data-scroll-target="#gpt-2-config-vocab-layers-heads-channels">21. GPT-2 Config: Vocab, Layers, Heads, Channels</a></li>
  <li><a href="#parameter-tensors-and-memory-layout" id="toc-parameter-tensors-and-memory-layout" class="nav-link" data-scroll-target="#parameter-tensors-and-memory-layout">22. Parameter Tensors and Memory Layout</a></li>
  <li><a href="#embedding-tables-token-positional" id="toc-embedding-tables-token-positional" class="nav-link" data-scroll-target="#embedding-tables-token-positional">23. Embedding Tables: Token + Positional</a></li>
  <li><a href="#attention-stack-qkv-projections-and-geometry" id="toc-attention-stack-qkv-projections-and-geometry" class="nav-link" data-scroll-target="#attention-stack-qkv-projections-and-geometry">24. Attention Stack: QKV Projections and Geometry</a></li>
  <li><a href="#mlp-block-linear-layers-activation" id="toc-mlp-block-linear-layers-activation" class="nav-link" data-scroll-target="#mlp-block-linear-layers-activation">25. MLP Block: Linear Layers + Activation</a></li>
  <li><a href="#layernorm-theory-and-implementation-doclayernorm" id="toc-layernorm-theory-and-implementation-doclayernorm" class="nav-link" data-scroll-target="#layernorm-theory-and-implementation-doclayernorm">26. LayerNorm: Theory and Implementation (<code>doc/layernorm</code>)</a></li>
  <li><a href="#residual-connections-keeping-the-signal-flowing" id="toc-residual-connections-keeping-the-signal-flowing" class="nav-link" data-scroll-target="#residual-connections-keeping-the-signal-flowing">27. Residual Connections: Keeping the Signal Flowing</a></li>
  <li><a href="#attention-masking-enforcing-causality" id="toc-attention-masking-enforcing-causality" class="nav-link" data-scroll-target="#attention-masking-enforcing-causality">28. Attention Masking: Enforcing Causality</a></li>
  <li><a href="#output-head-from-hidden-states-to-vocabulary" id="toc-output-head-from-hidden-states-to-vocabulary" class="nav-link" data-scroll-target="#output-head-from-hidden-states-to-vocabulary">29. Output Head: From Hidden States to Vocabulary</a></li>
  <li><a href="#loss-function-cross-entropy-over-vocabulary" id="toc-loss-function-cross-entropy-over-vocabulary" class="nav-link" data-scroll-target="#loss-function-cross-entropy-over-vocabulary">30. Loss Function: Cross-Entropy over Vocabulary</a></li>
  </ul></li>
  <li><a href="#chapter-4.-cpu-inference-forward-only" id="toc-chapter-4.-cpu-inference-forward-only" class="nav-link" data-scroll-target="#chapter-4.-cpu-inference-forward-only">Chapter 4. CPU Inference (Forward Only)</a>
  <ul class="collapse">
  <li><a href="#forward-pass-walkthrough" id="toc-forward-pass-walkthrough" class="nav-link" data-scroll-target="#forward-pass-walkthrough">31. Forward Pass Walkthrough</a></li>
  <li><a href="#token-and-positional-embedding-lookup" id="toc-token-and-positional-embedding-lookup" class="nav-link" data-scroll-target="#token-and-positional-embedding-lookup">32. Token and Positional Embedding Lookup</a></li>
  <li><a href="#attention-matmuls-masking-and-softmax-on-cpu" id="toc-attention-matmuls-masking-and-softmax-on-cpu" class="nav-link" data-scroll-target="#attention-matmuls-masking-and-softmax-on-cpu">33. Attention: Matmuls, Masking, and Softmax on CPU</a></li>
  <li><a href="#mlp-gemms-and-activation-functions" id="toc-mlp-gemms-and-activation-functions" class="nav-link" data-scroll-target="#mlp-gemms-and-activation-functions">34. MLP: GEMMs and Activation Functions</a></li>
  <li><a href="#layernorm-on-cpu-step-by-step" id="toc-layernorm-on-cpu-step-by-step" class="nav-link" data-scroll-target="#layernorm-on-cpu-step-by-step">35. LayerNorm on CPU (Step-by-Step)</a></li>
  <li><a href="#residual-adds-and-signal-flow" id="toc-residual-adds-and-signal-flow" class="nav-link" data-scroll-target="#residual-adds-and-signal-flow">36. Residual Adds and Signal Flow</a></li>
  <li><a href="#cross-entropy-loss-on-cpu" id="toc-cross-entropy-loss-on-cpu" class="nav-link" data-scroll-target="#cross-entropy-loss-on-cpu">37. Cross-Entropy Loss on CPU</a></li>
  <li><a href="#putting-it-all-together-the-gpt2_forward-function" id="toc-putting-it-all-together-the-gpt2_forward-function" class="nav-link" data-scroll-target="#putting-it-all-together-the-gpt2_forward-function">38. Putting It All Together: The <code>gpt2_forward</code> Function</a></li>
  <li><a href="#openmp-pragmas-for-parallel-loops" id="toc-openmp-pragmas-for-parallel-loops" class="nav-link" data-scroll-target="#openmp-pragmas-for-parallel-loops">39. OpenMP Pragmas for Parallel Loops</a></li>
  <li><a href="#cpu-memory-footprint-and-performance" id="toc-cpu-memory-footprint-and-performance" class="nav-link" data-scroll-target="#cpu-memory-footprint-and-performance">40. CPU Memory Footprint and Performance</a></li>
  </ul></li>
  <li><a href="#chapter-5.-training-loop-cpu-path" id="toc-chapter-5.-training-loop-cpu-path" class="nav-link" data-scroll-target="#chapter-5.-training-loop-cpu-path">Chapter 5. Training Loop (CPU Path)</a>
  <ul class="collapse">
  <li><a href="#backward-pass-walkthrough" id="toc-backward-pass-walkthrough" class="nav-link" data-scroll-target="#backward-pass-walkthrough">41. Backward Pass Walkthrough</a></li>
  <li><a href="#skeleton-of-training-loop" id="toc-skeleton-of-training-loop" class="nav-link" data-scroll-target="#skeleton-of-training-loop">42. Skeleton of Training Loop</a></li>
  <li><a href="#adamw-implementation-in-c" id="toc-adamw-implementation-in-c" class="nav-link" data-scroll-target="#adamw-implementation-in-c">43. AdamW Implementation in C</a></li>
  <li><a href="#gradient-accumulation-and-micro-batching" id="toc-gradient-accumulation-and-micro-batching" class="nav-link" data-scroll-target="#gradient-accumulation-and-micro-batching">44. Gradient Accumulation and Micro-Batching</a></li>
  <li><a href="#logging-and-progress-reporting" id="toc-logging-and-progress-reporting" class="nav-link" data-scroll-target="#logging-and-progress-reporting">45. Logging and Progress Reporting</a></li>
  <li><a href="#validation-runs-in-the-training-loop" id="toc-validation-runs-in-the-training-loop" class="nav-link" data-scroll-target="#validation-runs-in-the-training-loop">46. Validation Runs in the Training Loop</a></li>
  <li><a href="#checkpointing-parameters-and-optimizer-state" id="toc-checkpointing-parameters-and-optimizer-state" class="nav-link" data-scroll-target="#checkpointing-parameters-and-optimizer-state">47. Checkpointing Parameters and Optimizer State</a></li>
  <li><a href="#reproducibility-and-small-divergences" id="toc-reproducibility-and-small-divergences" class="nav-link" data-scroll-target="#reproducibility-and-small-divergences">48. Reproducibility and Small Divergences</a></li>
  <li><a href="#command-line-flags-and-defaults" id="toc-command-line-flags-and-defaults" class="nav-link" data-scroll-target="#command-line-flags-and-defaults">49. Command-Line Flags and Defaults</a></li>
  <li><a href="#example-cpu-training-logs-and-outputs" id="toc-example-cpu-training-logs-and-outputs" class="nav-link" data-scroll-target="#example-cpu-training-logs-and-outputs">50. Example CPU Training Logs and Outputs</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">The Book</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Small</span> file, giant dream,</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">llm.c</span> whispers tokens,</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="ex">worlds</span> unfold in text.</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<section id="chapter-1.-orientation" class="level2">
<h2 class="anchored" data-anchor-id="chapter-1.-orientation">Chapter 1. Orientation</h2>
<section id="what-llm.c-is" class="level3">
<h3 class="anchored" data-anchor-id="what-llm.c-is">1. What <em>llm.c</em> Is</h3>
<p>Imagine you wanted to peek inside a modern AI model-not by reading thousands of lines of optimized C++ or CUDA hidden inside a giant framework, but by opening a small, neat folder and seeing the entire training pipeline laid out in front of you. That is what <em>llm.c</em> gives you.</p>
<p>At its heart, <em>llm.c</em> is a reference implementation of how to train and run a GPT-2 style language model, written in pure C (and CUDA). The key word is <em>reference</em>: the code is meant to be minimal, readable, and educational. You don’t need to wade through abstraction layers or device-specific macros. Instead, you get a version that looks almost like pseudocode, but still compiles and runs on your computer.</p>
<section id="why-this-project-exists" class="level4">
<h4 class="anchored" data-anchor-id="why-this-project-exists">Why This Project Exists</h4>
<p>Deep learning frameworks like PyTorch and TensorFlow are amazing for getting models to work quickly, but they hide most of the actual mechanics. Under the hood, there’s a lot happening: tensors are allocated in memory, gradients are computed through backpropagation, optimizer states are updated, and schedulers adjust the learning rate. Most of us never see those details, because the framework handles them for us.</p>
<p><em>llm.c</em> flips this around. It says: <em>what if we removed the black box and showed you exactly how a GPT-2 model is trained, line by line?</em> It’s not about speed or production deployment. It’s about clarity, education, and demystifying how large language models work.</p>
</section>
<section id="key-characteristics" class="level4">
<h4 class="anchored" data-anchor-id="key-characteristics">Key Characteristics</h4>
<ul>
<li>Minimalism: The CPU version (<code>train_gpt2.c</code>) avoids complicated optimizations so that beginners can follow the logic. Even the CUDA version tries to stay simple, with only necessary calls to cuBLAS/cuDNN.</li>
<li>Self-contained: No external frameworks. The code defines its own tokenizer, dataloader, optimizer, and scheduler. Everything you need is in the repository.</li>
<li>Parallels to PyTorch: Each function in the C/CUDA implementation has a counterpart in PyTorch. The repo even ships with Python test files to prove that the outputs match within tolerance.</li>
<li>Step-by-step scalability: You can start with a tiny model on CPU and, once you understand the basics, switch to GPU, multi-GPU, or even multi-node training. The structure remains the same, just faster.</li>
</ul>
</section>
<section id="what-you-can-do-with-it" class="level4">
<h4 class="anchored" data-anchor-id="what-you-can-do-with-it">What You Can Do With It</h4>
<ol type="1">
<li>Train GPT-2 from scratch: Start with a small dataset (like Tiny Shakespeare) and see the model learn patterns in language.</li>
<li>Experiment with configurations: Change number of layers, sequence length, or hidden size, then watch how memory and training time scale.</li>
<li>Learn GPU training internals: Move from CPU to CUDA, and later to multi-GPU with MPI/NCCL, to see how real distributed training works under the hood.</li>
<li>Profile performance: The repo includes profiling tools so you can measure FLOPs, memory bandwidth, and kernel execution times.</li>
<li>Reproduce big models: With enough hardware, you can actually retrain GPT-2 124M or larger versions, using the exact same setup described in the README.</li>
</ol>
</section>
<section id="why-you-should-care" class="level4">
<h4 class="anchored" data-anchor-id="why-you-should-care">Why You Should Care</h4>
<p>If you’ve ever used a large language model and thought <em>“this feels like magic”</em>, <em>llm.c</em> is your chance to peel back the curtain. You’ll see:</p>
<ul>
<li>How text is converted into numbers (tokenization).</li>
<li>How those numbers are fed into matrices and transformed.</li>
<li>How attention actually looks in code.</li>
<li>How gradients are computed and weights updated.</li>
<li>How scaling from one GPU to many is managed.</li>
</ul>
<p>It’s not magic anymore-it’s math plus code you can read.</p>
</section>
<section id="the-big-picture" class="level4">
<h4 class="anchored" data-anchor-id="the-big-picture">The Big Picture</h4>
<p>Think of <em>llm.c</em> as a teaching lab for modern AI:</p>
<ul>
<li>On day one, you can run a small demo on CPU and see loss values dropping as the model learns.</li>
<li>On day ten, you might be profiling CUDA kernels or debugging NCCL synchronization across GPUs.</li>
<li>At every step, the code remains approachable because it avoids unnecessary complexity.</li>
</ul>
<p>This philosophy makes <em>llm.c</em> unique: it’s not trying to be the fastest or the most feature-rich. It’s trying to be the clearest window into how large language models actually work when stripped down to their essentials.</p>
<p>The Takeaway: <em>llm.c</em> is a minimal, transparent re-implementation of GPT-2 training and inference, designed to teach you how language models operate under the hood. It’s not about production performance-it’s about giving you a guided tour through the machinery of modern AI, one C file at a time.</p>
<p>Would you like me to now expand Section 2 (“Repository Tour”) in the same friendly beginner tone, showing what’s inside each folder and file?</p>
</section>
</section>
<section id="repository-tour" class="level3">
<h3 class="anchored" data-anchor-id="repository-tour">2. Repository Tour</h3>
<p>When you first open the <em>llm.c</em> repository, it may look like a random collection of <code>.c</code>, <code>.cu</code>, <code>.py</code>, and shell scripts. But there’s a clear structure hiding underneath. Each part has a purpose, and once you know where things live, navigating the repo feels natural. Let’s take a guided walk through the main pieces.</p>
<section id="root-directory-the-entry-points" class="level4">
<h4 class="anchored" data-anchor-id="root-directory-the-entry-points">Root Directory: The Entry Points</h4>
<p>At the top level, you’ll find the core training programs. These are the files you actually compile and run:</p>
<ul>
<li><code>train_gpt2.c</code> - The CPU reference implementation. This is the simplest, most readable version of GPT-2 training. It avoids special optimizations so you can follow the math and logic step by step.</li>
<li><code>train_gpt2.cu</code> - The CUDA implementation. Faster, uses GPU kernels, cuBLAS, and optional cuDNN FlashAttention. This is the version you’d use for serious training runs.</li>
<li><code>train_gpt2_fp32.cu</code> - A legacy CUDA path, using plain FP32 precision instead of mixed precision. It’s slower but useful as a debugging baseline.</li>
<li><code>train_gpt2.py</code> - The PyTorch reference. This is the oracle: a tiny script in Python/PyTorch that trains the same GPT-2 so you can compare outputs and verify correctness.</li>
</ul>
<p>Other important root-level files:</p>
<ul>
<li><code>Makefile</code> - Defines how to build different versions. Targets like <code>make train_gpt2</code> or <code>make train_gpt2cu</code> are your entry points.</li>
<li><code>README.md</code> - The main guide for running experiments, installing dependencies, and reproducing models.</li>
</ul>
</section>
<section id="llmc-directory-utilities-and-building-blocks" class="level4">
<h4 class="anchored" data-anchor-id="llmc-directory-utilities-and-building-blocks"><code>llmc/</code> Directory: Utilities and Building Blocks</h4>
<p>This folder holds reusable C utilities that the main training files include:</p>
<ul>
<li><code>utils.h</code> - Safety wrappers (<code>fopenCheck</code>, <code>mallocCheck</code>) and helper functions.</li>
<li><code>tokenizer.h</code> - Implements GPT-2’s tokenizer in C: encoding text into token IDs and decoding back to text.</li>
<li><code>dataloader.h</code> - Defines how training batches are loaded and served, handling dataset splits and iteration.</li>
<li><code>rand.h</code> - Random number utilities, mirroring PyTorch’s <code>manual_seed</code> and normal distributions.</li>
<li><code>schedulers.h</code> - Learning rate scheduling, like cosine decay with warmup.</li>
<li><code>sampler.h</code> - Implements softmax sampling for text generation and helper RNG.</li>
<li><code>logger.h</code> - Minimal logging functionality for tracking progress.</li>
</ul>
<p>Think of <code>llmc/</code> as the library that keeps the main files clean and readable. Instead of cluttering <code>train_gpt2.c</code> with helpers, everything is modularized here.</p>
</section>
<section id="dev-directory-scripts-and-extras" class="level4">
<h4 class="anchored" data-anchor-id="dev-directory-scripts-and-extras"><code>dev/</code> Directory: Scripts and Extras</h4>
<p>This folder is full of supporting tools that make experiments easier:</p>
<ul>
<li><code>dev/download_starter_pack.sh</code> - Fetches the GPT-2 124M weights, tokenizer, and datasets. This is the quickest way to get started.</li>
<li><code>dev/data/</code> - Contains scripts for preparing datasets like Tiny Shakespeare or OpenWebText in the binary format that <em>llm.c</em> expects.</li>
<li><code>dev/cuda/</code> - A place for experimenting with standalone CUDA kernels. This is where you’d go if you want to tinker with custom GPU code beyond the main trainer.</li>
</ul>
</section>
<section id="doc-directory-learning-resources" class="level4">
<h4 class="anchored" data-anchor-id="doc-directory-learning-resources"><code>doc/</code> Directory: Learning Resources</h4>
<p>Documentation that digs deeper into specific topics. For example:</p>
<ul>
<li><code>doc/layernorm/layernorm.md</code> - A tutorial-style explanation of Layer Normalization, complete with math and code. It helps you understand one of GPT-2’s core components before diving into the C implementation.</li>
</ul>
<p>This folder is a learning aid. Whenever a concept feels too dense, check here for a more gentle walkthrough.</p>
</section>
<section id="test-files" class="level4">
<h4 class="anchored" data-anchor-id="test-files">Test Files</h4>
<p>Testing is taken seriously in <em>llm.c</em>, because the goal is to prove that the C/CUDA implementation is correct compared to PyTorch:</p>
<ul>
<li><code>test_gpt2.c</code> - Runs forward passes and training steps on CPU and compares outputs to PyTorch.</li>
<li><code>test_gpt2cu.cu</code> - Same idea but for CUDA, including both FP32 and mixed-precision runs.</li>
</ul>
<p>These files keep everything honest: you can always verify that your build produces the same results as the canonical PyTorch model.</p>
</section>
<section id="profiling-tools" class="level4">
<h4 class="anchored" data-anchor-id="profiling-tools">Profiling Tools</h4>
<p>For performance deep dives:</p>
<ul>
<li><code>profile_gpt2.cu</code> - A CUDA profiling harness that benchmarks kernels and measures throughput.</li>
<li><code>profile_gpt2cu.py</code> - Python-side profiler for analyzing GPU utilization, memory bandwidth, and FLOPs.</li>
</ul>
<p>If you’re curious about where time is being spent in training, these files show you how to measure it.</p>
</section>
<section id="datasets-and-artifacts" class="level4">
<h4 class="anchored" data-anchor-id="datasets-and-artifacts">Datasets and Artifacts</h4>
<p>When you run <code>download_starter_pack.sh</code>, you’ll get:</p>
<ul>
<li><code>gpt2_tokenizer.bin</code> - GPT-2’s byte-pair encoding tokenizer, serialized in binary.</li>
<li>Dataset <code>.bin</code> files - Training and validation sets, tokenized and ready for the dataloader.</li>
</ul>
<p>These files are not in the repo by default but are downloaded or generated locally.</p>
</section>
<section id="putting-it-together" class="level4">
<h4 class="anchored" data-anchor-id="putting-it-together">Putting It Together</h4>
<p>The repository is structured like a teaching lab:</p>
<ul>
<li>Root files are the main experiments.</li>
<li><code>llmc/</code> is the library of building blocks.</li>
<li><code>dev/</code> provides extra tools and scripts.</li>
<li><code>doc/</code> explains tricky concepts in tutorial form.</li>
<li>Tests and profilers make sure everything matches PyTorch and runs efficiently.</li>
</ul>
<p>Once you see the pattern, the repo feels less intimidating. Every file has a role in telling the story of how a GPT-2 model is built from scratch in C and CUDA.</p>
</section>
</section>
<section id="makefile-targets-flags" class="level3">
<h3 class="anchored" data-anchor-id="makefile-targets-flags">3. Makefile Targets &amp; Flags</h3>
<p>Every C or CUDA program needs a build system, and in <em>llm.c</em> that role is handled by a simple but powerful Makefile. If you’ve never used <code>make</code> before, think of it as a recipe book: you type <code>make &lt;target&gt;</code> in your terminal, and it follows the instructions for compiling the code into an executable. In <em>llm.c</em>, this file is your control center for choosing which trainer to build, whether to enable GPUs, and which optional features to turn on.</p>
<section id="why-a-makefile" class="level4">
<h4 class="anchored" data-anchor-id="why-a-makefile">Why a Makefile?</h4>
<p>Instead of memorizing long <code>gcc</code> or <code>nvcc</code> compile commands with dozens of flags, the Makefile captures those instructions once and gives them a short name. For example, building the CPU trainer is as easy as:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">make</span> train_gpt2</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Behind the scenes, this calls <code>gcc</code>, sets optimization flags, includes the right headers, and links everything together. The same applies to CUDA builds with <code>nvcc</code>.</p>
</section>
<section id="core-targets" class="level4">
<h4 class="anchored" data-anchor-id="core-targets">Core Targets</h4>
<p>Here are the most important build targets you’ll find:</p>
<ul>
<li><code>train_gpt2</code> - Builds the CPU-only reference trainer. Uses <code>gcc</code> (or <code>clang</code>) and links against OpenMP for parallel loops.</li>
<li><code>train_gpt2cu</code> - Builds the CUDA trainer with mixed precision and optional cuDNN FlashAttention. Uses <code>nvcc</code>.</li>
<li><code>train_gpt2_fp32</code> - Builds the legacy CUDA trainer that stays in pure FP32 (slower but simpler).</li>
<li><code>test_gpt2</code> - Compiles the CPU test program to compare results against PyTorch.</li>
<li><code>test_gpt2cu</code> - Compiles the CUDA test program to check GPU parity with PyTorch.</li>
<li><code>profile_gpt2.cu</code> - Compiles the CUDA profiler harness, used to benchmark kernels and FLOPs.</li>
</ul>
<p>Each of these produces a binary you can run directly, for example:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./train_gpt2</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">./train_gpt2cu</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="ex">./test_gpt2</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="key-flags-you-can-toggle" class="level4">
<h4 class="anchored" data-anchor-id="key-flags-you-can-toggle">Key Flags You Can Toggle</h4>
<p>The Makefile also exposes several switches that let you customize the build. You set them when running <code>make</code>, like this:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">make</span> train_gpt2cu USE_CUDNN=1</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Here are the most important flags:</p>
<ul>
<li><code>USE_CUDNN</code> - Enables cuDNN FlashAttention if your system has cuDNN installed. This can give big speedups for attention, but it’s optional. By default, it’s off.</li>
<li><code>OMP=1</code> - Tells the CPU trainer to compile with OpenMP enabled. This allows multithreaded execution, making CPU runs much faster. Usually on by default if OpenMP is detected.</li>
<li><code>DEBUG=1</code> - Compiles with debugging symbols (<code>-g</code>) instead of maximum optimization. Useful when stepping through code in an IDE or using a debugger.</li>
<li><code>PROFILE=1</code> - Adds profiling hooks, helping you analyze execution time and performance.</li>
</ul>
</section>
<section id="optimization-choices" class="level4">
<h4 class="anchored" data-anchor-id="optimization-choices">Optimization Choices</h4>
<p>The default build uses <code>-O3</code> optimization, which makes the code run fast but sometimes harder to debug. If you’re just learning and want clarity, you can switch to:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">make</span> train_gpt2 DEBUG=1</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This creates a binary that runs slower but lets you step through line by line in a debugger. For performance benchmarking, stick with the optimized default.</p>
</section>
<section id="multi-gpu-and-mpi-support" class="level4">
<h4 class="anchored" data-anchor-id="multi-gpu-and-mpi-support">Multi-GPU and MPI Support</h4>
<p>When building the CUDA trainer, the Makefile can also link against MPI and NCCL if they’re installed. That’s what enables multi-GPU and multi-node training. You usually don’t need to change anything-the Makefile automatically detects these libraries and includes them if available.</p>
</section>
<section id="putting-it-all-together" class="level4">
<h4 class="anchored" data-anchor-id="putting-it-all-together">Putting It All Together</h4>
<p>Think of the Makefile as a switchboard for the whole project:</p>
<ul>
<li>Want to run the simple CPU demo? → <code>make train_gpt2</code></li>
<li>Want to train faster on GPU? → <code>make train_gpt2cu</code></li>
<li>Want to debug kernels? → <code>make train_gpt2cu DEBUG=1</code></li>
<li>Want to test parity with PyTorch? → <code>make test_gpt2</code> or <code>make test_gpt2cu</code></li>
</ul>
<p>With just a few keystrokes, you control whether you’re running a beginner-friendly CPU demo, a high-performance GPU build, or a debugging session.</p>
<p>The takeaway: The Makefile is your control center. It abstracts away complicated compiler commands and gives you a clean menu of options: CPU vs GPU, FP32 vs mixed precision, debug vs optimized, and single vs multi-GPU. Mastering it is the first step to feeling comfortable experimenting inside <em>llm.c</em>.</p>
</section>
</section>
<section id="quickstart-cpu-reference-path-train_gpt2.c" class="level3">
<h3 class="anchored" data-anchor-id="quickstart-cpu-reference-path-train_gpt2.c">4. Quickstart: CPU Reference Path (<code>train_gpt2.c</code>)</h3>
<p>The simplest way to begin exploring <em>llm.c</em> is with the CPU-only reference implementation. This file, <code>train_gpt2.c</code>, is deliberately designed to be minimal, readable, and approachable. It doesn’t hide complexity behind libraries or macros. Instead, it shows you exactly how a GPT-2 model is trained, step by step, using plain C and a sprinkle of OpenMP for speed.</p>
<section id="why-start-with-cpu" class="level4">
<h4 class="anchored" data-anchor-id="why-start-with-cpu">Why Start with CPU?</h4>
<ul>
<li>Clarity first: GPUs add layers of complexity (CUDA kernels, memory transfers, cuBLAS). On CPU, you can focus on the core algorithm without distraction.</li>
<li>Portability: Any machine with a C compiler can run it-no special hardware required.</li>
<li>Debuggability: Errors are easier to trace, and you can single-step through the code in an IDE.</li>
</ul>
<p>The CPU version is slower, but that’s a feature here-it forces you to really see what’s happening under the hood.</p>
</section>
<section id="building-the-cpu-trainer" class="level4">
<h4 class="anchored" data-anchor-id="building-the-cpu-trainer">Building the CPU Trainer</h4>
<p>From the root of the repository, you just type:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">make</span> train_gpt2</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This compiles <code>train_gpt2.c</code> into an executable named <code>train_gpt2</code>. If your system has OpenMP, the Makefile will detect it and add the right flags.</p>
</section>
<section id="running-your-first-training-run" class="level4">
<h4 class="anchored" data-anchor-id="running-your-first-training-run">Running Your First Training Run</h4>
<p>Before running, download the starter pack (tokenizer, dataset, configs):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./dev/download_starter_pack.sh</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Now launch training:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./train_gpt2</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>You’ll see output like:</p>
<pre><code>[GPT-2]
max_seq_len: 1024
vocab_size: 50257
padded_vocab_size: 50304
num_layers: 12
num_heads: 12
channels: 768
num_parameters: 124475904
train dataset num_batches: 1192
val dataset num_batches: 128
num_activations: 73347840
val loss 5.325529
step 0: train loss 4.677779 (took 1987.546000 ms)
step 1: train loss 5.191576 (took 1927.230000 ms)
...</code></pre>
<p>Each line tells you:</p>
<ul>
<li>Model size and config (sequence length, vocabulary size, layers, heads, channels).</li>
<li>Dataset stats (how many batches for training and validation).</li>
<li>Activation memory size (a measure of how big the intermediate states are).</li>
<li>Training progress (step number, train loss, validation loss, time per step).</li>
</ul>
</section>
<section id="inside-the-training-loop" class="level4">
<h4 class="anchored" data-anchor-id="inside-the-training-loop">Inside the Training Loop</h4>
<p>Although you don’t need to dive into the code yet, here’s the high-level flow in <code>train_gpt2.c</code>:</p>
<ol type="1">
<li><p>Load tokenizer and dataset → turns text into tokens.</p></li>
<li><p>Initialize model parameters → embeddings, attention weights, MLPs, norms.</p></li>
<li><p>For each batch:</p>
<ul>
<li>Forward pass → compute logits and loss.</li>
<li>Backward pass → compute gradients.</li>
<li>Update parameters → optimizer step.</li>
</ul></li>
<li><p>Log progress → print losses, occasionally run validation.</p></li>
</ol>
<p>This mirrors exactly what happens in PyTorch, just spelled out in C.</p>
</section>
<section id="performance-notes" class="level4">
<h4 class="anchored" data-anchor-id="performance-notes">Performance Notes</h4>
<p>On CPU, don’t expect speed. Training GPT-2 124M can take days or weeks. But that’s not the point. The CPU reference path is like a glass box: everything is visible, no shortcuts. You’ll use this to learn the mechanics and to verify that your GPU runs match the same results.</p>
<p>If you want to speed things up slightly, you can:</p>
<ul>
<li><p>Increase OpenMP threads:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="va">OMP_NUM_THREADS</span><span class="op">=</span>8 <span class="ex">./train_gpt2</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Use a smaller dataset (Tiny Shakespeare) to see faster progress.</p></li>
<li><p>Reduce model size by changing config values (fewer layers, smaller channels).</p></li>
</ul>
</section>
<section id="when-to-move-on" class="level4">
<h4 class="anchored" data-anchor-id="when-to-move-on">When to Move On</h4>
<p>Once you’re comfortable with how training looks on CPU-loss values going down, checkpoints being written, logs appearing-you’ll be ready to graduate to the GPU version (<code>train_gpt2.cu</code>). That’s where performance and scaling come in, but the CPU run gives you the conceptual foundation.</p>
<p>The takeaway: Running <code>train_gpt2.c</code> is your first hands-on encounter with GPT-2 training in <em>llm.c</em>. It’s slow, transparent, and designed for learning. You’ll see every piece of the model at work, one step at a time, before diving into the complexity of CUDA.</p>
</section>
</section>
<section id="quickstart-1-gpu-legacy-path-train_gpt2_fp32.cu" class="level3">
<h3 class="anchored" data-anchor-id="quickstart-1-gpu-legacy-path-train_gpt2_fp32.cu">5. Quickstart: 1-GPU Legacy Path (<code>train_gpt2_fp32.cu</code>)</h3>
<p>Once you’ve seen the CPU trainer in action, the natural next step is to try training on a GPU. The file <code>train_gpt2_fp32.cu</code> is the simplest GPU entry point. It predates the more advanced mixed-precision trainer (<code>train_gpt2.cu</code>), and it runs everything in full 32-bit floating point (FP32) precision. That makes it easier to follow and debug, even though it’s slower than modern approaches. Think of it as the “training wheels” for GPU training in <em>llm.c</em>.</p>
<section id="why-this-path-exists" class="level4">
<h4 class="anchored" data-anchor-id="why-this-path-exists">Why This Path Exists</h4>
<p>Modern GPU training almost always uses mixed precision (FP16/BF16 for speed and memory savings, FP32 for stability). But mixed precision introduces extra complexity: scaling losses, maintaining master weights, checking for overflows. For beginners, all that can be distracting.</p>
<p>The FP32 path avoids those complications:</p>
<ul>
<li>Every tensor (activations, weights, gradients) is stored as 32-bit floats.</li>
<li>No special handling of loss scaling is needed.</li>
<li>Debugging mismatches with PyTorch is straightforward.</li>
</ul>
<p>The trade-off is performance-this version runs significantly slower and uses more memory.</p>
</section>
<section id="building-the-fp32-cuda-trainer" class="level4">
<h4 class="anchored" data-anchor-id="building-the-fp32-cuda-trainer">Building the FP32 CUDA Trainer</h4>
<p>From the root of the repository:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">make</span> train_gpt2_fp32</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This invokes <code>nvcc</code> (the NVIDIA CUDA compiler) and links against cuBLAS for matrix multiplications. The output is an executable named <code>train_gpt2_fp32</code>.</p>
</section>
<section id="running-it" class="level4">
<h4 class="anchored" data-anchor-id="running-it">Running It</h4>
<p>Just like the CPU version, make sure you’ve downloaded the starter pack first:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./dev/download_starter_pack.sh</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then launch training on your GPU:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./train_gpt2_fp32</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>If CUDA is installed correctly, the program will detect your GPU and start training. You’ll see logs that look similar to the CPU trainer’s, but with much shorter step times. For example, a training step that took ~2 seconds on CPU might take ~50 milliseconds on GPU.</p>
</section>
<section id="under-the-hood" class="level4">
<h4 class="anchored" data-anchor-id="under-the-hood">Under the Hood</h4>
<p>Although the training loop looks the same on the surface, a lot changes under the hood when running on GPU:</p>
<ol type="1">
<li>Tensors are allocated in GPU memory (not system RAM).</li>
<li>Matrix multiplications (the core of attention and MLP layers) are executed by cuBLAS, NVIDIA’s high-performance linear algebra library.</li>
<li>Kernels for elementwise operations (like adding residuals, applying softmax, or normalizing) are written in CUDA or use built-in primitives.</li>
<li>Gradients and optimizer states are updated entirely on the device, with minimal CPU↔︎GPU transfers.</li>
</ol>
<p>This makes training dramatically faster, but the structure of the code is still recognizable compared to the CPU version.</p>
</section>
<section id="when-to-use-fp32-vs-mixed-precision" class="level4">
<h4 class="anchored" data-anchor-id="when-to-use-fp32-vs-mixed-precision">When to Use FP32 vs Mixed Precision</h4>
<ul>
<li><p>Use FP32 (this path) when:</p>
<ul>
<li>You’re learning how GPU training works step by step.</li>
<li>You want a clean comparison with the CPU trainer.</li>
<li>You’re debugging correctness issues without worrying about loss scaling.</li>
</ul></li>
<li><p>Use Mixed Precision (<code>train_gpt2.cu</code>) when:</p>
<ul>
<li>You want real performance (2–4× faster training).</li>
<li>You’re training larger models (774M, 1.6B parameters) where memory efficiency matters.</li>
<li>You’re aiming to reproduce published GPT-2 runs on modern GPUs.</li>
</ul></li>
</ul>
</section>
<section id="common-pitfalls" class="level4">
<h4 class="anchored" data-anchor-id="common-pitfalls">Common Pitfalls</h4>
<ol type="1">
<li>CUDA not installed → If <code>nvcc</code> isn’t found, the Makefile will fail. You’ll need the CUDA Toolkit installed.</li>
<li>Driver mismatch → Your NVIDIA driver must match the CUDA version.</li>
<li>Out of memory errors → FP32 uses more GPU memory, so you may need to lower batch size if you’re on a smaller GPU.</li>
</ol>
</section>
<section id="why-this-step-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-this-step-matters">Why This Step Matters</h4>
<p>The FP32 trainer is like a bridge:</p>
<ul>
<li>On one side is the CPU reference path, slow but crystal-clear.</li>
<li>On the other is the mixed-precision CUDA path, fast but more complex.</li>
</ul>
<p>By walking across this bridge, you learn how GPU acceleration works without being overwhelmed by optimizations.</p>
<p>The takeaway: <code>train_gpt2_fp32.cu</code> is your first taste of real GPU training in <em>llm.c</em>. It skips advanced tricks and shows you a clean, one-GPU, full-precision implementation. It’s not the fastest, but it’s the friendliest way to understand how training moves from CPU to GPU.</p>
</section>
</section>
<section id="quickstart-modern-cuda-path-train_gpt2.cu" class="level3">
<h3 class="anchored" data-anchor-id="quickstart-modern-cuda-path-train_gpt2.cu">6. Quickstart: Modern CUDA Path (<code>train_gpt2.cu</code>)</h3>
<p>This is the high-performance trainer most people use day to day. It runs on a single NVIDIA GPU (and also forms the basis for multi-GPU), uses mixed precision (FP16/BF16 where safe, FP32 where needed), and can optionally enable cuDNN FlashAttention for fast attention. Compared to the FP32 legacy path, it’s significantly faster and uses less memory, while keeping the training loop easy to follow.</p>
<section id="what-mixed-precision-means-in-plain-words" class="level4">
<h4 class="anchored" data-anchor-id="what-mixed-precision-means-in-plain-words">What “mixed precision” means (in plain words)</h4>
<ul>
<li>Weights &amp; activations: stored/processed in FP16 or BF16 for speed and lower memory.</li>
<li>Master weights: a FP32 copy of parameters kept for stable updates.</li>
<li>Loss scaling: multiply the loss before backward to avoid underflow; unscale the grads before the optimizer step.</li>
<li>Autocast-like behavior: the code picks safe dtypes for each op (GEMMs in tensor cores, reductions in FP32, etc.).</li>
</ul>
<p>You get 2–4× speedups on many GPUs, and the same final accuracy when configured properly.</p>
</section>
<section id="build-the-modern-cuda-trainer" class="level4">
<h4 class="anchored" data-anchor-id="build-the-modern-cuda-trainer">Build the modern CUDA trainer</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">make</span> train_gpt2cu</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Common variants:</p>
<ul>
<li><p>With cuDNN FlashAttention (if available):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">make</span> train_gpt2cu USE_CUDNN=1</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>With debug symbols (slower, but easier to step through):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">make</span> train_gpt2cu DEBUG=1</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ul>
<p>This produces an executable named <code>train_gpt2cu</code>.</p>
</section>
<section id="one-time-data-artifacts" class="level4">
<h4 class="anchored" data-anchor-id="one-time-data-artifacts">One-time data &amp; artifacts</h4>
<p>If you haven’t already:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./dev/download_starter_pack.sh</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This fetches the tokenizer and a small dataset so you can run immediately.</p>
</section>
<section id="run-your-first-gpu-training-session" class="level4">
<h4 class="anchored" data-anchor-id="run-your-first-gpu-training-session">Run your first GPU training session</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./train_gpt2cu</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>You should see a config header (model dims, vocab, sequence length) followed by step-by-step loss prints. Step times will be much shorter than CPU and noticeably faster than FP32, especially on tensor-core GPUs (Turing and newer).</p>
<p>Speed tips right away:</p>
<ul>
<li><p>Use a larger global batch if memory allows-it improves GPU utilization.</p></li>
<li><p>Set environment threads for any CPU preprocessing:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="va">OMP_NUM_THREADS</span><span class="op">=</span>8 <span class="ex">./train_gpt2cu</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ul>
</section>
<section id="whats-different-under-the-hood-vs.-fp32" class="level4">
<h4 class="anchored" data-anchor-id="whats-different-under-the-hood-vs.-fp32">What’s different under the hood vs.&nbsp;FP32</h4>
<ul>
<li>Tensor cores: GEMMs run in FP16/BF16 paths via cuBLAS/cuBLASLt for big throughput.</li>
<li>Scaled loss &amp; unscale pass: Forward computes the loss, multiplies it by a scale factor; backward divides gradients by the same factor before updates.</li>
<li>Master FP32 copy: Optimizer (AdamW) updates this copy, then casts back to low precision for the next forward.</li>
<li>Fused/fast attention (optional): With <code>USE_CUDNN=1</code>, attention may route through cuDNN FlashAttention backends.</li>
</ul>
<p>You still recognize the same loop: load batch → forward → loss → backward → AdamW step → log.</p>
</section>
<section id="choosing-fp16-vs.-bf16" class="level4">
<h4 class="anchored" data-anchor-id="choosing-fp16-vs.-bf16">Choosing FP16 vs.&nbsp;BF16</h4>
<ul>
<li>FP16: best speed, needs loss scaling; widely supported.</li>
<li>BF16: more numerically forgiving (often needs little/no scaling), requires hardware support (Ampere+); slightly larger memory than FP16 but often simpler.</li>
</ul>
<p>The trainer picks what your GPU supports or what the code defaults to; you can expose a flag later if you want to force one.</p>
</section>
<section id="common-command-patterns" class="level4">
<h4 class="anchored" data-anchor-id="common-command-patterns">Common command patterns</h4>
<ul>
<li><p>Small GPU (less VRAM):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./train_gpt2cu</span> <span class="at">--batch_size</span> 4 <span class="at">--micro_batch_size</span> 1 <span class="at">--seq_len</span> 512</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Faster warmup with cosine schedule:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb21"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./train_gpt2cu</span> <span class="at">--warmup_steps</span> 1000 <span class="at">--lr</span> 6e-4 <span class="at">--scheduler</span> cosine</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Periodic eval to sanity-check:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb22"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./train_gpt2cu</span> <span class="at">--eval_interval</span> 200 <span class="at">--eval_batches</span> 50</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ul>
<p>(Flag names above mirror typical patterns; adjust to match the binary’s printed help.)</p>
</section>
<section id="validating-correctness-highly-recommended" class="level4">
<h4 class="anchored" data-anchor-id="validating-correctness-highly-recommended">Validating correctness (highly recommended)</h4>
<ul>
<li><p>Run the CUDA test binary to compare against the PyTorch reference on small batches:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb23"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">make</span> test_gpt2cu</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="ex">./test_gpt2cu</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Check that logits/loss match within a small tolerance. If mismatches happen, recompile without optimizations or disable cuDNN fast paths (<code>USE_CUDNN=0</code>) to isolate the issue.</p></li>
</ul>
</section>
<section id="enabling-flashattention-when-available" class="level4">
<h4 class="anchored" data-anchor-id="enabling-flashattention-when-available">Enabling FlashAttention (when available)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb24"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">make</span> train_gpt2cu USE_CUDNN=1</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="ex">./train_gpt2cu</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Good signs: faster attention time and lower step latency. If you hit build/runtime errors, ensure your CUDA, cuDNN, and driver versions are compatible; fall back to <code>USE_CUDNN=0</code> while you sort it out.</p>
</section>
<section id="memory-performance-tuning-checklist" class="level4">
<h4 class="anchored" data-anchor-id="memory-performance-tuning-checklist">Memory &amp; performance tuning checklist</h4>
<ul>
<li>Batching: Increase <code>micro_batch_size</code> until you reach ~90% GPU utilization without OOM.</li>
<li>Sequence length: Longer sequences increase compute quadratically in attention; reduce <code>--seq_len</code> if memory is tight.</li>
<li>Grad accumulation: Keep global batch size large by accumulating over multiple micro-batches.</li>
<li>Pinned host memory &amp; async copies: Already used where sensible; keep CPU↔︎GPU transfers minimal.</li>
<li>Profiler: Once it runs, profile hotspots to confirm GEMMs dominate (as expected) and attention isn’t a bottleneck unless FlashAttention is off.</li>
</ul>
</section>
<section id="troubleshooting" class="level4">
<h4 class="anchored" data-anchor-id="troubleshooting">Troubleshooting</h4>
<ul>
<li><code>cudaErrorNoKernelImageForDevice</code>: Toolkit too new/old for your GPU; rebuild with proper <code>-arch=</code> or update drivers.</li>
<li><code>CUBLAS_STATUS_ALLOC_FAILED</code> / OOM: Lower batch size, sequence length, or switch to BF16 if supported.</li>
<li>Diverging loss with FP16: Increase loss scale (if configurable) or try BF16; confirm master-weight updates are in FP32.</li>
<li>cuDNN errors: Rebuild without <code>USE_CUDNN</code> to verify the base path works, then revisit versions/paths.</li>
</ul>
<p>The takeaway: <code>train_gpt2.cu</code> is the practical, fast trainer: mixed precision, optional FlashAttention, and ready to scale. You keep the same readable training loop while tapping your GPU’s tensor cores for large speedups and much better memory efficiency.</p>
</section>
</section>
<section id="starter-artifacts-data-prep-devdownload_starter_pack.sh-devdata" class="level3">
<h3 class="anchored" data-anchor-id="starter-artifacts-data-prep-devdownload_starter_pack.sh-devdata">7. Starter Artifacts &amp; Data Prep (<code>dev/download_starter_pack.sh</code>, <code>dev/data/</code>)</h3>
<p>Before you can actually train or test a model in <em>llm.c</em>, you need a few essential artifacts: the tokenizer, a dataset, and a config. These files aren’t stored in the repo directly (they’re too large and often under different licenses), so the project provides scripts to fetch or generate them. This is where the <code>dev/</code> folder comes into play.</p>
<section id="the-starter-pack-script" class="level4">
<h4 class="anchored" data-anchor-id="the-starter-pack-script">The Starter Pack Script</h4>
<p>The easiest way to get going is with:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb25"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./dev/download_starter_pack.sh</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This script pulls down a ready-made bundle containing:</p>
<ul>
<li><code>gpt2_tokenizer.bin</code> - The GPT-2 byte-pair encoding (BPE) tokenizer in binary format.</li>
<li><code>train.bin</code> / <code>val.bin</code> - Pre-tokenized training and validation datasets, often based on OpenWebText or Tiny Shakespeare for demos.</li>
<li>Model configs - A JSON or header file that sets hyperparameters like layers, hidden size, and number of heads for GPT-2 124M.</li>
</ul>
<p>Think of this as your “starter kit”: it contains just enough to run a demo and see training loss decreasing without setting up a full-scale dataset pipeline yourself.</p>
</section>
<section id="the-tokenizer-file-gpt2_tokenizer.bin" class="level4">
<h4 class="anchored" data-anchor-id="the-tokenizer-file-gpt2_tokenizer.bin">The Tokenizer File (<code>gpt2_tokenizer.bin</code>)</h4>
<p>This is a binary representation of GPT-2’s tokenizer vocabulary. It maps raw text (like <code>"Hello world"</code>) into integer token IDs, which are the actual inputs to the model.</p>
<ul>
<li>Why binary? It’s faster to load in C than parsing a text-based vocabulary.</li>
<li>Size? ~500 KB, representing ~50,000 tokens.</li>
<li>Role in training? Used in both the dataloader (to prepare inputs) and the sampler (to decode outputs).</li>
</ul>
<p>Without this file, the model can’t understand text at all-it would just be manipulating meaningless numbers.</p>
</section>
<section id="dataset-files-train.bin-val.bin" class="level4">
<h4 class="anchored" data-anchor-id="dataset-files-train.bin-val.bin">Dataset Files (<code>train.bin</code>, <code>val.bin</code>)</h4>
<p>Each dataset file is a binary blob containing:</p>
<ol type="1">
<li>A header (about 1 KB) describing sequence length, vocab size, and other metadata.</li>
<li>A stream of token IDs (<code>uint16</code>), representing the text corpus already tokenized.</li>
</ol>
<p>This design means the C dataloader can simply <code>fread()</code> chunks of tokens into memory, without needing to tokenize text on the fly. It’s fast and memory-efficient, perfect for a lean project like <em>llm.c</em>.</p>
<p>The script usually fetches two versions:</p>
<ul>
<li>Training set (<code>train.bin</code>)</li>
<li>Validation set (<code>val.bin</code>)</li>
</ul>
<p>That way, the training loop can occasionally switch to validation mode and report a validation loss, helping you track overfitting.</p>
</section>
<section id="the-devdata-folder" class="level4">
<h4 class="anchored" data-anchor-id="the-devdata-folder">The <code>dev/data/</code> Folder</h4>
<p>If you want to generate your own datasets, this is where you’ll find the tools:</p>
<ul>
<li>Scripts for Tiny Shakespeare, OpenWebText, or other corpora.</li>
<li>Utilities to tokenize text using the GPT-2 tokenizer and write out the <code>.bin</code> format.</li>
<li>Small Python snippets to check dataset statistics (like number of tokens or average sequence length).</li>
</ul>
<p>For example, if you wanted to try fine-tuning GPT-2 on your own text files, you’d:</p>
<ol type="1">
<li>Run a preprocessing script in <code>dev/data/</code> to tokenize and save your corpus.</li>
<li>Point <code>train_gpt2.c</code> or <code>train_gpt2.cu</code> to your new <code>train.bin</code> and <code>val.bin</code>.</li>
<li>Kick off training as usual.</li>
</ol>
</section>
<section id="why-preprocessing-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-preprocessing-matters">Why Preprocessing Matters</h4>
<p>Tokenization and dataset preparation can be surprisingly heavy in Python, especially for large corpora. By precomputing everything into compact <code>.bin</code> files, <em>llm.c</em> keeps the runtime training loop as simple as possible-just reading arrays of integers and feeding them into the model.</p>
<p>This separation of concerns (preprocessing vs.&nbsp;training) is what makes the training code clean and focused.</p>
</section>
<section id="quick-sanity-check" class="level4">
<h4 class="anchored" data-anchor-id="quick-sanity-check">Quick Sanity Check</h4>
<p>After running <code>download_starter_pack.sh</code>, you should see these files in your working directory:</p>
<pre><code>gpt2_tokenizer.bin
train.bin
val.bin</code></pre>
<p>If any are missing, re-run the script. Without them, the trainer will exit with a file-not-found error.</p>
<p>The takeaway: The starter pack is your ticket to running <em>llm.c</em> right away. It gives you a tokenizer and datasets in exactly the format the C code expects. Later, when you’re ready to train on your own text or scale up, the <code>dev/data/</code> folder shows you how to prepare custom datasets the same way.</p>
</section>
</section>
<section id="debugging-tips-ide-stepping--g" class="level3">
<h3 class="anchored" data-anchor-id="debugging-tips-ide-stepping--g">8. Debugging Tips &amp; IDE Stepping (<code>-g</code>)</h3>
<p>Even though <em>llm.c</em> is designed to be small and readable, training a transformer model is still a big program with lots of moving parts. When something goes wrong-whether it’s a segmentation fault, <code>NaN</code> losses, or unexpected results-you’ll want to be able to debug effectively. That’s where debug builds and IDE stepping come in.</p>
<section id="why-debug-mode-exists" class="level4">
<h4 class="anchored" data-anchor-id="why-debug-mode-exists">Why Debug Mode Exists</h4>
<p>By default, the Makefile compiles with heavy optimization (<code>-O3</code>). That makes the code run fast, but it also makes debugging harder:</p>
<ul>
<li>Variables may be optimized away.</li>
<li>Functions might get inlined so you can’t step through them clearly.</li>
<li>The debugger may jump around unpredictably.</li>
</ul>
<p>Adding the <code>-g</code> flag (enabled with <code>DEBUG=1</code>) tells the compiler to include extra information in the binary so you can see exactly what the code is doing at runtime.</p>
</section>
<section id="building-a-debug-binary" class="level4">
<h4 class="anchored" data-anchor-id="building-a-debug-binary">Building a Debug Binary</h4>
<p>To build with debug info:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb27"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">make</span> train_gpt2 DEBUG=1</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This produces a slower executable, but one that works seamlessly with tools like:</p>
<ul>
<li>gdb - the classic GNU debugger.</li>
<li>lldb - default on macOS.</li>
<li>VS Code / CLion / Xcode - IDEs with integrated debuggers and GUI interfaces.</li>
</ul>
</section>
<section id="using-gdb-on-linuxmacos" class="level4">
<h4 class="anchored" data-anchor-id="using-gdb-on-linuxmacos">Using gdb on Linux/macOS</h4>
<p>Start your program under gdb:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb28"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">gdb</span> ./train_gpt2</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Inside gdb:</p>
<ul>
<li>Run the program: <code>run</code></li>
<li>Set a breakpoint at <code>main</code>: <code>break main</code></li>
<li>Step through line by line: <code>step</code> or <code>next</code></li>
<li>Inspect variables: <code>print loss</code>, <code>print i</code></li>
<li>Quit: <code>quit</code></li>
</ul>
<p>This is the fastest way to see exactly where a crash happens.</p>
</section>
<section id="using-an-ide" class="level4">
<h4 class="anchored" data-anchor-id="using-an-ide">Using an IDE</h4>
<p>If command-line debugging feels intimidating, you can use an IDE like VS Code or CLion:</p>
<ul>
<li>Open the project folder.</li>
<li>Configure the debugger (choose <code>gdb</code> or <code>lldb</code> backend).</li>
<li>Add breakpoints by clicking next to line numbers.</li>
<li>Run the debug build (<code>train_gpt2</code> with <code>DEBUG=1</code>).</li>
<li>Step through forward pass, backward pass, or optimizer updates.</li>
</ul>
<p>This way, you can visually watch variables update with each step.</p>
</section>
<section id="debugging-cuda-code" class="level4">
<h4 class="anchored" data-anchor-id="debugging-cuda-code">Debugging CUDA Code</h4>
<p>CUDA debugging is a bit trickier, but still possible:</p>
<ul>
<li><code>cuda-gdb</code> - NVIDIA’s GPU debugger, works like gdb but supports stepping into kernels.</li>
<li>Nsight Systems / Nsight Compute - graphical profilers/debuggers that let you trace kernel launches, memory transfers, and GPU utilization.</li>
</ul>
<p>If your CUDA code crashes with cryptic messages like <code>illegal memory access</code>, <code>cuda-gdb</code> can help pinpoint the kernel and even the exact line.</p>
</section>
<section id="debugging-common-issues-in-llm.c" class="level4">
<h4 class="anchored" data-anchor-id="debugging-common-issues-in-llm.c">Debugging Common Issues in llm.c</h4>
<ol type="1">
<li><p>File not found → Make sure <code>gpt2_tokenizer.bin</code>, <code>train.bin</code>, and <code>val.bin</code> are downloaded.</p></li>
<li><p>Segfault at malloc/fread → Check file paths and dataset sizes.</p></li>
<li><p>Loss becomes NaN →</p>
<ul>
<li>On CPU: check for division by zero in normalization.</li>
<li>On GPU: check loss scaling (mixed precision) or try FP32 path for comparison.</li>
</ul></li>
<li><p>Mismatch with PyTorch tests → Run <code>test_gpt2</code> or <code>test_gpt2cu</code> and compare outputs; this usually isolates whether the bug is in forward pass, backward pass, or optimizer.</p></li>
</ol>
</section>
<section id="logging-sanity-checks" class="level4">
<h4 class="anchored" data-anchor-id="logging-sanity-checks">Logging &amp; Sanity Checks</h4>
<p>When debugging, it helps to add extra logging. The repo already has a lightweight logger, but you can also sprinkle <code>printf</code>s (on CPU) or <code>cudaDeviceSynchronize(); printf(...)</code> (on GPU) to track values. For example:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb29"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>printf<span class="op">(</span><span class="st">"Step </span><span class="sc">%d</span><span class="st">: loss=</span><span class="sc">%f\n</span><span class="st">"</span><span class="op">,</span> step<span class="op">,</span> loss<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Sometimes the quickest fix is just to print and see what’s going on.</p>
</section>
<section id="best-practices-for-beginners" class="level4">
<h4 class="anchored" data-anchor-id="best-practices-for-beginners">Best Practices for Beginners</h4>
<ul>
<li>Start with the CPU build when learning-it’s easier to debug than CUDA.</li>
<li>Always keep a small dataset (like Tiny Shakespeare) for fast iteration.</li>
<li>Compare against the PyTorch reference for the same batch to catch subtle errors.</li>
<li>Use <code>DEBUG=1</code> whenever you hit strange behavior-you’ll trade speed for clarity, which is usually worth it when learning.</li>
</ul>
<p>The takeaway: Debug builds (<code>-g</code>) turn <em>llm.c</em> from a black box into a step-through learning tool. With gdb, lldb, or an IDE, you can pause at any line, inspect variables, and understand exactly how GPT-2 training works inside C or CUDA. It’s slower, but it’s the clearest way to learn and fix issues.</p>
</section>
</section>
<section id="project-constraints-readability-contract" class="level3">
<h3 class="anchored" data-anchor-id="project-constraints-readability-contract">9. Project Constraints &amp; Readability Contract</h3>
<p>The <em>llm.c</em> project isn’t trying to be the fastest or most feature-rich GPT-2 trainer. Instead, it has a very deliberate set of constraints-rules the author imposes on the codebase to keep it approachable and educational. You can think of these as the “contract” between the code and the reader: certain things are kept simple on purpose, even if they cost some performance.</p>
<section id="minimalism-over-optimizations" class="level4">
<h4 class="anchored" data-anchor-id="minimalism-over-optimizations">Minimalism over Optimizations</h4>
<ul>
<li>No processor-specific intrinsics: You won’t see AVX, NEON, or other hardware-tuned assembly calls in the CPU path.</li>
<li>No fancy template metaprogramming: Unlike in C++ frameworks, here you get plain C structs and functions.</li>
<li>No exotic libraries: Aside from cuBLAS/cuDNN for GPU acceleration, most functionality is implemented directly.</li>
</ul>
<p>This means the code runs almost anywhere, and you don’t need to understand deep compiler tricks to follow what’s going on.</p>
</section>
<section id="transparency-over-abstraction" class="level4">
<h4 class="anchored" data-anchor-id="transparency-over-abstraction">Transparency over Abstraction</h4>
<ul>
<li>Every operation is visible in the source. For example, instead of calling a framework function like <code>nn.CrossEntropyLoss</code>, you’ll find an explicit forward and backward pass coded in C.</li>
<li>Data loading, tokenization, optimizer steps, and schedulers are all implemented as separate, small modules in <code>llmc/</code>.</li>
<li>You don’t need to guess what’s happening-if you’re curious, you can open the corresponding <code>.h</code> file and see the exact code.</li>
</ul>
<p>The guiding idea: if something is central to training GPT-2, you should be able to read and understand it.</p>
</section>
<section id="performance-where-it-matters-but-no-more" class="level4">
<h4 class="anchored" data-anchor-id="performance-where-it-matters-but-no-more">Performance Where It Matters (but No More)</h4>
<ul>
<li>OpenMP pragmas are allowed in CPU builds, because they give large speedups with minimal extra code.</li>
<li>cuBLAS/cuDNN are used for GPU matmuls and attention, because re-implementing them would be a distraction and would make the project impossibly large.</li>
<li>But the project avoids unnecessary complexity-no kernel fusion, no elaborate caching layers, no half-implemented “framework” abstractions.</li>
</ul>
<p>This balance ensures you can still run experiments at a reasonable speed, but the code never sacrifices readability.</p>
</section>
<section id="educational-first" class="level4">
<h4 class="anchored" data-anchor-id="educational-first">Educational First</h4>
<p>The code is written to teach, not to win benchmarks. That means:</p>
<ul>
<li>Variable names are descriptive, not cryptic.</li>
<li>Comments explain not just <em>what</em> happens, but also <em>why</em>.</li>
<li>Files are kept small and focused, rather than sprawling across dozens of layers of abstraction.</li>
<li>There’s a matching PyTorch reference implementation so you can always check your understanding against a familiar baseline.</li>
</ul>
</section>
<section id="limitations-you-should-expect" class="level4">
<h4 class="anchored" data-anchor-id="limitations-you-should-expect">Limitations You Should Expect</h4>
<ul>
<li>Training is slower than PyTorch/XLA/JAX or DeepSpeed-tuned runs.</li>
<li>Multi-GPU scaling is functional but not heavily optimized.</li>
<li>Only GPT-2 architectures are covered-don’t expect GPT-3 or transformer variants.</li>
<li>Features like dataset streaming, checkpoint sharding, or advanced distributed tricks are intentionally left out.</li>
</ul>
<p>These are not bugs-they’re conscious trade-offs to keep the codebase small, sharp, and didactic.</p>
</section>
<section id="why-this-matters-for-you" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-for-you">Why This Matters for You</h4>
<p>If you’re learning how transformers work, this contract is a gift:</p>
<ul>
<li>You won’t get lost in performance hacks.</li>
<li>You won’t fight through an abstraction jungle.</li>
<li>You’ll always know that what you’re reading is close to the “pure” algorithmic idea.</li>
</ul>
<p>On the flip side, if you’re aiming for production-grade speed, you’ll need to layer more on top. But that’s outside the mission of <em>llm.c</em>.</p>
<p>The takeaway: <em>llm.c</em> is bound by a readability contract: clarity over raw speed, transparency over abstraction, minimalism over complexity. These constraints keep the project small enough to fit in your head, while still powerful enough to reproduce GPT-2 training. It’s a teaching lab, not a racing car-and that’s exactly why it’s valuable.</p>
</section>
</section>
<section id="community-discussions-and-learning-path" class="level3">
<h3 class="anchored" data-anchor-id="community-discussions-and-learning-path">10. Community, Discussions, and Learning Path</h3>
<p>The last piece of the quickstart isn’t about code at all-it’s about the people and resources around the project. <em>llm.c</em> has grown into more than just a single repository; it has become a meeting point for learners, tinkerers, and researchers who want to strip large language models down to their essentials. Understanding this community layer is just as important as understanding the code itself.</p>
<section id="discussions-and-issues-on-github" class="level4">
<h4 class="anchored" data-anchor-id="discussions-and-issues-on-github">Discussions and Issues on GitHub</h4>
<p>The project’s Discussions tab is full of valuable context:</p>
<ul>
<li>Developers asking about build errors on different platforms (Linux, macOS, Windows).</li>
<li>Explorations of how to extend <em>llm.c</em> to train larger GPT-2 models (355M, 774M, 1.6B).</li>
<li>Reports on multi-GPU and MPI runs, including troubleshooting NCCL hangs and performance bottlenecks.</li>
<li>Debates on mixed precision vs FP32 vs BF16 stability.</li>
</ul>
<p>Reading these threads is like looking over the shoulders of hundreds of other learners. You’ll see not only the official answers but also the thought process of people solving problems in real time.</p>
</section>
<section id="roadmap-and-contributions" class="level4">
<h4 class="anchored" data-anchor-id="roadmap-and-contributions">Roadmap and Contributions</h4>
<p>The README and issues sometimes hint at where the project might grow:</p>
<ul>
<li>Making the CUDA kernels more modular in <code>dev/cuda/</code>.</li>
<li>Simplifying multi-GPU startup for clusters.</li>
<li>Adding small tutorial-style docs (like the LayerNorm walkthrough).</li>
</ul>
<p>The project is open to contributions, but it follows the same minimalist philosophy. If you’re thinking of contributing, remember: the goal is clarity first, performance second.</p>
</section>
<section id="external-learning-resources" class="level4">
<h4 class="anchored" data-anchor-id="external-learning-resources">External Learning Resources</h4>
<p>While <em>llm.c</em> is self-contained, it pairs nicely with outside material:</p>
<ul>
<li>The PyTorch reference implementation in <code>train_gpt2.py</code> is your canonical oracle for correctness.</li>
<li>The GPT-2 paper gives the architecture background.</li>
<li>CUDA and cuBLAS/cuDNN docs explain the GPU APIs that the project calls into.</li>
<li>Community blog posts often walk through specific sections of the code in plain English, making it easier to digest.</li>
</ul>
<p>By combining the code, the paper, and these resources, you can triangulate a much deeper understanding.</p>
</section>
<section id="a-suggested-learning-path" class="level4">
<h4 class="anchored" data-anchor-id="a-suggested-learning-path">A Suggested Learning Path</h4>
<p>If you’re coming to <em>llm.c</em> as a beginner, here’s a natural progression:</p>
<ol type="1">
<li>Run the CPU trainer (<code>train_gpt2.c</code>) on Tiny Shakespeare. Watch the loss decrease.</li>
<li>Step through the code with <code>DEBUG=1</code>, confirming that you understand forward, backward, and optimizer steps.</li>
<li>Move to the FP32 CUDA trainer to see how the same loop runs on GPU.</li>
<li>Switch to the modern CUDA trainer (<code>train_gpt2.cu</code>) and learn how mixed precision works.</li>
<li>Experiment with dataset scripts in <code>dev/data/</code>-try your own text corpus.</li>
<li>Read the LayerNorm doc in <code>doc/</code> to deepen your theory-practice connection.</li>
<li>Explore multi-GPU runs with MPI/NCCL if you have access to multiple GPUs.</li>
<li>Follow GitHub Discussions for real-world debugging and scaling stories.</li>
</ol>
</section>
<section id="why-this-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters">Why This Matters</h4>
<p>Code alone is not enough. The community context, the discussions, and the learning path make <em>llm.c</em> a living project. By engaging with them, you avoid the feeling of learning in isolation. You’ll see others wrestling with the same challenges, and you’ll have a clearer sense of what to try next.</p>
<p>The takeaway: Beyond the files and scripts, <em>llm.c</em> is a community-driven learning environment. GitHub issues, discussions, reference docs, and external tutorials all form part of the “extended classroom.” If the code is the lab bench, the community is the set of lab partners who help you figure things out along the way.</p>
</section>
</section>
</section>
<section id="chapter-2.-data-tokenization-and-loaders" class="level2">
<h2 class="anchored" data-anchor-id="chapter-2.-data-tokenization-and-loaders">Chapter 2. Data, Tokenization, and Loaders</h2>
<section id="gpt-2-tokenizer-artifacts-gpt2_tokenizer.bin" class="level3">
<h3 class="anchored" data-anchor-id="gpt-2-tokenizer-artifacts-gpt2_tokenizer.bin">11. GPT-2 Tokenizer Artifacts (<code>gpt2_tokenizer.bin</code>)</h3>
<p>A language model like GPT-2 doesn’t directly understand English, Vietnamese, or any other natural language. Instead, it understands numbers. These numbers are called tokens. A tokenizer is the tool that translates between human text and tokens. In <em>llm.c</em>, the GPT-2 tokenizer is stored in a small file called <code>gpt2_tokenizer.bin</code>. This file is the key that lets the model read input text and produce output text that we can understand.</p>
<section id="what-this-file-contains" class="level4">
<h4 class="anchored" data-anchor-id="what-this-file-contains">What This File Contains</h4>
<p>The file <code>gpt2_tokenizer.bin</code> is a binary version of GPT-2’s tokenizer. It includes:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 76%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Byte vocabulary (0–255)</td>
<td>Makes sure every possible character can be represented.</td>
</tr>
<tr class="even">
<td>Merge rules (BPE)</td>
<td>Combines frequent sequences like “ing” or ” the” into single tokens for efficiency.</td>
</tr>
<tr class="odd">
<td>Vocabulary size (~50,257)</td>
<td>Defines how many distinct tokens GPT-2 can work with.</td>
</tr>
<tr class="even">
<td>Mapping IDs ↔︎ text</td>
<td>Lets the program turn model outputs (numbers) back into human-readable strings.</td>
</tr>
</tbody>
</table>
<p>Instead of being written as JSON or text, the tokenizer is stored in binary form. This allows <em>llm.c</em> to load it very quickly using a simple file read, which keeps the code clean and fast.</p>
</section>
<section id="where-it-comes-from" class="level4">
<h4 class="anchored" data-anchor-id="where-it-comes-from">Where It Comes From</h4>
<p>You don’t need to build this file by hand. The repository provides a script to download it, along with small training and validation datasets:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb30"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./dev/download_starter_pack.sh</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>After running the script, you should see <code>gpt2_tokenizer.bin</code>, <code>train.bin</code>, and <code>val.bin</code> in your working directory. If the tokenizer is missing, the program cannot run because it won’t know how to interpret text.</p>
</section>
<section id="how-the-code-uses-it" class="level4">
<h4 class="anchored" data-anchor-id="how-the-code-uses-it">How the Code Uses It</h4>
<p>During training, the tokenizer is not active because the datasets (<code>train.bin</code> and <code>val.bin</code>) are already pre-tokenized into integers. This keeps the training loop fast and simple.</p>
<p>During sampling or evaluation, the tokenizer becomes important again. After the model predicts a sequence of token IDs, the tokenizer translates those numbers back into text that you can read on your screen.</p>
<p>The C API for the tokenizer, defined in <code>llmc/tokenizer.h</code>, provides just three main functions:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb31"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> tokenizer_init<span class="op">(</span>Tokenizer <span class="op">*</span>t<span class="op">,</span> <span class="dt">const</span> <span class="dt">char</span> <span class="op">*</span>filename<span class="op">);</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> tokenizer_decode<span class="op">(</span>Tokenizer <span class="op">*</span>t<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> <span class="op">*</span>ids<span class="op">,</span> <span class="dt">int</span> n<span class="op">,</span> <span class="dt">char</span> <span class="op">*</span>out<span class="op">);</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> tokenizer_free<span class="op">(</span>Tokenizer <span class="op">*</span>t<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This is all you need: initialize the tokenizer from the file, decode tokens into text, and free memory when done.</p>
</section>
<section id="example-workflow-in-practice" class="level4">
<h4 class="anchored" data-anchor-id="example-workflow-in-practice">Example Workflow in Practice</h4>
<ol type="1">
<li><p>Initialize the tokenizer:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb32"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>Tokenizer t<span class="op">;</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>tokenizer_init<span class="op">(&amp;</span>t<span class="op">,</span> <span class="st">"gpt2_tokenizer.bin"</span><span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Decode a sequence of tokens back to text:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb33"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="dt">char</span> buf<span class="op">[</span><span class="dv">512</span><span class="op">];</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>tokenizer_decode<span class="op">(&amp;</span>t<span class="op">,</span> tokens<span class="op">,</span> ntokens<span class="op">,</span> buf<span class="op">);</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>printf<span class="op">(</span><span class="st">"</span><span class="sc">%s\n</span><span class="st">"</span><span class="op">,</span> buf<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Clean up memory when you no longer need it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb34"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>tokenizer_free<span class="op">(&amp;</span>t<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ol>
<p>This small cycle is enough to turn model outputs into readable sentences.</p>
</section>
<section id="why-it-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters">Why It Matters</h4>
<p>Without the tokenizer, the model cannot communicate. The tokenizer is like a shared dictionary between humans and the neural network. If you give the model text, the tokenizer converts it into numbers the model understands. When the model responds, the tokenizer converts its numbers back into text. If the tokenizer does not match the dataset, the model’s predictions will come out as gibberish. Keeping the tokenizer and dataset in sync is essential for correct training and evaluation.</p>
</section>
<section id="try-it-yourself" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself">Try It Yourself</h4>
<p>Here are a few small exercises you can do to understand the tokenizer better:</p>
<ol type="1">
<li>Check that the file exists: After running the starter pack script, verify that <code>gpt2_tokenizer.bin</code> is in your directory. Try running the trainer without it and observe the error message.</li>
<li>Inspect vocab size: Run the trainer and look for the line that prints <code>vocab_size: 50257</code>. Compare this with <code>padded_vocab_size: 50304</code>. Why do you think padding helps GPUs?</li>
<li>Decode a sequence manually: Write a short C program that loads the tokenizer and decodes a fixed list of token IDs (for example <code>[464, 3290, 318]</code>). Observe what text you get.</li>
<li>Mismatch experiment: If you build your own dataset with a different tokenizer (say, a custom vocabulary), try decoding it with <code>gpt2_tokenizer.bin</code>. Notice how the output becomes meaningless, showing why consistency matters.</li>
<li>Dataset + tokenizer link: Open <code>train.bin</code> in a hex viewer. You’ll see it’s just numbers. Use the tokenizer to decode the first few hundred tokens and see real text emerge.</li>
</ol>
</section>
<section id="the-takeaway" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway">The Takeaway</h4>
<p><code>gpt2_tokenizer.bin</code> is a tiny but vital file. It is the bridge that allows the model and humans to speak the same language. Training is efficient because all data is pre-tokenized, and when you want to see what the model has written, the tokenizer turns raw numbers back into words. Without it, the entire system would be silent.</p>
</section>
</section>
<section id="binary-dataset-format-train.bin-and-val.bin" class="level3">
<h3 class="anchored" data-anchor-id="binary-dataset-format-train.bin-and-val.bin">12. Binary Dataset Format (<code>train.bin</code> and <code>val.bin</code>)</h3>
<p>Just like the tokenizer turns text into numbers, the datasets in <em>llm.c</em> are stored as numbers too. Instead of reading plain text files like <code>.txt</code>, the training and validation data are kept in simple binary files: <code>train.bin</code> and <code>val.bin</code>. These files are the fuel for the training loop.</p>
<section id="what-these-files-look-like" class="level4">
<h4 class="anchored" data-anchor-id="what-these-files-look-like">What These Files Look Like</h4>
<p>At first glance, <code>train.bin</code> and <code>val.bin</code> look like unreadable blobs if you open them in a text editor. That’s because they are not meant to be human-readable. They contain:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 68%">
</colgroup>
<thead>
<tr class="header">
<th>Part</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A tiny header (about 1 KB)</td>
<td>Stores metadata such as sequence length and vocab size.</td>
</tr>
<tr class="even">
<td>A stream of token IDs (<code>uint16</code>)</td>
<td>Every tokenized word piece from the dataset, saved as 16-bit integers.</td>
</tr>
</tbody>
</table>
<p>Each integer represents one token from the tokenizer’s vocabulary. Since GPT-2 has a vocabulary of about 50,000 tokens, 16-bit integers (<code>uint16_t</code>) are enough to store them all.</p>
</section>
<section id="why-binary-format" class="level4">
<h4 class="anchored" data-anchor-id="why-binary-format">Why Binary Format?</h4>
<ul>
<li>Efficiency: Instead of re-tokenizing text every time, the data is pre-tokenized once and stored as numbers. The trainer just reads them directly.</li>
<li>Speed: Reading integers from a file is faster than parsing and processing raw text.</li>
<li>Simplicity: The training loop only has to deal with arrays of integers-no string handling, no parsing, no surprises.</li>
</ul>
<p>This choice makes the training code in <em>llm.c</em> much cleaner and faster.</p>
</section>
<section id="how-the-dataloader-uses-them" class="level4">
<h4 class="anchored" data-anchor-id="how-the-dataloader-uses-them">How the Dataloader Uses Them</h4>
<p>When training starts, the dataloader reads chunks of numbers from <code>train.bin</code>. Each chunk corresponds to one batch of size B × T:</p>
<ul>
<li>B = batch size (number of examples in a batch).</li>
<li>T = sequence length (number of tokens per example).</li>
</ul>
<p>For example, if <code>B = 8</code> and <code>T = 1024</code>, the dataloader will read <code>8 × 1024 = 8192</code> token IDs from the file, reshape them into sequences, and feed them to the model.</p>
<p>The validation file (<code>val.bin</code>) works the same way but is only used occasionally during training to measure validation loss. This helps detect overfitting.</p>
</section>
<section id="workflow-in-code" class="level4">
<h4 class="anchored" data-anchor-id="workflow-in-code">Workflow in Code</h4>
<p>Inside the repo, you’ll see functions like these in <code>llmc/dataloader.h</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb35"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> dataloader_init<span class="op">(</span>Dataloader <span class="op">*</span>loader<span class="op">,</span> <span class="dt">const</span> <span class="dt">char</span> <span class="op">*</span>filename<span class="op">,</span> <span class="dt">int</span> B<span class="op">,</span> <span class="dt">int</span> T<span class="op">);</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> dataloader_next_batch<span class="op">(</span>Dataloader <span class="op">*</span>loader<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>inputs<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>targets<span class="op">);</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> dataloader_reset<span class="op">(</span>Dataloader <span class="op">*</span>loader<span class="op">);</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> dataloader_free<span class="op">(</span>Dataloader <span class="op">*</span>loader<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Here’s what happens step by step:</p>
<ol type="1">
<li>Initialize with the binary file and batch/sequence sizes.</li>
<li>Next batch reads the next B × T tokens into an input array and a target array.</li>
<li>Reset allows re-reading from the beginning when you start a new epoch.</li>
<li>Free cleans up resources when training ends.</li>
</ol>
<p>The target array is simply the same sequence shifted by one token-because language modeling predicts the <em>next</em> token.</p>
</section>
<section id="why-it-matters-1" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-1">Why It Matters</h4>
<p>The dataset format is what makes <em>llm.c</em> practical. Without it, the code would need to handle messy text, encodings, and tokenization during every training step. By storing clean arrays of token IDs, the training loop becomes very short and easy to follow. It’s a design decision that keeps the project minimal yet faithful to real training pipelines.</p>
</section>
<section id="try-it-yourself-1" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-1">Try It Yourself</h4>
<ol type="1">
<li>Check file size: Run <code>ls -lh train.bin</code> and notice how large it is compared to a plain <code>.txt</code> file. Why is it smaller or larger?</li>
<li>Peek inside: Use a hex viewer (<code>xxd train.bin | head</code>) to see raw numbers. They won’t look like text, but they are the tokens the model trains on.</li>
<li>Count tokens: Write a short Python or C script to count how many token IDs are stored in <code>train.bin</code>. This gives you a sense of dataset size.</li>
<li>Mini-dataset: Try generating your own dataset from a small <code>.txt</code> file using the scripts in <code>dev/data/</code>. See how the <code>.bin</code> file is created.</li>
<li>Validation experiment: During training, reduce the validation set to only a few batches and observe how the validation loss stabilizes or fluctuates compared to training loss.</li>
</ol>
</section>
<section id="the-takeaway-1" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-1">The Takeaway</h4>
<p><code>train.bin</code> and <code>val.bin</code> may look like gibberish, but they are carefully prepared binary files containing token IDs. They make training faster, simpler, and more reproducible. The dataloader in <em>llm.c</em> reads these numbers in neat chunks and serves them directly to the model, letting you focus on learning how transformers work instead of wrestling with raw text parsing.</p>
</section>
</section>
<section id="dataset-scripts-in-devdata" class="level3">
<h3 class="anchored" data-anchor-id="dataset-scripts-in-devdata">13. Dataset Scripts in <code>dev/data/</code></h3>
<p>The repository doesn’t just give you ready-made binary datasets like <code>train.bin</code> and <code>val.bin</code>. It also provides scripts inside the <code>dev/data/</code> folder that show you how to create your own. These scripts are important because they demonstrate how raw text gets transformed into the binary format that the dataloader in <em>llm.c</em> expects.</p>
<section id="whats-inside-devdata" class="level4">
<h4 class="anchored" data-anchor-id="whats-inside-devdata">What’s Inside <code>dev/data/</code></h4>
<p>This folder contains small Python scripts that:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 75%">
</colgroup>
<thead>
<tr class="header">
<th>Script</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>prepare_shakespeare.py</code></td>
<td>Turns the Tiny Shakespeare dataset into <code>train.bin</code> and <code>val.bin</code>.</td>
</tr>
<tr class="even">
<td><code>prepare_openwebtext.py</code></td>
<td>Prepares a large-scale dataset similar to the one GPT-2 was trained on.</td>
</tr>
<tr class="odd">
<td>Other helpers</td>
<td>Tokenize raw <code>.txt</code> files, split them into train/val, and save to binary.</td>
</tr>
</tbody>
</table>
<p>Each script follows the same basic recipe:</p>
<ol type="1">
<li>Read raw text from a source file.</li>
<li>Apply the GPT-2 tokenizer to turn text into token IDs.</li>
<li>Split the tokens into training and validation portions.</li>
<li>Write the IDs into binary files that <em>llm.c</em> can read directly.</li>
</ol>
</section>
<section id="why-preprocessing-happens-outside-c" class="level4">
<h4 class="anchored" data-anchor-id="why-preprocessing-happens-outside-c">Why Preprocessing Happens Outside C</h4>
<p>In C, handling text files with Unicode, punctuation, and different encodings is messy. Instead, preprocessing is done once in Python, where tokenizers are easier to use. The results are saved in a simple binary format (<code>uint16</code> IDs). From then on, C only has to deal with arrays of integers-clean and efficient.</p>
<p>This design keeps the training loop minimal: no text parsing, no string handling, just numbers.</p>
</section>
<section id="example-tiny-shakespeare" class="level4">
<h4 class="anchored" data-anchor-id="example-tiny-shakespeare">Example: Tiny Shakespeare</h4>
<p>One of the simplest datasets is Tiny Shakespeare, about 1 MB of text from Shakespeare’s plays. The script <code>prepare_shakespeare.py</code> will:</p>
<ul>
<li>Read <code>input.txt</code> (the raw text).</li>
<li>Use the GPT-2 tokenizer (<code>gpt2_tokenizer.bin</code>) to turn every word and symbol into token IDs.</li>
<li>Split 90% of the data into <code>train.bin</code> and 10% into <code>val.bin</code>.</li>
</ul>
<p>After running the script, you’ll have small binary files that let you train GPT-2 from scratch in minutes on CPU or GPU.</p>
</section>
<section id="example-openwebtext" class="level4">
<h4 class="anchored" data-anchor-id="example-openwebtext">Example: OpenWebText</h4>
<p>The script <code>prepare_openwebtext.py</code> shows how to tokenize a much larger dataset, closer to what GPT-2 was originally trained on. This is heavier and requires more disk space, but it’s useful if you want to try scaling up training to bigger models.</p>
</section>
<section id="why-it-matters-2" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-2">Why It Matters</h4>
<p>These scripts are more than convenience tools-they are examples of how to adapt llm.c to your own data. If you have a collection of emails, poems, or programming code, you can:</p>
<ol type="1">
<li>Put them into a single <code>.txt</code> file.</li>
<li>Modify one of the scripts in <code>dev/data/</code>.</li>
<li>Generate new <code>train.bin</code> and <code>val.bin</code> files.</li>
<li>Train GPT-2 on your own text.</li>
</ol>
<p>By separating dataset creation from training, <em>llm.c</em> keeps the C code small and makes experimentation flexible.</p>
</section>
<section id="try-it-yourself-2" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-2">Try It Yourself</h4>
<ol type="1">
<li><p>Run the Shakespeare script:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb36"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> dev/data/prepare_shakespeare.py</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then check that <code>train.bin</code> and <code>val.bin</code> were created.</p></li>
<li><p>Open the binary files with a hex viewer and confirm that they contain only numbers.</p></li>
<li><p>Modify the script to tokenize a different text file (for example, your own writing).</p></li>
<li><p>Compare dataset sizes: Tiny Shakespeare is tiny (MBs), OpenWebText is huge (GBs). Observe how training speed changes depending on dataset size.</p></li>
<li><p>Re-run training with your custom dataset and watch how the model starts generating text in your style.</p></li>
</ol>
</section>
<section id="the-takeaway-2" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-2">The Takeaway</h4>
<p>The <code>dev/data/</code> scripts are the bridge between raw human text and the binary datasets used in training. They let you prepare small demo datasets or scale up to larger corpora. By experimenting with these scripts, you learn how to bring your own data into <em>llm.c</em> and train a GPT-style model on anything you like.</p>
</section>
</section>
<section id="dataloader-design-batching-strides-epochs" class="level3">
<h3 class="anchored" data-anchor-id="dataloader-design-batching-strides-epochs">14. DataLoader Design (Batching, Strides, Epochs)</h3>
<p>Now that the datasets are prepared as <code>.bin</code> files, we need a way to feed them into the model during training. This is the job of the DataLoader in <em>llm.c</em>. You’ll find its interface in <code>llmc/dataloader.h</code>, and its purpose is very simple: take a big stream of token IDs from <code>train.bin</code> or <code>val.bin</code>, cut it into manageable chunks, and serve those chunks to the training loop as batches.</p>
<section id="the-core-idea" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea">The Core Idea</h4>
<p>Training a language model requires two arrays for every batch:</p>
<ul>
<li>Inputs: a sequence of token IDs, like <code>[The, cat, sat, on]</code></li>
<li>Targets: the same sequence shifted by one, like <code>[cat, sat, on, the]</code></li>
</ul>
<p>The model learns to predict each next token in the sequence. The DataLoader automates slicing these arrays out of the giant dataset file.</p>
</section>
<section id="the-interface" class="level4">
<h4 class="anchored" data-anchor-id="the-interface">The Interface</h4>
<p>In the code you’ll see function declarations like these:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb37"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> dataloader_init<span class="op">(</span>Dataloader <span class="op">*</span>loader<span class="op">,</span> <span class="dt">const</span> <span class="dt">char</span> <span class="op">*</span>filename<span class="op">,</span> <span class="dt">int</span> B<span class="op">,</span> <span class="dt">int</span> T<span class="op">);</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> dataloader_next_batch<span class="op">(</span>Dataloader <span class="op">*</span>loader<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>inputs<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>targets<span class="op">);</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> dataloader_reset<span class="op">(</span>Dataloader <span class="op">*</span>loader<span class="op">);</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> dataloader_free<span class="op">(</span>Dataloader <span class="op">*</span>loader<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Here’s what each does:</p>
<ul>
<li><code>dataloader_init</code>: opens the dataset file, remembers batch size <code>B</code> and sequence length <code>T</code>.</li>
<li><code>dataloader_next_batch</code>: returns the next chunk of <code>B × T</code> tokens (inputs) and their shifted version (targets).</li>
<li><code>dataloader_reset</code>: rewinds to the start of the file when an epoch ends.</li>
<li><code>dataloader_free</code>: closes the file and releases memory.</li>
</ul>
<p>This design keeps the training loop clean: just call <code>next_batch</code> and you get the data ready for forward/backward passes.</p>
</section>
<section id="b-t-explained" class="level4">
<h4 class="anchored" data-anchor-id="b-t-explained">B × T Explained</h4>
<p>The two most important parameters are:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Symbol</th>
<th>Meaning</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>B</td>
<td>Batch size (how many sequences per step)</td>
<td>16</td>
</tr>
<tr class="even">
<td>T</td>
<td>Sequence length (how many tokens per sequence)</td>
<td>1024</td>
</tr>
</tbody>
</table>
<p>So one batch contains <code>B × T</code> tokens. For example, with <code>B = 16</code> and <code>T = 1024</code>, each batch holds 16,384 tokens. The DataLoader simply reads that many numbers from the binary file and arranges them in memory.</p>
</section>
<section id="strides-through-the-dataset" class="level4">
<h4 class="anchored" data-anchor-id="strides-through-the-dataset">Strides Through the Dataset</h4>
<p>As you call <code>dataloader_next_batch</code>, the loader moves forward through the dataset by <code>B × T</code> tokens each time. When it reaches the end of the dataset file, it either:</p>
<ul>
<li>Resets back to the beginning (<code>dataloader_reset</code>), or</li>
<li>Switches from training to validation, depending on the training loop’s needs.</li>
</ul>
<p>This stride-based reading is efficient: no random access, just sequential reads from a file.</p>
</section>
<section id="epochs-and-shuffling" class="level4">
<h4 class="anchored" data-anchor-id="epochs-and-shuffling">Epochs and Shuffling</h4>
<p>In deep learning, an epoch means one full pass through the dataset. The DataLoader in <em>llm.c</em> is simple: it goes linearly from start to finish. It doesn’t shuffle data like PyTorch’s <code>DataLoader</code>. Why? Because language data is already very diverse, and the project values minimal code over extra features. If you want shuffling, you can preprocess the dataset differently before creating <code>.bin</code> files.</p>
</section>
<section id="why-it-matters-3" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-3">Why It Matters</h4>
<p>The DataLoader is the quiet workhorse of training. It ensures that every step sees a fresh batch of token sequences, always with matching inputs and targets. By separating dataset reading from the training loop, the code stays clean and focused. This design also makes it easy to swap datasets-once you generate a <code>.bin</code> file, the loader doesn’t care where it came from.</p>
</section>
<section id="try-it-yourself-3" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-3">Try It Yourself</h4>
<ol type="1">
<li>Print the first batch: Modify the code to print the first 20 input tokens and their targets. See how each input token aligns with the next target token.</li>
<li>Experiment with B and T: Set <code>B = 2</code> and <code>T = 8</code> and observe how the loader slices the dataset into tiny chunks. Then try larger values and see how memory usage changes.</li>
<li>Check epoch length: Write a small loop to count how many batches you get before <code>dataloader_reset</code> is called. Does this match the total tokens divided by <code>B × T</code>?</li>
<li>Validation check: Observe how often the training loop switches to <code>val.bin</code>. How does validation loss compare to training loss over time?</li>
<li>Custom stride: Modify the code so the DataLoader skips some tokens between batches. What effect does this have on training?</li>
</ol>
</section>
<section id="the-takeaway-3" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-3">The Takeaway</h4>
<p>The DataLoader in <em>llm.c</em> is intentionally simple. It streams token IDs in fixed-sized batches, moves forward stride by stride, and resets when done. This straightforward design avoids complexity and keeps the focus on the model itself, while still teaching you the essential mechanics of batching and sequence handling in language model training.</p>
</section>
</section>
<section id="evalloader-and-validation-workflow" class="level3">
<h3 class="anchored" data-anchor-id="evalloader-and-validation-workflow">15. EvalLoader and Validation Workflow</h3>
<p>Training a model isn’t just about watching the training loss go down. To know whether the model is actually learning patterns that generalize-and not just memorizing the training data-you need to run validation. In <em>llm.c</em>, validation is handled by a component called the EvalLoader, which works just like the DataLoader but reads from the validation dataset (<code>val.bin</code>) instead of the training dataset (<code>train.bin</code>).</p>
<section id="why-we-need-validation" class="level4">
<h4 class="anchored" data-anchor-id="why-we-need-validation">Why We Need Validation</h4>
<p>Imagine teaching a student only by drilling them with the same math problems over and over. They might get really good at those problems, but fail completely when given new ones. Validation is like giving the student a pop quiz with unseen questions. If they do well, you know they’ve actually learned the concepts.</p>
<p>For language models, validation helps detect overfitting: when the training loss keeps improving but the validation loss stays flat or even gets worse.</p>
</section>
<section id="how-evalloader-works" class="level4">
<h4 class="anchored" data-anchor-id="how-evalloader-works">How EvalLoader Works</h4>
<p>EvalLoader lives in the same code file as the DataLoader (<code>llmc/dataloader.h</code>), but it points to a different dataset file. Its workflow is nearly identical:</p>
<ol type="1">
<li>Open <code>val.bin</code> and prepare for reading.</li>
<li>Serve up batches of size <code>B × T</code> (batch size × sequence length).</li>
<li>Provide inputs and targets the same way as the training DataLoader.</li>
<li>Reset after one full pass through the file.</li>
</ol>
<p>The training loop typically calls the EvalLoader at intervals-for example, every few hundred steps-so you get a snapshot of validation loss during training.</p>
</section>
<section id="what-happens-during-validation" class="level4">
<h4 class="anchored" data-anchor-id="what-happens-during-validation">What Happens During Validation</h4>
<p>When validation is triggered:</p>
<ol type="1">
<li>The current model parameters are frozen (no gradient updates).</li>
<li>A few batches are read from <code>val.bin</code>.</li>
<li>The model runs forward passes only, computing the loss on each batch.</li>
<li>The losses are averaged and reported as validation loss.</li>
</ol>
<p>This doesn’t take long because it usually samples just a subset of the validation dataset, not the entire file.</p>
</section>
<section id="training-loop-with-validation" class="level4">
<h4 class="anchored" data-anchor-id="training-loop-with-validation">Training Loop with Validation</h4>
<p>In pseudocode, the loop looks like this:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb38"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span>step <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> step <span class="op">&lt;</span> max_steps<span class="op">;</span> step<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    dataloader_next_batch<span class="op">(&amp;</span>train_loader<span class="op">,</span> inputs<span class="op">,</span> targets<span class="op">);</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>    forward_backward_update<span class="op">(</span>model<span class="op">,</span> inputs<span class="op">,</span> targets<span class="op">);</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>step <span class="op">%</span> eval_interval <span class="op">==</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>        <span class="dt">float</span> val_loss <span class="op">=</span> <span class="fl">0.0</span><span class="bu">f</span><span class="op">;</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> eval_batches<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>            evalloader_next_batch<span class="op">(&amp;</span>val_loader<span class="op">,</span> inputs<span class="op">,</span> targets<span class="op">);</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>            val_loss <span class="op">+=</span> forward_only<span class="op">(</span>model<span class="op">,</span> inputs<span class="op">,</span> targets<span class="op">);</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>        val_loss <span class="op">/=</span> eval_batches<span class="op">;</span></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>        printf<span class="op">(</span><span class="st">"step </span><span class="sc">%d</span><span class="st">: val loss </span><span class="sc">%.4f\n</span><span class="st">"</span><span class="op">,</span> step<span class="op">,</span> val_loss<span class="op">);</span></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This is simplified, but it shows the idea: the validation loop is nested inside the training loop, running occasionally instead of every step.</p>
</section>
<section id="why-it-matters-4" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-4">Why It Matters</h4>
<p>Validation is the reality check of training. Without it, you could train forever and celebrate low training losses, only to discover that your model produces nonsense on new text. By tracking validation loss, you can:</p>
<ul>
<li>Detect overfitting early.</li>
<li>Adjust hyperparameters (like learning rate or batch size).</li>
<li>Know when training has plateaued and it’s time to stop.</li>
</ul>
<p>In professional setups, validation curves are often plotted live, but in <em>llm.c</em>, the minimalist approach is to just print numbers to the console.</p>
</section>
<section id="try-it-yourself-4" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-4">Try It Yourself</h4>
<ol type="1">
<li>Watch val loss: Run training and note how validation loss compares to training loss. Do they both decrease together?</li>
<li>Overfitting demo: Train on a very tiny dataset (like 10 KB of text). Notice how training loss plummets but validation loss stalls or rises.</li>
<li>Change eval interval: Reduce <code>eval_interval</code> so validation runs every step. How much slower does training feel?</li>
<li>Change eval batches: Set <code>eval_batches</code> to 1 vs 100. What difference does this make in the stability of the reported validation loss?</li>
<li>Validation as stopping rule: Stop training when validation loss stops improving for many intervals. How does this affect final performance?</li>
</ol>
</section>
<section id="the-takeaway-4" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-4">The Takeaway</h4>
<p>The EvalLoader is a twin of the DataLoader, but for validation. It feeds the model data it has never seen during training, and the resulting validation loss tells you whether your model is learning useful patterns or just memorizing. It’s the simplest safeguard against wasted compute, and it’s an essential part of every training loop-even in the stripped-down world of <em>llm.c</em>.</p>
</section>
</section>
<section id="sequence-length-and-memory-budgeting" class="level3">
<h3 class="anchored" data-anchor-id="sequence-length-and-memory-budgeting">16. Sequence Length and Memory Budgeting</h3>
<p>When training GPT-2 in <em>llm.c</em>, one of the most important decisions you make is choosing the sequence length (often called T). This value determines how many tokens the model processes in a single forward pass. It might sound like just another parameter, but sequence length has a huge impact on what the model can learn, how much memory it uses, and how fast training runs.</p>
<section id="what-sequence-length-means" class="level4">
<h4 class="anchored" data-anchor-id="what-sequence-length-means">What Sequence Length Means</h4>
<p>Sequence length is simply the number of tokens per training example. If <code>T = 1024</code>, the model reads 1,024 tokens in a row (like words or subwords) and tries to predict the next token at each position.</p>
<p>Think of it like this: if you give the model a paragraph of text, sequence length is how much of that paragraph it sees at once. Shorter lengths give the model less context, while longer lengths allow it to capture bigger patterns, like whole paragraphs or even multiple pages.</p>
</section>
<section id="where-it-appears-in-the-code" class="level4">
<h4 class="anchored" data-anchor-id="where-it-appears-in-the-code">Where It Appears in the Code</h4>
<p>In the logs, you’ll often see lines like:</p>
<pre><code>max_seq_len: 1024</code></pre>
<p>This number is defined in the GPT-2 configuration and passed into the DataLoader. The DataLoader slices chunks of exactly <code>T</code> tokens from <code>train.bin</code> and <code>val.bin</code>. The model itself has fixed positional embeddings of size <code>T</code>, so it cannot process sequences longer than this maximum.</p>
</section>
<section id="memory-costs-of-longer-sequences" class="level4">
<h4 class="anchored" data-anchor-id="memory-costs-of-longer-sequences">Memory Costs of Longer Sequences</h4>
<p>Transformers are powerful but expensive. The attention mechanism compares every token to every other token in the sequence. This means memory and compute scale with the square of sequence length:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Sequence Length (T)</th>
<th>Relative Attention Cost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>256</td>
<td>1×</td>
</tr>
<tr class="even">
<td>512</td>
<td>4×</td>
</tr>
<tr class="odd">
<td>1024</td>
<td>16×</td>
</tr>
<tr class="even">
<td>2048</td>
<td>64×</td>
</tr>
</tbody>
</table>
<p>So doubling <code>T</code> doesn’t just double the cost-it multiplies it by four. That’s why training at long context lengths requires a lot of GPU memory.</p>
</section>
<section id="trade-offs" class="level4">
<h4 class="anchored" data-anchor-id="trade-offs">Trade-offs</h4>
<ul>
<li>Shorter sequences: Faster, less memory, but limited context. Good for quick experiments or tiny datasets like Tiny Shakespeare.</li>
<li>Longer sequences: More memory, slower, but the model can understand larger spans of text. Required for large-scale GPT-2 training.</li>
</ul>
<p>You can think of sequence length as a dial: turning it up increases the model’s ability to “remember,” but it also makes training much heavier.</p>
</section>
<section id="practical-choices-in-llm.c" class="level4">
<h4 class="anchored" data-anchor-id="practical-choices-in-llm.c">Practical Choices in <em>llm.c</em></h4>
<ul>
<li>Tiny Shakespeare example: often trained with <code>T = 64</code> or <code>128</code> for speed.</li>
<li>GPT-2 small (124M): typically uses <code>T = 1024</code>, the same as the original paper.</li>
<li>If your GPU has limited memory, you might need to shrink <code>T</code> and/or batch size <code>B</code>.</li>
</ul>
</section>
<section id="why-it-matters-5" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-5">Why It Matters</h4>
<p>Choosing sequence length is about balancing learning power against hardware limits. A too-small sequence length can prevent the model from capturing long-term dependencies. A too-large one can make training impossible on your hardware. Every run of <em>llm.c</em> is a negotiation between what you’d like the model to see and what your system can handle.</p>
</section>
<section id="try-it-yourself-5" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-5">Try It Yourself</h4>
<ol type="1">
<li>Short vs long: Train Tiny Shakespeare with <code>T = 64</code> and then <code>T = 256</code>. Compare both the speed and the coherence of generated text.</li>
<li>Memory test: Increase <code>T</code> step by step until you hit an out-of-memory (OOM) error. Note the maximum your GPU can handle.</li>
<li>Batch trade-off: Try reducing batch size <code>B</code> while increasing <code>T</code>. Can you keep GPU memory stable while giving the model more context?</li>
<li>Validation impact: Run with different <code>T</code> values and watch how validation loss behaves. Does longer context always help?</li>
<li>Inspect embeddings: Print out the shape of the positional embeddings. Notice how they are always tied to <code>T</code>.</li>
</ol>
</section>
<section id="the-takeaway-5" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-5">The Takeaway</h4>
<p>Sequence length (<code>T</code>) controls how much context the model sees. It directly determines the size of the positional embeddings, the structure of batches, and the memory required for attention. In <em>llm.c</em>, adjusting <code>T</code> is one of the fastest ways to explore the trade-offs between speed, memory, and model capability.</p>
</section>
</section>
<section id="reproducibility-and-seeding-across-runs" class="level3">
<h3 class="anchored" data-anchor-id="reproducibility-and-seeding-across-runs">17. Reproducibility and Seeding Across Runs</h3>
<p>When training machine learning models, it’s common to notice that two runs-using the same code and the same dataset-don’t produce exactly the same results. This happens because many parts of training involve randomness. In <em>llm.c</em>, reproducibility is controlled by random seeds. A seed is a starting point for a random number generator. If you always start from the same seed, the sequence of “random” numbers will be identical, and so will the training run.</p>
<section id="where-randomness-appears" class="level4">
<h4 class="anchored" data-anchor-id="where-randomness-appears">Where Randomness Appears</h4>
<p>Even in a small project like <em>llm.c</em>, randomness shows up in several places:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 80%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Random Role</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Weight initialization</td>
<td>The model’s parameters (like attention matrices) are set randomly at the start.</td>
</tr>
<tr class="even">
<td>Optimizer states</td>
<td>Some optimizers use random noise (though AdamW is mostly deterministic).</td>
</tr>
<tr class="odd">
<td>Sampling outputs</td>
<td>When generating text, randomness decides which token to pick if probabilities are close.</td>
</tr>
<tr class="even">
<td>Parallelism</td>
<td>On GPU, threads may execute in slightly different orders, sometimes introducing small nondeterminism.</td>
</tr>
</tbody>
</table>
<p>Without a fixed seed, every training run can drift apart, even if all settings look the same.</p>
</section>
<section id="how-llm.c-handles-seeds" class="level4">
<h4 class="anchored" data-anchor-id="how-llm.c-handles-seeds">How <em>llm.c</em> Handles Seeds</h4>
<p>The repository provides a small random utilities module: <code>llmc/rand.h</code>. Inside you’ll find functions such as:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb40"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> manual_seed<span class="op">(</span><span class="dt">uint64_t</span> seed<span class="op">);</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="dt">float</span> normal_<span class="op">(</span><span class="dt">float</span> mean<span class="op">,</span> <span class="dt">float</span> std<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><code>manual_seed</code> sets the seed for the internal random number generator, ensuring reproducibility.</li>
<li><code>normal_</code> is used for initializing weights with Gaussian noise, similar to PyTorch’s <code>torch.nn.init.normal_</code>.</li>
</ul>
<p>When you call <code>manual_seed(1337);</code>, the model weights will be initialized the same way every time.</p>
</section>
<section id="why-seeds-dont-guarantee-perfect-reproducibility" class="level4">
<h4 class="anchored" data-anchor-id="why-seeds-dont-guarantee-perfect-reproducibility">Why Seeds Don’t Guarantee Perfect Reproducibility</h4>
<p>Even with a fixed seed, you may still see small differences:</p>
<ul>
<li>GPU kernels sometimes use parallel algorithms that are not bitwise deterministic.</li>
<li>Floating-point math can produce slightly different rounding on different hardware.</li>
<li>Multi-GPU runs (via NCCL/MPI) may introduce nondeterministic reduce operations.</li>
</ul>
<p>These differences are usually tiny-validation loss might vary by 0.001-but they exist. For most educational purposes, <em>llm.c</em> seeds are enough to make experiments repeatable.</p>
</section>
<section id="typical-defaults" class="level4">
<h4 class="anchored" data-anchor-id="typical-defaults">Typical Defaults</h4>
<p>In many examples, you’ll see:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb41"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>manual_seed<span class="op">(</span><span class="dv">1337</span><span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This “magic number” 1337 is just a convention. You can change it to any integer. Using the same seed across runs guarantees the same starting weights, which helps when comparing hyperparameters.</p>
</section>
<section id="why-it-matters-6" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-6">Why It Matters</h4>
<p>Reproducibility is crucial in machine learning because it lets you:</p>
<ul>
<li>Debug effectively: If a bug appears, you want it to appear consistently.</li>
<li>Compare settings: You can test learning rates or batch sizes fairly by keeping everything else the same.</li>
<li>Share results: Other people can run your exact setup and see the same outcomes.</li>
</ul>
<p>Without seeds, it becomes hard to tell whether a difference came from your hyperparameter change or just random luck.</p>
</section>
<section id="try-it-yourself-6" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-6">Try It Yourself</h4>
<ol type="1">
<li>Run twice with same seed: Train GPT-2 with <code>manual_seed(1337)</code> set. Do you get identical training loss curves?</li>
<li>Change the seed: Try <code>manual_seed(42)</code> and compare the loss curve. How similar are they? Do they converge to about the same final validation loss?</li>
<li>Remove seeding: Comment out the seed line and run again. Notice how runs diverge.</li>
<li>Sampling experiment: With a fixed seed, generate text multiple times. Then change the seed and generate again. See how outputs change.</li>
<li>Multi-GPU test: If you have more than one GPU, run the same seed across devices. Do results stay exactly the same or only approximately?</li>
</ol>
</section>
<section id="the-takeaway-6" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-6">The Takeaway</h4>
<p>Reproducibility in <em>llm.c</em> comes from setting seeds for random number generators. While floating-point quirks mean you can’t always get perfect bit-for-bit matches, seeds let you control the biggest source of randomness: weight initialization and sampling. With seeding, you can debug, compare, and share results confidently.</p>
</section>
</section>
<section id="error-surfaces-from-bad-data-bounds-asserts" class="level3">
<h3 class="anchored" data-anchor-id="error-surfaces-from-bad-data-bounds-asserts">18. Error Surfaces from Bad Data (Bounds, Asserts)</h3>
<p>When training a model in <em>llm.c</em>, everything depends on the quality and correctness of the data you feed in. If the dataset or batches contain mistakes, the training process can go off track quickly-sometimes by crashing outright, other times by producing strange loss values like <code>NaN</code>. To guard against this, the code uses bounds checks and asserts that catch problems early.</p>
<section id="what-can-go-wrong-with-data" class="level4">
<h4 class="anchored" data-anchor-id="what-can-go-wrong-with-data">What Can Go Wrong with Data</h4>
<p>There are several common data issues:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 87%">
</colgroup>
<thead>
<tr class="header">
<th>Problem</th>
<th>What Happens</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Token ID out of range</td>
<td>The model expects IDs between 0 and <code>vocab_size-1</code>. A wrong ID can cause array indexing errors.</td>
</tr>
<tr class="even">
<td>Empty or short dataset</td>
<td>The DataLoader may run out of tokens before filling a batch.</td>
</tr>
<tr class="odd">
<td>Mismatched tokenizer</td>
<td>If you build a dataset with a different tokenizer, IDs may not correspond to the GPT-2 tokenizer in <code>gpt2_tokenizer.bin</code>. This produces nonsense outputs.</td>
</tr>
<tr class="even">
<td>Corrupt <code>.bin</code> files</td>
<td>If files are incomplete or written incorrectly, the DataLoader might read garbage values.</td>
</tr>
</tbody>
</table>
<p>These errors show up as segfaults, invalid memory access, or exploding losses during training.</p>
</section>
<section id="how-llm.c-defends-against-bad-data" class="level4">
<h4 class="anchored" data-anchor-id="how-llm.c-defends-against-bad-data">How <em>llm.c</em> Defends Against Bad Data</h4>
<p>The repository makes heavy use of asserts-simple checks that stop the program immediately if something unexpected happens. For example, in <code>llmc/utils.h</code>, functions like <code>freadCheck</code> and <code>mallocCheck</code> ensure that file reads and memory allocations succeed. If not, they print an error message and abort instead of silently failing.</p>
<p>Inside the DataLoader, token IDs are often validated to make sure they fall inside the expected vocabulary range. If you try to access an invalid index in the embedding table, the program will crash quickly, which is better than continuing with corrupted values.</p>
</section>
<section id="example-vocab-range-check" class="level4">
<h4 class="anchored" data-anchor-id="example-vocab-range-check">Example: Vocab Range Check</h4>
<p>During training, every input token is used to look up a row in the embedding matrix. If a token ID is too large, you’d access memory outside the matrix. This is why checking <code>0 &lt;= id &lt; vocab_size</code> is essential. In C, asserts provide this safety net.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb42"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>assert<span class="op">(</span>id <span class="op">&gt;=</span> <span class="dv">0</span> <span class="op">&amp;&amp;</span> id <span class="op">&lt;</span> vocab_size<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This kind of check may look simple, but it saves hours of debugging mysterious crashes.</p>
</section>
<section id="error-surfaces-in-loss" class="level4">
<h4 class="anchored" data-anchor-id="error-surfaces-in-loss">Error Surfaces in Loss</h4>
<p>Even if your program doesn’t crash, bad data can create “error surfaces” in the loss function:</p>
<ul>
<li>NaNs: Appear when invalid values propagate through softmax, layernorm, or division operations.</li>
<li>Flat loss: If the dataset is empty or repetitive, the model never improves.</li>
<li>Mismatch behavior: Training loss decreases but validation loss stays high if training and validation sets use inconsistent tokenization.</li>
</ul>
<p>These are signs that something is wrong with the dataset or preprocessing.</p>
</section>
<section id="why-it-matters-7" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-7">Why It Matters</h4>
<p>C is a low-level language with very little safety by default. One out-of-range index can corrupt memory and cause unpredictable bugs. By aggressively checking assumptions (file sizes, vocab bounds, token IDs), <em>llm.c</em> turns hard-to-find errors into immediate, clear failures. For learners, this makes it much easier to understand what went wrong.</p>
</section>
<section id="try-it-yourself-7" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-7">Try It Yourself</h4>
<ol type="1">
<li>Corrupt a dataset: Open <code>train.bin</code> and delete a few bytes. Run training and see what error appears. Notice how quickly asserts catch it.</li>
<li>Force a bad ID: Modify the DataLoader to add <code>+100000</code> to a token. Does the model crash with an assertion?</li>
<li>Skip asserts: Temporarily disable checks and rerun. Compare how much harder it is to figure out what went wrong.</li>
<li>Validation mismatch: Tokenize a file with a different tokenizer and save it as <code>val.bin</code>. Watch how the validation loss behaves compared to training loss.</li>
<li>Print debug info: Add logging to display the first 20 tokens of each batch. Can you spot bad data before it crashes?</li>
</ol>
</section>
<section id="the-takeaway-7" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-7">The Takeaway</h4>
<p>Bad data can silently sabotage training, but <em>llm.c</em> uses asserts and bounds checks to make errors loud and clear. This design choice helps learners focus on the real logic of transformers instead of chasing hidden bugs caused by corrupted or mismatched datasets. In machine learning, good data hygiene and strict validation are as important as the model itself.</p>
</section>
</section>
<section id="tokenization-edge-cases-unks-eos-bos" class="level3">
<h3 class="anchored" data-anchor-id="tokenization-edge-cases-unks-eos-bos">19. Tokenization Edge Cases (UNKs, EOS, BOS)</h3>
<p>Tokenization looks simple at first: take text, split it into tokens, and assign each token an ID. But in practice, there are always tricky situations. <em>llm.c</em> inherits the quirks of the GPT-2 tokenizer, which is byte-level BPE (Byte Pair Encoding). This design mostly avoids “unknown” tokens, but it still has details you need to understand when preparing datasets or interpreting outputs.</p>
<section id="no-true-unk-in-gpt-2" class="level4">
<h4 class="anchored" data-anchor-id="no-true-unk-in-gpt-2">No True “UNK” in GPT-2</h4>
<p>Some tokenizers, like those used in earlier NLP systems, include a special <code>UNK</code> (unknown) token for words that aren’t in the vocabulary. GPT-2 avoids this problem by working at the byte level:</p>
<ul>
<li>Every possible byte (0–255) is in the base vocabulary.</li>
<li>If the tokenizer doesn’t know how to split a character or word, it just falls back to raw bytes.</li>
</ul>
<p>That means you will never see an <code>UNK</code> token in <em>llm.c</em>. Any input text is always representable. This is one of the main reasons GPT-2’s tokenizer is so robust.</p>
</section>
<section id="special-tokens-eos-and-bos" class="level4">
<h4 class="anchored" data-anchor-id="special-tokens-eos-and-bos">Special Tokens: EOS and BOS</h4>
<p>Even though GPT-2 doesn’t use <code>UNK</code>, it does use other special tokens:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 14%">
<col style="width: 64%">
</colgroup>
<thead>
<tr class="header">
<th>Token</th>
<th>ID</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>EOS (End of Sequence)</td>
<td>50256</td>
<td>Marks the end of a text segment. Used during training and sampling.</td>
</tr>
<tr class="even">
<td>BOS (Beginning of Sequence)</td>
<td>Not explicit in GPT-2</td>
<td>GPT-2 doesn’t use a fixed BOS token. Instead, the model assumes generation starts at position 0.</td>
</tr>
</tbody>
</table>
<p>In <em>llm.c</em>, you’ll often see <code>EOS</code> at the end of training sequences or when sampling text. If you generate text and see strange endings, it’s usually because the model predicted <code>EOS</code>.</p>
</section>
<section id="whitespace-quirks" class="level4">
<h4 class="anchored" data-anchor-id="whitespace-quirks">Whitespace Quirks</h4>
<p>The tokenizer also handles whitespace in a slightly unusual way. For example, the word “hello” and the word ” hello” (with a leading space) map to different tokens. This is why generated text sometimes starts with a space-it’s part of the token definition.</p>
<p>Example:</p>
<ul>
<li><code>"hello"</code> → token ID 31373</li>
<li><code>" hello"</code> → token ID 15496</li>
</ul>
<p>This is normal behavior for GPT-2. It helps the model capture spacing and punctuation consistently.</p>
</section>
<section id="unicode-and-rare-characters" class="level4">
<h4 class="anchored" data-anchor-id="unicode-and-rare-characters">Unicode and Rare Characters</h4>
<p>Because it’s byte-level, GPT-2 can encode emojis, accented characters, or even binary junk data. But the BPE merges are optimized for English, so rare characters often get split into multiple byte tokens. That means sequences with lots of rare symbols (like Chinese or emojis) will use more tokens than plain English text.</p>
</section>
<section id="why-it-matters-8" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-8">Why It Matters</h4>
<p>Edge cases in tokenization affect both dataset preparation and model outputs. If you see weird spacing or early <code>EOS</code> tokens, it’s not a bug-it’s just how the tokenizer works. Understanding these quirks helps you debug outputs and prepare datasets without surprises.</p>
</section>
<section id="try-it-yourself-8" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-8">Try It Yourself</h4>
<ol type="1">
<li>EOS inspection: Open <code>val.bin</code> with a hex viewer and look for token ID <code>50256</code>. These mark the ends of text segments.</li>
<li>Whitespace check: Use the tokenizer to encode <code>"hello"</code> and <code>" hello"</code>. Compare the token IDs.</li>
<li>Emoji test: Encode a string with emojis (e.g., <code>"🙂🙂🙂"</code>) and see how many tokens it becomes.</li>
<li>Rare character dataset: Create a small <code>.txt</code> file with accented characters and tokenize it. How many bytes does each character consume?</li>
<li>Sampling experiment: Generate text until you see the EOS token appear. Notice how the model “knows” to stop.</li>
</ol>
</section>
<section id="the-takeaway-8" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-8">The Takeaway</h4>
<p>Tokenization in GPT-2 is robust, but it has quirks. There are no unknown tokens thanks to byte-level encoding, but whitespace and special tokens like <code>EOS</code> play important roles. By experimenting with these edge cases, you’ll develop an intuition for how raw text is mapped into the numbers that drive training and generation in <em>llm.c</em>.</p>
</section>
</section>
<section id="data-hygiene-and-logging" class="level3">
<h3 class="anchored" data-anchor-id="data-hygiene-and-logging">20. Data Hygiene and Logging</h3>
<p>When training with <em>llm.c</em>, having clean data is just as important as having the right model code. If the dataset contains errors, duplicates, or formatting issues, the model may waste capacity memorizing noise instead of learning useful patterns. This is where data hygiene comes in-making sure your training and validation sets are prepared properly. Alongside this, logging ensures you can monitor what’s happening during training and catch problems early.</p>
<section id="what-data-hygiene-means" class="level4">
<h4 class="anchored" data-anchor-id="what-data-hygiene-means">What Data Hygiene Means</h4>
<p>Data hygiene is about making sure your dataset is both valid and useful. For language models, this includes:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 77%">
</colgroup>
<thead>
<tr class="header">
<th>Check</th>
<th>Why It Matters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Correct tokenization</td>
<td>Must match the tokenizer (<code>gpt2_tokenizer.bin</code>), otherwise IDs won’t line up.</td>
</tr>
<tr class="even">
<td>No corrupt files</td>
<td>Binary <code>.bin</code> files must be complete; partial writes cause crashes.</td>
</tr>
<tr class="odd">
<td>Balanced splits</td>
<td>Training and validation sets should come from the same distribution.</td>
</tr>
<tr class="even">
<td>Reasonable size</td>
<td>Too small → overfitting. Too large → slow or infeasible.</td>
</tr>
<tr class="odd">
<td>Deduplication</td>
<td>Repeated passages (e.g., web scrapes) make models memorize instead of generalize.</td>
</tr>
</tbody>
</table>
<p>The scripts in <code>dev/data/</code> handle basic hygiene by tokenizing consistently and splitting into train/val sets. But if you bring your own dataset, you are responsible for cleaning it first.</p>
</section>
<section id="logging-during-training" class="level4">
<h4 class="anchored" data-anchor-id="logging-during-training">Logging During Training</h4>
<p>Once training starts, logging becomes your window into what’s happening. <em>llm.c</em> uses a minimal logging system (<code>llmc/logger.h</code>) to print progress to the console. Typical logs include:</p>
<pre><code>step 0: train loss 5.19, val loss 5.32
step 100: train loss 4.87, val loss 5.01
step 200: train loss 4.62, val loss 4.88</code></pre>
<p>These numbers let you track:</p>
<ul>
<li>Training loss: Is the model fitting the data?</li>
<li>Validation loss: Is it generalizing, or overfitting?</li>
<li>Step timing: How long each batch takes, useful for profiling.</li>
</ul>
<p>Even in such a small project, this logging loop gives you most of what you need to debug runs.</p>
</section>
<section id="why-hygiene-and-logging-go-together" class="level4">
<h4 class="anchored" data-anchor-id="why-hygiene-and-logging-go-together">Why Hygiene and Logging Go Together</h4>
<p>Bad data often reveals itself in the logs. For example:</p>
<ul>
<li>If validation loss is much higher than training loss, your validation set may be mismatched.</li>
<li>If loss suddenly becomes <code>NaN</code>, your dataset might contain corrupt tokens.</li>
<li>If loss plateaus at a high value, you may have too little data or poor preprocessing.</li>
</ul>
<p>By keeping your data clean and watching logs closely, you can detect these issues early instead of wasting hours of compute.</p>
</section>
<section id="try-it-yourself-9" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-9">Try It Yourself</h4>
<ol type="1">
<li>Dirty dataset test: Take a <code>.txt</code> file, add random symbols or binary junk, and prepare a <code>.bin</code> dataset. What happens to training loss?</li>
<li>Duplicate passages: Copy the same paragraph 100 times into a training file. Does validation loss improve, or does the model just memorize?</li>
<li>Log frequency: Modify the code to log every step instead of every N steps. How noisy are the results?</li>
<li>Custom logger: Extend the logger to also print gradient norms or learning rate values. Does this help you understand training dynamics better?</li>
<li>Compare splits: Build two datasets with different train/val splits. Which one gives more stable validation losses?</li>
</ol>
</section>
<section id="the-takeaway-9" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-9">The Takeaway</h4>
<p>Data hygiene ensures the model learns from clean, consistent input, while logging ensures you can see whether learning is actually happening. Together, they form the foundation of reliable experiments in <em>llm.c</em>. If you clean your data carefully and pay attention to the logs, you’ll catch most problems before they become serious.</p>
</section>
</section>
</section>
<section id="chapter-3.-model-definition-and-weights" class="level2">
<h2 class="anchored" data-anchor-id="chapter-3.-model-definition-and-weights">Chapter 3. Model Definition and Weights</h2>
<section id="gpt-2-config-vocab-layers-heads-channels" class="level3">
<h3 class="anchored" data-anchor-id="gpt-2-config-vocab-layers-heads-channels">21. GPT-2 Config: Vocab, Layers, Heads, Channels</h3>
<p>Every GPT-2 model, no matter how large or small, is defined by a handful of configuration numbers. These numbers decide how big the model is, how much memory it needs, and how powerful it can become. In <em>llm.c</em>, these settings are stored in a simple config struct and printed at the start of training. They describe the “blueprint” of the transformer.</p>
<section id="the-core-parameters" class="level4">
<h4 class="anchored" data-anchor-id="the-core-parameters">The Core Parameters</h4>
<p>Here are the most important values you’ll see in the logs:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 58%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Meaning</th>
<th>Example (GPT-2 Small)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>vocab_size</code></td>
<td>Number of distinct tokens (from tokenizer).</td>
<td>50,257</td>
</tr>
<tr class="even">
<td><code>padded_vocab_size</code></td>
<td>Vocab size rounded up to nearest multiple (for GPU efficiency).</td>
<td>50,304</td>
</tr>
<tr class="odd">
<td><code>max_seq_len</code></td>
<td>Longest sequence of tokens the model can handle.</td>
<td>1,024</td>
</tr>
<tr class="even">
<td><code>num_layers</code></td>
<td>Number of transformer blocks stacked on top of each other.</td>
<td>12</td>
</tr>
<tr class="odd">
<td><code>num_heads</code></td>
<td>Number of attention heads per block.</td>
<td>12</td>
</tr>
<tr class="even">
<td><code>channels</code></td>
<td>Width of hidden states (embedding dimension).</td>
<td>768</td>
</tr>
<tr class="odd">
<td><code>num_parameters</code></td>
<td>Total trainable weights in the model.</td>
<td>~124M</td>
</tr>
</tbody>
</table>
<p>Together, these values define both the structure and the capacity of the model.</p>
</section>
<section id="what-they-control" class="level4">
<h4 class="anchored" data-anchor-id="what-they-control">What They Control</h4>
<ul>
<li>Vocabulary size connects the model to the tokenizer. Every input token ID must be less than <code>vocab_size</code>. The padded version makes GPU matrix multiplications easier.</li>
<li>Max sequence length fixes the size of the positional embeddings. If you set this to 1024, the model can’t read beyond 1024 tokens in one pass.</li>
<li>Layers control model depth. Each layer contains an attention block and an MLP. More layers = more representational power.</li>
<li>Heads divide attention into parallel “subspaces.” With 12 heads, the model can track different types of relationships in the text at the same time.</li>
<li>Channels set the dimensionality of embeddings and hidden vectors. Larger channels mean more expressive representations but also more computation.</li>
<li>Parameters are the sum of it all. This number tells you how heavy the model is to train and how much memory it will consume.</li>
</ul>
</section>
<section id="configs-across-gpt-2-sizes" class="level4">
<h4 class="anchored" data-anchor-id="configs-across-gpt-2-sizes">Configs Across GPT-2 Sizes</h4>
<p>The original GPT-2 models come in several sizes:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Layers</th>
<th>Heads</th>
<th>Channels</th>
<th>Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPT-2 Small</td>
<td>12</td>
<td>12</td>
<td>768</td>
<td>124M</td>
</tr>
<tr class="even">
<td>GPT-2 Medium</td>
<td>24</td>
<td>16</td>
<td>1024</td>
<td>355M</td>
</tr>
<tr class="odd">
<td>GPT-2 Large</td>
<td>36</td>
<td>20</td>
<td>1280</td>
<td>774M</td>
</tr>
<tr class="even">
<td>GPT-2 XL</td>
<td>48</td>
<td>25</td>
<td>1600</td>
<td>1.6B</td>
</tr>
</tbody>
</table>
<p><em>llm.c</em> can scale between these by just changing a few numbers in the config struct.</p>
</section>
<section id="where-config-appears-in-the-code" class="level4">
<h4 class="anchored" data-anchor-id="where-config-appears-in-the-code">Where Config Appears in the Code</h4>
<p>In <code>train_gpt2.c</code> and <code>train_gpt2.cu</code>, you’ll see something like:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb44"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>GPT2Config config <span class="op">=</span> <span class="op">{</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>    <span class="op">.</span>vocab_size <span class="op">=</span> <span class="dv">50257</span><span class="op">,</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>    <span class="op">.</span>max_seq_len <span class="op">=</span> <span class="dv">1024</span><span class="op">,</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">.</span>num_layers <span class="op">=</span> <span class="dv">12</span><span class="op">,</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>    <span class="op">.</span>num_heads <span class="op">=</span> <span class="dv">12</span><span class="op">,</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>    <span class="op">.</span>channels <span class="op">=</span> <span class="dv">768</span><span class="op">,</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="op">};</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Later, the model is initialized using this struct, and the log prints all the derived information (like <code>num_parameters</code>).</p>
</section>
<section id="why-it-matters-9" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-9">Why It Matters</h4>
<p>The config is the contract between your dataset and your model.</p>
<ul>
<li>If <code>vocab_size</code> doesn’t match your tokenizer, you’ll get crashes.</li>
<li>If <code>max_seq_len</code> is too small, you’ll lose context.</li>
<li>If <code>num_layers</code> or <code>channels</code> are too large for your GPU, you’ll run out of memory.</li>
</ul>
<p>By tweaking the config, you decide whether you want a tiny model for learning or a massive one closer to GPT-2 XL.</p>
</section>
<section id="try-it-yourself-10" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-10">Try It Yourself</h4>
<ol type="1">
<li>Print config: Run the trainer and note the printed values. Compare them with the GPT-2 sizes in the table.</li>
<li>Shrink the model: Change <code>num_layers = 4</code>, <code>num_heads = 4</code>, and <code>channels = 256</code>. Train on Tiny Shakespeare and see how fast it runs.</li>
<li>Increase sequence length: Try setting <code>max_seq_len = 2048</code>. Does your GPU still handle it, or do you get out-of-memory errors?</li>
<li>Parameter count check: Compute how many parameters your custom config has. Compare it to the reported <code>num_parameters</code>.</li>
<li>Tokenizer mismatch test: Intentionally set <code>vocab_size = 30000</code> and watch what error appears when loading the tokenizer.</li>
</ol>
</section>
<section id="the-takeaway-10" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-10">The Takeaway</h4>
<p>The GPT-2 config struct in <em>llm.c</em> is small but powerful. It defines everything about the model’s architecture: vocabulary, sequence length, depth, width, and total parameters. By adjusting just a few integers, you can scale from a toy model that runs on CPU to a billion-parameter giant (if your hardware allows it). Understanding these numbers is the first step to understanding how transformer capacity is controlled.</p>
</section>
</section>
<section id="parameter-tensors-and-memory-layout" class="level3">
<h3 class="anchored" data-anchor-id="parameter-tensors-and-memory-layout">22. Parameter Tensors and Memory Layout</h3>
<p>Once the GPT-2 configuration is set, the next big step is to allocate the parameters of the model. These are the trainable numbers-weights and biases-that define how the model processes input tokens. In <em>llm.c</em>, parameters are stored in flat arrays of floats rather than in deeply nested objects like in PyTorch. This choice makes the code easier to read and keeps memory access predictable.</p>
<section id="what-are-parameters" class="level4">
<h4 class="anchored" data-anchor-id="what-are-parameters">What Are Parameters?</h4>
<p>Every part of the transformer has its own trainable weights:</p>
<ul>
<li>Embedding tables: one for tokens and one for positions.</li>
<li>Attention layers: query, key, value, and output projections.</li>
<li>MLP layers: two linear layers plus their biases.</li>
<li>LayerNorms: scale (<code>gamma</code>) and shift (<code>beta</code>) values.</li>
<li>Final projection: maps hidden states back to vocab size for logits.</li>
</ul>
<p>Together, these add up to hundreds of millions of numbers, even for GPT-2 Small.</p>
</section>
<section id="flat-memory-design-in-llm.c" class="level4">
<h4 class="anchored" data-anchor-id="flat-memory-design-in-llm.c">Flat Memory Design in <em>llm.c</em></h4>
<p>Instead of allocating each parameter separately, <em>llm.c</em> stores all parameters in one contiguous block of memory. Each layer is given a slice of this big array.</p>
<p>This has two benefits:</p>
<ol type="1">
<li>Simplicity: You only need one malloc (or cudaMalloc) for all parameters.</li>
<li>Performance: Contiguous memory access is faster on both CPU and GPU.</li>
</ol>
<p>To keep track of where each layer’s weights live inside the block, the code uses offsets.</p>
</section>
<section id="example-in-code" class="level4">
<h4 class="anchored" data-anchor-id="example-in-code">Example in Code</h4>
<p>In <code>train_gpt2.c</code>, parameters are packed into a single array:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb45"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="dt">float</span><span class="op">*</span> params <span class="op">=</span> <span class="op">(</span><span class="dt">float</span><span class="op">*)</span>mallocCheck<span class="op">(</span>config<span class="op">.</span>num_parameters <span class="op">*</span> <span class="kw">sizeof</span><span class="op">(</span><span class="dt">float</span><span class="op">));</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Later, helper functions compute pointers into this array for each sub-module. For example, the token embedding weights are just the first slice:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb46"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="dt">float</span><span class="op">*</span> token_embedding_table <span class="op">=</span> params<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then the program moves forward, assigning chunks to positional embeddings, attention weights, and so on.</p>
</section>
<section id="shapes-of-the-tensors" class="level4">
<h4 class="anchored" data-anchor-id="shapes-of-the-tensors">Shapes of the Tensors</h4>
<p>Even though parameters are stored in 1D memory, they conceptually form 2D or 3D tensors. For example:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 45%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Shape</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Token embeddings</td>
<td><code>[vocab_size, channels]</code></td>
<td>Maps token IDs to vectors.</td>
</tr>
<tr class="even">
<td>Positional embeddings</td>
<td><code>[max_seq_len, channels]</code></td>
<td>Adds position info.</td>
</tr>
<tr class="odd">
<td>Attention weights (Q, K, V, O)</td>
<td><code>[channels, channels]</code></td>
<td>Project hidden states.</td>
</tr>
<tr class="even">
<td>MLP layers</td>
<td><code>[channels, 4×channels]</code> and <code>[4×channels, channels]</code></td>
<td>Expand and contract hidden states.</td>
</tr>
<tr class="odd">
<td>LayerNorm scale/shift</td>
<td><code>[channels]</code></td>
<td>Normalize and rescale features.</td>
</tr>
</tbody>
</table>
<p>When you look at the code, remember: these shapes are “virtual.” They’re just views into slices of the big 1D array.</p>
</section>
<section id="why-this-layout-works-well" class="level4">
<h4 class="anchored" data-anchor-id="why-this-layout-works-well">Why This Layout Works Well</h4>
<p>PyTorch or TensorFlow manage parameter tensors with lots of abstractions. <em>llm.c</em> strips this away: you see the raw memory, the exact number of parameters, and the order they’re laid out in. This makes it clear how large the model really is and why it uses so much RAM or VRAM.</p>
<p>It also means you can easily save and load checkpoints by writing or reading the flat array directly to disk. No need for complicated serialization formats.</p>
</section>
<section id="why-it-matters-10" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-10">Why It Matters</h4>
<p>Understanding parameter layout helps you:</p>
<ul>
<li>See how the model’s size explodes as you increase layers, heads, or channels.</li>
<li>Debug memory issues by checking how big each slice is.</li>
<li>Realize how much of training is just linear algebra on big arrays of floats.</li>
</ul>
<p>This perspective is powerful because it demystifies deep learning: at its core, GPT-2 is just multiplying slices of one giant float array again and again.</p>
</section>
<section id="try-it-yourself-11" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-11">Try It Yourself</h4>
<ol type="1">
<li>Print parameter count: Add a line in the code to print <code>config.num_parameters</code>. Compare it with the table for GPT-2 Small/Medium/Large.</li>
<li>Inspect a slice: Print the first 10 numbers of the embedding table. They’ll look random (from initialization).</li>
<li>Change precision: Modify the code to allocate <code>half</code> (FP16) instead of <code>float</code>. How much memory do you save?</li>
<li>Checkpoint peek: Save a checkpoint, then open it in a hex viewer. It’s just raw floats-proof that parameters are stored flat.</li>
<li>Parameter scaling: Double the number of layers and see how <code>num_parameters</code> changes. Can you predict the increase?</li>
</ol>
</section>
<section id="the-takeaway-11" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-11">The Takeaway</h4>
<p>In <em>llm.c</em>, parameters are not hidden inside classes or objects. They live in one flat block of memory, sliced up by convention into embeddings, attention matrices, MLP weights, and norms. This design makes the relationship between model architecture and memory crystal clear-and reminds you that even a billion-parameter transformer is “just” a giant array of numbers.</p>
</section>
</section>
<section id="embedding-tables-token-positional" class="level3">
<h3 class="anchored" data-anchor-id="embedding-tables-token-positional">23. Embedding Tables: Token + Positional</h3>
<p>Before a transformer can reason about text, it first needs to turn tokens into vectors. In <em>llm.c</em>, this job is handled by the embedding tables: one for tokens, one for positions. These tables are the very first layer of GPT-2, and they transform plain integer IDs into continuous values that the neural network can process.</p>
<section id="token-embedding-table" class="level4">
<h4 class="anchored" data-anchor-id="token-embedding-table">Token Embedding Table</h4>
<p>When you feed in a batch of token IDs, the model looks up their corresponding vectors in the token embedding table.</p>
<ul>
<li><p>Shape: <code>[vocab_size, channels]</code></p>
<ul>
<li><code>vocab_size ≈ 50,257</code> (for GPT-2)</li>
<li><code>channels = hidden size</code> (768 for GPT-2 Small)</li>
</ul></li>
<li><p>Each row corresponds to one token in the vocabulary.</p></li>
<li><p>Each row is a dense vector of size <code>channels</code>.</p></li>
</ul>
<p>So if your input batch has size <code>(B, T)</code>, looking up embeddings gives you a tensor of shape <code>(B, T, channels)</code>.</p>
<p>In the code, this is implemented as an array slice from the flat parameter block:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb47"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="dt">float</span><span class="op">*</span> token_embedding_table <span class="op">=</span> params<span class="op">;</span>  <span class="co">// first slice of parameters</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>At runtime, token IDs index directly into this table.</p>
</section>
<section id="positional-embedding-table" class="level4">
<h4 class="anchored" data-anchor-id="positional-embedding-table">Positional Embedding Table</h4>
<p>Transformers don’t inherently know about word order. That’s what positional embeddings are for.</p>
<ul>
<li><p>Shape: <code>[max_seq_len, channels]</code></p>
<ul>
<li><code>max_seq_len = 1024</code> in GPT-2 Small</li>
<li>Same channel dimension as token embeddings</li>
</ul></li>
<li><p>Each position (0, 1, 2, …, 1023) has its own vector.</p></li>
</ul>
<p>During training, when the model sees token <code>i</code> at position <code>j</code>, it takes the token embedding vector and adds the positional embedding vector for <code>j</code>. This gives the model both word identity and word position.</p>
<p>In <em>llm.c</em>, positional embeddings immediately follow the token embeddings in the flat parameter array.</p>
</section>
<section id="adding-them-together" class="level4">
<h4 class="anchored" data-anchor-id="adding-them-together">Adding Them Together</h4>
<p>The embedding layer’s forward pass is simple:</p>
<pre><code>embedding_out[token, pos] = token_embedding[token] + positional_embedding[pos]</code></pre>
<p>This results in a <code>(B, T, channels)</code> tensor that becomes the input to the first transformer block.</p>
</section>
<section id="why-this-matters-1" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-1">Why This Matters</h4>
<p>Embeddings are the bridge between discrete tokens and continuous math. Without them, the model couldn’t use linear algebra to learn patterns. By adding positional embeddings, GPT-2 knows the difference between:</p>
<ul>
<li>“dog bites man” → <code>dog</code> comes first, <code>man</code> comes last</li>
<li>“man bites dog” → same tokens, but swapped positions change the meaning</li>
</ul>
<p>This small step is essential: order and identity must both be captured before attention can begin.</p>
</section>
<section id="try-it-yourself-12" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-12">Try It Yourself</h4>
<ol type="1">
<li>Inspect shapes: Print the sizes of the token and positional embedding tables during initialization. Confirm they match <code>[vocab_size, channels]</code> and <code>[max_seq_len, channels]</code>.</li>
<li>Look at first rows: Print the first 5 vectors of the token embedding table. They should look like small random floats from initialization.</li>
<li>Change max_seq_len: Double <code>max_seq_len</code> in the config. How does this change the size of the positional table? Does training still work?</li>
<li>Overwrite embeddings: Try setting the token embedding table to all zeros. What happens to training loss?</li>
<li>Sampling experiment: After training a few steps, decode outputs without adding positional embeddings. Do the results become nonsensical or repetitive?</li>
</ol>
</section>
<section id="the-takeaway-12" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-12">The Takeaway</h4>
<p>The embedding tables are the foundation of GPT-2. Token embeddings give meaning to symbols, while positional embeddings give structure to sequences. In <em>llm.c</em>, they are just two slices of the flat parameter array, added together at the very start of the forward pass-but without them, the transformer would be blind to both words and order.</p>
</section>
</section>
<section id="attention-stack-qkv-projections-and-geometry" class="level3">
<h3 class="anchored" data-anchor-id="attention-stack-qkv-projections-and-geometry">24. Attention Stack: QKV Projections and Geometry</h3>
<p>After embeddings, the real magic of transformers begins: the attention mechanism. In GPT-2, every transformer block contains an attention stack. This is where the model learns how each token relates to others in the sequence-whether it’s paying attention to the previous word, the beginning of a sentence, or even punctuation marks far away.</p>
<section id="what-attention-does" class="level4">
<h4 class="anchored" data-anchor-id="what-attention-does">What Attention Does</h4>
<p>Attention lets the model answer the question:</p>
<blockquote class="blockquote">
<p>“Given the current word, which other words in the context should I care about, and how much?”</p>
</blockquote>
<p>Instead of treating words independently, the model uses attention to build connections across the sequence.</p>
</section>
<section id="the-q-k-v-projections" class="level4">
<h4 class="anchored" data-anchor-id="the-q-k-v-projections">The Q, K, V Projections</h4>
<p>Each attention block starts with three linear projections:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 25%">
<col style="width: 58%">
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Shape</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Q (Query)</td>
<td><code>[channels, channels]</code></td>
<td>Represents what each token is <em>asking</em> about.</td>
</tr>
<tr class="even">
<td>K (Key)</td>
<td><code>[channels, channels]</code></td>
<td>Represents how each token can be <em>recognized</em>.</td>
</tr>
<tr class="odd">
<td>V (Value)</td>
<td><code>[channels, channels]</code></td>
<td>Represents the actual <em>information</em> to pass along.</td>
</tr>
</tbody>
</table>
<p>Here’s the flow:</p>
<ol type="1">
<li>Each input vector (from embeddings or previous block) is multiplied by these three matrices to produce Q, K, and V vectors.</li>
<li>Attention scores are computed by comparing Qs with Ks.</li>
<li>These scores are used to weight the Vs, mixing information from other tokens into the current one.</li>
</ol>
</section>
<section id="geometry-of-attention" class="level4">
<h4 class="anchored" data-anchor-id="geometry-of-attention">Geometry of Attention</h4>
<ul>
<li>Q and K define a similarity score: how well does this token match another one?</li>
<li>V carries the actual features (like meaning, grammar cues).</li>
<li>The result is a weighted sum: tokens borrow information from others based on attention scores.</li>
</ul>
<p>In equations:</p>
<pre><code>scores = Q × K^T / sqrt(d_k)
weights = softmax(scores + mask)
output  = weights × V</code></pre>
<p>The division by <code>sqrt(d_k)</code> normalizes scores so they don’t blow up as dimensions grow.</p>
</section>
<section id="multi-head-attention" class="level4">
<h4 class="anchored" data-anchor-id="multi-head-attention">Multi-Head Attention</h4>
<p>GPT-2 doesn’t use just one attention projection-it uses many in parallel, called heads. Each head learns to focus on different types of relationships:</p>
<ul>
<li>One head might track subject–verb agreement.</li>
<li>Another might watch punctuation and quotes.</li>
<li>Another might connect pronouns to their referents.</li>
</ul>
<p>For GPT-2 Small:</p>
<ul>
<li>12 heads per layer</li>
<li>Each head works on a reduced dimension (<code>channels / num_heads</code>)</li>
<li>Outputs are concatenated and projected back to <code>channels</code></li>
</ul>
<p>This setup is what gives transformers their flexibility.</p>
</section>
<section id="implementation-in-llm.c" class="level4">
<h4 class="anchored" data-anchor-id="implementation-in-llm.c">Implementation in <em>llm.c</em></h4>
<p>In the parameter array, each transformer block has slices for Q, K, V, and output projection (O). During forward pass:</p>
<ol type="1">
<li>Multiply input by Q, K, V matrices.</li>
<li>Reshape into heads.</li>
<li>Compute attention scores (masked to prevent looking forward).</li>
<li>Apply softmax.</li>
<li>Multiply by V to get weighted values.</li>
<li>Concatenate heads and apply the O projection.</li>
</ol>
<p>All of this is done with plain matrix multiplications and softmax calls-no magic beyond linear algebra.</p>
</section>
<section id="why-it-matters-11" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-11">Why It Matters</h4>
<p>Attention is the beating heart of GPT-2. It’s how the model captures dependencies across text, from short-term grammar to long-range coherence. Without QKV, embeddings would stay isolated, and the model could never build context-aware representations.</p>
</section>
<section id="try-it-yourself-13" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-13">Try It Yourself</h4>
<ol type="1">
<li>Print shapes: Log the shapes of Q, K, V matrices in one layer. Confirm they match <code>[channels, channels]</code>.</li>
<li>Visualize scores: After a forward pass, print the attention weights for one head. Do they concentrate on recent tokens or spread across the sequence?</li>
<li>Reduce heads: Change <code>num_heads</code> from 12 to 4. What happens to validation loss?</li>
<li>Break symmetry: Initialize all Q, K, V matrices with zeros. Does training loss decrease at all?</li>
<li>Mask experiment: Disable the causal mask (allow looking ahead). Does the model “cheat” by predicting future tokens perfectly?</li>
</ol>
</section>
<section id="the-takeaway-13" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-13">The Takeaway</h4>
<p>The attention stack is where tokens stop being isolated and start talking to each other. Q, K, and V projections turn context into weighted relationships, and multi-head attention lets the model juggle many types of dependencies at once. In <em>llm.c</em>, this is implemented with straightforward linear algebra, making the most powerful idea in modern NLP visible and accessible.</p>
</section>
</section>
<section id="mlp-block-linear-layers-activation" class="level3">
<h3 class="anchored" data-anchor-id="mlp-block-linear-layers-activation">25. MLP Block: Linear Layers + Activation</h3>
<p>After attention mixes information across tokens, GPT-2 applies a second transformation inside each block: the MLP (Multi-Layer Perceptron). This part doesn’t look at other tokens-it processes each position independently. But it’s just as important because it gives the model extra capacity to transform and refine the hidden features before passing them to the next layer.</p>
<section id="what-the-mlp-looks-like" class="level4">
<h4 class="anchored" data-anchor-id="what-the-mlp-looks-like">What the MLP Looks Like</h4>
<p>Every transformer block contains an MLP with two linear layers and a nonlinear activation in between:</p>
<pre><code>hidden = Linear1(x)
hidden = GELU(hidden)
out    = Linear2(hidden)</code></pre>
<p>This structure expands the feature dimension and then compresses it back down, which lets the network learn richer representations.</p>
</section>
<section id="shapes-of-the-layers" class="level4">
<h4 class="anchored" data-anchor-id="shapes-of-the-layers">Shapes of the Layers</h4>
<p>If the hidden size (channels) is <code>d_model</code>, the MLP works as follows:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 30%">
<col style="width: 55%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Shape</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Input</td>
<td><code>[B, T, d_model]</code></td>
<td>Output of attention for each token.</td>
</tr>
<tr class="even">
<td>Linear1</td>
<td><code>[d_model, 4 × d_model]</code></td>
<td>Expands features 4× wider.</td>
</tr>
<tr class="odd">
<td>GELU</td>
<td>elementwise</td>
<td>Introduces nonlinearity.</td>
</tr>
<tr class="even">
<td>Linear2</td>
<td><code>[4 × d_model, d_model]</code></td>
<td>Projects back to original size.</td>
</tr>
<tr class="odd">
<td>Output</td>
<td><code>[B, T, d_model]</code></td>
<td>Same shape as input, ready for residual add.</td>
</tr>
</tbody>
</table>
<p>For GPT-2 Small (<code>d_model = 768</code>), Linear1 expands to 3072 channels, then Linear2 reduces back to 768.</p>
</section>
<section id="activation-gelu" class="level4">
<h4 class="anchored" data-anchor-id="activation-gelu">Activation: GELU</h4>
<p>The activation function in GPT-2 is GELU (Gaussian Error Linear Unit). It’s smoother than ReLU, giving the model a more nuanced way of handling values around zero. In code, GELU looks like:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb51"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="dt">float</span> gelu<span class="op">(</span><span class="dt">float</span> x<span class="op">)</span> <span class="op">{</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span><span class="bu">f</span> <span class="op">*</span> x <span class="op">*</span> <span class="op">(</span><span class="fl">1.0</span><span class="bu">f</span> <span class="op">+</span> tanhf<span class="op">(</span><span class="fl">0.79788456</span><span class="bu">f</span> <span class="op">*</span> <span class="op">(</span>x <span class="op">+</span> <span class="fl">0.044715</span><span class="bu">f</span> <span class="op">*</span> x <span class="op">*</span> x <span class="op">*</span> x<span class="op">)));</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This formula may look complicated, but the idea is simple: it smoothly squashes negative values toward zero and keeps positive values flowing through.</p>
</section>
<section id="why-expand-and-shrink" class="level4">
<h4 class="anchored" data-anchor-id="why-expand-and-shrink">Why Expand and Shrink?</h4>
<p>The expansion to <code>4 × d_model</code> may seem wasteful, but it’s deliberate:</p>
<ul>
<li>Expanding gives the model more capacity to represent patterns at each token.</li>
<li>Shrinking keeps the overall parameter count manageable.</li>
<li>Together, they act like a bottleneck layer that forces the model to transform information more effectively.</li>
</ul>
<p>This “expand → activate → shrink” design is one of the main reasons transformers scale so well.</p>
</section>
<section id="implementation-in-llm.c-1" class="level4">
<h4 class="anchored" data-anchor-id="implementation-in-llm.c-1">Implementation in <em>llm.c</em></h4>
<p>Just like attention, the MLP parameters live in the flat array of floats. Each block stores two weight matrices and two bias vectors. During forward pass:</p>
<ol type="1">
<li>Multiply input by <code>Linear1</code> weights, add bias.</li>
<li>Apply GELU elementwise.</li>
<li>Multiply by <code>Linear2</code> weights, add bias.</li>
<li>Pass result through residual connection.</li>
</ol>
<p>Because each position is processed independently, the MLP is easy to parallelize across tokens.</p>
</section>
<section id="why-it-matters-12" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-12">Why It Matters</h4>
<p>The MLP is the nonlinear refiner of transformer blocks. Attention spreads information, but MLPs transform it in-place, giving the model more expressive power. Without the MLP, the network would be mostly linear, limiting its ability to capture complex patterns in text.</p>
</section>
<section id="try-it-yourself-14" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-14">Try It Yourself</h4>
<ol type="1">
<li>Print shapes: Log the dimensions of Linear1 and Linear2 weights in one block. Do they match <code>[768, 3072]</code> and <code>[3072, 768]</code> for GPT-2 Small?</li>
<li>Swap activation: Replace GELU with ReLU in the code. Does training still work? How does validation loss compare?</li>
<li>Reduce expansion: Change expansion from 4× to 2× (<code>[768, 1536]</code>). What effect does this have on parameter count and performance?</li>
<li>Zero out MLP: Set MLP weights to zero. Does the model still learn anything, or does performance collapse?</li>
<li>Compare speed: Measure training step time with and without the MLP enabled. How much slower is it?</li>
</ol>
</section>
<section id="the-takeaway-14" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-14">The Takeaway</h4>
<p>The MLP block in GPT-2 is a simple two-layer network with GELU activation, applied independently to each token. It expands, activates, and compresses features, giving the model nonlinear power to reshape hidden states. In <em>llm.c</em>, it’s implemented with basic matrix multiplications and a smooth GELU function, proving that even small building blocks can have a big impact on the model’s ability to learn language.</p>
</section>
</section>
<section id="layernorm-theory-and-implementation-doclayernorm" class="level3">
<h3 class="anchored" data-anchor-id="layernorm-theory-and-implementation-doclayernorm">26. LayerNorm: Theory and Implementation (<code>doc/layernorm</code>)</h3>
<p>Deep neural networks often suffer from unstable training if activations drift too high or too low. To stabilize this, GPT-2 uses Layer Normalization (LayerNorm) inside every transformer block. In <em>llm.c</em>, LayerNorm is implemented directly in C, and there’s even a detailed explanation in the repo’s <code>doc/layernorm</code> file to help learners understand how it works.</p>
<section id="the-idea-of-normalization" class="level4">
<h4 class="anchored" data-anchor-id="the-idea-of-normalization">The Idea of Normalization</h4>
<p>When you pass vectors through many layers, their values can become unbalanced-some features dominate while others shrink. Normalization fixes this by:</p>
<ol type="1">
<li>Centering: subtracting the mean of the vector.</li>
<li>Scaling: dividing by the standard deviation.</li>
</ol>
<p>This makes every feature vector have mean 0 and variance 1, improving stability.</p>
</section>
<section id="why-layer-norm" class="level4">
<h4 class="anchored" data-anchor-id="why-layer-norm">Why “Layer” Norm?</h4>
<p>There are different kinds of normalization (BatchNorm, InstanceNorm, etc.). LayerNorm is special because:</p>
<ul>
<li>It normalizes across the features of a single token (the “layer”), not across the batch.</li>
<li>This makes it independent of batch size, which is important for NLP where batch sizes can vary.</li>
</ul>
<p>So if a hidden vector has 768 channels, LayerNorm computes the mean and variance over those 768 numbers for each token.</p>
</section>
<section id="trainable-parameters" class="level4">
<h4 class="anchored" data-anchor-id="trainable-parameters">Trainable Parameters</h4>
<p>LayerNorm isn’t just normalization-it also has two trainable vectors:</p>
<ul>
<li>γ (gamma): scales each feature after normalization.</li>
<li>β (beta): shifts each feature after normalization.</li>
</ul>
<p>These allow the network to “undo” normalization when necessary, giving it flexibility.</p>
</section>
<section id="formula" class="level4">
<h4 class="anchored" data-anchor-id="formula">Formula</h4>
<p>For each input vector <code>x</code> of size <code>d</code>:</p>
<pre><code>mean = (1/d) * Σ x_i
var  = (1/d) * Σ (x_i - mean)^2
x_norm = (x - mean) / sqrt(var + eps)
y = γ * x_norm + β</code></pre>
<p>Where <code>eps</code> is a tiny constant (like <code>1e-5</code>) to avoid dividing by zero.</p>
</section>
<section id="implementation-in-llm.c-2" class="level4">
<h4 class="anchored" data-anchor-id="implementation-in-llm.c-2">Implementation in <em>llm.c</em></h4>
<p>In the code, LayerNorm is implemented as a simple function that loops over features, computes mean and variance, and applies the formula above. It’s not hidden inside a framework-it’s right there in C, so you can step through it line by line.</p>
<p>For example, the forward pass looks like this (simplified):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb53"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> layernorm_forward<span class="op">(</span><span class="dt">float</span><span class="op">*</span> out<span class="op">,</span> <span class="dt">float</span><span class="op">*</span> inp<span class="op">,</span> <span class="dt">float</span><span class="op">*</span> weight<span class="op">,</span> <span class="dt">float</span><span class="op">*</span> bias<span class="op">,</span> <span class="dt">int</span> N<span class="op">)</span> <span class="op">{</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>    <span class="dt">float</span> mean <span class="op">=</span> <span class="fl">0.0</span><span class="bu">f</span><span class="op">,</span> var <span class="op">=</span> <span class="fl">0.0</span><span class="bu">f</span><span class="op">;</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> N<span class="op">;</span> i<span class="op">++)</span> mean <span class="op">+=</span> inp<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">/=</span> N<span class="op">;</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> N<span class="op">;</span> i<span class="op">++)</span> var <span class="op">+=</span> <span class="op">(</span>inp<span class="op">[</span>i<span class="op">]</span> <span class="op">-</span> mean<span class="op">)</span> <span class="op">*</span> <span class="op">(</span>inp<span class="op">[</span>i<span class="op">]</span> <span class="op">-</span> mean<span class="op">);</span></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>    var <span class="op">/=</span> N<span class="op">;</span></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>    <span class="dt">float</span> inv_std <span class="op">=</span> <span class="fl">1.0</span><span class="bu">f</span> <span class="op">/</span> sqrtf<span class="op">(</span>var <span class="op">+</span> <span class="fl">1e-5</span><span class="bu">f</span><span class="op">);</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> N<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>        out<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> <span class="op">(</span>inp<span class="op">[</span>i<span class="op">]</span> <span class="op">-</span> mean<span class="op">)</span> <span class="op">*</span> inv_std <span class="op">*</span> weight<span class="op">[</span>i<span class="op">]</span> <span class="op">+</span> bias<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This is the kind of clear, low-level implementation that makes <em>llm.c</em> educational.</p>
</section>
<section id="where-it-fits-in-gpt-2" class="level4">
<h4 class="anchored" data-anchor-id="where-it-fits-in-gpt-2">Where It Fits in GPT-2</h4>
<p>Each transformer block contains two LayerNorms:</p>
<ul>
<li>One before attention.</li>
<li>One before the MLP.</li>
</ul>
<p>GPT-2 uses Pre-LN architecture: inputs are normalized before each sublayer. This makes training more stable and gradients flow better.</p>
</section>
<section id="why-it-matters-13" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-13">Why It Matters</h4>
<p>LayerNorm may look like a small detail, but without it, GPT-2 would fail to train reliably. It smooths out the flow of activations so attention and MLP layers can do their job. In practice, this is one of the critical “glue” components that makes deep transformers trainable at scale.</p>
</section>
<section id="try-it-yourself-15" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-15">Try It Yourself</h4>
<ol type="1">
<li>Print statistics: After applying LayerNorm, print the mean and variance of the output. Do they stay close to 0 and 1?</li>
<li>Remove γ and β: Force gamma to 1 and beta to 0. Does the model still train? Compare losses.</li>
<li>Disable normalization: Comment out LayerNorm and train. How unstable does training become?</li>
<li>Compare positions: Try switching to Post-LN (apply normalization after attention/MLP). Does this change convergence speed?</li>
<li>Vary epsilon: Change <code>1e-5</code> to <code>1e-2</code> or <code>1e-8</code>. How sensitive is training?</li>
</ol>
</section>
<section id="the-takeaway-15" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-15">The Takeaway</h4>
<p>LayerNorm is the quiet stabilizer of GPT-2. It makes sure each token’s features stay balanced, while γ and β keep flexibility. In <em>llm.c</em>, it’s implemented directly with clear C code, letting you see exactly how normalization is calculated. It’s a small but indispensable piece of the transformer puzzle.</p>
</section>
</section>
<section id="residual-connections-keeping-the-signal-flowing" class="level3">
<h3 class="anchored" data-anchor-id="residual-connections-keeping-the-signal-flowing">27. Residual Connections: Keeping the Signal Flowing</h3>
<p>Transformers like GPT-2 don’t just stack layers on top of each other blindly. They use residual connections-a trick that allows the input of a layer to be added back to its output. This simple addition helps signals flow through the network without vanishing or exploding, and it makes training deep models possible.</p>
<section id="the-basic-idea" class="level4">
<h4 class="anchored" data-anchor-id="the-basic-idea">The Basic Idea</h4>
<p>Imagine you have a function <code>F(x)</code> representing some transformation (like attention or an MLP). Instead of just computing:</p>
<pre><code>y = F(x)</code></pre>
<p>the transformer does:</p>
<pre><code>y = F(x) + x</code></pre>
<p>This means the layer learns only the <em>difference</em> it needs to add to the input, instead of replacing it entirely.</p>
</section>
<section id="why-this-helps" class="level4">
<h4 class="anchored" data-anchor-id="why-this-helps">Why This Helps</h4>
<p>Residuals solve two big problems in deep networks:</p>
<ol type="1">
<li>Gradient flow: During backpropagation, gradients can get smaller and smaller as they pass through many layers. Adding the input back ensures gradients always have a path straight through.</li>
<li>Information preservation: Even if <code>F(x)</code> distorts the signal, the original <code>x</code> is still there. This prevents the model from “forgetting” important information.</li>
<li>Faster training: The network doesn’t have to re-learn identity mappings-it can just pass them through the skip connection.</li>
</ol>
</section>
<section id="implementation-in-llm.c-3" class="level4">
<h4 class="anchored" data-anchor-id="implementation-in-llm.c-3">Implementation in <em>llm.c</em></h4>
<p>Residuals in <em>llm.c</em> are implemented as a straightforward elementwise addition:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb56"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> residual_forward<span class="op">(</span><span class="dt">float</span><span class="op">*</span> out<span class="op">,</span> <span class="dt">float</span><span class="op">*</span> inp1<span class="op">,</span> <span class="dt">float</span><span class="op">*</span> inp2<span class="op">,</span> <span class="dt">int</span> N<span class="op">)</span> <span class="op">{</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> N<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>        out<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> inp1<span class="op">[</span>i<span class="op">]</span> <span class="op">+</span> inp2<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Here:</p>
<ul>
<li><code>inp1</code> is the output of the layer (like attention).</li>
<li><code>inp2</code> is the original input.</li>
<li><code>out</code> is the combined result.</li>
</ul>
<p>This is done for every token position and feature channel.</p>
</section>
<section id="where-residuals-are-used" class="level4">
<h4 class="anchored" data-anchor-id="where-residuals-are-used">Where Residuals Are Used</h4>
<p>In GPT-2, every transformer block has two residuals:</p>
<ol type="1">
<li>Attention residual: Adds the input of the attention layer to its output.</li>
<li>MLP residual: Adds the input of the MLP to its output.</li>
</ol>
<p>So the data flowing through the network always carries both the new transformation and the original signal.</p>
</section>
<section id="why-it-matters-14" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-14">Why It Matters</h4>
<p>Without residual connections, stacking 12–48 transformer blocks would be nearly impossible to train. Gradients would vanish, and the model would either stop learning or take forever to converge. Residuals let deep transformers scale smoothly.</p>
<p>They also add an intuitive interpretation: each block is like a “refinement step” rather than a full rewrite of the representation.</p>
</section>
<section id="try-it-yourself-16" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-16">Try It Yourself</h4>
<ol type="1">
<li>Remove residuals: Comment out the addition in the code. Does training collapse?</li>
<li>Scale residuals: Multiply the input by 0.5 before adding. Does this slow convergence?</li>
<li>Check loss curves: Compare training with and without residuals for the first 500 steps.</li>
<li>Inspect outputs: Print the norms of <code>inp1</code>, <code>inp2</code>, and <code>out</code>. Are the scales balanced?</li>
<li>Deeper models: Increase the number of layers from 12 to 24. Does the importance of residuals become more obvious?</li>
</ol>
</section>
<section id="the-takeaway-16" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-16">The Takeaway</h4>
<p>Residual connections are the “lifeline” of deep transformers. By simply adding inputs back into outputs, they make it possible to train very deep networks without losing gradients or information. In <em>llm.c</em>, the implementation is as simple as looping over arrays and adding them-but the effect is profound: residuals are what let GPT-2 go deep and still work.</p>
</section>
</section>
<section id="attention-masking-enforcing-causality" class="level3">
<h3 class="anchored" data-anchor-id="attention-masking-enforcing-causality">28. Attention Masking: Enforcing Causality</h3>
<p>One of the defining traits of GPT-2 is that it’s a causal language model. That means it predicts the <em>next</em> token given all the tokens before it, but never cheats by looking ahead. To enforce this, GPT-2 applies an attention mask inside every attention layer.</p>
<section id="why-a-mask-is-needed" class="level4">
<h4 class="anchored" data-anchor-id="why-a-mask-is-needed">Why a Mask Is Needed</h4>
<p>Without a mask, attention is free to connect any token to any other, including future ones. For example:</p>
<ul>
<li>Input: “The cat sat on the”</li>
<li>Target: “mat”</li>
</ul>
<p>If the model could peek at “mat” while computing attention, the task would be trivial-it could just copy the next word. That would break the training objective.</p>
<p>The mask forces the model to only use tokens at or before the current position when making predictions.</p>
</section>
<section id="how-the-mask-works" class="level4">
<h4 class="anchored" data-anchor-id="how-the-mask-works">How the Mask Works</h4>
<p>When computing attention scores (<code>Q × K^T / sqrt(d_k)</code>), the result is a matrix of size <code>[T, T]</code> where each row corresponds to one token attending to all others.</p>
<p>The mask modifies this matrix:</p>
<ul>
<li>Allowed positions (past and present): keep scores as is.</li>
<li>Disallowed positions (future): set scores to <code>-inf</code>.</li>
</ul>
<p>After applying softmax, those <code>-inf</code> entries become zero probability, effectively blocking attention to the future.</p>
</section>
<section id="implementation-in-llm.c-4" class="level4">
<h4 class="anchored" data-anchor-id="implementation-in-llm.c-4">Implementation in <em>llm.c</em></h4>
<p>The causal mask is applied during the attention forward pass. The code uses a loop to zero out invalid positions:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb57"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> t <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> t <span class="op">&lt;</span> T<span class="op">;</span> t<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> u <span class="op">=</span> t <span class="op">+</span> <span class="dv">1</span><span class="op">;</span> u <span class="op">&lt;</span> T<span class="op">;</span> u<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>        scores<span class="op">[</span>t<span class="op">][</span>u<span class="op">]</span> <span class="op">=</span> <span class="op">-</span><span class="fl">1e9</span><span class="op">;</span> <span class="co">// block future positions</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Here <code>T</code> is the sequence length. This ensures that token <code>t</code> can only attend to itself and earlier tokens.</p>
</section>
<section id="visualizing-the-mask" class="level4">
<h4 class="anchored" data-anchor-id="visualizing-the-mask">Visualizing the Mask</h4>
<p>Think of the mask as a triangular matrix:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>✓</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>1</td>
<td>✓</td>
<td>✓</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>2</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td></td>
</tr>
<tr class="even">
<td>3</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
</tr>
</tbody>
</table>
<p>Each row shows which past tokens a given position can look at. Future positions remain blank.</p>
</section>
<section id="why-it-matters-15" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-15">Why It Matters</h4>
<p>The mask is what makes GPT-2 a predictive model instead of a bidirectional encoder like BERT. Without it, the model could “cheat” and the training objective would no longer match how it’s used at inference time (generating text step by step).</p>
<p>This small detail-just filling part of a matrix with <code>-inf</code>-is critical to making autoregressive text generation possible.</p>
</section>
<section id="try-it-yourself-17" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-17">Try It Yourself</h4>
<ol type="1">
<li>Disable the mask: Comment out the masking code. Watch validation loss drop unrealistically, then notice that text generation produces garbage.</li>
<li>Reverse the mask: Block the past and allow the future. Does the model still train? What does it predict?</li>
<li>Partial mask: Only allow attention to the previous 5 tokens (a sliding window). How does this affect learning long-range structure?</li>
<li>Print scores: Before and after masking, log a row of attention scores. Notice how future positions become huge negatives.</li>
<li>Visualize: Write a small script to plot the attention mask as a matrix. It should look strictly lower-triangular.</li>
</ol>
</section>
<section id="the-takeaway-17" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-17">The Takeaway</h4>
<p>Attention masking is a simple but essential trick. By filling future positions with <code>-inf</code> before softmax, GPT-2 ensures that each token can only attend to its past. In <em>llm.c</em>, this is implemented with just a couple of loops-but it’s what turns a generic transformer into a true causal language model.</p>
</section>
</section>
<section id="output-head-from-hidden-states-to-vocabulary" class="level3">
<h3 class="anchored" data-anchor-id="output-head-from-hidden-states-to-vocabulary">29. Output Head: From Hidden States to Vocabulary</h3>
<p>After tokens pass through embeddings, attention, MLPs, LayerNorm, and residuals, we end up with hidden states for every position in the sequence. But GPT-2’s final job is not to output vectors-it must predict the next token from the vocabulary. This is handled by the output head, the last stage of the model.</p>
<section id="what-the-output-head-does" class="level4">
<h4 class="anchored" data-anchor-id="what-the-output-head-does">What the Output Head Does</h4>
<p>The output head maps hidden states of shape <code>(B, T, channels)</code> into logits of shape <code>(B, T, vocab_size)</code>. Each logit represents the model’s “raw score” for how likely a particular token is at the next step.</p>
<p>The pipeline looks like this:</p>
<pre><code>hidden states → Linear projection → Logits → Softmax → Probabilities</code></pre>
<ul>
<li>Logits: real numbers, one per token in the vocabulary.</li>
<li>Softmax: converts logits into probabilities that sum to 1.</li>
<li>Predicted token: the token with the highest probability (or sampled from the distribution).</li>
</ul>
</section>
<section id="tied-weights-with-embeddings" class="level4">
<h4 class="anchored" data-anchor-id="tied-weights-with-embeddings">Tied Weights with Embeddings</h4>
<p>In GPT-2, the token embedding table and the output head share weights. This means the same matrix is used both for:</p>
<ul>
<li>Mapping tokens to vectors at the start (embedding lookup).</li>
<li>Mapping vectors back to tokens at the end (output head).</li>
</ul>
<p>Mathematically, this improves efficiency and helps align input and output representations.</p>
<p>In <em>llm.c</em>, this is done by simply pointing both embedding and output head to the same parameter slice.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb59"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co">// token embedding table</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="dt">float</span><span class="op">*</span> token_embedding_table <span class="op">=</span> params<span class="op">;</span></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="co">// output head reuses the same memory</span></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="dt">float</span><span class="op">*</span> output_head <span class="op">=</span> token_embedding_table<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>When the model projects hidden states back to vocab space, it does a matrix multiply with this shared matrix.</p>
</section>
<section id="shapes-in-action" class="level4">
<h4 class="anchored" data-anchor-id="shapes-in-action">Shapes in Action</h4>
<p>For GPT-2 Small:</p>
<ul>
<li>Hidden states: <code>[B, T, 768]</code></li>
<li>Output projection (embedding transpose): <code>[768, 50257]</code></li>
<li>Logits: <code>[B, T, 50257]</code></li>
</ul>
<p>That’s more than 50k scores per position, one for each token in the vocabulary.</p>
</section>
<section id="why-weight-tying-helps" class="level4">
<h4 class="anchored" data-anchor-id="why-weight-tying-helps">Why Weight Tying Helps</h4>
<ol type="1">
<li>Memory efficiency: You don’t need a separate giant matrix for the output head.</li>
<li>Better learning: The same vectors that represent tokens going in also represent them going out, which reinforces consistency.</li>
<li>Simpler code: Just reuse the same parameter slice.</li>
</ol>
<p>This trick is why GPT-2 can scale vocab sizes without blowing up parameter counts too much.</p>
</section>
<section id="why-it-matters-16" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-16">Why It Matters</h4>
<p>The output head is where everything comes together. For each position, the model collapses its hidden representation into a distribution over possible next tokens. This is how GPT-2 generates text one step at a time. Without this step, you’d only have abstract hidden states-useful internally, but not something you can read.</p>
</section>
<section id="try-it-yourself-18" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-18">Try It Yourself</h4>
<ol type="1">
<li>Print logits: After a forward pass, print the logits for the last token. Do they look like random floats at initialization?</li>
<li>Check probability sum: Apply softmax to logits and verify the probabilities sum to 1.</li>
<li>Untie weights: Make the output head its own matrix instead of reusing embeddings. Does training still work? How does the parameter count change?</li>
<li>Top-k sampling: Modify sampling to keep only the top 5 logits before softmax. What kind of text does this produce?</li>
<li>Greedy vs random: Compare greedy decoding (argmax) vs random sampling from probabilities. Which one gives more interesting outputs?</li>
</ol>
</section>
<section id="the-takeaway-18" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-18">The Takeaway</h4>
<p>The output head is the final bridge between hidden vectors and actual words. By reusing the token embedding matrix, GPT-2 projects hidden states back into vocabulary space and produces logits for every possible token. In <em>llm.c</em>, this step is just another matrix multiplication-but it’s the one that turns internal math into real text predictions.</p>
</section>
</section>
<section id="loss-function-cross-entropy-over-vocabulary" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-cross-entropy-over-vocabulary">30. Loss Function: Cross-Entropy over Vocabulary</h3>
<p>Training GPT-2 means teaching it to predict the next token in a sequence. To measure how well it’s doing, we need a loss function that compares the model’s predicted probabilities with the true token IDs. In <em>llm.c</em>, this is done with the cross-entropy loss-a standard choice for classification tasks.</p>
<section id="from-logits-to-probabilities" class="level4">
<h4 class="anchored" data-anchor-id="from-logits-to-probabilities">From Logits to Probabilities</h4>
<p>After the output head, we have logits of shape <code>(B, T, vocab_size)</code>. These are raw scores. To turn them into probabilities:</p>
<pre><code>probs = softmax(logits)</code></pre>
<p>Softmax ensures two things:</p>
<ul>
<li>All values are between 0 and 1.</li>
<li>They sum to 1 across the vocabulary.</li>
</ul>
<p>So for each position, you get a probability distribution over all possible next tokens.</p>
</section>
<section id="cross-entropy-definition" class="level4">
<h4 class="anchored" data-anchor-id="cross-entropy-definition">Cross-Entropy Definition</h4>
<p>Cross-entropy compares the predicted distribution <code>p</code> with the true distribution <code>q</code>. For language modeling:</p>
<ul>
<li><code>q</code> is a one-hot vector (all zeros, except 1 at the true token index).</li>
<li><code>p</code> is the probability vector from softmax.</li>
</ul>
<p>The formula for one token:</p>
<pre><code>loss = -log(p[true_token])</code></pre>
<p>For a batch, you average across all tokens in all sequences.</p>
</section>
<section id="implementation-in-llm.c-5" class="level4">
<h4 class="anchored" data-anchor-id="implementation-in-llm.c-5">Implementation in <em>llm.c</em></h4>
<p>In C, this boils down to:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb62"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="dt">float</span> loss <span class="op">=</span> <span class="fl">0.0</span><span class="bu">f</span><span class="op">;</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> b <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> b <span class="op">&lt;</span> B<span class="op">;</span> b<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> t <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> t <span class="op">&lt;</span> T<span class="op">;</span> t<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int</span> target <span class="op">=</span> targets<span class="op">[</span>b<span class="op">*</span>T <span class="op">+</span> t<span class="op">];</span></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>        <span class="dt">float</span> logit_max <span class="op">=</span> <span class="op">-</span><span class="fl">1e9</span><span class="op">;</span></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> v <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> v <span class="op">&lt;</span> vocab_size<span class="op">;</span> v<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="op">(</span>logits<span class="op">[</span>b<span class="op">*</span>T<span class="op">*</span>vocab_size <span class="op">+</span> t<span class="op">*</span>vocab_size <span class="op">+</span> v<span class="op">]</span> <span class="op">&gt;</span> logit_max<span class="op">)</span> <span class="op">{</span></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>                logit_max <span class="op">=</span> logits<span class="op">[</span>b<span class="op">*</span>T<span class="op">*</span>vocab_size <span class="op">+</span> t<span class="op">*</span>vocab_size <span class="op">+</span> v<span class="op">];</span></span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>            <span class="op">}</span></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">// compute softmax denominator</span></span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a>        <span class="dt">float</span> sum <span class="op">=</span> <span class="fl">0.0</span><span class="bu">f</span><span class="op">;</span></span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> v <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> v <span class="op">&lt;</span> vocab_size<span class="op">;</span> v<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a>            sum <span class="op">+=</span> expf<span class="op">(</span>logits<span class="op">[</span>b<span class="op">*</span>T<span class="op">*</span>vocab_size <span class="op">+</span> t<span class="op">*</span>vocab_size <span class="op">+</span> v<span class="op">]</span> <span class="op">-</span> logit_max<span class="op">);</span></span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a>        <span class="dt">float</span> logprob <span class="op">=</span> logits<span class="op">[</span>b<span class="op">*</span>T<span class="op">*</span>vocab_size <span class="op">+</span> t<span class="op">*</span>vocab_size <span class="op">+</span> target<span class="op">]</span> <span class="op">-</span> logit_max <span class="op">-</span> logf<span class="op">(</span>sum<span class="op">);</span></span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">+=</span> <span class="op">-</span>logprob<span class="op">;</span></span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a>loss <span class="op">/=</span> <span class="op">(</span>B <span class="op">*</span> T<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This snippet shows how <em>llm.c</em> explicitly computes softmax and cross-entropy in loops. No black boxes-just raw math.</p>
</section>
<section id="intuition" class="level4">
<h4 class="anchored" data-anchor-id="intuition">Intuition</h4>
<ul>
<li>If the model assigns high probability to the correct token → loss is small.</li>
<li>If the model assigns low probability to the correct token → loss is large.</li>
<li>Minimizing loss means pushing probability mass toward the right answers.</li>
</ul>
</section>
<section id="why-cross-entropy-works-for-language" class="level4">
<h4 class="anchored" data-anchor-id="why-cross-entropy-works-for-language">Why Cross-Entropy Works for Language</h4>
<p>Language modeling is essentially a huge multi-class classification problem: at each step, which word comes next? Cross-entropy is perfect here because it directly penalizes wrong predictions proportional to how confident the model was.</p>
</section>
<section id="why-it-matters-17" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-17">Why It Matters</h4>
<p>The loss function is the only signal the model gets about how well it’s doing. Everything else-parameter updates, weight tuning, learning dynamics-flows from this single number. A well-implemented cross-entropy ensures training is stable and meaningful.</p>
</section>
<section id="try-it-yourself-19" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-19">Try It Yourself</h4>
<ol type="1">
<li>Check values: Print the loss after the first few steps. It should be close to <code>log(vocab_size)</code> (≈10.8 for 50k vocab) before training.</li>
<li>Overfit tiny batch: Train on just one sequence. Does the loss go near 0 after enough steps?</li>
<li>Change target: Replace the true token with a random one. Does the loss increase immediately?</li>
<li>Compare vocab sizes: Train with a smaller vocabulary (e.g., 100 tokens). Does initial loss drop to <code>log(100) ≈ 4.6</code>?</li>
<li>Inspect probabilities: For one token, print the top 5 predicted probabilities. Does the true token climb to the top as training progresses?</li>
</ol>
</section>
<section id="the-takeaway-19" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-19">The Takeaway</h4>
<p>The cross-entropy loss is the compass guiding GPT-2 during training. It turns raw logits into probabilities and measures how well the model predicts the correct next token. In <em>llm.c</em>, it’s implemented with explicit loops and math, letting you see exactly how probabilities and losses are computed. Without this step, the model would have no way to learn from its mistakes.</p>
</section>
</section>
</section>
<section id="chapter-4.-cpu-inference-forward-only" class="level2">
<h2 class="anchored" data-anchor-id="chapter-4.-cpu-inference-forward-only">Chapter 4. CPU Inference (Forward Only)</h2>
<section id="forward-pass-walkthrough" class="level3">
<h3 class="anchored" data-anchor-id="forward-pass-walkthrough">31. Forward Pass Walkthrough</h3>
<p>When we talk about the <em>forward pass</em> in GPT-2, we mean the process of turning an input sentence (like “The cat sat on the”) into predictions for the next word. In simple terms, it’s how the model “thinks” before giving an answer. In <code>train_gpt2.c</code>, this happens inside the function <code>gpt2_forward</code>. Let’s walk through it slowly, step by step, so you can see how numbers flow through the model and transform along the way.</p>
<section id="from-words-to-numbers" class="level4">
<h4 class="anchored" data-anchor-id="from-words-to-numbers">1. From Words to Numbers</h4>
<p>Computers don’t understand words like <em>cat</em> or <em>sat</em>. They only understand numbers. Before the forward pass starts, text is already tokenized into IDs (integers). For example:</p>
<pre><code>"The cat sat" → [464, 3290, 616]</code></pre>
<p>Each number is a token ID. The model doesn’t yet know what “464” means in plain English-it just knows it’s a number that points into a table.</p>
</section>
<section id="embedding-giving-words-meaning" class="level4">
<h4 class="anchored" data-anchor-id="embedding-giving-words-meaning">2. Embedding: Giving Words Meaning</h4>
<p>The first real step in the forward pass is embedding lookup. Imagine we have a huge dictionary, but instead of definitions in English, each word ID points to a long vector of numbers (say, 768 numbers for GPT-2 small).</p>
<ul>
<li>Word embeddings (<code>wte</code>): Each token ID becomes a vector that captures the meaning of the word.</li>
<li>Position embeddings (<code>wpe</code>): Each token also gets a vector for its position: first word, second word, third word, etc.</li>
</ul>
<p>The model adds these two vectors together. This way, it knows not just what the word is, but also where it is in the sentence.</p>
<p>For example:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 29%">
<col style="width: 34%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th>Token</th>
<th>Word</th>
<th>Word Embedding (shortened)</th>
<th>Position Embedding (shortened)</th>
<th>Combined Vector</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>464</td>
<td>“The”</td>
<td>[0.2, -0.5, 0.1, …]</td>
<td>[0.0, 0.1, -0.3, …]</td>
<td>[0.2, -0.4, -0.2, …]</td>
</tr>
<tr class="even">
<td>3290</td>
<td>“cat”</td>
<td>[0.9, -0.2, 0.4, …]</td>
<td>[0.1, -0.1, -0.2, …]</td>
<td>[1.0, -0.3, 0.2, …]</td>
</tr>
</tbody>
</table>
<p>Now every token is a vector with both meaning and position built in.</p>
</section>
<section id="transformer-layers-the-thinking-steps" class="level4">
<h4 class="anchored" data-anchor-id="transformer-layers-the-thinking-steps">3. Transformer Layers: The Thinking Steps</h4>
<p>GPT-2 has multiple identical layers stacked on top of each other. Each layer has two big parts: attention and MLP (feed-forward network).</p>
<p>Attention (looking around):</p>
<ul>
<li>Each word asks: “Which other words should I pay attention to right now?”</li>
<li>For “sat,” attention might focus heavily on “cat,” because those words are related.</li>
<li>The code computes <em>queries</em>, <em>keys</em>, and <em>values</em> for every word, then does dot-products, softmax, and weighted sums to mix information.</li>
</ul>
<p>MLP (processing deeply):</p>
<ul>
<li>After attention, each token passes through a mini neural network (two matrix multiplications with a nonlinear GELU function in between).</li>
<li>This helps each word refine its understanding, even if it doesn’t directly attend to another word.</li>
</ul>
<p>Both blocks have residual connections: the input is added back to the output, like keeping the original notes while adding new insights. This prevents information loss.</p>
</section>
<section id="normalization-keeping-numbers-stable" class="level4">
<h4 class="anchored" data-anchor-id="normalization-keeping-numbers-stable">4. Normalization: Keeping Numbers Stable</h4>
<p>At many points, the model normalizes vectors so they don’t explode in size or shrink too small. This is called LayerNorm. It ensures training is stable, like making sure your cooking pot doesn’t boil over or dry out.</p>
</section>
<section id="the-final-prediction-layer" class="level4">
<h4 class="anchored" data-anchor-id="the-final-prediction-layer">5. The Final Prediction Layer</h4>
<p>After all layers, the model produces a final vector for each position. Then:</p>
<ul>
<li>It multiplies those vectors by the embedding table again (but transposed).</li>
<li>This gives logits: raw scores for each word in the vocabulary (about 50k options).</li>
</ul>
<p>Example: for the last token “on the,” the logits might be:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Word</th>
<th>Logit</th>
<th>Probability (after softmax)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>“mat”</td>
<td>7.2</td>
<td>0.85</td>
</tr>
<tr class="even">
<td>“dog”</td>
<td>5.1</td>
<td>0.10</td>
</tr>
<tr class="odd">
<td>“car”</td>
<td>3.0</td>
<td>0.05</td>
</tr>
</tbody>
</table>
<p>The highest probability is “mat.”</p>
</section>
<section id="softmax-turning-scores-into-probabilities" class="level4">
<h4 class="anchored" data-anchor-id="softmax-turning-scores-into-probabilities">6. Softmax: Turning Scores into Probabilities</h4>
<p>The logits are big numbers, but they don’t mean much until we apply softmax. Softmax makes them into probabilities that sum to 1. This way, we can interpret them as chances: “There’s an 85% chance the next word is <em>mat</em>.”</p>
</section>
<section id="cross-entropy-loss-measuring-mistakes" class="level4">
<h4 class="anchored" data-anchor-id="cross-entropy-loss-measuring-mistakes">7. Cross-Entropy Loss: Measuring Mistakes</h4>
<p>If we’re training, we also give the model the correct next word. The model checks how much probability it gave to that word. If it gave it high probability, the loss is low. If it gave it low probability, the loss is high.</p>
<ul>
<li>Correct: “mat” (probability 0.85 → loss ≈ 0.16, small).</li>
<li>Wrong: “car” (probability 0.05 → loss ≈ 3.0, large).</li>
</ul>
<p>This loss is averaged across all tokens, and it’s the signal that tells the backward pass how to update the model.</p>
</section>
<section id="why-it-matters-18" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-18">8. Why It Matters</h4>
<p>The forward pass is the part of GPT-2 that generates predictions. Without it, the model can’t “think” or make sense of input. It’s like the brain processing sensory input before deciding what to do. In <code>train_gpt2.c</code>, the forward pass is written with plain C loops, which makes the math crystal clear instead of hidden inside deep learning libraries.</p>
</section>
<section id="try-it-yourself-20" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-20">9. Try It Yourself</h4>
<ol type="1">
<li>Print embeddings: Modify the code to print the vector for the first token. See how it’s just numbers, but those numbers are the “meaning” of the word.</li>
<li>Inspect probabilities: After the forward pass, print the softmax probabilities for one position. They should sum to 1.0.</li>
<li>Change sequence length: Increase <code>T</code> from 64 to 128. Notice how validation slows down, because attention compares all tokens with all others (<code>T²</code> scaling).</li>
<li>Baseline loss: Before training, measure the loss. It should be around <code>log(vocab_size)</code> (≈10.8 for GPT-2 small). That’s the loss of random guessing.</li>
<li>Mask experiment: Temporarily remove the causal mask in attention. The model will “cheat” by looking ahead, and loss will drop unrealistically.</li>
</ol>
</section>
<section id="the-takeaway-20" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-20">The Takeaway</h4>
<p>The forward pass is like the thought process of GPT-2. Input words become vectors, vectors mix through attention and MLPs, everything gets normalized, and finally the model produces probabilities for the next word. It’s a carefully choreographed dance of math operations, all coded in plain C loops in <code>train_gpt2.c</code>. Once you understand this flow, you can follow exactly how GPT-2 turns raw tokens into intelligent predictions.</p>
</section>
</section>
<section id="token-and-positional-embedding-lookup" class="level3">
<h3 class="anchored" data-anchor-id="token-and-positional-embedding-lookup">32. Token and Positional Embedding Lookup</h3>
<p>Before GPT-2 can do anything intelligent with text, it needs to turn raw numbers (token IDs) into vectors that capture meaning and context. This is the role of embeddings. In <code>train_gpt2.c</code>, this step is handled by the function <code>encoder_forward</code>. Let’s take a closer look at how it works and why it matters.</p>
<section id="tokens-are-just-numbers" class="level4">
<h4 class="anchored" data-anchor-id="tokens-are-just-numbers">Tokens Are Just Numbers</h4>
<p>Suppose you type:</p>
<pre><code>"The cat sat on the mat."</code></pre>
<p>After tokenization, this sentence might look like:</p>
<pre><code>[464, 3290, 616, 319, 262, 1142, 13]</code></pre>
<p>These are just IDs. The model doesn’t inherently know that <code>3290</code> means “cat.” It only knows it needs to use these numbers to fetch vectors from a table.</p>
</section>
<section id="the-embedding-tables" class="level4">
<h4 class="anchored" data-anchor-id="the-embedding-tables">The Embedding Tables</h4>
<p>The model has two important tables stored in memory:</p>
<ol type="1">
<li><p>Word Token Embeddings (<code>wte</code>)</p>
<ul>
<li>Size: <code>(V, C)</code> where <code>V</code> is vocab size (~50,000 for GPT-2 small) and <code>C</code> is channels (768).</li>
<li>Each row corresponds to a token ID.</li>
<li>Example: row 3290 might be <code>[0.12, -0.45, 0.88, …]</code>.</li>
</ul></li>
<li><p>Positional Embeddings (<code>wpe</code>)</p>
<ul>
<li>Size: <code>(maxT, C)</code> where <code>maxT</code> is the maximum sequence length (e.g.&nbsp;1024).</li>
<li>Each row corresponds to a position index: 0 for the first token, 1 for the second, etc.</li>
<li>Example: position 2 might be <code>[0.07, 0.31, -0.22, …]</code>.</li>
</ul></li>
</ol>
<p>Both tables are filled with trainable values. At the start, they’re random. As training progresses, the optimizer updates them so they encode useful patterns.</p>
</section>
<section id="adding-them-together-1" class="level4">
<h4 class="anchored" data-anchor-id="adding-them-together-1">Adding Them Together</h4>
<p>For each token at position <code>t</code>:</p>
<ul>
<li>Look up its word vector from <code>wte</code>.</li>
<li>Look up its position vector from <code>wpe</code>.</li>
<li>Add them elementwise.</li>
</ul>
<p>This gives a final vector of size <code>C</code> that represents what the token is and where it is.</p>
<p>Example with simplified numbers:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 6%">
<col style="width: 26%">
<col style="width: 26%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th>Token ID</th>
<th>Word</th>
<th>Word Embedding</th>
<th>Position</th>
<th>Combined</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>464</td>
<td>The</td>
<td>[0.1, -0.2, 0.3]</td>
<td>[0.2, 0.0, -0.1]</td>
<td>[0.3, -0.2, 0.2]</td>
</tr>
<tr class="even">
<td>3290</td>
<td>cat</td>
<td>[0.4, 0.5, -0.3]</td>
<td>[0.0, 0.1, 0.2]</td>
<td>[0.4, 0.6, -0.1]</td>
</tr>
</tbody>
</table>
<p>Now the vector doesn’t just mean “cat,” it means “cat at position 1.”</p>
</section>
<section id="why-position-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-position-matters">Why Position Matters</h4>
<p>Without positions, the model would treat:</p>
<ul>
<li>“The cat sat”</li>
<li>“Sat cat the”</li>
</ul>
<p>as identical, because they use the same tokens. But word order is essential in language. By adding positional embeddings, GPT-2 knows the difference between “dog bites man” and “man bites dog.”</p>
</section>
<section id="inside-the-code" class="level4">
<h4 class="anchored" data-anchor-id="inside-the-code">Inside the Code</h4>
<p>The embedding lookup is written explicitly with loops in C:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb66"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> b <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> b <span class="op">&lt;</span> B<span class="op">;</span> b<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> t <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> t <span class="op">&lt;</span> T<span class="op">;</span> t<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>        <span class="dt">float</span><span class="op">*</span> out_bt <span class="op">=</span> out <span class="op">+</span> b <span class="op">*</span> T <span class="op">*</span> C <span class="op">+</span> t <span class="op">*</span> C<span class="op">;</span></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int</span> ix <span class="op">=</span> inp<span class="op">[</span>b <span class="op">*</span> T <span class="op">+</span> t<span class="op">];</span></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>        <span class="dt">float</span><span class="op">*</span> wte_ix <span class="op">=</span> wte <span class="op">+</span> ix <span class="op">*</span> C<span class="op">;</span></span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>        <span class="dt">float</span><span class="op">*</span> wpe_t <span class="op">=</span> wpe <span class="op">+</span> t <span class="op">*</span> C<span class="op">;</span></span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> C<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>            out_bt<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> wte_ix<span class="op">[</span>i<span class="op">]</span> <span class="op">+</span> wpe_t<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>What’s happening here:</p>
<ul>
<li>Loop over batches (<code>b</code>) and sequence positions (<code>t</code>).</li>
<li>Find the token ID <code>ix</code>.</li>
<li>Fetch its embedding <code>wte_ix</code>.</li>
<li>Fetch its position embedding <code>wpe_t</code>.</li>
<li>Add them element by element.</li>
</ul>
<p>The result, <code>out_bt</code>, is the vector for this token at this position.</p>
</section>
<section id="analogy" class="level4">
<h4 class="anchored" data-anchor-id="analogy">Analogy</h4>
<p>Think of it like name tags at a conference:</p>
<ul>
<li>The word embedding is your name: “Alice.”</li>
<li>The position embedding is your table number: “Table 7.”</li>
<li>Together, they tell the conference staff who you are and where you are seated.</li>
</ul>
<p>Without the table number, they might know who you are but not where to find you. Without your name, they just know there’s someone at Table 7 but not who. Both are needed for proper context.</p>
</section>
<section id="why-it-matters-19" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-19">Why It Matters</h4>
<p>Embeddings are the foundation of the whole model. If this step is wrong, everything else collapses. They transform meaningless IDs into rich vectors that carry semantic and positional information. This is the entry point where language starts becoming something a neural network can reason about.</p>
</section>
<section id="try-it-yourself-21" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-21">Try It Yourself</h4>
<ol type="1">
<li>Print a token embedding: Modify the code to print out <code>wte_ix</code> for a specific token ID like “cat.” You’ll see a vector of floats, the learned representation.</li>
<li>Print a position embedding: Do the same for <code>wpe_t</code> at position 0, 1, 2… Notice how positions have unique but consistent patterns.</li>
<li>Check the sum: Verify that <code>out_bt[i] = wte_ix[i] + wpe_t[i]</code>. This is literally how word and position are fused.</li>
<li>Shuffle words: Try feeding “cat sat” vs.&nbsp;“sat cat.” The embeddings will differ because the position vectors change, even though the words are the same.</li>
<li>Observe growth during training: After some training steps, dump the embeddings again. You’ll notice they stop being random and start showing structure.</li>
</ol>
</section>
<section id="the-takeaway-21" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-21">The Takeaway</h4>
<p>The embedding lookup is the very first step of the forward pass. It takes raw numbers and makes them meaningful by combining token identity and position. This prepares the input for the deeper transformer layers. Even though the C code looks simple-a few nested loops-it’s doing the crucial work of giving words a mathematical shape the model can understand.</p>
</section>
</section>
<section id="attention-matmuls-masking-and-softmax-on-cpu" class="level3">
<h3 class="anchored" data-anchor-id="attention-matmuls-masking-and-softmax-on-cpu">33. Attention: Matmuls, Masking, and Softmax on CPU</h3>
<p>The attention mechanism is the heart of GPT-2. It’s where each word in the input sequence decides which other words to look at when forming its representation. In <code>train_gpt2.c</code>, this happens inside the <code>attention_forward</code> function, which implements multi-head self-attention using plain C loops and matrix multiplications. Let’s break it down carefully, step by step, so even an absolute beginner can follow the flow.</p>
<section id="the-big-idea-of-attention" class="level4">
<h4 class="anchored" data-anchor-id="the-big-idea-of-attention">The Big Idea of Attention</h4>
<p>Imagine you’re reading:</p>
<blockquote class="blockquote">
<p>“The cat sat on the mat.”</p>
</blockquote>
<p>When the model is trying to understand the word <em>sat</em>, it doesn’t just look at <em>sat</em> by itself. It wants to consider other words like <em>cat</em> (the subject) and <em>mat</em> (likely the object). Attention gives each token a way to “consult” earlier tokens and decide how important they are.</p>
<p>This is done mathematically by projecting each token into three roles: Query (Q), Key (K), and Value (V).</p>
<ul>
<li>Query (Q): “What am I looking for?”</li>
<li>Key (K): “What do I offer?”</li>
<li>Value (V): “What information do I carry?”</li>
</ul>
</section>
<section id="step-1-creating-q-k-and-v" class="level4">
<h4 class="anchored" data-anchor-id="step-1-creating-q-k-and-v">Step 1: Creating Q, K, and V</h4>
<p>For every input vector of size <code>C</code> (e.g., 768), the code performs three separate linear projections (matrix multiplications). These produce Q, K, and V vectors of smaller size, divided among attention heads.</p>
<p>In the code:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb67"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>matmul_forward<span class="op">(</span>acts<span class="op">.</span>q<span class="op">,</span> acts<span class="op">.</span>ln1<span class="op">,</span> params<span class="op">.</span>wq<span class="op">,</span> params<span class="op">.</span>bq<span class="op">,</span> B<span class="op">,</span> T<span class="op">,</span> C<span class="op">,</span> C<span class="op">);</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>matmul_forward<span class="op">(</span>acts<span class="op">.</span>k<span class="op">,</span> acts<span class="op">.</span>ln1<span class="op">,</span> params<span class="op">.</span>wk<span class="op">,</span> params<span class="op">.</span>bk<span class="op">,</span> B<span class="op">,</span> T<span class="op">,</span> C<span class="op">,</span> C<span class="op">);</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>matmul_forward<span class="op">(</span>acts<span class="op">.</span>v<span class="op">,</span> acts<span class="op">.</span>ln1<span class="op">,</span> params<span class="op">.</span>wv<span class="op">,</span> params<span class="op">.</span>bv<span class="op">,</span> B<span class="op">,</span> T<span class="op">,</span> C<span class="op">,</span> C<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Here:</p>
<ul>
<li><code>acts.ln1</code> is the normalized input from the previous step.</li>
<li><code>params.wq</code>, <code>params.wk</code>, <code>params.wv</code> are the weight matrices.</li>
<li>The output shapes are <code>(B, T, C)</code>.</li>
</ul>
<p>So each token now has three new representations: Q, K, and V.</p>
</section>
<section id="step-2-computing-attention-scores" class="level4">
<h4 class="anchored" data-anchor-id="step-2-computing-attention-scores">Step 2: Computing Attention Scores</h4>
<p>For each token at position <code>t</code>, we want to know how much it should pay attention to every earlier token (including itself). This is done with a dot product between its Query and all Keys.</p>
<p>Mathematically:</p>
<pre><code>score[t][u] = (Q[t] ⋅ K[u]) / sqrt(dk)</code></pre>
<ul>
<li><code>t</code> = current token.</li>
<li><code>u</code> = another token at or before <code>t</code>.</li>
<li><code>sqrt(dk)</code> is a scaling factor (dk = size of each head) to keep values stable.</li>
</ul>
<p>In the code, these dot products are done explicitly in loops.</p>
</section>
<section id="step-3-applying-the-causal-mask" class="level4">
<h4 class="anchored" data-anchor-id="step-3-applying-the-causal-mask">Step 3: Applying the Causal Mask</h4>
<p>GPT-2 is an autoregressive model, meaning it only predicts the future from the past, not the other way around. To enforce this, the attention matrix is masked:</p>
<ul>
<li>Token at position <code>t</code> can only look at positions <code>≤ t</code>.</li>
<li>Anything beyond <code>t</code> is set to a very negative value (<code>-1e9</code>), which becomes effectively zero after softmax.</li>
</ul>
<p>This ensures, for example, that when predicting the 3rd word, the model doesn’t cheat by looking at the 4th.</p>
</section>
<section id="step-4-turning-scores-into-probabilities" class="level4">
<h4 class="anchored" data-anchor-id="step-4-turning-scores-into-probabilities">Step 4: Turning Scores into Probabilities</h4>
<p>The scores are raw numbers that can be large and unstable. To convert them into meaningful weights, the code applies softmax:</p>
<pre><code>attention_weights[t][u] = exp(score[t][u]) / Σ exp(score[t][v])</code></pre>
<p>This makes all weights positive and ensures they sum to 1. Now each token has a probability distribution over earlier tokens.</p>
<p>Example for the word <em>sat</em>:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Attended Token</th>
<th>Raw Score</th>
<th>After Softmax</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>The</td>
<td>1.2</td>
<td>0.10</td>
</tr>
<tr class="even">
<td>cat</td>
<td>3.4</td>
<td>0.80</td>
</tr>
<tr class="odd">
<td>sat (itself)</td>
<td>0.7</td>
<td>0.10</td>
</tr>
<tr class="even">
<td>on</td>
<td>masked</td>
<td>0.00</td>
</tr>
</tbody>
</table>
<p>Clearly, <em>sat</em> focuses most strongly on <em>cat</em>.</p>
</section>
<section id="step-5-weighted-sum-of-values" class="level4">
<h4 class="anchored" data-anchor-id="step-5-weighted-sum-of-values">Step 5: Weighted Sum of Values</h4>
<p>Once the attention weights are computed, the model uses them to take a weighted sum of the Value vectors:</p>
<pre><code>output[t] = Σ attention_weights[t][u] * V[u]</code></pre>
<p>This produces a new representation for each token that blends in information from others.</p>
<p>For <em>sat</em>, its new vector will be mostly influenced by <em>cat</em>, but also a little by <em>The</em> and itself.</p>
</section>
<section id="step-6-multi-head-attention" class="level4">
<h4 class="anchored" data-anchor-id="step-6-multi-head-attention">Step 6: Multi-Head Attention</h4>
<p>In practice, attention is split into multiple heads (12 for GPT-2 small). Each head works on smaller chunks of the vector (C/heads).</p>
<ul>
<li>Head 1 might focus on subject–verb relationships.</li>
<li>Head 2 might track distances (like “how far back was this token?”).</li>
<li>Head 3 might specialize in punctuation.</li>
</ul>
<p>After all heads compute their outputs, the results are concatenated and projected back into size <code>C</code> with another matrix multiply.</p>
</section>
<section id="step-7-residual-connection" class="level4">
<h4 class="anchored" data-anchor-id="step-7-residual-connection">Step 7: Residual Connection</h4>
<p>Finally, the output of the attention block is added back to the original input (residual connection). This keeps the original signal flowing, even if the attention introduces distortions.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb71"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>residual_forward<span class="op">(</span>acts<span class="op">.</span>residual2<span class="op">,</span> acts<span class="op">.</span>ln1<span class="op">,</span> acts<span class="op">.</span>att<span class="op">,</span> B<span class="op">*</span>T<span class="op">,</span> C<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This ensures information isn’t lost and gradients flow smoothly during training.</p>
</section>
<section id="why-it-matters-20" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-20">Why It Matters</h4>
<p>Attention is the mechanism that lets GPT-2 capture relationships between words. Without it, the model would treat each token independently, losing context. By explicitly computing “who should I look at?” for every token, GPT-2 learns patterns like subject–verb agreement, long-distance dependencies, and even stylistic nuances.</p>
</section>
<section id="try-it-yourself-22" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-22">Try It Yourself</h4>
<ol type="1">
<li>Inspect the attention mask: Print out the scores before and after masking. Notice how future tokens are set to huge negative values.</li>
<li>Visualize weights: Run attention on a short sentence and plot the weights. You’ll see which words attend to which.</li>
<li>Change sequence length: Try increasing <code>T</code> and observe how computation grows quadratically (<code>T²</code>). Attention is expensive!</li>
<li>Experiment with heads: Force the model to use only 1 head instead of 12. See how this limits the diversity of patterns it can capture.</li>
<li>Check sum of weights: For one token, verify that all attention weights add up to 1.0 after softmax.</li>
</ol>
</section>
<section id="the-takeaway-22" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-22">The Takeaway</h4>
<p>Attention is what makes transformers powerful. It allows each word to dynamically decide which other words matter for understanding its role in a sentence. In <code>train_gpt2.c</code>, this process is spelled out with explicit loops and matrix multiplications, so you can follow every step of the math. Understanding this section gives you the key to why GPT-2-and all modern LLMs-work so well.</p>
</section>
</section>
<section id="mlp-gemms-and-activation-functions" class="level3">
<h3 class="anchored" data-anchor-id="mlp-gemms-and-activation-functions">34. MLP: GEMMs and Activation Functions</h3>
<p>After the attention block lets tokens “talk to each other,” GPT-2 applies a second kind of transformation called the MLP block (multi-layer perceptron). Unlike attention, which mixes information between tokens, the MLP processes each token independently, enriching its internal representation. Even though it looks simpler than attention, the MLP is essential for capturing complex relationships in language.</p>
<section id="what-the-mlp-does" class="level4">
<h4 class="anchored" data-anchor-id="what-the-mlp-does">What the MLP Does</h4>
<p>Every token’s vector (size <code>C</code>, e.g., 768 for GPT-2 small) goes through:</p>
<ol type="1">
<li>Linear expansion: project from size <code>C</code> to size <code>4C</code> (3072 in GPT-2 small).</li>
<li>Nonlinear activation: apply the GELU function, which adds flexibility.</li>
<li>Linear projection back: reduce size from <code>4C</code> back to <code>C</code>.</li>
<li>Residual connection: add the input vector back to the output, keeping the original signal intact.</li>
</ol>
<p>This allows the model to not only share information between tokens (via attention) but also refine how each token represents itself.</p>
</section>
<section id="step-1-expanding-with-a-matrix-multiply" class="level4">
<h4 class="anchored" data-anchor-id="step-1-expanding-with-a-matrix-multiply">Step 1: Expanding with a Matrix Multiply</h4>
<p>The first step is to expand each token’s vector from 768 to 3072 dimensions. This is done with a general matrix multiply (GEMM):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb72"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>matmul_forward<span class="op">(</span>acts<span class="op">.</span>mlp_in<span class="op">,</span> acts<span class="op">.</span>ln2<span class="op">,</span> params<span class="op">.</span>wfc<span class="op">,</span> params<span class="op">.</span>bfc<span class="op">,</span> B<span class="op">,</span> T<span class="op">,</span> C<span class="op">,</span> <span class="dv">4</span><span class="op">*</span>C<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><code>acts.ln2</code>: the normalized input from the previous residual.</li>
<li><code>params.wfc</code>: the weight matrix of size <code>(C, 4C)</code>.</li>
<li><code>params.bfc</code>: bias vector of size <code>(4C)</code>.</li>
<li><code>acts.mlp_in</code>: the result, shape <code>(B, T, 4C)</code>.</li>
</ul>
<p>Think of it like stretching a rubber band-suddenly, the token has much more room to express richer features.</p>
</section>
<section id="step-2-gelu-activation" class="level4">
<h4 class="anchored" data-anchor-id="step-2-gelu-activation">Step 2: GELU Activation</h4>
<p>After expansion, each number passes through GELU (Gaussian Error Linear Unit).</p>
<p>The formula:</p>
<pre><code>GELU(x) = 0.5 * x * (1 + tanh(√(2/π) * (x + 0.044715 * x^3)))</code></pre>
<p>This looks complicated, but the key idea is:</p>
<ul>
<li>For small negative numbers, output ≈ 0 (ignore weak signals).</li>
<li>For large positive numbers, output ≈ x (pass strong signals).</li>
<li>For numbers in between, it smoothly blends.</li>
</ul>
<p>Unlike ReLU, which just chops off negatives, GELU lets small signals through in a probabilistic way. This makes it better for language, where even small hints matter.</p>
<p>Analogy: Imagine you’re grading homework. If an answer is completely wrong, you give 0 points (ReLU style). If it’s perfect, you give full credit. But if it’s partially right, you give partial credit. GELU behaves like that-soft, nuanced grading.</p>
</section>
<section id="step-3-projecting-back-down" class="level4">
<h4 class="anchored" data-anchor-id="step-3-projecting-back-down">Step 3: Projecting Back Down</h4>
<p>Once the token vector has been expanded and passed through GELU, it’s projected back to the original size <code>C</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb74"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>matmul_forward<span class="op">(</span>acts<span class="op">.</span>mlp_out<span class="op">,</span> acts<span class="op">.</span>mlp_in_gelu<span class="op">,</span> params<span class="op">.</span>wproj<span class="op">,</span> params<span class="op">.</span>bproj<span class="op">,</span> B<span class="op">,</span> T<span class="op">,</span> <span class="dv">4</span><span class="op">*</span>C<span class="op">,</span> C<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><code>params.wproj</code>: the projection weights, size <code>(4C, C)</code>.</li>
<li><code>params.bproj</code>: bias, size <code>(C)</code>.</li>
<li><code>acts.mlp_out</code>: result, shape <code>(B, T, C)</code>.</li>
</ul>
<p>Now each token has gone through a non-linear “thinking step,” mixing and reshaping features.</p>
</section>
<section id="step-4-residual-connection" class="level4">
<h4 class="anchored" data-anchor-id="step-4-residual-connection">Step 4: Residual Connection</h4>
<p>Just like with attention, the MLP output is added back to the input:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb75"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>residual_forward<span class="op">(</span>acts<span class="op">.</span>residual3<span class="op">,</span> acts<span class="op">.</span>residual2<span class="op">,</span> acts<span class="op">.</span>mlp_out<span class="op">,</span> B<span class="op">*</span>T<span class="op">,</span> C<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This means the token keeps its old representation while adding the new refinements. If the MLP makes a mistake early in training, the residual ensures the token doesn’t lose all its meaning.</p>
</section>
<section id="inside-the-code-simplicity" class="level4">
<h4 class="anchored" data-anchor-id="inside-the-code-simplicity">Inside the Code: Simplicity</h4>
<p>Even though MLPs in deep learning libraries like PyTorch are one-liners (<code>nn.Linear</code> + <code>nn.GELU</code> + <code>nn.Linear</code>), here in C you see every step spelled out:</p>
<ul>
<li>First GEMM expands to 4C.</li>
<li>Loop applies GELU element by element.</li>
<li>Second GEMM projects back to C.</li>
<li>Residual adds input and output.</li>
</ul>
<p>It’s like watching a magician reveal the trick instead of just seeing the final illusion.</p>
</section>
<section id="why-the-expansion-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-the-expansion-matters">Why the Expansion Matters</h4>
<p>You might ask: why expand to 4C and then shrink back? Why not just keep the size the same?</p>
<p>The expansion allows the model to capture more complicated combinations of features. By spreading information out, applying a nonlinear transformation, and then compressing it again, the model can discover patterns that wouldn’t fit in the smaller space.</p>
<p>Think of it like brainstorming on a huge whiteboard. You spread out all your ideas (4C), reorganize them, and then condense the best ones into a neat summary (C).</p>
</section>
<section id="example-walkthrough" class="level4">
<h4 class="anchored" data-anchor-id="example-walkthrough">Example Walkthrough</h4>
<p>Let’s say we’re processing the token “cat” in the sentence <em>The cat sat</em>.</p>
<ol type="1">
<li>Input vector (size 768): <code>[0.12, -0.08, 0.33, …]</code></li>
<li>After first matrix multiply: expanded to <code>[1.2, -0.9, 0.5, …]</code> (size 3072).</li>
<li>After GELU: <code>[1.1, -0.0, 0.4, …]</code> (smooth nonlinearity).</li>
<li>After projection: back to <code>[0.15, -0.02, 0.27, …]</code> (size 768).</li>
<li>Add back original input: <code>[0.27, -0.10, 0.60, …]</code>.</li>
</ol>
<p>Now “cat” has been enriched with new internal features that help the model predict what comes next.</p>
</section>
<section id="why-it-matters-21" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-21">Why It Matters</h4>
<p>The MLP is the part of GPT-2 that lets each token refine itself. Attention gives context from neighbors, but the MLP deepens the representation of the token itself. Without it, the model would lack the ability to detect fine-grained patterns.</p>
</section>
<section id="try-it-yourself-23" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-23">Try It Yourself</h4>
<ol type="1">
<li>Print intermediate sizes: Add debug prints to see how token vectors grow to 4C and shrink back to C.</li>
<li>Swap activation: Replace GELU with ReLU in the code and train. Compare losses-you’ll notice GPT-2 prefers GELU.</li>
<li>Disable residual: Temporarily remove the residual add. Watch how the model struggles to learn, because it can’t preserve information.</li>
<li>Visualize values: Track how many values are near 0 before and after GELU. You’ll see GELU softly zeroes out weak signals.</li>
<li>Smaller expansion: Try changing 4C to 2C in the code. You’ll save memory but lose accuracy, since the MLP has less expressive power.</li>
</ol>
</section>
<section id="the-takeaway-23" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-23">The Takeaway</h4>
<p>The MLP block is a token’s personal deep thinker. It stretches the representation wide, filters it through GELU, compresses it again, and then adds it back to the original. While attention handles the conversation between words, the MLP ensures each word processes and refines its own role. Together, they create the layered reasoning ability that makes GPT-2 so powerful.</p>
</section>
</section>
<section id="layernorm-on-cpu-step-by-step" class="level3">
<h3 class="anchored" data-anchor-id="layernorm-on-cpu-step-by-step">35. LayerNorm on CPU (Step-by-Step)</h3>
<p>One of the most important but often overlooked ingredients in GPT-2 is Layer Normalization, or LayerNorm for short. While attention and MLPs are the big stars, LayerNorm is like the stage crew keeping everything running smoothly behind the scenes. It ensures the numbers flowing through the network stay stable and balanced, preventing explosions or collapses that could make training impossible. In <code>train_gpt2.c</code>, LayerNorm is implemented with explicit loops so you can see every calculation. Let’s walk through it carefully.</p>
<section id="why-do-we-need-normalization" class="level4">
<h4 class="anchored" data-anchor-id="why-do-we-need-normalization">Why Do We Need Normalization?</h4>
<p>Imagine a classroom where every student talks at different volumes. Some whisper, some shout. If you try to listen to all of them at once, the loud voices drown out the quiet ones.</p>
<p>Neural networks face a similar problem. The outputs of layers can have wildly different scales. If one dimension of a vector is much larger than the others, it dominates. Training becomes unstable, and gradients may vanish or explode.</p>
<p>LayerNorm fixes this by ensuring that, for each token at each layer, the vector has:</p>
<ul>
<li>Mean = 0 (centered around zero)</li>
<li>Variance = 1 (consistent spread of values)</li>
</ul>
<p>After that, trainable parameters scale and shift the result so the model can still learn flexible transformations.</p>
</section>
<section id="the-math-behind-layernorm" class="level4">
<h4 class="anchored" data-anchor-id="the-math-behind-layernorm">The Math Behind LayerNorm</h4>
<p>For a given token vector <code>x</code> of size <code>C</code> (e.g., 768):</p>
<ol type="1">
<li><p>Compute mean:</p>
<pre><code>μ = (1/C) * Σ x[i]</code></pre></li>
<li><p>Compute variance:</p>
<pre><code>σ² = (1/C) * Σ (x[i] - μ)²</code></pre></li>
<li><p>Normalize:</p>
<pre><code>x_norm[i] = (x[i] - μ) / sqrt(σ² + ε)</code></pre>
<p>where <code>ε</code> is a tiny constant (like 1e-5) to avoid division by zero.</p></li>
<li><p>Scale and shift with trainable weights <code>g</code> (gamma) and <code>b</code> (beta):</p>
<pre><code>y[i] = g[i] * x_norm[i] + b[i]</code></pre></li>
</ol>
<p>So the final output has controlled statistics, but still enough flexibility for the model to adjust.</p>
</section>
<section id="the-code-in-train_gpt2.c" class="level4">
<h4 class="anchored" data-anchor-id="the-code-in-train_gpt2.c">The Code in <code>train_gpt2.c</code></h4>
<p>Here’s a simplified version from the repository:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb80"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> layernorm_forward<span class="op">(</span><span class="dt">float</span><span class="op">*</span> out<span class="op">,</span> <span class="dt">float</span><span class="op">*</span> inp<span class="op">,</span> <span class="dt">float</span><span class="op">*</span> weight<span class="op">,</span> <span class="dt">float</span><span class="op">*</span> bias<span class="op">,</span> <span class="dt">int</span> B<span class="op">,</span> <span class="dt">int</span> T<span class="op">,</span> <span class="dt">int</span> C<span class="op">)</span> <span class="op">{</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> b <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> b <span class="op">&lt;</span> B<span class="op">;</span> b<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> t <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> t <span class="op">&lt;</span> T<span class="op">;</span> t<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>            <span class="dt">float</span><span class="op">*</span> x <span class="op">=</span> inp <span class="op">+</span> b<span class="op">*</span>T<span class="op">*</span>C <span class="op">+</span> t<span class="op">*</span>C<span class="op">;</span></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>            <span class="dt">float</span><span class="op">*</span> o <span class="op">=</span> out <span class="op">+</span> b<span class="op">*</span>T<span class="op">*</span>C <span class="op">+</span> t<span class="op">*</span>C<span class="op">;</span></span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>            <span class="co">// mean</span></span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>            <span class="dt">float</span> mean <span class="op">=</span> <span class="fl">0.0</span><span class="bu">f</span><span class="op">;</span></span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> C<span class="op">;</span> i<span class="op">++)</span> mean <span class="op">+=</span> x<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>            mean <span class="op">/=</span> C<span class="op">;</span></span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a>            <span class="co">// variance</span></span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>            <span class="dt">float</span> var <span class="op">=</span> <span class="fl">0.0</span><span class="bu">f</span><span class="op">;</span></span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> C<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a>                <span class="dt">float</span> diff <span class="op">=</span> x<span class="op">[</span>i<span class="op">]</span> <span class="op">-</span> mean<span class="op">;</span></span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a>                var <span class="op">+=</span> diff <span class="op">*</span> diff<span class="op">;</span></span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a>            <span class="op">}</span></span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a>            var <span class="op">/=</span> C<span class="op">;</span></span>
<span id="cb80-19"><a href="#cb80-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-20"><a href="#cb80-20" aria-hidden="true" tabindex="-1"></a>            <span class="co">// normalize, scale, shift</span></span>
<span id="cb80-21"><a href="#cb80-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> C<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb80-22"><a href="#cb80-22" aria-hidden="true" tabindex="-1"></a>                <span class="dt">float</span> norm <span class="op">=</span> <span class="op">(</span>x<span class="op">[</span>i<span class="op">]</span> <span class="op">-</span> mean<span class="op">)</span> <span class="op">/</span> sqrtf<span class="op">(</span>var <span class="op">+</span> <span class="fl">1e-5</span><span class="bu">f</span><span class="op">);</span></span>
<span id="cb80-23"><a href="#cb80-23" aria-hidden="true" tabindex="-1"></a>                o<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> norm <span class="op">*</span> weight<span class="op">[</span>i<span class="op">]</span> <span class="op">+</span> bias<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb80-24"><a href="#cb80-24" aria-hidden="true" tabindex="-1"></a>            <span class="op">}</span></span>
<span id="cb80-25"><a href="#cb80-25" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb80-26"><a href="#cb80-26" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb80-27"><a href="#cb80-27" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Notice the structure:</p>
<ul>
<li>Outer loops over batch <code>B</code> and sequence length <code>T</code>.</li>
<li>Inner loops compute mean, variance, and then apply normalization per token vector of length <code>C</code>.</li>
<li><code>weight</code> and <code>bias</code> are the learnable gamma and beta.</li>
</ul>
<p>This is exactly what LayerNorm means: normalize <em>each layer’s inputs per token</em>.</p>
</section>
<section id="example-walkthrough-1" class="level4">
<h4 class="anchored" data-anchor-id="example-walkthrough-1">Example Walkthrough</h4>
<p>Suppose we have a single token vector (C=4) = <code>[2.0, -1.0, 3.0, 0.0]</code>.</p>
<ol type="1">
<li><p>Mean: <code>(2 - 1 + 3 + 0)/4 = 1.0</code></p></li>
<li><p>Variance: <code>((2-1)² + (-1-1)² + (3-1)² + (0-1)²)/4 = (1 + 4 + 4 + 1)/4 = 2.5</code></p></li>
<li><p>Normalize: subtract mean and divide by sqrt(2.5):</p>
<pre><code>[ (2-1)/1.58, (-1-1)/1.58, (3-1)/1.58, (0-1)/1.58 ]
= [0.63, -1.26, 1.26, -0.63]</code></pre></li>
<li><p>Scale and shift (say weight=[1,1,1,1], bias=[0,0,0,0]):</p>
<pre><code>[0.63, -1.26, 1.26, -0.63]</code></pre></li>
</ol>
<p>Now the vector has mean 0, variance 1, and is ready for the next layer.</p>
</section>
<section id="analogy-1" class="level4">
<h4 class="anchored" data-anchor-id="analogy-1">Analogy</h4>
<p>Think of LayerNorm like a baking recipe. If one ingredient is way too strong (like adding five times too much salt), the whole dish is ruined. LayerNorm tastes the mixture, balances all the flavors, and then lets you adjust the seasoning with learnable gamma (scale) and beta (shift).</p>
</section>
<section id="why-it-matters-22" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-22">Why It Matters</h4>
<p>Without LayerNorm, the model would quickly become unstable:</p>
<ul>
<li>Some tokens would dominate, while others fade.</li>
<li>Gradients could explode, making loss jump wildly.</li>
<li>Training would be inconsistent between batches.</li>
</ul>
<p>With LayerNorm, each layer works with clean, normalized inputs. This allows deeper stacks of attention and MLP blocks to learn reliably.</p>
</section>
<section id="try-it-yourself-24" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-24">Try It Yourself</h4>
<ol type="1">
<li>Print statistics: Add debug code to check the mean and variance before and after LayerNorm. Before: mean ≠ 0, variance ≠ 1. After: mean ≈ 0, variance ≈ 1.</li>
<li>Remove LayerNorm: Comment out LayerNorm in the code. Watch training collapse-loss will not decrease properly.</li>
<li>Change epsilon: Try making <code>ε = 1e-1</code> or <code>ε = 1e-12</code>. See how too large or too small values can break stability.</li>
<li>Observe gamma and beta: Initialize gamma=1, beta=0. During training, watch how these parameters drift, fine-tuning normalization.</li>
<li>Experiment with batch norm: Replace LayerNorm with BatchNorm (not typical for transformers). You’ll see it doesn’t work well, because transformers process variable-length sequences where per-batch statistics vary too much.</li>
</ol>
</section>
<section id="the-takeaway-24" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-24">The Takeaway</h4>
<p>LayerNorm is the quiet but critical stabilizer in GPT-2. It ensures every token vector is balanced, centered, and scaled before moving into attention or MLP. In <code>train_gpt2.c</code>, you see exactly how it works: compute mean, compute variance, normalize, then scale and shift. Even though it’s just a few lines of C code, it’s one of the main reasons deep transformers can stack dozens of layers without breaking.</p>
</section>
</section>
<section id="residual-adds-and-signal-flow" class="level3">
<h3 class="anchored" data-anchor-id="residual-adds-and-signal-flow">36. Residual Adds and Signal Flow</h3>
<p>Once embeddings, attention, and MLP blocks are computed, there’s still one piece left to keep the whole network stable and effective: residual connections. In <code>train_gpt2.c</code>, these appear in functions like <code>residual_forward</code>, where outputs of a layer are added back to their inputs. This simple-looking step is one of the key reasons GPT-2 and other deep transformer models can stack many layers without collapsing.</p>
<section id="the-core-idea-1" class="level4">
<h4 class="anchored" data-anchor-id="the-core-idea-1">The Core Idea</h4>
<p>A residual connection says:</p>
<pre><code>output = input + transformation(input)</code></pre>
<p>Instead of replacing the old representation with the new one, the model adds them together. That way, the original signal always survives, even if the new transformation is noisy or imperfect.</p>
<p>Think of it like taking lecture notes. Each time the teacher explains more, you don’t throw away your old notes. You add new details next to them. That way, you preserve everything learned so far, while layering new insights on top.</p>
</section>
<section id="why-residuals-are-crucial" class="level4">
<h4 class="anchored" data-anchor-id="why-residuals-are-crucial">Why Residuals Are Crucial</h4>
<ol type="1">
<li><p>Preventing information loss: If you only applied transformations, some features might vanish forever. Adding the input back ensures no information is lost.</p></li>
<li><p>Helping gradients flow: During backpropagation, gradients must travel backward through many layers. Without shortcuts, they can vanish or explode. Residuals create direct paths for gradients, making learning stable.</p></li>
<li><p>Improving training speed: With residuals, deeper networks converge faster because the model can “skip” bad transformations while still using the identity mapping.</p></li>
</ol>
</section>
<section id="the-code-in-train_gpt2.c-1" class="level4">
<h4 class="anchored" data-anchor-id="the-code-in-train_gpt2.c-1">The Code in <code>train_gpt2.c</code></h4>
<p>Here’s the implementation of residual addition:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb84"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> residual_forward<span class="op">(</span><span class="dt">float</span><span class="op">*</span> out<span class="op">,</span> <span class="dt">float</span><span class="op">*</span> inp1<span class="op">,</span> <span class="dt">float</span><span class="op">*</span> inp2<span class="op">,</span> <span class="dt">int</span> N<span class="op">)</span> <span class="op">{</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> N<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>        out<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> inp1<span class="op">[</span>i<span class="op">]</span> <span class="op">+</span> inp2<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>It’s deceptively simple:</p>
<ul>
<li><code>inp1</code> is the original input.</li>
<li><code>inp2</code> is the new transformation (from attention or MLP).</li>
<li><code>out</code> stores the sum.</li>
<li><code>N</code> is the total number of floats (<code>B * T * C</code>).</li>
</ul>
<p>Even though it’s just a single line inside a loop, this is what makes stacking 12+ transformer blocks possible.</p>
</section>
<section id="example-walkthrough-2" class="level4">
<h4 class="anchored" data-anchor-id="example-walkthrough-2">Example Walkthrough</h4>
<p>Suppose we’re processing the token “cat.”</p>
<ul>
<li>Input vector (simplified, size=3): <code>[0.5, -0.3, 0.7]</code></li>
<li>After attention block: <code>[0.2, 0.1, -0.4]</code></li>
</ul>
<p>Residual addition:</p>
<pre><code>[0.5 + 0.2, -0.3 + 0.1, 0.7 - 0.4] = [0.7, -0.2, 0.3]</code></pre>
<p>Now the representation of “cat” contains both the original signal and the contextual information from attention.</p>
<p>Later, after the MLP:</p>
<ul>
<li>Input again: <code>[0.7, -0.2, 0.3]</code></li>
<li>MLP output: <code>[0.1, -0.5, 0.4]</code></li>
<li>Residual: <code>[0.8, -0.7, 0.7]</code></li>
</ul>
<p>Step by step, the vector grows richer without losing its foundation.</p>
</section>
<section id="analogy-2" class="level4">
<h4 class="anchored" data-anchor-id="analogy-2">Analogy</h4>
<p>Residual connections are like building layers in Photoshop. Each new layer adds adjustments, but you always keep the original photo underneath. If a new adjustment is bad, you can still see the original. This makes the final composition stronger and safer to experiment with.</p>
</section>
<section id="residuals-across-the-model" class="level4">
<h4 class="anchored" data-anchor-id="residuals-across-the-model">Residuals Across the Model</h4>
<p>In GPT-2’s forward pass, residuals appear in two main places inside each transformer block:</p>
<ol type="1">
<li><p>After attention:</p>
<pre><code>x = x + Attention(x)</code></pre></li>
<li><p>After MLP:</p>
<pre><code>x = x + MLP(x)</code></pre></li>
</ol>
<p>Together with LayerNorm before each block, these form the backbone of the transformer architecture:</p>
<pre><code>x → LayerNorm → Attention → Residual Add → LayerNorm → MLP → Residual Add</code></pre>
</section>
<section id="why-it-matters-23" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-23">Why It Matters</h4>
<p>Without residual connections, GPT-2 would struggle to train past a few layers. Deeper stacks would lose track of the original signal, gradients would vanish, and performance would stall. Residuals are the glue that holds the whole architecture together, enabling models with billions of parameters to train effectively.</p>
</section>
<section id="try-it-yourself-25" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-25">Try It Yourself</h4>
<ol type="1">
<li>Remove residuals: Temporarily comment out the <code>residual_forward</code> calls. Training will quickly fail-the loss won’t decrease properly.</li>
<li>Print before/after: Inspect a token’s vector before and after residual add. Notice how the numbers change smoothly rather than being overwritten.</li>
<li>Experiment with scaling: Try replacing <code>out[i] = inp1[i] + inp2[i];</code> with <code>out[i] = inp1[i] + 0.1 * inp2[i];</code>. This reduces the impact of the new transformation-sometimes used in advanced architectures.</li>
<li>Compare to skip-less RNNs: Research how older recurrent networks without residuals had trouble scaling deep. You’ll see why residuals are a game changer.</li>
<li>Chain of signals: Track how a single token’s vector evolves across all 12 layers. You’ll notice it keeps its core identity while absorbing new context.</li>
</ol>
</section>
<section id="the-takeaway-25" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-25">The Takeaway</h4>
<p>Residual connections may look like a simple addition, but they’re the key to deep learning’s success in transformers. They preserve information, stabilize training, and allow GPT-2 to stack many layers without falling apart. In <code>train_gpt2.c</code>, this idea is laid bare: a few lines of C code implementing one of the most powerful tricks in modern neural networks.</p>
</section>
</section>
<section id="cross-entropy-loss-on-cpu" class="level3">
<h3 class="anchored" data-anchor-id="cross-entropy-loss-on-cpu">37. Cross-Entropy Loss on CPU</h3>
<p>After embeddings, attention, MLPs, LayerNorm, and residuals have done their job, the model produces logits-raw scores for every word in the vocabulary at every position in the sequence. But logits alone don’t tell us if the model is “good” or “bad” at predicting the right word. To measure performance and guide learning, GPT-2 uses the cross-entropy loss function. In <code>train_gpt2.c</code>, this is implemented in the function <code>crossentropy_forward</code>.</p>
<section id="what-are-logits" class="level4">
<h4 class="anchored" data-anchor-id="what-are-logits">What Are Logits?</h4>
<p>At the final stage of the forward pass, each token position has a vector of length <code>V</code> (vocabulary size, ~50k). For example, the model might produce these logits for a tiny vocabulary of 3 words:</p>
<pre><code>logits = [5.2, 1.1, -2.7]</code></pre>
<p>Logits are just numbers-bigger means “more likely,” smaller means “less likely”-but they aren’t probabilities yet.</p>
</section>
<section id="step-1-softmax-turning-scores-into-probabilities" class="level4">
<h4 class="anchored" data-anchor-id="step-1-softmax-turning-scores-into-probabilities">Step 1: Softmax – Turning Scores Into Probabilities</h4>
<p>To compare predictions with the true target, we first convert logits into probabilities. The tool for this is the softmax function:</p>
<pre><code>p[i] = exp(logits[i]) / Σ exp(logits[j])</code></pre>
<p>Softmax has two important effects:</p>
<ol type="1">
<li>It makes all values positive.</li>
<li>It normalizes them so they sum to 1, forming a probability distribution.</li>
</ol>
<p>Example:</p>
<ul>
<li>Logits: <code>[5.2, 1.1, -2.7]</code></li>
<li>Subtract max (5.2) for stability → <code>[0.0, -4.1, -7.9]</code></li>
<li>Exponentiate → <code>[1.0, 0.017, 0.0004]</code></li>
<li>Normalize → <code>[0.98, 0.017, 0.0004]</code></li>
</ul>
<p>Now the model is saying:</p>
<ul>
<li>Word 0: 98% chance</li>
<li>Word 1: 1.7% chance</li>
<li>Word 2: 0.04% chance</li>
</ul>
</section>
<section id="step-2-cross-entropy-measuring-mistakes" class="level4">
<h4 class="anchored" data-anchor-id="step-2-cross-entropy-measuring-mistakes">Step 2: Cross-Entropy – Measuring Mistakes</h4>
<p>Cross-entropy compares the predicted probability for the correct word against the ideal case (probability = 1).</p>
<p>Formula:</p>
<pre><code>loss = -log(probability_of_correct_word)</code></pre>
<ul>
<li>If the model assigns high probability to the correct word, loss is small.</li>
<li>If the model assigns low probability, loss is large.</li>
</ul>
<p>Example:</p>
<ul>
<li>Correct word = Word 0, probability = 0.98 → loss = -log(0.98) ≈ 0.02 (excellent).</li>
<li>Correct word = Word 1, probability = 0.017 → loss = -log(0.017) ≈ 4.1 (bad).</li>
</ul>
</section>
<section id="step-3-averaging-over-the-batch" class="level4">
<h4 class="anchored" data-anchor-id="step-3-averaging-over-the-batch">Step 3: Averaging Over the Batch</h4>
<p>In practice, we don’t train on just one word, but a batch of sequences. The code loops over every token in every batch, collects their losses, and averages them.</p>
<p>From <code>train_gpt2.c</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb92"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="dt">float</span> loss <span class="op">=</span> <span class="fl">0.0</span><span class="bu">f</span><span class="op">;</span></span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> B<span class="op">*</span>T<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> target <span class="op">=</span> targets<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">float</span> logit_max <span class="op">=</span> <span class="op">-</span><span class="fl">1e9</span><span class="op">;</span></span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> Vp<span class="op">;</span> j<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(</span>logits<span class="op">[</span>i<span class="op">*</span>Vp <span class="op">+</span> j<span class="op">]</span> <span class="op">&gt;</span> logit_max<span class="op">)</span> logit_max <span class="op">=</span> logits<span class="op">[</span>i<span class="op">*</span>Vp <span class="op">+</span> j<span class="op">];</span></span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>    <span class="dt">float</span> sum <span class="op">=</span> <span class="fl">0.0</span><span class="bu">f</span><span class="op">;</span></span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> Vp<span class="op">;</span> j<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a>        sum <span class="op">+=</span> expf<span class="op">(</span>logits<span class="op">[</span>i<span class="op">*</span>Vp <span class="op">+</span> j<span class="op">]</span> <span class="op">-</span> logit_max<span class="op">);</span></span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb92-12"><a href="#cb92-12" aria-hidden="true" tabindex="-1"></a>    <span class="dt">float</span> log_sum <span class="op">=</span> logf<span class="op">(</span>sum<span class="op">);</span></span>
<span id="cb92-13"><a href="#cb92-13" aria-hidden="true" tabindex="-1"></a>    <span class="dt">float</span> correct_logit <span class="op">=</span> logits<span class="op">[</span>i<span class="op">*</span>Vp <span class="op">+</span> target<span class="op">];</span></span>
<span id="cb92-14"><a href="#cb92-14" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">+=</span> <span class="op">(</span>log_sum <span class="op">+</span> logit_max <span class="op">-</span> correct_logit<span class="op">);</span></span>
<span id="cb92-15"><a href="#cb92-15" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb92-16"><a href="#cb92-16" aria-hidden="true" tabindex="-1"></a>loss <span class="op">/=</span> <span class="op">(</span>B<span class="op">*</span>T<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>What’s happening here:</p>
<ol type="1">
<li>For each token, find the max logit (<code>logit_max</code>) to improve numerical stability.</li>
<li>Compute softmax denominator (<code>sum</code>).</li>
<li>Calculate log probability of the correct token.</li>
<li>Accumulate losses across all tokens.</li>
<li>Divide by total tokens (<code>B*T</code>) to get the average.</li>
</ol>
</section>
<section id="numerical-stability-tricks" class="level4">
<h4 class="anchored" data-anchor-id="numerical-stability-tricks">Numerical Stability Tricks</h4>
<p>Without subtracting <code>logit_max</code>, <code>exp(logits)</code> can overflow. For example, <code>exp(1000)</code> is infinite. By subtracting the max, the largest logit becomes 0, so its exponential is 1, and all others are ≤ 1. This keeps numbers manageable while preserving the probability ratios.</p>
</section>
<section id="example-with-a-sentence" class="level4">
<h4 class="anchored" data-anchor-id="example-with-a-sentence">Example With a Sentence</h4>
<p>Sentence: <em>The cat sat on the mat.</em></p>
<p>Suppose the model predicts probabilities for the last token:</p>
<ul>
<li>“mat”: 0.85</li>
<li>“dog”: 0.10</li>
<li>“car”: 0.05</li>
</ul>
<p>Correct word = “mat.”</p>
<p>Loss = <code>-log(0.85) ≈ 0.16</code>.</p>
<p>If instead the model guessed “dog” with 0.10, loss = <code>-log(0.10) ≈ 2.3</code>. Higher penalty for being wrong.</p>
</section>
<section id="analogy-3" class="level4">
<h4 class="anchored" data-anchor-id="analogy-3">Analogy</h4>
<p>Cross-entropy is like grading multiple-choice exams. If the student picks the right answer confidently (high probability), they lose almost no points. If they’re hesitant or wrong, they lose more points. Over many questions (tokens), you calculate their average score-the training loss.</p>
</section>
<section id="why-it-matters-24" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-24">Why It Matters</h4>
<p>Cross-entropy loss is the guiding signal for the entire training process. It tells the optimizer:</p>
<ul>
<li>“Increase probability for the right words.”</li>
<li>“Decrease probability for the wrong words.”</li>
</ul>
<p>Without it, GPT-2 would have no way of knowing whether its predictions are improving.</p>
</section>
<section id="try-it-yourself-26" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-26">Try It Yourself</h4>
<ol type="1">
<li>Check baseline loss: Before training, print the loss. It should be close to <code>log(vocab_size)</code> (~10.8 for GPT-2 small), which corresponds to random guessing.</li>
<li>Inspect softmax sums: For one token, sum all probabilities. It should equal ~1.0.</li>
<li>Force the wrong answer: Temporarily change the target to an incorrect word. Watch how the loss shoots up.</li>
<li>Observe loss during training: Print loss every step. It should steadily decrease as the model learns.</li>
<li>Compare with accuracy: Track how often the model’s top prediction matches the target. Loss and accuracy will move together, but loss is smoother and more informative.</li>
</ol>
</section>
<section id="the-takeaway-26" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-26">The Takeaway</h4>
<p>Cross-entropy loss turns raw model scores into a clear training signal. It penalizes wrong predictions, rewards confident correct ones, and ensures the optimizer knows exactly how to adjust weights. In <code>train_gpt2.c</code>, you see this implemented explicitly, without any library shortcuts-just loops, exponentials, and logs. Understanding this section is key to understanding how GPT-2 learns from its mistakes.</p>
</section>
</section>
<section id="putting-it-all-together-the-gpt2_forward-function" class="level3">
<h3 class="anchored" data-anchor-id="putting-it-all-together-the-gpt2_forward-function">38. Putting It All Together: The <code>gpt2_forward</code> Function</h3>
<p>Up to this point, we’ve explored the forward pass piece by piece - embeddings, attention, feed-forward layers, layer normalization, residual connections, and finally the loss. But a model doesn’t live as disconnected pieces; they all come together in a single function that drives inference: <code>gpt2_forward</code>. This function is where the code actually executes the story we’ve been telling. Let’s walk through it carefully so you can see how every building block plugs into the whole picture.</p>
<section id="the-role-of-gpt2_forward" class="level4">
<h4 class="anchored" data-anchor-id="the-role-of-gpt2_forward">The role of <code>gpt2_forward</code></h4>
<p>Think of <code>gpt2_forward</code> as the director of the play. The actors (embeddings, attention, MLP, layernorm, etc.) already know their roles. The director calls them on stage in the right order and makes sure they hand the script off smoothly to the next actor. In our case:</p>
<ol type="1">
<li>Tokens come in as integers (word IDs).</li>
<li>They’re turned into embeddings (token + position).</li>
<li>Each transformer block processes the sequence through attention, MLP, layernorm, and residuals.</li>
<li>The final hidden states are mapped back into vocabulary space.</li>
<li>If labels are provided, a loss is computed.</li>
</ol>
</section>
<section id="code-skeleton" class="level4">
<h4 class="anchored" data-anchor-id="code-skeleton">Code skeleton</h4>
<p>Here’s a simplified excerpt of the real function from <code>train_gpt2.c</code> (slightly shortened for readability):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb93"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> gpt2_forward<span class="op">(</span>GPT2 <span class="op">*</span>model<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>tokens<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>labels<span class="op">,</span> <span class="dt">int</span> B<span class="op">,</span> <span class="dt">int</span> T<span class="op">)</span> <span class="op">{</span></span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Step 1: Embedding lookup</span></span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>    embedding_forward<span class="op">(</span>model<span class="op">-&gt;</span>token_embedding<span class="op">,</span> tokens<span class="op">,</span> B<span class="op">,</span> T<span class="op">);</span></span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>    embedding_forward<span class="op">(</span>model<span class="op">-&gt;</span>position_embedding<span class="op">,</span> positions<span class="op">,</span> B<span class="op">,</span> T<span class="op">);</span></span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Step 2: Transformer blocks</span></span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> l <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> l <span class="op">&lt;</span> model<span class="op">-&gt;</span>config<span class="op">-&gt;</span>n_layer<span class="op">;</span> l<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a>        attention_forward<span class="op">(&amp;</span>model<span class="op">-&gt;</span>blocks<span class="op">[</span>l<span class="op">].</span>attn<span class="op">,</span> <span class="op">...);</span></span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a>        mlp_forward<span class="op">(&amp;</span>model<span class="op">-&gt;</span>blocks<span class="op">[</span>l<span class="op">].</span>mlp<span class="op">,</span> <span class="op">...);</span></span>
<span id="cb93-10"><a href="#cb93-10" aria-hidden="true" tabindex="-1"></a>        layernorm_forward<span class="op">(&amp;</span>model<span class="op">-&gt;</span>blocks<span class="op">[</span>l<span class="op">].</span>ln<span class="op">,</span> <span class="op">...);</span></span>
<span id="cb93-11"><a href="#cb93-11" aria-hidden="true" tabindex="-1"></a>        residual_forward<span class="op">(...);</span></span>
<span id="cb93-12"><a href="#cb93-12" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb93-13"><a href="#cb93-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-14"><a href="#cb93-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Step 3: Final normalization + logits</span></span>
<span id="cb93-15"><a href="#cb93-15" aria-hidden="true" tabindex="-1"></a>    layernorm_forward<span class="op">(</span>model<span class="op">-&gt;</span>final_ln<span class="op">,</span> <span class="op">...);</span></span>
<span id="cb93-16"><a href="#cb93-16" aria-hidden="true" tabindex="-1"></a>    matmul_forward<span class="op">(</span>model<span class="op">-&gt;</span>lm_head<span class="op">,</span> <span class="op">...);</span> <span class="co">// project to vocab</span></span>
<span id="cb93-17"><a href="#cb93-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-18"><a href="#cb93-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Step 4: Loss (optional)</span></span>
<span id="cb93-19"><a href="#cb93-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>labels <span class="op">!=</span> NULL<span class="op">)</span> <span class="op">{</span></span>
<span id="cb93-20"><a href="#cb93-20" aria-hidden="true" tabindex="-1"></a>        crossentropy_forward<span class="op">(...);</span></span>
<span id="cb93-21"><a href="#cb93-21" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb93-22"><a href="#cb93-22" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Don’t worry if this looks intimidating - we’ll decode each part in plain language.</p>
</section>
<section id="step-1-embedding-lookup" class="level4">
<h4 class="anchored" data-anchor-id="step-1-embedding-lookup">Step 1: Embedding lookup</h4>
<p>Before the model can reason about words, it has to map token IDs into continuous vectors. That’s where embedding tables come in:</p>
<ul>
<li><code>token_embedding</code> converts each integer token ID into a dense vector of size <code>C</code> (the channel dimension).</li>
<li><code>position_embedding</code> does the same for positions (0, 1, 2, …, T-1).</li>
<li>These two are added together, giving each token both a meaning (word identity) and a place in the sentence.</li>
</ul>
</section>
<section id="step-2-transformer-blocks" class="level4">
<h4 class="anchored" data-anchor-id="step-2-transformer-blocks">Step 2: Transformer blocks</h4>
<p>Each block is like a mini-pipeline that processes the sequence and passes it forward. Inside the loop:</p>
<ol type="1">
<li>Attention: compares tokens with each other, weighted by learned Q/K/V projections.</li>
<li>MLP: expands each token vector, applies a nonlinear GELU activation, then projects back down.</li>
<li>LayerNorm: normalizes values for stable training and inference.</li>
<li>Residual: adds the input of the block back to its output to keep information flowing.</li>
</ol>
<p>This loop runs <code>n_layer</code> times - for GPT-2 124M, that’s 12 blocks.</p>
</section>
<section id="step-3-final-normalization-and-logits" class="level4">
<h4 class="anchored" data-anchor-id="step-3-final-normalization-and-logits">Step 3: Final normalization and logits</h4>
<p>After the last block, the sequence of token representations goes through a final layer normalization. Then, a large matrix multiplication (<code>lm_head</code>) projects each token’s hidden state into the size of the vocabulary (≈50,000 for GPT-2). The result is a tensor of shape <code>(B, T, vocab_size)</code> containing the raw prediction scores for each next token.</p>
</section>
<section id="step-4-optional-loss-computation" class="level4">
<h4 class="anchored" data-anchor-id="step-4-optional-loss-computation">Step 4: Optional loss computation</h4>
<p>If you pass <code>labels</code> (the correct next tokens) into <code>gpt2_forward</code>, the function calls <code>crossentropy_forward</code>. This compares the predicted scores with the true tokens and outputs a single number: the loss. The loss tells you “how wrong” the model was, which is critical during training. But if you’re only doing inference, you don’t need this step.</p>
</section>
<section id="how-the-pieces-connect" class="level4">
<h4 class="anchored" data-anchor-id="how-the-pieces-connect">How the pieces connect</h4>
<p>Here’s a table that maps our earlier sections to the parts of <code>gpt2_forward</code>:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 52%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th>Code Step</th>
<th>Concept</th>
<th>Section Covered Earlier</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Embeddings</td>
<td>token + positional vectors</td>
<td>32</td>
</tr>
<tr class="even">
<td>Attention</td>
<td>QKV projections, masking, softmax</td>
<td>33</td>
</tr>
<tr class="odd">
<td>MLP</td>
<td>feed-forward expansion and compression</td>
<td>34</td>
</tr>
<tr class="even">
<td>LayerNorm</td>
<td>normalization for stability</td>
<td>35</td>
</tr>
<tr class="odd">
<td>Residual</td>
<td>skip connections for signal flow</td>
<td>36</td>
</tr>
<tr class="even">
<td>CrossEntropy</td>
<td>comparing predictions with labels</td>
<td>37</td>
</tr>
</tbody>
</table>
<p>So <code>gpt2_forward</code> is really just a neat orchestration of everything you’ve already learned.</p>
</section>
<section id="why-it-matters-25" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-25">Why it matters</h4>
<p>Understanding <code>gpt2_forward</code> gives you the complete mental picture of inference. It shows how embeddings, attention, MLP, normalization, and residuals work together in code to turn a batch of tokens into predictions. Without this integration step, the model would just be a collection of disconnected parts.</p>
</section>
<section id="try-it-yourself-27" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-27">Try it yourself</h4>
<ol type="1">
<li>Print shapes: Add <code>printf</code> statements inside <code>gpt2_forward</code> to print tensor shapes after embeddings, after each block, and after logits. This helps you see the data flow.</li>
<li>Use a single block: Change the loop to run only 1 transformer block instead of all 12. Watch how the outputs degrade - the model loses depth of reasoning.</li>
<li>Disable position embeddings: Comment out the line that adds <code>position_embedding</code>. Try running inference. You’ll notice the model becomes worse at handling word order.</li>
<li>Loss vs no loss: Call <code>gpt2_forward</code> with and without labels. Compare the difference - with labels you get a scalar loss, without labels you just get logits.</li>
<li>Smaller vocab: Try using a toy tokenizer with a small vocabulary and rerun the projection step. You’ll see the logits shrink to <code>(B, T, tiny_vocab_size)</code>.</li>
</ol>
</section>
<section id="the-takeaway-27" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-27">The takeaway</h4>
<p><code>gpt2_forward</code> is where GPT-2 inference really happens. It ties together every concept - embeddings, attention, feed-forward layers, normalization, residuals, and the final projection into vocabulary space. Once you understand this function, you don’t just know the pieces of GPT-2, you know how they actually work together to produce predictions. It’s the “main stage” of inference, and mastering it means you can confidently say you understand how a transformer runs forward on CPU.</p>
</section>
</section>
<section id="openmp-pragmas-for-parallel-loops" class="level3">
<h3 class="anchored" data-anchor-id="openmp-pragmas-for-parallel-loops">39. OpenMP Pragmas for Parallel Loops</h3>
<p>CPU training in <code>train_gpt2.c</code> is intentionally “plain C,” but it still squeezes out a lot of speed by adding a few OpenMP pragmas (<code>#pragma omp …</code>) in the hottest loops. OpenMP lets the compiler split a loop’s iterations across multiple CPU cores-no threads to create by hand, no locks to manage. If you compile without OpenMP support, these pragmas are simply ignored and the code still runs (just slower).</p>
<p>Below we’ll (1) show exactly where OpenMP is used, (2) explain why those loops are good candidates, and (3) offer practical tips to get solid speedups on your machine.</p>
<section id="openmp-in-this-file-where-it-appears-and-why" class="level4">
<h4 class="anchored" data-anchor-id="openmp-in-this-file-where-it-appears-and-why">OpenMP in this file: where it appears and why</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 16%">
<col style="width: 23%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th>Location / Function</th>
<th>Pragma used</th>
<th>What’s parallelized</th>
<th>Why it’s a great fit</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>matmul_forward_naive</code></td>
<td><code>#pragma omp parallel for collapse(2)</code></td>
<td>Outer loops over <code>b</code> (batch) and <code>t</code> (time)</td>
<td>Each <code>(b,t)</code> row computes an independent output vector; no write conflicts. Large, regular work = easy scaling.</td>
</tr>
<tr class="even">
<td><code>matmul_forward</code> (tiled)</td>
<td><code>#pragma omp parallel for</code></td>
<td>Collapsed <code>B*T</code> loop in tiles of <code>LOOP_UNROLL</code></td>
<td>Heaviest compute in the model; tiling + per-thread tiles keep caches warm.</td>
</tr>
<tr class="odd">
<td><code>matmul_backward</code> (part 1)</td>
<td><code>#pragma omp parallel for collapse(2)</code></td>
<td>Backprop into <code>inp</code> over <code>(b,t)</code></td>
<td>Each <code>(b,t)</code> reads weights and <code>dout</code>, writes a private slice of <code>dinp</code> → no overlap.</td>
</tr>
<tr class="even">
<td><code>matmul_backward</code> (part 2)</td>
<td><code>#pragma omp parallel for</code></td>
<td>Backprop into <code>weight</code>/<code>bias</code> over <code>o</code> (output channel)</td>
<td>Each thread owns one output channel’s gradient row, avoiding atomics.</td>
</tr>
<tr class="odd">
<td><code>softmax_forward</code></td>
<td><code>#pragma omp parallel for collapse(2)</code></td>
<td>Over <code>(b,t)</code> positions</td>
<td>Each softmax is independent; perfect “embarrassingly parallel” loop.</td>
</tr>
<tr class="even">
<td><code>attention_forward</code></td>
<td><code>#pragma omp parallel for collapse(3)</code></td>
<td>Over <code>(b, t, h)</code> = batch, time, head</td>
<td>Per <code>(b,t,h)</code> head’s work is independent; big 3-D grid parallelizes extremely well.</td>
</tr>
</tbody>
</table>
<p>A few key patterns to notice:</p>
<ul>
<li>Collapse clauses (<code>collapse(2)</code> / <code>collapse(3)</code>) fuse nested loops into one big iteration space so the scheduler can distribute more, smaller chunks-great for load-balancing when <code>B</code>, <code>T</code>, or <code>NH</code> are modest.</li>
<li>Parallelizing along independent dimensions avoids race conditions. For example, in <code>matmul_backward</code> the pass that writes <code>dinp[b,t,:]</code> is parallelized over <code>(b,t)</code> so no two threads update the same memory.</li>
<li>Own-your-row strategy: when accumulating <code>dweight</code>, the loop goes over <code>o</code> (output channels) so each thread writes its own gradient row <code>dweight[o,:]</code>. No atomics needed.</li>
</ul>
</section>
<section id="quick-refresher-what-openmp-is-doing" class="level4">
<h4 class="anchored" data-anchor-id="quick-refresher-what-openmp-is-doing">Quick refresher: what OpenMP is doing</h4>
<p>A typical pattern looks like:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb94"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma omp parallel for collapse(2)</span></span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> b <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> b <span class="op">&lt;</span> B<span class="op">;</span> b<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> t <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> t <span class="op">&lt;</span> T<span class="op">;</span> t<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>        <span class="co">// compute outputs for (b, t) independently</span></span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>When compiled with OpenMP, the compiler creates a team of threads and divides the iteration space (<code>B*T</code> in this example) among them. Each thread executes its assigned iterations; when the loop finishes, threads sync at an implicit barrier.</p>
<p>Because each <code>(b,t)</code> (or <code>(b,t,h)</code>) writes to a disjoint slice of the output arrays, there’s no need for locks or atomics. This is why these loops scale cleanly across cores.</p>
</section>
<section id="enabling-openmp-safely" class="level4">
<h4 class="anchored" data-anchor-id="enabling-openmp-safely">Enabling OpenMP, safely</h4>
<ul>
<li><p>The source guards the OpenMP header with:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb95"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#ifdef OMP</span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;omp.h&gt;</span></span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a><span class="pp">#endif</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>so you can define <code>OMP</code> in your build and add your compiler switch. Example (GCC/Clang):</p>
<pre><code>-D OMP -fopenmp</code></pre></li>
<li><p>If you forget <code>-fopenmp</code> (or your platform’s equivalent), the pragmas are ignored and the program runs single-threaded.</p></li>
<li><p>You can control threads at runtime:</p>
<pre><code>export OMP_NUM_THREADS=8</code></pre>
<p>A good rule of thumb is to start with the number of physical cores on your CPU.</p></li>
</ul>
</section>
<section id="why-these-loops-benefit-the-most" class="level4">
<h4 class="anchored" data-anchor-id="why-these-loops-benefit-the-most">Why these loops benefit the most</h4>
<ol type="1">
<li><p>Matrix multiplies dominate runtime. <code>matmul_forward</code>/<code>matmul_backward</code> consume the bulk of CPU time. Parallelizing them yields the largest end-to-end speedups.</p></li>
<li><p>Softmax is independent per position. Each <code>(b,t)</code> softmax computes a max, then exponentials and a sum-no cross-talk between positions.</p></li>
<li><p>Attention splits across batch/time/head. The triple loop over <code>(b,t,h)</code> has lots of work per iteration (Q·K, softmax, weighted sum), making thread overhead negligible compared to useful compute.</p></li>
<li><p>Minimal synchronization and no atomics. By choosing iteration spaces that own exclusive output slices, we avoid costly synchronization.</p></li>
</ol>
</section>
<section id="practical-tips-for-better-scaling" class="level4">
<h4 class="anchored" data-anchor-id="practical-tips-for-better-scaling">Practical tips for better scaling</h4>
<ul>
<li>Set <code>OMP_NUM_THREADS</code> to your CPU. Too many threads can hurt (oversubscription). Start with physical cores, then experiment.</li>
<li>Pin threads (optional, advanced). Some OpenMP runtimes support <code>OMP_PROC_BIND=close</code> to improve cache locality.</li>
<li>Mind memory bandwidth. On wide CPUs, GEMMs may become bandwidth-bound. Bigger <code>B</code>/<code>T</code> improves arithmetic intensity; tiny batches underutilize cores.</li>
<li>Warm caches with tiling. The “tiled” <code>matmul_forward</code> keeps small accumulators in registers and reuses loaded weights across <code>LOOP_UNROLL</code> inner iterations.</li>
<li>Avoid hidden sharing. If you add new parallel loops, ensure each thread writes to unique memory regions. If you must accumulate to the same place, restructure (like “own-your-row”) or use per-thread scratch buffers then reduce.</li>
</ul>
</section>
<section id="micro-walkthrough-why-collapse-helps" class="level4">
<h4 class="anchored" data-anchor-id="micro-walkthrough-why-collapse-helps">Micro-walkthrough: why <code>collapse</code> helps</h4>
<p>Consider <code>softmax_forward</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb98"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#pragma omp parallel for collapse(2)</span></span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> b <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> b <span class="op">&lt;</span> B<span class="op">;</span> b<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> t <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> t <span class="op">&lt;</span> T<span class="op">;</span> t<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>        <span class="co">// 1) find max over V</span></span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a>        <span class="co">// 2) exp/log-sum</span></span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">// 3) normalize first V entries; zero out padded [V..Vp)</span></span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>If <code>B=4</code>, <code>T=64</code>, that’s 256 independent softmaxes. With <code>collapse(2)</code>, OpenMP sees a single loop of 256 iterations to distribute evenly; without <code>collapse</code>, it might chunk by <code>b</code> first (only 4 big chunks), which can load-imbalance.</p>
</section>
<section id="common-pitfalls-and-how-this-code-avoids-them" class="level4">
<h4 class="anchored" data-anchor-id="common-pitfalls-and-how-this-code-avoids-them">Common pitfalls (and how this code avoids them)</h4>
<ul>
<li><p>Race conditions: Two threads writing the same <code>out[i]</code>. <em>Avoided by design:</em> each parallel loop writes distinct slices (e.g., per <code>(b,t)</code> or per <code>o</code>).</p></li>
<li><p>False sharing: Threads write adjacent memory locations on the same cache line. It’s minimized by the large, contiguous slices per thread (entire rows/tiles), but if you extend the code with fine-grained parallelism, keep this in mind.</p></li>
<li><p>Tiny loops: Overhead can exceed work. The file parallelizes only large, hot loops (GEMMs, attention, softmax), not small scalar ops.</p></li>
</ul>
</section>
<section id="try-it-yourself-28" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-28">Try it yourself</h4>
<ol type="1">
<li>Change thread count: Run with <code>OMP_NUM_THREADS=1,2,4,8,…</code> and log step time. Plot speedup vs.&nbsp;threads.</li>
<li>Toggle a pragma: Comment out <code>#pragma omp</code> in <code>matmul_forward</code> only. Measure the slowdown; you’ll see where most time goes.</li>
<li>Experiment with <code>collapse</code>: Remove <code>collapse(2)</code> in <code>softmax_forward</code>. On small <code>B</code>, you’ll likely see worse scaling.</li>
<li>Per-layer profiling: Print elapsed time around <code>matmul_forward</code>, <code>attention_forward</code>, and <code>softmax_forward</code> to see which benefits most on your CPU.</li>
<li>Schedule policy (advanced): Try <code>#pragma omp parallel for schedule(static)</code> vs.&nbsp;<code>dynamic</code> on a heavy loop to see if it changes load balance (defaults are usually fine here).</li>
</ol>
</section>
<section id="the-takeaway-28" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-28">The takeaway</h4>
<p>A handful of well-placed OpenMP pragmas deliver big wins on CPU by parallelizing the most expensive loops (GEMMs, attention, softmax) across cores-without complicating the code. The design ensures each thread works on independent slices, so there’s no locking, no atomics, and very little overhead. If you compile with OpenMP enabled, you get fast, multi-core training; if not, you still have a clean, readable reference implementation.</p>
</section>
</section>
<section id="cpu-memory-footprint-and-performance" class="level3">
<h3 class="anchored" data-anchor-id="cpu-memory-footprint-and-performance">40. CPU Memory Footprint and Performance</h3>
<p>When you train GPT-2 on your CPU using <code>train_gpt2.c</code>, two big questions usually pop up almost immediately: <em>how much memory is this going to take?</em> and <em>how fast will it run?</em> Let’s walk through both of these in a beginner-friendly way, so you understand not just what happens in the code, but why it behaves the way it does.</p>
<section id="memory-where-does-it-all-go" class="level4">
<h4 class="anchored" data-anchor-id="memory-where-does-it-all-go">Memory: where does it all go?</h4>
<p>Imagine training GPT-2 is like cooking a big meal in a small kitchen. You need space for ingredients, bowls for mixing, and counter space for preparing. Memory on your CPU is that kitchen. GPT-2 needs several “bowls” to hold different parts of the computation:</p>
<ol type="1">
<li><p>Parameters (the weights of the model). These are the “fixed recipe” - the actual numbers the network learns. They come from the checkpoint file you load at the start. For GPT-2 124M, this is about 124 million floating-point numbers. Each one takes 4 bytes, so just the weights are around 500 MB.</p></li>
<li><p>Optimizer state (AdamW). Training doesn’t just adjust weights blindly; it keeps track of two extra moving averages for each weight, called <em>m</em> and <em>v</em>. That means for every single parameter, you store three numbers: the weight, m, and v. So memory for optimizer state is often double the size of the weights themselves. For GPT-2 124M, that’s about 1 GB more.</p></li>
<li><p>Gradients. Every time we run a backward pass, we store how much each weight should change. That’s another buffer roughly the same size as the weights - another 500 MB.</p></li>
<li><p>Activations (intermediate results). This is the sneaky one. Every forward pass produces temporary tensors like embeddings, attention maps, and feed-forward outputs. Their size depends on batch size (B) and sequence length (T). If B=4 and T=64, activations are a few hundred MB. If B=32 and T=1024, they can balloon to many gigabytes.</p></li>
</ol>
<p>Here’s a rough mental budget for GPT-2 124M with a small setup (B=4, T=64):</p>
<ul>
<li>Parameters: ~500 MB</li>
<li>Optimizer state: ~1 GB</li>
<li>Gradients: ~500 MB</li>
<li>Activations: ~200–300 MB Total: ~2–2.5 GB</li>
</ul>
<p>Even for the “tiny” GPT-2, you already need a couple gigabytes of RAM to train. On a laptop, this can quickly push you to the limit.</p>
</section>
<section id="performance-where-does-time-go" class="level4">
<h4 class="anchored" data-anchor-id="performance-where-does-time-go">Performance: where does time go?</h4>
<p>Now let’s talk speed. When you run <code>train_gpt2.c</code> on CPU, you’ll see lines like:</p>
<pre><code>step 1: train loss 5.191576 (took 1927.230000 ms)</code></pre>
<p>That “took X ms” tells you how long one step took. Why is it slow? Three main reasons:</p>
<ol type="1">
<li><p>Matrix multiplications (matmuls). These are the heart of neural networks. Every attention head and every MLP layer does them. On CPU, most of your step time is spent here. That’s why the code uses OpenMP pragmas (<code>#pragma omp</code>) to parallelize loops across cores.</p></li>
<li><p>Attention softmax. Attention compares every token in a sequence with every other token. If your sequence length is 1024, that’s over a million comparisons per head per layer. On CPU, this quadratic growth is painful.</p></li>
<li><p>Memory bandwidth. CPUs can only move numbers from RAM to cores so fast. Even if you had infinite FLOPs, you’d still be slowed down by how quickly you can fetch and store these huge tensors.</p></li>
</ol>
</section>
<section id="a-simple-experiment" class="level4">
<h4 class="anchored" data-anchor-id="a-simple-experiment">A simple experiment</h4>
<p>You can see these effects yourself:</p>
<ul>
<li>Change batch size (B). Run with B=1, then with B=8. Notice how memory usage and step time scale up.</li>
<li>Change sequence length (T). Try T=16, then T=256. You’ll see attention costs grow dramatically.</li>
<li>Change threads. Set <code>OMP_NUM_THREADS=1</code> versus <code>OMP_NUM_THREADS=8</code>. With more threads, you’ll often see speedups, but only up to the number of physical cores your CPU has.</li>
</ul>
</section>
<section id="why-this-matters-2" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-2">Why this matters</h4>
<p>For beginners, CPU runs are perfect for learning:</p>
<ul>
<li>You can debug with small batches and short sequences.</li>
<li>You can step into functions with a debugger and watch tensors being created.</li>
<li>You don’t need a GPU just to understand how training works.</li>
</ul>
<p>But when it comes to <em>serious</em> training - larger GPT-2 models or even long sequences - CPU is simply too slow. What takes seconds on GPU may take minutes on CPU. That’s why in practice, people use CPUs for learning and testing, and GPUs for large-scale training.</p>
</section>
<section id="the-takeaway-29" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-29">The takeaway</h4>
<p>Training GPT-2 on CPU is like practicing piano on a small keyboard. It’s slower, limited, and you can’t play the biggest pieces, but it’s great for learning the fundamentals. Memory usage comes from weights, optimizer state, gradients, and activations, and performance is dominated by matmuls and attention. Once you understand where the resources go, you can adjust batch size, sequence length, and threads to find the sweet spot for your machine.</p>
</section>
</section>
</section>
<section id="chapter-5.-training-loop-cpu-path" class="level2">
<h2 class="anchored" data-anchor-id="chapter-5.-training-loop-cpu-path">Chapter 5. Training Loop (CPU Path)</h2>
<section id="backward-pass-walkthrough" class="level3">
<h3 class="anchored" data-anchor-id="backward-pass-walkthrough">41. Backward Pass Walkthrough</h3>
<p>Up until now, we’ve spent all our time looking at the forward pass. That’s the part of the model that takes tokens, pushes them through embeddings, attention, feed-forward layers, and finally produces logits or a loss. For inference, forward pass alone is enough. But if you want to train a model, forward is only half the story.</p>
<p>Training means adjusting the weights of the model so that its predictions become better over time. To do this, we need a way to figure out how wrong each weight was and in what direction it should move to reduce the loss. That’s the job of the backward pass.</p>
<p>The backward pass is also called backpropagation. It’s the algorithm that moves information in reverse through the network: from the loss, back through the final logits, through every transformer block, down to the embeddings. Along the way, it calculates gradients - small numbers that tell us how much each weight contributed to the error.</p>
<section id="the-big-idea-chain-rule-in-action" class="level4">
<h4 class="anchored" data-anchor-id="the-big-idea-chain-rule-in-action">The big idea: chain rule in action</h4>
<p>At the heart of backpropagation is something very familiar from calculus: the chain rule. If the output of the network depends on many functions stacked together (embedding → attention → MLP → … → loss), then the derivative of the loss with respect to an early parameter is a product of partial derivatives through the entire chain.</p>
<p>Instead of writing long formulas, the code in <code>train_gpt2.c</code> simply calls each layer’s backward function in reverse order. The gradient flows backward, step by step, and each layer computes its own contribution using local rules.</p>
<p>Think of it like a relay race, but run backwards: the loss hands a “blame baton” to the output head, which hands it back to the last transformer block, and so on, until it reaches the very first embedding table.</p>
</section>
<section id="walking-through-gpt2_backward" class="level4">
<h4 class="anchored" data-anchor-id="walking-through-gpt2_backward">Walking through <code>gpt2_backward</code></h4>
<p>Here’s a simplified sketch of how the backward function looks in the code (names shortened for readability):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb100"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> gpt2_backward<span class="op">(</span>GPT2 <span class="op">*</span>model<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>tokens<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>labels<span class="op">,</span> <span class="dt">int</span> B<span class="op">,</span> <span class="dt">int</span> T<span class="op">)</span> <span class="op">{</span></span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Step 1: loss gradient</span></span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>    crossentropy_backward<span class="op">(...);</span></span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Step 2: final projection (lm_head)</span></span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a>    matmul_backward<span class="op">(</span>model<span class="op">-&gt;</span>lm_head<span class="op">,</span> <span class="op">...);</span></span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Step 3: transformer blocks in reverse</span></span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> l <span class="op">=</span> model<span class="op">-&gt;</span>config<span class="op">-&gt;</span>n_layer <span class="op">-</span> <span class="dv">1</span><span class="op">;</span> l <span class="op">&gt;=</span> <span class="dv">0</span><span class="op">;</span> l<span class="op">--)</span> <span class="op">{</span></span>
<span id="cb100-10"><a href="#cb100-10" aria-hidden="true" tabindex="-1"></a>        residual_backward<span class="op">(...);</span></span>
<span id="cb100-11"><a href="#cb100-11" aria-hidden="true" tabindex="-1"></a>        layernorm_backward<span class="op">(&amp;</span>model<span class="op">-&gt;</span>blocks<span class="op">[</span>l<span class="op">].</span>ln<span class="op">,</span> <span class="op">...);</span></span>
<span id="cb100-12"><a href="#cb100-12" aria-hidden="true" tabindex="-1"></a>        mlp_backward<span class="op">(&amp;</span>model<span class="op">-&gt;</span>blocks<span class="op">[</span>l<span class="op">].</span>mlp<span class="op">,</span> <span class="op">...);</span></span>
<span id="cb100-13"><a href="#cb100-13" aria-hidden="true" tabindex="-1"></a>        attention_backward<span class="op">(&amp;</span>model<span class="op">-&gt;</span>blocks<span class="op">[</span>l<span class="op">].</span>attn<span class="op">,</span> <span class="op">...);</span></span>
<span id="cb100-14"><a href="#cb100-14" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb100-15"><a href="#cb100-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-16"><a href="#cb100-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Step 4: embeddings</span></span>
<span id="cb100-17"><a href="#cb100-17" aria-hidden="true" tabindex="-1"></a>    embedding_backward<span class="op">(</span>model<span class="op">-&gt;</span>token_embedding<span class="op">,</span> tokens<span class="op">,</span> <span class="op">...);</span></span>
<span id="cb100-18"><a href="#cb100-18" aria-hidden="true" tabindex="-1"></a>    embedding_backward<span class="op">(</span>model<span class="op">-&gt;</span>position_embedding<span class="op">,</span> positions<span class="op">,</span> <span class="op">...);</span></span>
<span id="cb100-19"><a href="#cb100-19" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Let’s unpack this line by line.</p>
</section>
<section id="step-1-starting-from-the-loss" class="level4">
<h4 class="anchored" data-anchor-id="step-1-starting-from-the-loss">Step 1: Starting from the loss</h4>
<p>The journey begins with the loss function. In training, the most common loss is cross-entropy. Its backward function compares the predicted probabilities with the true labels and produces a gradient for the logits.</p>
<ul>
<li>If the model predicted “cat” with high confidence and the true label was “dog,” the gradient will push the logits away from “cat” and toward “dog.”</li>
<li>This gradient is the starting signal that propagates backward through the entire network.</li>
</ul>
</section>
<section id="step-2-back-through-the-output-head" class="level4">
<h4 class="anchored" data-anchor-id="step-2-back-through-the-output-head">Step 2: Back through the output head</h4>
<p>After the loss, the next stop is the final linear projection (<code>lm_head</code>). This is just a big matrix multiply that turns hidden states into vocabulary logits. Its backward function computes two things:</p>
<ol type="1">
<li>The gradient with respect to the weights of <code>lm_head</code>.</li>
<li>The gradient with respect to the hidden states that fed into it.</li>
</ol>
<p>This hidden-state gradient is then passed back to the last transformer block.</p>
</section>
<section id="step-3-transformer-blocks-in-reverse" class="level4">
<h4 class="anchored" data-anchor-id="step-3-transformer-blocks-in-reverse">Step 3: Transformer blocks in reverse</h4>
<p>Here comes the heavy lifting. Each block has multiple components, and their backward functions are called in the exact opposite order of the forward pass.</p>
<ol type="1">
<li>Residual backward: the skip connection splits the gradient into two paths - one flowing back into the transformed output, one flowing back into the original input.</li>
<li>LayerNorm backward: computes gradients with respect to its scale (<code>gamma</code>) and shift (<code>beta</code>), and also passes gradients back to the normalized input.</li>
<li>MLP backward: applies the chain rule to the two linear layers and the GELU activation. The code reuses temporary values from the forward pass (like activations) to make this efficient.</li>
<li>Attention backward: this is the trickiest. It computes gradients for Q, K, and V projections, as well as for the softmaxed attention weights. It has to apply the causal mask again to ensure no illegal gradient flows.</li>
</ol>
<p>This loop continues until all transformer blocks have been processed.</p>
</section>
<section id="step-4-back-to-embeddings" class="level4">
<h4 class="anchored" data-anchor-id="step-4-back-to-embeddings">Step 4: Back to embeddings</h4>
<p>Finally, the gradient reaches the embedding tables. This is where the model first looked up vectors for tokens and positions. Now it calculates how much each embedding contributed to the error. These gradients are added into the embedding matrices, telling the optimizer how to update them.</p>
</section>
<section id="why-this-matters-3" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-3">Why this matters</h4>
<p>The backward pass is what makes learning possible. Without it, the model would forever output the same predictions, never improving. By flowing “blame” backwards, each parameter learns how to nudge itself so that the next forward pass is a little bit better.</p>
<p>Even though the code looks like a lot of function calls, the principle is simple: start from the loss, step backward through each layer, apply the chain rule locally, and collect gradients.</p>
</section>
<section id="try-it-yourself-29" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-29">Try it yourself</h4>
<ol type="1">
<li>Print gradient norms: Add a <code>printf</code> to see the average gradient magnitude at each layer. Notice how they change - sometimes exploding, sometimes vanishing.</li>
<li>Freeze a layer: Comment out <code>mlp_backward</code> for one block and see how the model fails to update properly.</li>
<li>Inspect embeddings: After training a few steps, dump a few rows of the token embedding matrix. You’ll see the numbers changing because of gradient updates.</li>
<li>Tiny dataset experiment: Train on a very small dataset (like a 10-word corpus) and watch how the backward pass quickly pushes embeddings to memorize it.</li>
<li>Check symmetry: Compare the order of calls in <code>gpt2_forward</code> with <code>gpt2_backward</code>. They’re exact opposites - forward builds, backward unbuilds.</li>
</ol>
</section>
<section id="the-takeaway-30" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-30">The takeaway</h4>
<p>Backpropagation is the learning engine of neural networks. In <code>llm.c</code>, the backward pass is written out explicitly, showing how gradients flow from the loss, through the output head, back through every transformer block, and finally into embeddings. Once you understand this flow, you can see how training stitches forward and backward together to slowly shape a random model into a working language model.</p>
</section>
</section>
<section id="skeleton-of-training-loop" class="level3">
<h3 class="anchored" data-anchor-id="skeleton-of-training-loop">42. Skeleton of Training Loop</h3>
<p>The backward pass gave us gradients, but gradients by themselves don’t train a model. Training requires a loop: a cycle that repeatedly runs forward, backward, and update steps over and over until the model improves. This cycle is called the training loop, and it is the heartbeat of every deep learning program. In <code>train_gpt2.c</code>, the loop is written explicitly in C, which means you can see every piece instead of it being hidden away in a framework.</p>
<section id="the-basic-rhythm" class="level4">
<h4 class="anchored" data-anchor-id="the-basic-rhythm">The basic rhythm</h4>
<p>Every training step follows the same rhythm:</p>
<ol type="1">
<li>Get a batch of data (input tokens and their labels).</li>
<li>Run the forward pass to compute predictions and loss.</li>
<li>Run the backward pass to compute gradients.</li>
<li>Update weights using an optimizer like AdamW.</li>
<li>Log progress and, occasionally, validate.</li>
</ol>
<p>This rhythm repeats thousands or millions of times. With each repetition, the weights shift slightly, nudging the model toward lower loss and better predictions.</p>
</section>
<section id="how-the-loop-looks-in-code" class="level4">
<h4 class="anchored" data-anchor-id="how-the-loop-looks-in-code">How the loop looks in code</h4>
<p>Here’s a simplified sketch from <code>train_gpt2.c</code> (with some details omitted for clarity):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb101"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> step <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> step <span class="op">&lt;</span> max_steps<span class="op">;</span> step<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">// 1. Load batch of tokens and labels</span></span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>    dataloader_next_batch<span class="op">(&amp;</span>train_loader<span class="op">,</span> tokens<span class="op">,</span> labels<span class="op">,</span> B<span class="op">,</span> T<span class="op">);</span></span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">// 2. Forward pass</span></span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a>    gpt2_forward<span class="op">(&amp;</span>model<span class="op">,</span> tokens<span class="op">,</span> labels<span class="op">,</span> B<span class="op">,</span> T<span class="op">);</span></span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-8"><a href="#cb101-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">// 3. Zero gradients</span></span>
<span id="cb101-9"><a href="#cb101-9" aria-hidden="true" tabindex="-1"></a>    gpt2_zero_grad<span class="op">(&amp;</span>model<span class="op">);</span></span>
<span id="cb101-10"><a href="#cb101-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-11"><a href="#cb101-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">// 4. Backward pass</span></span>
<span id="cb101-12"><a href="#cb101-12" aria-hidden="true" tabindex="-1"></a>    gpt2_backward<span class="op">(&amp;</span>model<span class="op">,</span> tokens<span class="op">,</span> labels<span class="op">,</span> B<span class="op">,</span> T<span class="op">);</span></span>
<span id="cb101-13"><a href="#cb101-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-14"><a href="#cb101-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">// 5. Optimizer step (AdamW)</span></span>
<span id="cb101-15"><a href="#cb101-15" aria-hidden="true" tabindex="-1"></a>    adamw_update<span class="op">(&amp;</span>opt<span class="op">,</span> <span class="op">&amp;</span>model<span class="op">,</span> learning_rate<span class="op">);</span></span>
<span id="cb101-16"><a href="#cb101-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-17"><a href="#cb101-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">// 6. Logging and validation</span></span>
<span id="cb101-18"><a href="#cb101-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>step <span class="op">%</span> log_interval <span class="op">==</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span> print_loss<span class="op">(</span>step<span class="op">,</span> model<span class="op">.</span>loss<span class="op">);</span> <span class="op">}</span></span>
<span id="cb101-19"><a href="#cb101-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>step <span class="op">%</span> val_interval <span class="op">==</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span> run_validation<span class="op">(...);</span> <span class="op">}</span></span>
<span id="cb101-20"><a href="#cb101-20" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This loop captures the full training lifecycle: data, forward, backward, update, and monitoring.</p>
</section>
<section id="step-1-batching-data" class="level4">
<h4 class="anchored" data-anchor-id="step-1-batching-data">Step 1: Batching data</h4>
<p>The dataloader feeds the loop with small chunks of tokens. Instead of sending the whole dataset at once, it breaks it down into batches of size <code>B</code> (number of sequences per batch) and length <code>T</code> (number of tokens per sequence).</p>
<ul>
<li>Example: if <code>B=4</code> and <code>T=128</code>, each batch is 512 tokens long.</li>
<li>Each sequence has a matching set of labels, which are simply the same tokens shifted one position ahead (so the model always predicts the <em>next</em> word).</li>
</ul>
<p>This batching keeps memory use manageable and helps the model see many small samples instead of a few giant ones.</p>
</section>
<section id="step-2-forward-pass" class="level4">
<h4 class="anchored" data-anchor-id="step-2-forward-pass">Step 2: Forward pass</h4>
<p>The forward pass computes predictions for all tokens in the batch and calculates the loss. This is the “evaluation” step - how well did the model do on this batch? The result is stored in <code>model.loss</code>.</p>
</section>
<section id="step-3-zeroing-gradients" class="level4">
<h4 class="anchored" data-anchor-id="step-3-zeroing-gradients">Step 3: Zeroing gradients</h4>
<p>Before calculating new gradients, the old ones must be cleared out. If you skip this step, gradients from previous batches would accumulate and corrupt the update. In frameworks like PyTorch you’d call <code>optimizer.zero_grad()</code>. Here it’s a plain C function:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb102"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>gpt2_zero_grad<span class="op">(&amp;</span>model<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>It walks through all parameters and resets their gradient buffers to zero.</p>
</section>
<section id="step-4-backward-pass" class="level4">
<h4 class="anchored" data-anchor-id="step-4-backward-pass">Step 4: Backward pass</h4>
<p>Now the backward function is called. It pushes gradients back through the network, computing how each weight influenced the error. At this point, every parameter has an associated gradient stored in memory.</p>
</section>
<section id="step-5-optimizer-update" class="level4">
<h4 class="anchored" data-anchor-id="step-5-optimizer-update">Step 5: Optimizer update</h4>
<p>With gradients ready, the optimizer (AdamW in this code) updates each parameter:</p>
<pre><code>new_weight = old_weight - learning_rate * gradient (with AdamW tweaks)</code></pre>
<p>This step is what actually changes the model. Without it, the model would never learn - the forward and backward passes would just repeat the same results forever.</p>
</section>
<section id="step-6-logging-and-validation" class="level4">
<h4 class="anchored" data-anchor-id="step-6-logging-and-validation">Step 6: Logging and validation</h4>
<p>Every few steps, the loop prints out useful numbers: current step, loss, time taken, and sometimes throughput (tokens per second). This feedback is important to check whether training is actually working.</p>
<p>Every few hundred or thousand steps, the loop also runs a validation pass on held-out data. This tells you whether the model is just memorizing training data or genuinely learning patterns that generalize.</p>
</section>
<section id="why-the-training-loop-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-the-training-loop-matters">Why the training loop matters</h4>
<p>The training loop is deceptively simple, but it is the engine room of machine learning. Every improvement in model performance happens because this loop runs many times. By writing it explicitly in C, <code>llm.c</code> exposes details that high-level frameworks usually hide: zeroing gradients, passing pointers to arrays, calling backward and optimizer functions directly.</p>
<p>This makes it a perfect learning tool. You can see clearly:</p>
<ul>
<li>Where the data comes in,</li>
<li>Where predictions are made,</li>
<li>Where gradients are calculated,</li>
<li>And where learning actually happens.</li>
</ul>
</section>
<section id="try-it-yourself-30" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-30">Try it yourself</h4>
<ol type="1">
<li>Print the loss curve: Add a <code>printf</code> inside the loop and write the loss to a file. Plot it - you should see it decrease over time.</li>
<li>Change batch size: Set <code>B=1</code> vs.&nbsp;<code>B=8</code>. Notice how the loop becomes noisier with smaller batches but smoother with larger ones.</li>
<li>Skip backward: Comment out <code>gpt2_backward</code> and optimizer update. Run the loop. You’ll see the loss never decreases - a clear demonstration that forward alone doesn’t train.</li>
<li>Experiment with steps: Try <code>max_steps=10</code> vs.&nbsp;<code>max_steps=1000</code>. Short runs show no improvement; longer runs start to reduce the loss.</li>
<li>Slow it down: Insert a <code>sleep(1);</code> inside the loop. This makes the rhythm visible step by step, so you can literally watch the model “breathe” as it trains.</li>
</ol>
</section>
<section id="the-takeaway-31" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-31">The takeaway</h4>
<p>The skeleton of the training loop is the core cycle of learning. It feeds data into the model, computes predictions, finds errors, sends them backward, updates weights, and logs progress. Everything else - optimizers, schedulers, distributed training, mixed precision - is just an enhancement of this basic loop. If you understand how this loop works in <code>llm.c</code>, you understand the beating heart of deep learning training.</p>
</section>
</section>
<section id="adamw-implementation-in-c" class="level3">
<h3 class="anchored" data-anchor-id="adamw-implementation-in-c">43. AdamW Implementation in C</h3>
<p>Training a neural network is about adjusting millions of parameters so that the model gradually becomes better at predicting text. The function <code>gpt2_update</code> in <code>train_gpt2.c</code> is responsible for this adjustment. It implements the AdamW optimizer, one of the most widely used algorithms in deep learning. Let’s walk through both the theory and the actual implementation.</p>
<section id="from-gradient-descent-to-adamw" class="level4">
<h4 class="anchored" data-anchor-id="from-gradient-descent-to-adamw">From Gradient Descent to AdamW</h4>
<p>The most basic optimizer is gradient descent:</p>
<pre class="text"><code>new_param = old_param - learning_rate * gradient</code></pre>
<p>This approach works, but it has weaknesses. The step size (learning rate) must be tuned carefully: too small and training is slow, too large and training diverges. Moreover, all parameters use the same step size, even though some may need gentler updates.</p>
<p>AdamW improves this by keeping track of moving averages of gradients and scaling updates adaptively. It also introduces weight decay, which prevents parameters from growing too large and helps regularize the model.</p>
</section>
<section id="how-adamw-works" class="level4">
<h4 class="anchored" data-anchor-id="how-adamw-works">How AdamW Works</h4>
<p>AdamW combines several techniques into a single update rule. First, it uses momentum: instead of relying only on the current gradient, it averages recent gradients. This smooths noisy updates. Second, it maintains a running estimate of the squared gradient, which scales down steps in directions where gradients are consistently large. These are sometimes called the first and second moments.</p>
<p>Since both running averages start at zero, the algorithm applies bias correction during the first few steps. Without this, the early updates would be too small. Finally, AdamW applies weight decay directly in the update, shrinking parameter values slightly each step.</p>
<p>Putting it together, each parameter update looks like this:</p>
<pre class="text"><code>m_t = β1 * m_(t-1) + (1 - β1) * g_t
v_t = β2 * v_(t-1) + (1 - β2) * g_t²
m̂_t = m_t / (1 - β1^t)
v̂_t = v_t / (1 - β2^t)

new_param = old_param - lr * ( m̂_t / (sqrt(v̂_t) + ε) + λ * old_param )</code></pre>
<p>Here <code>m</code> is momentum, <code>v</code> is variance, <code>lr</code> is learning rate, <code>ε</code> is a small constant for stability, and <code>λ</code> is the weight decay factor.</p>
</section>
<section id="the-implementation-in-train_gpt2.c" class="level4">
<h4 class="anchored" data-anchor-id="the-implementation-in-train_gpt2.c">The Implementation in <code>train_gpt2.c</code></h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb106"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> gpt2_update<span class="op">(</span>GPT2 <span class="op">*</span>model<span class="op">,</span> <span class="dt">float</span> learning_rate<span class="op">,</span> <span class="dt">float</span> beta1<span class="op">,</span> <span class="dt">float</span> beta2<span class="op">,</span></span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>                 <span class="dt">float</span> eps<span class="op">,</span> <span class="dt">float</span> weight_decay<span class="op">,</span> <span class="dt">int</span> t<span class="op">)</span> <span class="op">{</span></span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>model<span class="op">-&gt;</span>m_memory <span class="op">==</span> NULL<span class="op">)</span> <span class="op">{</span></span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a>        model<span class="op">-&gt;</span>m_memory <span class="op">=</span> <span class="op">(</span><span class="dt">float</span><span class="op">*)</span>calloc<span class="op">(</span>model<span class="op">-&gt;</span>num_parameters<span class="op">,</span> <span class="kw">sizeof</span><span class="op">(</span><span class="dt">float</span><span class="op">));</span></span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a>        model<span class="op">-&gt;</span>v_memory <span class="op">=</span> <span class="op">(</span><span class="dt">float</span><span class="op">*)</span>calloc<span class="op">(</span>model<span class="op">-&gt;</span>num_parameters<span class="op">,</span> <span class="kw">sizeof</span><span class="op">(</span><span class="dt">float</span><span class="op">));</span></span>
<span id="cb106-6"><a href="#cb106-6" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb106-7"><a href="#cb106-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-8"><a href="#cb106-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">size_t</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> model<span class="op">-&gt;</span>num_parameters<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb106-9"><a href="#cb106-9" aria-hidden="true" tabindex="-1"></a>        <span class="dt">float</span> param <span class="op">=</span> model<span class="op">-&gt;</span>params_memory<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb106-10"><a href="#cb106-10" aria-hidden="true" tabindex="-1"></a>        <span class="dt">float</span> grad <span class="op">=</span> model<span class="op">-&gt;</span>grads_memory<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb106-11"><a href="#cb106-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-12"><a href="#cb106-12" aria-hidden="true" tabindex="-1"></a>        <span class="dt">float</span> m <span class="op">=</span> beta1 <span class="op">*</span> model<span class="op">-&gt;</span>m_memory<span class="op">[</span>i<span class="op">]</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1.0</span><span class="bu">f</span> <span class="op">-</span> beta1<span class="op">)</span> <span class="op">*</span> grad<span class="op">;</span></span>
<span id="cb106-13"><a href="#cb106-13" aria-hidden="true" tabindex="-1"></a>        <span class="dt">float</span> v <span class="op">=</span> beta2 <span class="op">*</span> model<span class="op">-&gt;</span>v_memory<span class="op">[</span>i<span class="op">]</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1.0</span><span class="bu">f</span> <span class="op">-</span> beta2<span class="op">)</span> <span class="op">*</span> grad <span class="op">*</span> grad<span class="op">;</span></span>
<span id="cb106-14"><a href="#cb106-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-15"><a href="#cb106-15" aria-hidden="true" tabindex="-1"></a>        <span class="dt">float</span> m_hat <span class="op">=</span> m <span class="op">/</span> <span class="op">(</span><span class="fl">1.0</span><span class="bu">f</span> <span class="op">-</span> powf<span class="op">(</span>beta1<span class="op">,</span> t<span class="op">));</span></span>
<span id="cb106-16"><a href="#cb106-16" aria-hidden="true" tabindex="-1"></a>        <span class="dt">float</span> v_hat <span class="op">=</span> v <span class="op">/</span> <span class="op">(</span><span class="fl">1.0</span><span class="bu">f</span> <span class="op">-</span> powf<span class="op">(</span>beta2<span class="op">,</span> t<span class="op">));</span></span>
<span id="cb106-17"><a href="#cb106-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-18"><a href="#cb106-18" aria-hidden="true" tabindex="-1"></a>        model<span class="op">-&gt;</span>m_memory<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> m<span class="op">;</span></span>
<span id="cb106-19"><a href="#cb106-19" aria-hidden="true" tabindex="-1"></a>        model<span class="op">-&gt;</span>v_memory<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> v<span class="op">;</span></span>
<span id="cb106-20"><a href="#cb106-20" aria-hidden="true" tabindex="-1"></a>        model<span class="op">-&gt;</span>params_memory<span class="op">[</span>i<span class="op">]</span> <span class="op">-=</span> learning_rate <span class="op">*</span></span>
<span id="cb106-21"><a href="#cb106-21" aria-hidden="true" tabindex="-1"></a>            <span class="op">(</span>m_hat <span class="op">/</span> <span class="op">(</span>sqrtf<span class="op">(</span>v_hat<span class="op">)</span> <span class="op">+</span> eps<span class="op">)</span> <span class="op">+</span> weight_decay <span class="op">*</span> param<span class="op">);</span></span>
<span id="cb106-22"><a href="#cb106-22" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb106-23"><a href="#cb106-23" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The first <code>if</code> block allocates memory for the moving averages <code>m</code> and <code>v</code> the first time the optimizer runs. Then, for each parameter, the code computes the new averages, applies bias correction, and finally updates the parameter with the AdamW formula.</p>
</section>
<section id="example-walkthrough-3" class="level4">
<h4 class="anchored" data-anchor-id="example-walkthrough-3">Example Walkthrough</h4>
<p>Suppose we have a parameter <code>w = 0.5</code> with gradient <code>g = 0.2</code> on the first training step. Using β1 = 0.9 and β2 = 0.999:</p>
<ul>
<li><p>Momentum:</p>
<pre><code>m = 0.9 * 0 + 0.1 * 0.2 = 0.02</code></pre></li>
<li><p>Variance:</p>
<pre><code>v = 0.999 * 0 + 0.001 * 0.04 = 0.00004</code></pre></li>
<li><p>Bias correction:</p>
<pre><code>m̂ = 0.02 / (1 - 0.9) = 0.2
v̂ = 0.00004 / (1 - 0.999) = 0.04</code></pre></li>
<li><p>Final update (lr = 0.001, weight_decay = 0.01):</p>
<pre><code>update = 0.001 * (0.2 / sqrt(0.04) + 0.01 * 0.5)
       = 0.001 * (1.0 + 0.005)
       = 0.001005</code></pre></li>
</ul>
<p>So the parameter becomes <code>w = 0.498995</code>.</p>
</section>
<section id="intuition-1" class="level4">
<h4 class="anchored" data-anchor-id="intuition-1">Intuition</h4>
<p>Think of a ball rolling down a slope. The gradient is the slope itself. Momentum makes the ball keep rolling even if the slope flattens briefly. The variance term makes the ball slow down on rocky ground where the slope changes rapidly. Bias correction ensures the ball doesn’t move too timidly at the start. Weight decay adds friction so the ball doesn’t roll out of control.</p>
</section>
<section id="why-it-matters-26" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-26">Why It Matters</h4>
<p>Optimizers are the difference between a model that trains smoothly and one that diverges or gets stuck. AdamW became popular because it combines stability with efficiency. It automatically adapts to each parameter’s scale, reduces the need for manual learning rate tuning, and includes weight decay in a principled way. For GPT-style models with hundreds of millions of parameters, these qualities make training feasible.</p>
</section>
<section id="try-it-yourself-31" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-31">Try It Yourself</h4>
<ol type="1">
<li>Change the learning rate from <code>0.001</code> to <code>0.01</code> in the code and see how quickly the model diverges.</li>
<li>Set <code>weight_decay = 0</code> and compare validation loss after a few epochs. The model might overfit more quickly.</li>
<li>Print out the first 10 values of <code>m_memory</code> and <code>v_memory</code> during training to watch how they evolve over steps.</li>
<li>Replace AdamW with plain SGD (just <code>param -= lr * grad</code>) and compare training speed and stability.</li>
<li>Experiment with β1 = 0 (no momentum) or β2 = 0 (no variance smoothing) and see how noisy updates become.</li>
</ol>
</section>
<section id="the-takeaway-32" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-32">The Takeaway</h4>
<p>AdamW provides a balance of speed, stability, and generalization. In practice, it allows models like GPT-2 to train much more reliably than with vanilla gradient descent. The C implementation in <code>llm.c</code> demonstrates that beneath the math, it’s just a simple loop applying a few arithmetic operations for each parameter.</p>
</section>
</section>
<section id="gradient-accumulation-and-micro-batching" class="level3">
<h3 class="anchored" data-anchor-id="gradient-accumulation-and-micro-batching">44. Gradient Accumulation and Micro-Batching</h3>
<p>Modern language models are enormous, and so are the batches of text we would like to feed them during training. But real hardware has limits: a single GPU or CPU may not have enough memory to process a large batch in one go. To solve this, training code often uses gradient accumulation and micro-batching. Both ideas allow us to simulate training with larger batches without requiring more memory than our hardware can provide.</p>
<section id="what-problem-are-we-solving" class="level4">
<h4 class="anchored" data-anchor-id="what-problem-are-we-solving">What Problem Are We Solving?</h4>
<p>When you process a batch of data, you run forward and backward passes to calculate gradients. If your batch size is very large, you get smoother gradients (less noisy), which often helps the model converge better. But large batches may not fit in memory.</p>
<p>Imagine trying to train with a batch of 1024 sequences on a GPU that can only handle 128 sequences at once. Without tricks, you would be forced to use the smaller batch size and give up the benefits of larger batches. Gradient accumulation fixes this problem by letting you split the big batch into smaller micro-batches, process them one at a time, and accumulate the results as if you had processed the big batch all at once.</p>
</section>
<section id="how-it-works-in-practice" class="level4">
<h4 class="anchored" data-anchor-id="how-it-works-in-practice">How It Works in Practice</h4>
<p>Let’s say we want an effective batch size of 1024, but our hardware only supports 128. We split the big batch into 8 micro-batches of 128 each:</p>
<ol type="1">
<li>Run forward + backward on micro-batch 1, store the gradients.</li>
<li>Run forward + backward on micro-batch 2, add its gradients to the stored ones.</li>
<li>Repeat until all 8 micro-batches are processed.</li>
<li>Once gradients for all 8 are accumulated, perform the optimizer update.</li>
</ol>
<p>The important part is step 4: we only update the parameters once per effective batch, not after each micro-batch. This preserves the effect of training with a large batch.</p>
</section>
<section id="pseudocode-example" class="level4">
<h4 class="anchored" data-anchor-id="pseudocode-example">Pseudocode Example</h4>
<p>Here’s how this might look in simplified pseudocode:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb111"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> accumulation_steps <span class="op">=</span> <span class="dv">8</span><span class="op">;</span></span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> step <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> step <span class="op">&lt;</span> total_steps<span class="op">;</span> step<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a>    zero_grad<span class="op">(&amp;</span>model<span class="op">);</span></span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> accumulation_steps<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a>        dataloader_next_batch<span class="op">(&amp;</span>train_loader<span class="op">,</span> tokens<span class="op">,</span> labels<span class="op">,</span> B<span class="op">,</span> T<span class="op">);</span></span>
<span id="cb111-6"><a href="#cb111-6" aria-hidden="true" tabindex="-1"></a>        forward<span class="op">(&amp;</span>model<span class="op">,</span> tokens<span class="op">,</span> labels<span class="op">,</span> B<span class="op">,</span> T<span class="op">);</span></span>
<span id="cb111-7"><a href="#cb111-7" aria-hidden="true" tabindex="-1"></a>        backward<span class="op">(&amp;</span>model<span class="op">,</span> tokens<span class="op">,</span> labels<span class="op">,</span> B<span class="op">,</span> T<span class="op">);</span></span>
<span id="cb111-8"><a href="#cb111-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">// do NOT call optimizer update yet</span></span>
<span id="cb111-9"><a href="#cb111-9" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb111-10"><a href="#cb111-10" aria-hidden="true" tabindex="-1"></a>    adamw_update<span class="op">(&amp;</span>model<span class="op">,</span> lr<span class="op">,</span> beta1<span class="op">,</span> beta2<span class="op">,</span> eps<span class="op">,</span> weight_decay<span class="op">,</span> step<span class="op">+</span><span class="dv">1</span><span class="op">);</span></span>
<span id="cb111-11"><a href="#cb111-11" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Notice how the optimizer only runs once per outer loop iteration, even though gradients were accumulated across multiple micro-batches.</p>
</section>
<section id="why-gradient-accumulation-helps" class="level4">
<h4 class="anchored" data-anchor-id="why-gradient-accumulation-helps">Why Gradient Accumulation Helps</h4>
<ul>
<li>Memory efficiency: You can train with larger effective batch sizes without needing more hardware.</li>
<li>Training stability: Larger batches reduce the variance of gradients, making training less noisy.</li>
<li>Flexibility: You can scale effective batch size up or down depending on your needs without changing hardware.</li>
</ul>
</section>
<section id="micro-batching-vs.-accumulation" class="level4">
<h4 class="anchored" data-anchor-id="micro-batching-vs.-accumulation">Micro-Batching vs.&nbsp;Accumulation</h4>
<p>Micro-batching refers to the act of splitting a batch into smaller parts. Gradient accumulation is what you do after micro-batching: sum up the gradients across those parts. Together, they allow you to simulate training with any batch size you want, within memory constraints.</p>
</section>
<section id="why-it-matters-27" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-27">Why It Matters</h4>
<p>The quality of training often depends on batch size. If you can’t fit a large batch directly, gradient accumulation ensures you still reap the benefits. It’s one of those “engineering hacks” that makes training state-of-the-art models possible on limited resources.</p>
</section>
<section id="try-it-yourself-32" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-32">Try It Yourself</h4>
<ol type="1">
<li>Run training with batch size = 16 and no accumulation. Watch how noisy the loss curve looks.</li>
<li>Now set micro-batch size = 4 and accumulation_steps = 4. This simulates batch size = 16, but in smaller chunks. Compare the loss curve.</li>
<li>Increase accumulation_steps to simulate batch size = 32. Observe if training becomes smoother.</li>
<li>Experiment with turning accumulation off and on while keeping the same effective batch size. Notice how optimizer updates per epoch differ.</li>
<li>Print out how many times the optimizer is called. With accumulation, it should be fewer than the number of micro-batches.</li>
</ol>
</section>
<section id="the-takeaway-33" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-33">The Takeaway</h4>
<p>Gradient accumulation and micro-batching are techniques that let you train with large effective batch sizes while staying within the limits of your hardware. They preserve the benefits of large batches-stability and smoother gradients-without demanding extra memory. In <code>llm.c</code>, the simplicity of the training loop means you can clearly see where accumulation fits: gradients are summed across micro-batches, and only then does the optimizer step in. This is a small adjustment in code but a huge enabler in practice.</p>
</section>
</section>
<section id="logging-and-progress-reporting" class="level3">
<h3 class="anchored" data-anchor-id="logging-and-progress-reporting">45. Logging and Progress Reporting</h3>
<p>Every training loop needs a way to show what’s happening under the hood. Without logs, you wouldn’t know if the model is improving, if the code is running efficiently, or if something has silently gone wrong. In <code>train_gpt2.c</code>, logging is intentionally minimal but highly informative: each training step prints the step number, the current training loss, and how long that step took to run.</p>
<section id="the-real-code-for-logging" class="level4">
<h4 class="anchored" data-anchor-id="the-real-code-for-logging">The Real Code for Logging</h4>
<p>Here’s the relevant snippet from <code>train_gpt2.c</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb112"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="co">// do a training step</span></span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a>clock_gettime<span class="op">(</span>CLOCK_MONOTONIC<span class="op">,</span> <span class="op">&amp;</span>start<span class="op">);</span></span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a>dataloader_next_batch<span class="op">(&amp;</span>train_loader<span class="op">);</span></span>
<span id="cb112-4"><a href="#cb112-4" aria-hidden="true" tabindex="-1"></a>gpt2_forward<span class="op">(&amp;</span>model<span class="op">,</span> train_loader<span class="op">.</span>inputs<span class="op">,</span> train_loader<span class="op">.</span>targets<span class="op">,</span> B<span class="op">,</span> T<span class="op">);</span></span>
<span id="cb112-5"><a href="#cb112-5" aria-hidden="true" tabindex="-1"></a>gpt2_zero_grad<span class="op">(&amp;</span>model<span class="op">);</span></span>
<span id="cb112-6"><a href="#cb112-6" aria-hidden="true" tabindex="-1"></a>gpt2_backward<span class="op">(&amp;</span>model<span class="op">);</span></span>
<span id="cb112-7"><a href="#cb112-7" aria-hidden="true" tabindex="-1"></a>gpt2_update<span class="op">(&amp;</span>model<span class="op">,</span> <span class="fl">1e-4</span><span class="bu">f</span><span class="op">,</span> <span class="fl">0.9</span><span class="bu">f</span><span class="op">,</span> <span class="fl">0.999</span><span class="bu">f</span><span class="op">,</span> <span class="fl">1e-8</span><span class="bu">f</span><span class="op">,</span> <span class="fl">0.0</span><span class="bu">f</span><span class="op">,</span> step<span class="op">+</span><span class="dv">1</span><span class="op">);</span></span>
<span id="cb112-8"><a href="#cb112-8" aria-hidden="true" tabindex="-1"></a>clock_gettime<span class="op">(</span>CLOCK_MONOTONIC<span class="op">,</span> <span class="op">&amp;</span>end<span class="op">);</span></span>
<span id="cb112-9"><a href="#cb112-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-10"><a href="#cb112-10" aria-hidden="true" tabindex="-1"></a><span class="dt">double</span> time_elapsed_s <span class="op">=</span> <span class="op">(</span>end<span class="op">.</span>tv_sec <span class="op">-</span> start<span class="op">.</span>tv_sec<span class="op">)</span> <span class="op">+</span></span>
<span id="cb112-11"><a href="#cb112-11" aria-hidden="true" tabindex="-1"></a>                        <span class="op">(</span>end<span class="op">.</span>tv_nsec <span class="op">-</span> start<span class="op">.</span>tv_nsec<span class="op">)</span> <span class="op">/</span> <span class="fl">1e9</span><span class="op">;</span></span>
<span id="cb112-12"><a href="#cb112-12" aria-hidden="true" tabindex="-1"></a>printf<span class="op">(</span><span class="st">"step </span><span class="sc">%d</span><span class="st">: train loss </span><span class="sc">%f</span><span class="st"> (took </span><span class="sc">%f</span><span class="st"> ms)</span><span class="sc">\n</span><span class="st">"</span><span class="op">,</span></span>
<span id="cb112-13"><a href="#cb112-13" aria-hidden="true" tabindex="-1"></a>       step<span class="op">,</span> model<span class="op">.</span>mean_loss<span class="op">,</span> time_elapsed_s <span class="op">*</span> <span class="dv">1000</span><span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This small block accomplishes two things:</p>
<ol type="1">
<li>It measures how long the training step took using <code>clock_gettime</code>.</li>
<li>It reports the step number, the loss, and the elapsed time in milliseconds.</li>
</ol>
<p>The output looks like this when training:</p>
<pre><code>step 0: train loss 4.677779 (took 1987.546 ms)
step 1: train loss 5.191576 (took 1927.230 ms)
step 2: train loss 4.438685 (took 1902.987 ms)</code></pre>
</section>
<section id="understanding-whats-reported" class="level4">
<h4 class="anchored" data-anchor-id="understanding-whats-reported">Understanding What’s Reported</h4>
<ul>
<li><p>Step number (<code>step</code>) Tells you where you are in training. Since deep learning often runs for thousands of steps, this acts like a progress bar.</p></li>
<li><p>Training loss (<code>model.mean_loss</code>) Shows how well the model is fitting the training batch. A lower value generally means better predictions. Watching this number decrease over time is the main signal that learning is happening.</p></li>
<li><p>Step duration (<code>time_elapsed_s * 1000</code>) Measures performance. If one step takes 2000 ms, then 5000 steps would take about 3 hours. Monitoring this helps you estimate total training time and spot performance regressions (e.g., if a new change suddenly doubles the step time).</p></li>
</ul>
</section>
<section id="why-it-matters-28" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-28">Why It Matters</h4>
<p>Logs are your window into the training process. If the loss goes down smoothly, training is healthy. If it suddenly spikes or stays flat, something is wrong-maybe the learning rate is too high, or the model has run out of capacity. Timing information also matters: you need to know whether the code is running efficiently or wasting cycles.</p>
</section>
<section id="try-it-yourself-33" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-33">Try It Yourself</h4>
<ol type="1">
<li><p>Change the learning rate from <code>1e-4</code> to <code>1e-2</code> and watch how the loss behaves. If it jumps or becomes unstable, you’ll see it directly in the logs.</p></li>
<li><p>Add validation logging by running the model on a held-out dataset every 100 steps and printing <code>val_loss</code>. Compare it to <code>train_loss</code>.</p></li>
<li><p>Record the log output to a file with:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb114"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./train_gpt2</span> <span class="op">&gt;</span> log.txt</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then plot <code>train_loss</code> over steps in Python or Excel to visualize the curve.</p></li>
<li><p>Add throughput reporting: divide the batch size times sequence length (<code>B*T</code>) by the step time to print tokens per second. This gives a clearer sense of efficiency.</p></li>
<li><p>Try disabling <code>clock_gettime</code> and only print loss. Notice how much harder it becomes to judge performance without timing information.</p></li>
</ol>
</section>
<section id="the-takeaway-34" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-34">The Takeaway</h4>
<p>Even the simplest logs can tell you a lot. With just a single line-step, loss, and duration-you know how fast training is, whether it’s converging, and how long it will take. In larger frameworks, this kind of information is often hidden behind dashboards and monitoring tools, but the core idea is the same: training is only useful if you can see and interpret its progress.</p>
</section>
</section>
<section id="validation-runs-in-the-training-loop" class="level3">
<h3 class="anchored" data-anchor-id="validation-runs-in-the-training-loop">46. Validation Runs in the Training Loop</h3>
<p>When you train a model, it is not enough to look only at how well it does on the training data. The real test is whether the model has learned patterns that apply to new, unseen data. This is where validation comes in. Validation is like a quiz the model takes from time to time during training. It does not count toward learning-it is just a check to see how much the model has really understood.</p>
<p>In <code>train_gpt2.c</code>, validation is built right into the training loop. Every so often, instead of updating weights, the program pauses and runs the model on a set of tokens it has never trained on. It then prints out the average validation loss. This number tells you if the model is actually generalizing, not just memorizing.</p>
<section id="how-the-validation-code-looks" class="level4">
<h4 class="anchored" data-anchor-id="how-the-validation-code-looks">How the validation code looks</h4>
<p>Here is the actual block of code that handles validation:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb115"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="op">(</span>step <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>    <span class="dt">float</span> val_loss <span class="op">=</span> <span class="fl">0.0</span><span class="bu">f</span><span class="op">;</span></span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a>    dataloader_reset<span class="op">(&amp;</span>val_loader<span class="op">);</span></span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> val_num_batches<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a>        dataloader_next_batch<span class="op">(&amp;</span>val_loader<span class="op">);</span></span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true" tabindex="-1"></a>        gpt2_forward<span class="op">(&amp;</span>model<span class="op">,</span> val_loader<span class="op">.</span>inputs<span class="op">,</span> val_loader<span class="op">.</span>targets<span class="op">,</span> B<span class="op">,</span> T<span class="op">);</span></span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true" tabindex="-1"></a>        val_loss <span class="op">+=</span> model<span class="op">.</span>mean_loss<span class="op">;</span></span>
<span id="cb115-8"><a href="#cb115-8" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb115-9"><a href="#cb115-9" aria-hidden="true" tabindex="-1"></a>    val_loss <span class="op">/=</span> val_num_batches<span class="op">;</span></span>
<span id="cb115-10"><a href="#cb115-10" aria-hidden="true" tabindex="-1"></a>    printf<span class="op">(</span><span class="st">"val loss </span><span class="sc">%f\n</span><span class="st">"</span><span class="op">,</span> val_loss<span class="op">);</span></span>
<span id="cb115-11"><a href="#cb115-11" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>At first glance, this might look like just a few lines of C code. But behind it are several important ideas about how machine learning models are tested while they learn. Let’s go through this step by step.</p>
</section>
<section id="step-by-step-explanation" class="level4">
<h4 class="anchored" data-anchor-id="step-by-step-explanation">Step-by-step explanation</h4>
<p>The first line checks whether it is time to run validation:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb116"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="op">(</span>step <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This means that validation happens every 10 steps. The <code>%</code> operator is “modulo,” which returns the remainder of a division. If the step number is divisible by 10 (like 0, 10, 20, 30), then the block runs. By spacing it out this way, validation does not slow training too much but still gives you regular updates.</p>
<p>Next, the code sets up a place to store the running total of the validation loss:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb117"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a><span class="dt">float</span> val_loss <span class="op">=</span> <span class="fl">0.0</span><span class="bu">f</span><span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then it resets the validation dataloader:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb118"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a>dataloader_reset<span class="op">(&amp;</span>val_loader<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This makes sure the validation dataset starts from the beginning each time. That way, the results are consistent-you’re always checking the model on the same set of text, rather than starting from a random place.</p>
<p>Now comes the loop over validation batches:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb119"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> val_num_batches<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a>    dataloader_next_batch<span class="op">(&amp;</span>val_loader<span class="op">);</span></span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true" tabindex="-1"></a>    gpt2_forward<span class="op">(&amp;</span>model<span class="op">,</span> val_loader<span class="op">.</span>inputs<span class="op">,</span> val_loader<span class="op">.</span>targets<span class="op">,</span> B<span class="op">,</span> T<span class="op">);</span></span>
<span id="cb119-4"><a href="#cb119-4" aria-hidden="true" tabindex="-1"></a>    val_loss <span class="op">+=</span> model<span class="op">.</span>mean_loss<span class="op">;</span></span>
<span id="cb119-5"><a href="#cb119-5" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Here’s what’s happening inside:</p>
<ul>
<li><code>dataloader_next_batch</code> fetches the next chunk of tokens and labels from the validation set.</li>
<li><code>gpt2_forward</code> runs the model forward on those tokens, predicting the next word for each one, and computes the loss against the true labels.</li>
<li>The loss from that batch is added to <code>val_loss</code>.</li>
</ul>
<p>Notice that there is no call to <code>gpt2_zero_grad</code>, no <code>gpt2_backward</code>, and no <code>gpt2_update</code>. That is because validation does not train the model. It only measures performance.</p>
<p>Finally, the program averages the loss across the number of batches:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb120"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a>val_loss <span class="op">/=</span> val_num_batches<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>And prints the result:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb121"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a>printf<span class="op">(</span><span class="st">"val loss </span><span class="sc">%f\n</span><span class="st">"</span><span class="op">,</span> val_loss<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This gives you a single number that summarizes how well the model is performing on unseen data at this point in training.</p>
</section>
<section id="how-to-read-validation-loss" class="level4">
<h4 class="anchored" data-anchor-id="how-to-read-validation-loss">How to read validation loss</h4>
<p>Imagine you are training and see logs like this:</p>
<pre><code>step 0: train loss 4.677779 (took 1987.546 ms)
val loss 4.901234
step 1: train loss 5.191576 (took 1927.230 ms)
step 2: train loss 4.438685 (took 1902.987 ms)
...
step 10: train loss 3.912342 (took 1890.321 ms)
val loss 4.100321</code></pre>
<p>The training loss is printed every step, while the validation loss appears every 10 steps. If both numbers are going down, that is a sign the model is genuinely learning. If training loss drops but validation loss stays the same or starts going up, the model is probably memorizing the training set-this is called overfitting.</p>
</section>
<section id="why-validation-is-important" class="level4">
<h4 class="anchored" data-anchor-id="why-validation-is-important">Why validation is important</h4>
<p>Without validation, you could be tricked into thinking the model is improving just because the training loss is going down. But that might only mean it has memorized the training data. Validation checks prevent this by showing you whether the model can handle data it has not seen before. It is like a student practicing with old exam papers (training) versus being tested with new problems (validation).</p>
</section>
<section id="small-details-that-matter" class="level4">
<h4 class="anchored" data-anchor-id="small-details-that-matter">Small details that matter</h4>
<p>The code averages validation loss over <code>val_num_batches</code>, which is set earlier to 5. That means it only checks 5 batches, not the entire validation dataset. This is a shortcut-it makes validation much faster, at the cost of some accuracy in the measurement. But for training feedback, this is usually enough.</p>
<p>The batch size <code>B</code> and sequence length <code>T</code> for validation are the same as training. This keeps the loss comparable between training and validation.</p>
</section>
<section id="try-it-yourself-34" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-34">Try it yourself</h4>
<p>You can experiment with the validation process to understand it better. Here are some ideas:</p>
<ol type="1">
<li>Change the frequency from every 10 steps to every 5 or even every step. You’ll see more validation updates, but training will slow down.</li>
<li>Increase <code>val_num_batches</code> to 20. The validation loss will become less noisy, but each check will take longer.</li>
<li>Comment out the validation block and train again. Notice how you lose a sense of whether the model is really generalizing.</li>
<li>Save validation loss values to a file and plot them. Compare the curve against the training loss curve. You’ll see how they move together or diverge.</li>
<li>Try using a very small validation dataset. Watch how the loss jumps around more compared to a larger, more stable dataset.</li>
</ol>
</section>
<section id="the-takeaway-35" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-35">The takeaway</h4>
<p>Validation runs are short forward-only tests that give you confidence the model is learning patterns that apply to new text. They are easy to implement-a few lines of code in <code>train_gpt2.c</code>-but they are one of the most important tools for monitoring training. By checking validation loss regularly, you make sure your model is not just memorizing but actually becoming better at language modeling.</p>
</section>
</section>
<section id="checkpointing-parameters-and-optimizer-state" class="level3">
<h3 class="anchored" data-anchor-id="checkpointing-parameters-and-optimizer-state">47. Checkpointing Parameters and Optimizer State</h3>
<p>Training a model can take hours, days, or even weeks. If you stop the program halfway-whether by accident (a crash, a power cut) or on purpose (pausing to save compute)-you don’t want to start over from scratch. Checkpointing solves this problem by saving the model’s parameters and optimizer state to disk so you can resume training later.</p>
<section id="what-a-checkpoint-contains" class="level4">
<h4 class="anchored" data-anchor-id="what-a-checkpoint-contains">What a checkpoint contains</h4>
<p>A checkpoint is like a “save game” for machine learning. At a minimum, it needs:</p>
<ol type="1">
<li>Model parameters – the actual weights of the neural network, stored as floating-point numbers in memory. These define what the model has learned so far.</li>
<li>Optimizer state – for AdamW, this includes the running averages of gradients (<code>m_memory</code>) and squared gradients (<code>v_memory</code>). Without these, the optimizer would lose its “memory” of past updates, which could destabilize resumed training.</li>
<li>Step counter – the number of steps completed so far. This matters for bias correction in AdamW and for scheduling the learning rate.</li>
</ol>
<p>Together, these three things capture the full training state.</p>
</section>
<section id="saving-a-checkpoint" class="level4">
<h4 class="anchored" data-anchor-id="saving-a-checkpoint">Saving a checkpoint</h4>
<p>Although <code>train_gpt2.c</code> is kept minimal and does not include full checkpointing code, the idea is straightforward. You allocate a file, write all parameters, optimizer buffers, and metadata, then close the file. In pseudocode, it looks like this:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb123"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="dt">FILE</span> <span class="op">*</span>f <span class="op">=</span> fopen<span class="op">(</span><span class="st">"checkpoint.bin"</span><span class="op">,</span> <span class="st">"wb"</span><span class="op">);</span></span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a>fwrite<span class="op">(</span>model<span class="op">.</span>params_memory<span class="op">,</span> <span class="kw">sizeof</span><span class="op">(</span><span class="dt">float</span><span class="op">),</span> model<span class="op">.</span>num_parameters<span class="op">,</span> f<span class="op">);</span></span>
<span id="cb123-3"><a href="#cb123-3" aria-hidden="true" tabindex="-1"></a>fwrite<span class="op">(</span>model<span class="op">.</span>m_memory<span class="op">,</span> <span class="kw">sizeof</span><span class="op">(</span><span class="dt">float</span><span class="op">),</span> model<span class="op">.</span>num_parameters<span class="op">,</span> f<span class="op">);</span></span>
<span id="cb123-4"><a href="#cb123-4" aria-hidden="true" tabindex="-1"></a>fwrite<span class="op">(</span>model<span class="op">.</span>v_memory<span class="op">,</span> <span class="kw">sizeof</span><span class="op">(</span><span class="dt">float</span><span class="op">),</span> model<span class="op">.</span>num_parameters<span class="op">,</span> f<span class="op">);</span></span>
<span id="cb123-5"><a href="#cb123-5" aria-hidden="true" tabindex="-1"></a>fwrite<span class="op">(&amp;</span>step<span class="op">,</span> <span class="kw">sizeof</span><span class="op">(</span><span class="dt">int</span><span class="op">),</span> <span class="dv">1</span><span class="op">,</span> f<span class="op">);</span></span>
<span id="cb123-6"><a href="#cb123-6" aria-hidden="true" tabindex="-1"></a>fclose<span class="op">(</span>f<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This is a binary dump of the model and optimizer. Later, you can load the file back with <code>fread</code> calls into the same memory locations.</p>
</section>
<section id="loading-a-checkpoint" class="level4">
<h4 class="anchored" data-anchor-id="loading-a-checkpoint">Loading a checkpoint</h4>
<p>Loading is the reverse:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb124"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="dt">FILE</span> <span class="op">*</span>f <span class="op">=</span> fopen<span class="op">(</span><span class="st">"checkpoint.bin"</span><span class="op">,</span> <span class="st">"rb"</span><span class="op">);</span></span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a>fread<span class="op">(</span>model<span class="op">.</span>params_memory<span class="op">,</span> <span class="kw">sizeof</span><span class="op">(</span><span class="dt">float</span><span class="op">),</span> model<span class="op">.</span>num_parameters<span class="op">,</span> f<span class="op">);</span></span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a>fread<span class="op">(</span>model<span class="op">.</span>m_memory<span class="op">,</span> <span class="kw">sizeof</span><span class="op">(</span><span class="dt">float</span><span class="op">),</span> model<span class="op">.</span>num_parameters<span class="op">,</span> f<span class="op">);</span></span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true" tabindex="-1"></a>fread<span class="op">(</span>model<span class="op">.</span>v_memory<span class="op">,</span> <span class="kw">sizeof</span><span class="op">(</span><span class="dt">float</span><span class="op">),</span> model<span class="op">.</span>num_parameters<span class="op">,</span> f<span class="op">);</span></span>
<span id="cb124-5"><a href="#cb124-5" aria-hidden="true" tabindex="-1"></a>fread<span class="op">(&amp;</span>step<span class="op">,</span> <span class="kw">sizeof</span><span class="op">(</span><span class="dt">int</span><span class="op">),</span> <span class="dv">1</span><span class="op">,</span> f<span class="op">);</span></span>
<span id="cb124-6"><a href="#cb124-6" aria-hidden="true" tabindex="-1"></a>fclose<span class="op">(</span>f<span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Once loaded, training can continue exactly where it left off.</p>
</section>
<section id="why-optimizer-state-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-optimizer-state-matters">Why optimizer state matters</h4>
<p>It might seem enough to save only the model’s parameters. But AdamW depends on moving averages of past gradients. If you throw those away and restart with only the parameters, the optimizer will behave differently. Learning may suddenly become unstable, or the effective learning rate may feel wrong. That’s why saving both the parameters and optimizer state gives the most faithful restart.</p>
</section>
<section id="why-checkpointing-is-essential" class="level4">
<h4 class="anchored" data-anchor-id="why-checkpointing-is-essential">Why checkpointing is essential</h4>
<p>Training is rarely smooth. Servers reboot, experiments are interrupted, bugs are found. Without checkpoints, any interruption means wasted compute and lost progress. With checkpoints, you can pause and resume at will. They also let you archive important moments in training-for example, saving the model when validation loss is lowest, not just at the end.</p>
</section>
<section id="try-it-yourself-35" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-35">Try it yourself</h4>
<ol type="1">
<li>Write a small function that saves the model’s parameters after every 100 steps. Then kill the program midway and reload from the saved file. Confirm that resumed training picks up where it left off.</li>
<li>Try saving only parameters but not optimizer state. Resume training and compare loss curves. You’ll see that the run diverges from the original.</li>
<li>Save checkpoints at multiple steps and later reload them to compare model generations (does the model produce more fluent text after 10 steps, 100 steps, 1000 steps?).</li>
<li>Intentionally corrupt part of a checkpoint file (flip a few bytes) and try reloading. This helps you understand why consistency checks or checksums are often added in real systems.</li>
<li>Store checkpoints in a versioned way (e.g., <code>checkpoint_step100.bin</code>, <code>checkpoint_step200.bin</code>) so you can roll back if a later training phase degrades performance.</li>
</ol>
</section>
<section id="the-takeaway-36" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-36">The takeaway</h4>
<p>Checkpointing is what makes long-running training practical. By saving parameters, optimizer state, and the step counter, you preserve not just what the model knows but how it is learning. In real projects, checkpoints are the bridge between experiments and production: they let you stop, resume, compare, and deploy models without ever starting from scratch. Even though <code>llm.c</code> does not fully implement it, the concept is simple and invaluable.</p>
</section>
</section>
<section id="reproducibility-and-small-divergences" class="level3">
<h3 class="anchored" data-anchor-id="reproducibility-and-small-divergences">48. Reproducibility and Small Divergences</h3>
<p>When training deep learning models, two runs that look identical on the surface can still behave differently. One run might converge quickly, another might take longer, and sometimes losses diverge even though you used the same dataset and code. This happens because of the way randomness and numerical precision interact during training. Reproducibility is about controlling these factors so that results are consistent and meaningful.</p>
<section id="sources-of-randomness" class="level4">
<h4 class="anchored" data-anchor-id="sources-of-randomness">Sources of randomness</h4>
<p>There are several places where randomness sneaks into training:</p>
<ul>
<li>Data order: if batches are shuffled differently, the model sees tokens in a new sequence. Early steps can influence the trajectory of training.</li>
<li>Weight initialization: initial parameters are usually set randomly. Different seeds lead to slightly different starting points.</li>
<li>Dropout and sampling: while <code>train_gpt2.c</code> is minimal and doesn’t include dropout layers, many neural networks do. Dropout randomly disables activations during training.</li>
<li>Floating-point arithmetic: on CPUs and GPUs, the order of summations or parallel reductions can cause tiny rounding differences. Over many steps, these small changes accumulate.</li>
</ul>
</section>
<section id="how-llm.c-handles-reproducibility" class="level4">
<h4 class="anchored" data-anchor-id="how-llm.c-handles-reproducibility">How llm.c handles reproducibility</h4>
<p>The repository includes functions like <code>manual_seed</code> and <code>random_f32</code> in <code>llmc/rand.h</code>. These are simple random number generators that can be seeded with a fixed value. For example:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb125"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>manual_seed<span class="op">(</span><span class="dv">1337</span><span class="op">);</span>  </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>If you call this before training, the random number generator starts from the same state every run. That means weight initialization and sampling will be reproducible.</p>
<p>The dataloaders also have reproducibility options. When you initialize a <code>DataLoader</code>, you can decide whether it shuffles batches or not. Keeping this consistent ensures the model sees the same data order each run.</p>
</section>
<section id="why-small-divergences-happen-anyway" class="level4">
<h4 class="anchored" data-anchor-id="why-small-divergences-happen-anyway">Why small divergences happen anyway</h4>
<p>Even with fixed seeds, you might notice that two runs are not perfectly identical. On CPUs, differences often come from OpenMP parallel loops-threads may sum numbers in a different order, producing slightly different results. On GPUs, parallelism and library implementations (like cuBLAS or cuDNN) can do the same.</p>
<p>These differences are usually very small, but deep learning systems are chaotic: tiny changes in the early steps can grow into visible differences later. This doesn’t mean the code is wrong-it just means floating-point math has limits.</p>
</section>
<section id="why-reproducibility-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-reproducibility-matters">Why reproducibility matters</h4>
<p>Reproducibility isn’t just about peace of mind. It has real uses:</p>
<ul>
<li>Debugging: if a bug appears, you want to reproduce the exact same run to diagnose it.</li>
<li>Comparisons: when testing new optimizers, schedulers, or architectures, you want fair comparisons on identical conditions.</li>
<li>Science: reproducible results are essential for research papers and benchmarks.</li>
</ul>
<p>At the same time, absolute bit-for-bit reproducibility is often unrealistic in large parallel systems. Instead, the goal is practical reproducibility: ensuring that runs are <em>similar enough</em> to reach the same conclusions.</p>
</section>
<section id="example-experiment" class="level4">
<h4 class="anchored" data-anchor-id="example-experiment">Example experiment</h4>
<p>Suppose you seed training with <code>manual_seed(1337)</code> and use the same dataset. You might get a loss curve like this:</p>
<pre><code>Run A: step 1000 → val loss 3.42  
Run B: step 1000 → val loss 3.43  </code></pre>
<p>The numbers are not identical, but they are close. The important part is that the model’s learning trajectory is stable and results are comparable.</p>
<p>If you remove the seed and allow full randomness, you might get:</p>
<pre><code>Run A: step 1000 → val loss 3.42  
Run B: step 1000 → val loss 3.89  </code></pre>
<p>Both are valid, but harder to compare.</p>
</section>
<section id="try-it-yourself-36" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-36">Try it yourself</h4>
<ol type="1">
<li>Run training twice without setting a seed. Compare how training loss and validation loss differ at step 500.</li>
<li>Set a fixed seed with <code>manual_seed(42)</code> before building the model. Run training twice and compare again. You should see closer numbers.</li>
<li>Enable OpenMP with multiple threads and then run with a single thread. Notice how results differ slightly due to floating-point summation order.</li>
<li>Save two checkpoints from runs with different seeds. Use the model to generate text and compare outputs. You’ll see different wording, but both grammatically plausible.</li>
<li>Increase the dataset size and check if differences between runs shrink. With more data, randomness matters less.</li>
</ol>
</section>
<section id="the-takeaway-37" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-37">The takeaway</h4>
<p>Reproducibility in training is about controlling randomness where possible and accepting small divergences where not. In <code>llm.c</code>, reproducibility is made clear through simple seeding functions and deterministic dataloader options. Perfect bit-level reproducibility isn’t the point-the goal is to ensure results are stable, comparable, and scientifically sound, even if tiny numerical differences creep in.</p>
</section>
</section>
<section id="command-line-flags-and-defaults" class="level3">
<h3 class="anchored" data-anchor-id="command-line-flags-and-defaults">49. Command-Line Flags and Defaults</h3>
<p>When you run a training program, you often want to change certain settings without editing the source code. For example, you might want to try a different batch size, adjust the learning rate, or train for more steps. Command-line flags make this possible. In <code>train_gpt2.c</code>, defaults are set inside the program, but it can also be compiled to accept arguments, giving you flexibility while keeping the code minimal.</p>
<section id="why-flags-exist" class="level4">
<h4 class="anchored" data-anchor-id="why-flags-exist">Why flags exist</h4>
<p>Deep learning experiments are highly sensitive to hyperparameters-values like learning rate, batch size, sequence length, or number of steps. If every change required modifying source code, recompiling, and rerunning, experimentation would be slow and error-prone. Flags allow you to configure these parameters quickly at runtime.</p>
<p>In many large frameworks (like PyTorch or TensorFlow), command-line arguments are parsed with helper libraries. In <code>llm.c</code>, the philosophy is simplicity: flags are either defined in code as constants, or you can extend <code>main</code> with standard C argument parsing to override defaults.</p>
</section>
<section id="defaults-in-train_gpt2.c" class="level4">
<h4 class="anchored" data-anchor-id="defaults-in-train_gpt2.c">Defaults in <code>train_gpt2.c</code></h4>
<p>Looking at the code, here are the main defaults hardcoded in the <code>main</code> function:</p>
<ul>
<li><p>Batch size (<code>B</code>):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb128"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> B <span class="op">=</span> <span class="dv">4</span><span class="op">;</span> <span class="co">// number of sequences per batch</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Sequence length (<code>T</code>):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb129"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> T <span class="op">=</span> <span class="dv">64</span><span class="op">;</span> <span class="co">// tokens per sequence</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Validation batches:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb130"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> val_num_batches <span class="op">=</span> <span class="dv">5</span><span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Training steps:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb131"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> step <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> step <span class="op">&lt;=</span> <span class="dv">40</span><span class="op">;</span> step<span class="op">++)</span> <span class="op">{</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>By default, only 40 steps are run in this example.</p></li>
<li><p>Optimizer hyperparameters (inside <code>gpt2_update</code> call):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb132"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a>gpt2_update<span class="op">(&amp;</span>model<span class="op">,</span> <span class="fl">1e-4</span><span class="bu">f</span><span class="op">,</span> <span class="fl">0.9</span><span class="bu">f</span><span class="op">,</span> <span class="fl">0.999</span><span class="bu">f</span><span class="op">,</span> <span class="fl">1e-8</span><span class="bu">f</span><span class="op">,</span> <span class="fl">0.0</span><span class="bu">f</span><span class="op">,</span> step<span class="op">+</span><span class="dv">1</span><span class="op">);</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Here the learning rate is <code>1e-4</code>, beta values for AdamW are <code>0.9</code> and <code>0.999</code>, epsilon is <code>1e-8</code>, and weight decay is <code>0.0</code>.</p></li>
</ul>
<p>These defaults are chosen to make the reference training loop run quickly and predictably, especially on small datasets like Tiny Shakespeare or Tiny Stories.</p>
</section>
<section id="how-to-add-flags" class="level4">
<h4 class="anchored" data-anchor-id="how-to-add-flags">How to add flags</h4>
<p>If you want flexibility, you can extend <code>main</code> with argument parsing:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb133"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> main<span class="op">(</span><span class="dt">int</span> argc<span class="op">,</span> <span class="dt">char</span> argv<span class="op">)</span> <span class="op">{</span></span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> B <span class="op">=</span> <span class="dv">4</span><span class="op">;</span></span>
<span id="cb133-3"><a href="#cb133-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> T <span class="op">=</span> <span class="dv">64</span><span class="op">;</span></span>
<span id="cb133-4"><a href="#cb133-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> max_steps <span class="op">=</span> <span class="dv">40</span><span class="op">;</span></span>
<span id="cb133-5"><a href="#cb133-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>argc <span class="op">&gt;</span> <span class="dv">1</span><span class="op">)</span> B <span class="op">=</span> atoi<span class="op">(</span>argv<span class="op">[</span><span class="dv">1</span><span class="op">]);</span></span>
<span id="cb133-6"><a href="#cb133-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>argc <span class="op">&gt;</span> <span class="dv">2</span><span class="op">)</span> T <span class="op">=</span> atoi<span class="op">(</span>argv<span class="op">[</span><span class="dv">2</span><span class="op">]);</span></span>
<span id="cb133-7"><a href="#cb133-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>argc <span class="op">&gt;</span> <span class="dv">3</span><span class="op">)</span> max_steps <span class="op">=</span> atoi<span class="op">(</span>argv<span class="op">[</span><span class="dv">3</span><span class="op">]);</span></span>
<span id="cb133-8"><a href="#cb133-8" aria-hidden="true" tabindex="-1"></a>    <span class="op">...</span></span>
<span id="cb133-9"><a href="#cb133-9" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Now you can run:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb134"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./train_gpt2</span> 8 128 100</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This sets batch size to 8, sequence length to 128, and steps to 100, without changing source code.</p>
</section>
<section id="why-it-matters-29" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-29">Why it matters</h4>
<p>Command-line flags make experimentation far more efficient. You can try multiple configurations in one day without recompiling or editing the file repeatedly. This is especially useful when running jobs on clusters where you want scripts that launch many experiments automatically with different parameters.</p>
<p>Defaults are equally important: they give you a safe, predictable starting point. Beginners can run the code without thinking about flags, while advanced users can override values as needed.</p>
</section>
<section id="try-it-yourself-37" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-37">Try it yourself</h4>
<ol type="1">
<li>Keep the default batch size of 4 and sequence length of 64. Run training and note the time per step.</li>
<li>Change batch size to 8 by editing the code. Observe how training speed changes and how memory usage increases.</li>
<li>Modify the loop to train for 200 steps instead of 40. Watch how loss decreases further.</li>
<li>Add argument parsing to accept learning rate as a flag. Experiment with <code>1e-3</code> vs.&nbsp;<code>1e-5</code> and see how quickly training diverges or stalls.</li>
<li>Create a shell script that runs training multiple times with different values for <code>B</code> and <code>T</code>. Compare results.</li>
</ol>
</section>
<section id="the-takeaway-38" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-38">The takeaway</h4>
<p>Command-line flags and defaults balance simplicity with flexibility. Defaults make the code runnable out of the box, while flags let you scale experiments without constantly editing the source. In <code>train_gpt2.c</code>, this design keeps the training loop minimal but still adaptable, encouraging both clarity and experimentation.</p>
</section>
</section>
<section id="example-cpu-training-logs-and-outputs" class="level3">
<h3 class="anchored" data-anchor-id="example-cpu-training-logs-and-outputs">50. Example CPU Training Logs and Outputs</h3>
<p>One of the best ways to understand what a training loop is doing is by reading its logs. Logs are the program’s way of telling you how training is progressing: what the loss is, how fast it’s running, and whether validation checks are improving. In <code>train_gpt2.c</code>, logging is deliberately minimal so you can easily see the essentials without being overwhelmed.</p>
<section id="what-the-logs-look-like" class="level4">
<h4 class="anchored" data-anchor-id="what-the-logs-look-like">What the logs look like</h4>
<p>Here’s a snippet of output from running the CPU training loop on Tiny Shakespeare:</p>
<pre><code>train dataset num_batches: 1192
val dataset num_batches: 128
[GPT-2]
max_seq_len: 1024
vocab_size: 50257
padded_vocab_size: 50304
num_layers: 12
num_heads: 12
channels: 768
num_parameters: 124475904
num_activations: 73347840
val loss 5.325529
step 0: train loss 4.677779 (took 1987.546000 ms)
step 1: train loss 5.191576 (took 1927.230000 ms)
step 2: train loss 4.438685 (took 1902.987000 ms)
...</code></pre>
<p>Each part of this output has meaning:</p>
<ul>
<li>Dataset sizes: how many training and validation batches are available.</li>
<li>Model config: confirmation that the GPT-2 model was loaded correctly (sequence length, vocab size, number of layers, etc.).</li>
<li>Validation loss: an average measure of how well the model is doing on unseen data.</li>
<li>Training step logs: for each step, you see the training loss and how long the step took in milliseconds.</li>
</ul>
</section>
<section id="understanding-loss-values" class="level4">
<h4 class="anchored" data-anchor-id="understanding-loss-values">Understanding loss values</h4>
<p>Loss is the number that tells us how far the model’s predictions are from the correct answers. Lower is better.</p>
<ul>
<li>A loss around 5.3 means the model is essentially guessing.</li>
<li>As training progresses, you want to see this number slowly decrease.</li>
<li>If the number gets stuck, or goes up, it can indicate problems with the learning rate, dataset, or implementation.</li>
</ul>
<p>Think of it like a report card: at the beginning, the model is failing every test, but as it practices (trains), the grades (loss values) improve.</p>
</section>
<section id="speed-measurements" class="level4">
<h4 class="anchored" data-anchor-id="speed-measurements">Speed measurements</h4>
<p>The “took … ms” part shows how long each step took. On CPU, this is usually slow, sometimes a couple of seconds per step. On GPU, the same step might only take tens of milliseconds.</p>
<p>Timing logs are useful because they help you:</p>
<ul>
<li>Estimate how long full training will take.</li>
<li>Compare performance between machines.</li>
<li>Spot problems if training suddenly slows down.</li>
</ul>
</section>
<section id="occasional-validation-checks" class="level4">
<h4 class="anchored" data-anchor-id="occasional-validation-checks">Occasional validation checks</h4>
<p>Every few steps, the code switches to validation data and prints a <code>val loss</code>. This is crucial: training loss always goes down if the model memorizes the training set, but validation loss tells you if it is <em>actually learning patterns</em> that generalize.</p>
<p>If training loss goes down but validation loss stays high, that’s a sign of overfitting.</p>
</section>
<section id="generated-samples" class="level4">
<h4 class="anchored" data-anchor-id="generated-samples">Generated samples</h4>
<p>At certain steps, the code also prints generated text like this:</p>
<pre><code>generating:

 The King had not
 that the Duke of Northumberland and the Duke of
...
</code></pre>
<p>Even though the text might look strange at first, it’s a powerful sign that the model is learning. At the beginning, output is pure gibberish, but as training continues, you start to see recognizable words and patterns.</p>
</section>
<section id="why-it-matters-30" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-30">Why it matters</h4>
<p>Logs are your window into the training process. Without them, training would be a black box-you’d wait hours and have no idea if it was working. By watching loss curves, step times, and sample outputs, you can make informed adjustments and gain confidence that the model is on the right track.</p>
</section>
<section id="try-it-yourself-38" class="level4">
<h4 class="anchored" data-anchor-id="try-it-yourself-38">Try it yourself</h4>
<ol type="1">
<li>Run the training loop as-is and save the console output. Mark how loss changes between step 0 and step 40.</li>
<li>Increase the number of steps to 200 and compare how the losses evolve.</li>
<li>Change the batch size from 4 to 8 and note both the training speed and the loss behavior.</li>
<li>Edit the code to print validation loss every step instead of every 10 steps. Does the trend look smoother?</li>
<li>Save the generated samples at steps 20 and 40. Compare how the quality changes.</li>
</ol>
</section>
<section id="the-takeaway-39" class="level4">
<h4 class="anchored" data-anchor-id="the-takeaway-39">The takeaway</h4>
<p>Training logs are like a diary of the model’s progress. They show you how quickly the model is learning, how well it generalizes, and how fast the computation runs. By reading and interpreting logs carefully, you can guide experiments, detect problems early, and appreciate the progress that’s happening inside the model.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../index.html" class="pagination-link" aria-label="Content">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Content</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->




</body></html>