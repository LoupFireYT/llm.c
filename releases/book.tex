% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={The Little Book of llm.c},
  pdfauthor={Duc-Tam Nguyen},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{The Little Book of llm.c}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Version 0.1.1}
\author{Duc-Tam Nguyen}
\date{2025-09-24}
\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter{Content}\label{content}

\subsubsection{Chapter 1 --- Orientation}\label{chapter-1-orientation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What \emph{llm.c} Is (scope, goals, philosophy)
\item
  Repository Tour (folders, files, structure)
\item
  Makefile Targets \& Flags (CPU, CUDA, options)
\item
  Quickstart: CPU Reference Path (\texttt{train\_gpt2.c})
\item
  Quickstart: 1-GPU Legacy Path (\texttt{train\_gpt2\_fp32.cu})
\item
  Quickstart: Modern CUDA Path (\texttt{train\_gpt2.cu})
\item
  Starter Artifacts \& Data Prep
  (\texttt{dev/download\_starter\_pack.sh}, \texttt{dev/data/})
\item
  Debugging Tips \& IDE Stepping (\texttt{-g}, gdb, lldb, IDEs)
\item
  Project Constraints \& Readability Contract
\item
  Community, Discussions, and Learning Path
\end{enumerate}

\subsubsection{Chapter 2 --- Data, Tokenization, and
Loaders}\label{chapter-2-data-tokenization-and-loaders}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\tightlist
\item
  GPT-2 Tokenizer Artifacts (\texttt{gpt2\_tokenizer.bin})
\item
  Binary Dataset Format (\texttt{.bin} with header + tokens)
\item
  Dataset Scripts in \texttt{dev/data/} (Tiny Shakespeare, OpenWebText)
\item
  DataLoader Design (batching, strides, epochs)
\item
  EvalLoader and Validation Workflow
\item
  Sequence Length and Memory Budgeting
\item
  Reproducibility and Seeding Across Runs
\item
  Error Surfaces from Bad Data (bounds, asserts)
\item
  Tokenization Edge Cases (UNKs, EOS, BOS)
\item
  Data Hygiene and Logging
\end{enumerate}

\subsubsection{Chapter 3 --- Model Definition \&
Weights}\label{chapter-3-model-definition-weights}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{20}
\tightlist
\item
  GPT-2 Config: vocab, layers, heads, channels
\item
  Parameter Tensors and Memory Layout
\item
  Embedding Tables: token + positional
\item
  Attention Stack: QKV projections and geometry
\item
  MLP Block: linear layers + activation
\item
  LayerNorm: theory and implementation (\texttt{doc/layernorm})
\item
  Residual Streams: skip connections explained
\item
  Loss Head: tied embeddings and logits
\item
  Checkpoint Loading from PyTorch
\item
  Parameter Counting and Sanity Checks
\end{enumerate}

\subsubsection{Chapter 4 --- CPU Inference (Forward
only)}\label{chapter-4-cpu-inference-forward-only}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{30}
\tightlist
\item
  Forward Pass Walkthrough
\item
  Token and Positional Embedding Lookup
\item
  Attention: matmuls, masking, softmax on CPU
\item
  MLP: GEMMs and activation functions
\item
  LayerNorm on CPU (step-by-step)
\item
  Residual Adds and Signal Flow
\item
  Cross-Entropy Loss on CPU
\item
  Putting It All Together: The \texttt{gpt2\_forward}
\item
  OpenMP Pragmas for Parallel Loops
\item
  CPU Memory Footprint and Performance
\end{enumerate}

\subsubsection{Chapter 5 --- Training Loop (CPU
Path)}\label{chapter-5-training-loop-cpu-path}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{40}
\tightlist
\item
  Skeleton of Training Loop
\item
  AdamW Implementation in C
\item
  Learning Rate Schedulers (cosine, warmup)
\item
  Gradient Accumulation and Micro-Batching
\item
  Logging and Progress Reporting
\item
  Validation Runs in Training Loop
\item
  Checkpointing Parameters and Optimizer State
\item
  Reproducibility and Small Divergences
\item
  Command-Line Flags and Defaults
\item
  Example CPU Training Logs and Outputs
\end{enumerate}

\subsubsection{Chapter 6 --- Testing, Profiling, \&
Parity}\label{chapter-6-testing-profiling-parity}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{50}
\tightlist
\item
  Debug State Structs and Their Role
\item
  \texttt{test\_gpt2.c}: CPU vs PyTorch
\item
  \texttt{test\_gpt2cu.cu}: CUDA vs PyTorch
\item
  Matching Outputs Within Tolerances
\item
  Profiling with \texttt{profile\_gpt2.cu}
\item
  Measuring FLOPs and GPU Utilization
\item
  Reproducing Known Loss Curves
\item
  Common CUDA Pitfalls (toolchain, PTX)
\item
  cuDNN FlashAttention Testing (\texttt{USE\_CUDNN})
\item
  From Unit Test to Full Training Readiness
\end{enumerate}

\subsubsection{\texorpdfstring{Chapter 7 --- CUDA Training Internals
(\texttt{train\_gpt2.cu})}{Chapter 7 --- CUDA Training Internals (train\_gpt2.cu)}}\label{chapter-7-cuda-training-internals-train_gpt2.cu}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{60}
\tightlist
\item
  CUDA Architecture Overview (streams, kernels)
\item
  Matrix Multiplication via cuBLAS/cuBLASLt
\item
  Attention Kernels: cuDNN FlashAttention
\item
  Mixed Precision: FP16/BF16 with Master FP32 Weights
\item
  Loss Scaling in Mixed Precision Training
\item
  Activation Checkpointing and Memory Tradeoffs
\item
  GPU Memory Planning: params, grads, states
\item
  Kernel Launch Configurations and Occupancy
\item
  CUDA Error Handling and Debugging
\item
  \texttt{dev/cuda/}: From Simple Kernels to High Performance
\end{enumerate}

\subsubsection{Chapter 8 --- Multi-GPU \& Multi-Node
Training}\label{chapter-8-multi-gpu-multi-node-training}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{70}
\tightlist
\item
  Data Parallelism in \emph{llm.c}
\item
  MPI Process Model and GPU Affinity
\item
  NCCL All-Reduce for Gradient Sync
\item
  Building and Running Multi-GPU Trainers
\item
  Multi-Node Bootstrapping with MPI
\item
  SLURM and PMIx Caveats
\item
  Debugging Multi-GPU Hangs and Stalls
\item
  Scaling Stories: GPT-2 124M → 774M → 1.6B
\item
  NCCL Tuning and Overlap Opportunities
\item
  Common Multi-GPU Errors and Fixes
\end{enumerate}

\subsubsection{Chapter 9 --- Extending the
Codebase}\label{chapter-9-extending-the-codebase}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{80}
\tightlist
\item
  The \texttt{dev/cuda} Library for Custom Kernels
\item
  Adding New Dataset Pipelines (\texttt{dev/data/*})
\item
  Adding a New Optimizer to the Codebase
\item
  Adding a New Scheduler (cosine, step, etc.)
\item
  Alternative Attention Mechanisms
\item
  Profiling and Testing New Kernels
\item
  Using PyTorch Reference as Oracle
\item
  Exploring Beyond GPT-2: LLaMA Example
\item
  Porting Playbook: C → Go/Rust/Metal
\item
  Keeping the Repo Minimal and Clean
\end{enumerate}

\subsubsection{Chapter 10 --- Reproductions, Community, and
Roadmap}\label{chapter-10-reproductions-community-and-roadmap}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{90}
\tightlist
\item
  Reproducing GPT-2 124M on Single Node
\item
  Reproducing GPT-2 355M (constraints and tricks)
\item
  Reproducing GPT-2 774M (scaling up)
\item
  Reproducing GPT-2 1.6B on 8×H100 (24h run)
\item
  CPU-only Fine-Tune Demo (Tiny Shakespeare)
\item
  Cost and Time Estimation for Runs
\item
  Hyperparameter Sweeps (\texttt{sweep.sh})
\item
  Validating Evaluation and Loss Curves
\item
  Future Work: Kernel Library, Less cuDNN Dependence
\item
  Community, GitHub Discussions, and Suggested Learning Path
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{The Book}\label{the-book}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{Small}\NormalTok{ file, giant dream,}
\ExtensionTok{llm.c}\NormalTok{ whispers tokens,}
\ExtensionTok{worlds}\NormalTok{ unfold in text.}
\end{Highlighting}
\end{Shaded}

\section{Chapter 1. Orientation}\label{chapter-1.-orientation}

\subsection{\texorpdfstring{1. What \emph{llm.c}
Is}{1. What llm.c Is}}\label{what-llm.c-is}

Imagine you wanted to peek inside a modern AI model-not by reading
thousands of lines of optimized C++ or CUDA hidden inside a giant
framework, but by opening a small, neat folder and seeing the entire
training pipeline laid out in front of you. That is what \emph{llm.c}
gives you.

At its heart, \emph{llm.c} is a reference implementation of how to train
and run a GPT-2 style language model, written in pure C (and CUDA). The
key word is \emph{reference}: the code is meant to be minimal, readable,
and educational. You don't need to wade through abstraction layers or
device-specific macros. Instead, you get a version that looks almost
like pseudocode, but still compiles and runs on your computer.

\subsubsection{Why This Project Exists}\label{why-this-project-exists}

Deep learning frameworks like PyTorch and TensorFlow are amazing for
getting models to work quickly, but they hide most of the actual
mechanics. Under the hood, there's a lot happening: tensors are
allocated in memory, gradients are computed through backpropagation,
optimizer states are updated, and schedulers adjust the learning rate.
Most of us never see those details, because the framework handles them
for us.

\emph{llm.c} flips this around. It says: \emph{what if we removed the
black box and showed you exactly how a GPT-2 model is trained, line by
line?} It's not about speed or production deployment. It's about
clarity, education, and demystifying how large language models work.

\subsubsection{Key Characteristics}\label{key-characteristics}

\begin{itemize}
\tightlist
\item
  Minimalism: The CPU version (\texttt{train\_gpt2.c}) avoids
  complicated optimizations so that beginners can follow the logic. Even
  the CUDA version tries to stay simple, with only necessary calls to
  cuBLAS/cuDNN.
\item
  Self-contained: No external frameworks. The code defines its own
  tokenizer, dataloader, optimizer, and scheduler. Everything you need
  is in the repository.
\item
  Parallels to PyTorch: Each function in the C/CUDA implementation has a
  counterpart in PyTorch. The repo even ships with Python test files to
  prove that the outputs match within tolerance.
\item
  Step-by-step scalability: You can start with a tiny model on CPU and,
  once you understand the basics, switch to GPU, multi-GPU, or even
  multi-node training. The structure remains the same, just faster.
\end{itemize}

\subsubsection{What You Can Do With It}\label{what-you-can-do-with-it}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train GPT-2 from scratch: Start with a small dataset (like Tiny
  Shakespeare) and see the model learn patterns in language.
\item
  Experiment with configurations: Change number of layers, sequence
  length, or hidden size, then watch how memory and training time scale.
\item
  Learn GPU training internals: Move from CPU to CUDA, and later to
  multi-GPU with MPI/NCCL, to see how real distributed training works
  under the hood.
\item
  Profile performance: The repo includes profiling tools so you can
  measure FLOPs, memory bandwidth, and kernel execution times.
\item
  Reproduce big models: With enough hardware, you can actually retrain
  GPT-2 124M or larger versions, using the exact same setup described in
  the README.
\end{enumerate}

\subsubsection{Why You Should Care}\label{why-you-should-care}

If you've ever used a large language model and thought \emph{``this
feels like magic''}, \emph{llm.c} is your chance to peel back the
curtain. You'll see:

\begin{itemize}
\tightlist
\item
  How text is converted into numbers (tokenization).
\item
  How those numbers are fed into matrices and transformed.
\item
  How attention actually looks in code.
\item
  How gradients are computed and weights updated.
\item
  How scaling from one GPU to many is managed.
\end{itemize}

It's not magic anymore-it's math plus code you can read.

\subsubsection{The Big Picture}\label{the-big-picture}

Think of \emph{llm.c} as a teaching lab for modern AI:

\begin{itemize}
\tightlist
\item
  On day one, you can run a small demo on CPU and see loss values
  dropping as the model learns.
\item
  On day ten, you might be profiling CUDA kernels or debugging NCCL
  synchronization across GPUs.
\item
  At every step, the code remains approachable because it avoids
  unnecessary complexity.
\end{itemize}

This philosophy makes \emph{llm.c} unique: it's not trying to be the
fastest or the most feature-rich. It's trying to be the clearest window
into how large language models actually work when stripped down to their
essentials.

The Takeaway: \emph{llm.c} is a minimal, transparent re-implementation
of GPT-2 training and inference, designed to teach you how language
models operate under the hood. It's not about production
performance-it's about giving you a guided tour through the machinery of
modern AI, one C file at a time.

Would you like me to now expand Section 2 (``Repository Tour'') in the
same friendly beginner tone, showing what's inside each folder and file?

\subsection{2. Repository Tour}\label{repository-tour}

When you first open the \emph{llm.c} repository, it may look like a
random collection of \texttt{.c}, \texttt{.cu}, \texttt{.py}, and shell
scripts. But there's a clear structure hiding underneath. Each part has
a purpose, and once you know where things live, navigating the repo
feels natural. Let's take a guided walk through the main pieces.

\subsubsection{Root Directory: The Entry
Points}\label{root-directory-the-entry-points}

At the top level, you'll find the core training programs. These are the
files you actually compile and run:

\begin{itemize}
\tightlist
\item
  \texttt{train\_gpt2.c} - The CPU reference implementation. This is the
  simplest, most readable version of GPT-2 training. It avoids special
  optimizations so you can follow the math and logic step by step.
\item
  \texttt{train\_gpt2.cu} - The CUDA implementation. Faster, uses GPU
  kernels, cuBLAS, and optional cuDNN FlashAttention. This is the
  version you'd use for serious training runs.
\item
  \texttt{train\_gpt2\_fp32.cu} - A legacy CUDA path, using plain FP32
  precision instead of mixed precision. It's slower but useful as a
  debugging baseline.
\item
  \texttt{train\_gpt2.py} - The PyTorch reference. This is the oracle: a
  tiny script in Python/PyTorch that trains the same GPT-2 so you can
  compare outputs and verify correctness.
\end{itemize}

Other important root-level files:

\begin{itemize}
\tightlist
\item
  \texttt{Makefile} - Defines how to build different versions. Targets
  like \texttt{make\ train\_gpt2} or \texttt{make\ train\_gpt2cu} are
  your entry points.
\item
  \texttt{README.md} - The main guide for running experiments,
  installing dependencies, and reproducing models.
\end{itemize}

\subsubsection{\texorpdfstring{\texttt{llmc/} Directory: Utilities and
Building
Blocks}{llmc/ Directory: Utilities and Building Blocks}}\label{llmc-directory-utilities-and-building-blocks}

This folder holds reusable C utilities that the main training files
include:

\begin{itemize}
\tightlist
\item
  \texttt{utils.h} - Safety wrappers (\texttt{fopenCheck},
  \texttt{mallocCheck}) and helper functions.
\item
  \texttt{tokenizer.h} - Implements GPT-2's tokenizer in C: encoding
  text into token IDs and decoding back to text.
\item
  \texttt{dataloader.h} - Defines how training batches are loaded and
  served, handling dataset splits and iteration.
\item
  \texttt{rand.h} - Random number utilities, mirroring PyTorch's
  \texttt{manual\_seed} and normal distributions.
\item
  \texttt{schedulers.h} - Learning rate scheduling, like cosine decay
  with warmup.
\item
  \texttt{sampler.h} - Implements softmax sampling for text generation
  and helper RNG.
\item
  \texttt{logger.h} - Minimal logging functionality for tracking
  progress.
\end{itemize}

Think of \texttt{llmc/} as the library that keeps the main files clean
and readable. Instead of cluttering \texttt{train\_gpt2.c} with helpers,
everything is modularized here.

\subsubsection{\texorpdfstring{\texttt{dev/} Directory: Scripts and
Extras}{dev/ Directory: Scripts and Extras}}\label{dev-directory-scripts-and-extras}

This folder is full of supporting tools that make experiments easier:

\begin{itemize}
\tightlist
\item
  \texttt{dev/download\_starter\_pack.sh} - Fetches the GPT-2 124M
  weights, tokenizer, and datasets. This is the quickest way to get
  started.
\item
  \texttt{dev/data/} - Contains scripts for preparing datasets like Tiny
  Shakespeare or OpenWebText in the binary format that \emph{llm.c}
  expects.
\item
  \texttt{dev/cuda/} - A place for experimenting with standalone CUDA
  kernels. This is where you'd go if you want to tinker with custom GPU
  code beyond the main trainer.
\end{itemize}

\subsubsection{\texorpdfstring{\texttt{doc/} Directory: Learning
Resources}{doc/ Directory: Learning Resources}}\label{doc-directory-learning-resources}

Documentation that digs deeper into specific topics. For example:

\begin{itemize}
\tightlist
\item
  \texttt{doc/layernorm/layernorm.md} - A tutorial-style explanation of
  Layer Normalization, complete with math and code. It helps you
  understand one of GPT-2's core components before diving into the C
  implementation.
\end{itemize}

This folder is a learning aid. Whenever a concept feels too dense, check
here for a more gentle walkthrough.

\subsubsection{Test Files}\label{test-files}

Testing is taken seriously in \emph{llm.c}, because the goal is to prove
that the C/CUDA implementation is correct compared to PyTorch:

\begin{itemize}
\tightlist
\item
  \texttt{test\_gpt2.c} - Runs forward passes and training steps on CPU
  and compares outputs to PyTorch.
\item
  \texttt{test\_gpt2cu.cu} - Same idea but for CUDA, including both FP32
  and mixed-precision runs.
\end{itemize}

These files keep everything honest: you can always verify that your
build produces the same results as the canonical PyTorch model.

\subsubsection{Profiling Tools}\label{profiling-tools}

For performance deep dives:

\begin{itemize}
\tightlist
\item
  \texttt{profile\_gpt2.cu} - A CUDA profiling harness that benchmarks
  kernels and measures throughput.
\item
  \texttt{profile\_gpt2cu.py} - Python-side profiler for analyzing GPU
  utilization, memory bandwidth, and FLOPs.
\end{itemize}

If you're curious about where time is being spent in training, these
files show you how to measure it.

\subsubsection{Datasets and Artifacts}\label{datasets-and-artifacts}

When you run \texttt{download\_starter\_pack.sh}, you'll get:

\begin{itemize}
\tightlist
\item
  \texttt{gpt2\_tokenizer.bin} - GPT-2's byte-pair encoding tokenizer,
  serialized in binary.
\item
  Dataset \texttt{.bin} files - Training and validation sets, tokenized
  and ready for the dataloader.
\end{itemize}

These files are not in the repo by default but are downloaded or
generated locally.

\subsubsection{Putting It Together}\label{putting-it-together}

The repository is structured like a teaching lab:

\begin{itemize}
\tightlist
\item
  Root files are the main experiments.
\item
  \texttt{llmc/} is the library of building blocks.
\item
  \texttt{dev/} provides extra tools and scripts.
\item
  \texttt{doc/} explains tricky concepts in tutorial form.
\item
  Tests and profilers make sure everything matches PyTorch and runs
  efficiently.
\end{itemize}

Once you see the pattern, the repo feels less intimidating. Every file
has a role in telling the story of how a GPT-2 model is built from
scratch in C and CUDA.

\subsection{3. Makefile Targets \& Flags}\label{makefile-targets-flags}

Every C or CUDA program needs a build system, and in \emph{llm.c} that
role is handled by a simple but powerful Makefile. If you've never used
\texttt{make} before, think of it as a recipe book: you type
\texttt{make\ \textless{}target\textgreater{}} in your terminal, and it
follows the instructions for compiling the code into an executable. In
\emph{llm.c}, this file is your control center for choosing which
trainer to build, whether to enable GPUs, and which optional features to
turn on.

\subsubsection{Why a Makefile?}\label{why-a-makefile}

Instead of memorizing long \texttt{gcc} or \texttt{nvcc} compile
commands with dozens of flags, the Makefile captures those instructions
once and gives them a short name. For example, building the CPU trainer
is as easy as:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make}\NormalTok{ train\_gpt2}
\end{Highlighting}
\end{Shaded}

Behind the scenes, this calls \texttt{gcc}, sets optimization flags,
includes the right headers, and links everything together. The same
applies to CUDA builds with \texttt{nvcc}.

\subsubsection{Core Targets}\label{core-targets}

Here are the most important build targets you'll find:

\begin{itemize}
\tightlist
\item
  \texttt{train\_gpt2} - Builds the CPU-only reference trainer. Uses
  \texttt{gcc} (or \texttt{clang}) and links against OpenMP for parallel
  loops.
\item
  \texttt{train\_gpt2cu} - Builds the CUDA trainer with mixed precision
  and optional cuDNN FlashAttention. Uses \texttt{nvcc}.
\item
  \texttt{train\_gpt2\_fp32} - Builds the legacy CUDA trainer that stays
  in pure FP32 (slower but simpler).
\item
  \texttt{test\_gpt2} - Compiles the CPU test program to compare results
  against PyTorch.
\item
  \texttt{test\_gpt2cu} - Compiles the CUDA test program to check GPU
  parity with PyTorch.
\item
  \texttt{profile\_gpt2.cu} - Compiles the CUDA profiler harness, used
  to benchmark kernels and FLOPs.
\end{itemize}

Each of these produces a binary you can run directly, for example:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{./train\_gpt2}
\ExtensionTok{./train\_gpt2cu}
\ExtensionTok{./test\_gpt2}
\end{Highlighting}
\end{Shaded}

\subsubsection{Key Flags You Can Toggle}\label{key-flags-you-can-toggle}

The Makefile also exposes several switches that let you customize the
build. You set them when running \texttt{make}, like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make}\NormalTok{ train\_gpt2cu USE\_CUDNN=1}
\end{Highlighting}
\end{Shaded}

Here are the most important flags:

\begin{itemize}
\tightlist
\item
  \texttt{USE\_CUDNN} - Enables cuDNN FlashAttention if your system has
  cuDNN installed. This can give big speedups for attention, but it's
  optional. By default, it's off.
\item
  \texttt{OMP=1} - Tells the CPU trainer to compile with OpenMP enabled.
  This allows multithreaded execution, making CPU runs much faster.
  Usually on by default if OpenMP is detected.
\item
  \texttt{DEBUG=1} - Compiles with debugging symbols (\texttt{-g})
  instead of maximum optimization. Useful when stepping through code in
  an IDE or using a debugger.
\item
  \texttt{PROFILE=1} - Adds profiling hooks, helping you analyze
  execution time and performance.
\end{itemize}

\subsubsection{Optimization Choices}\label{optimization-choices}

The default build uses \texttt{-O3} optimization, which makes the code
run fast but sometimes harder to debug. If you're just learning and want
clarity, you can switch to:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make}\NormalTok{ train\_gpt2 DEBUG=1}
\end{Highlighting}
\end{Shaded}

This creates a binary that runs slower but lets you step through line by
line in a debugger. For performance benchmarking, stick with the
optimized default.

\subsubsection{Multi-GPU and MPI
Support}\label{multi-gpu-and-mpi-support}

When building the CUDA trainer, the Makefile can also link against MPI
and NCCL if they're installed. That's what enables multi-GPU and
multi-node training. You usually don't need to change anything-the
Makefile automatically detects these libraries and includes them if
available.

\subsubsection{Putting It All Together}\label{putting-it-all-together}

Think of the Makefile as a switchboard for the whole project:

\begin{itemize}
\tightlist
\item
  Want to run the simple CPU demo? → \texttt{make\ train\_gpt2}
\item
  Want to train faster on GPU? → \texttt{make\ train\_gpt2cu}
\item
  Want to debug kernels? → \texttt{make\ train\_gpt2cu\ DEBUG=1}
\item
  Want to test parity with PyTorch? → \texttt{make\ test\_gpt2} or
  \texttt{make\ test\_gpt2cu}
\end{itemize}

With just a few keystrokes, you control whether you're running a
beginner-friendly CPU demo, a high-performance GPU build, or a debugging
session.

The takeaway: The Makefile is your control center. It abstracts away
complicated compiler commands and gives you a clean menu of options: CPU
vs GPU, FP32 vs mixed precision, debug vs optimized, and single vs
multi-GPU. Mastering it is the first step to feeling comfortable
experimenting inside \emph{llm.c}.

\subsection{\texorpdfstring{4. Quickstart: CPU Reference Path
(\texttt{train\_gpt2.c})}{4. Quickstart: CPU Reference Path (train\_gpt2.c)}}\label{quickstart-cpu-reference-path-train_gpt2.c}

The simplest way to begin exploring \emph{llm.c} is with the CPU-only
reference implementation. This file, \texttt{train\_gpt2.c}, is
deliberately designed to be minimal, readable, and approachable. It
doesn't hide complexity behind libraries or macros. Instead, it shows
you exactly how a GPT-2 model is trained, step by step, using plain C
and a sprinkle of OpenMP for speed.

\subsubsection{Why Start with CPU?}\label{why-start-with-cpu}

\begin{itemize}
\tightlist
\item
  Clarity first: GPUs add layers of complexity (CUDA kernels, memory
  transfers, cuBLAS). On CPU, you can focus on the core algorithm
  without distraction.
\item
  Portability: Any machine with a C compiler can run it-no special
  hardware required.
\item
  Debuggability: Errors are easier to trace, and you can single-step
  through the code in an IDE.
\end{itemize}

The CPU version is slower, but that's a feature here-it forces you to
really see what's happening under the hood.

\subsubsection{Building the CPU Trainer}\label{building-the-cpu-trainer}

From the root of the repository, you just type:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make}\NormalTok{ train\_gpt2}
\end{Highlighting}
\end{Shaded}

This compiles \texttt{train\_gpt2.c} into an executable named
\texttt{train\_gpt2}. If your system has OpenMP, the Makefile will
detect it and add the right flags.

\subsubsection{Running Your First Training
Run}\label{running-your-first-training-run}

Before running, download the starter pack (tokenizer, dataset, configs):

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{./dev/download\_starter\_pack.sh}
\end{Highlighting}
\end{Shaded}

Now launch training:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{./train\_gpt2}
\end{Highlighting}
\end{Shaded}

You'll see output like:

\begin{verbatim}
[GPT-2]
max_seq_len: 1024
vocab_size: 50257
padded_vocab_size: 50304
num_layers: 12
num_heads: 12
channels: 768
num_parameters: 124475904
train dataset num_batches: 1192
val dataset num_batches: 128
num_activations: 73347840
val loss 5.325529
step 0: train loss 4.677779 (took 1987.546000 ms)
step 1: train loss 5.191576 (took 1927.230000 ms)
...
\end{verbatim}

Each line tells you:

\begin{itemize}
\tightlist
\item
  Model size and config (sequence length, vocabulary size, layers,
  heads, channels).
\item
  Dataset stats (how many batches for training and validation).
\item
  Activation memory size (a measure of how big the intermediate states
  are).
\item
  Training progress (step number, train loss, validation loss, time per
  step).
\end{itemize}

\subsubsection{Inside the Training Loop}\label{inside-the-training-loop}

Although you don't need to dive into the code yet, here's the high-level
flow in \texttt{train\_gpt2.c}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Load tokenizer and dataset → turns text into tokens.
\item
  Initialize model parameters → embeddings, attention weights, MLPs,
  norms.
\item
  For each batch:

  \begin{itemize}
  \tightlist
  \item
    Forward pass → compute logits and loss.
  \item
    Backward pass → compute gradients.
  \item
    Update parameters → optimizer step.
  \end{itemize}
\item
  Log progress → print losses, occasionally run validation.
\end{enumerate}

This mirrors exactly what happens in PyTorch, just spelled out in C.

\subsubsection{Performance Notes}\label{performance-notes}

On CPU, don't expect speed. Training GPT-2 124M can take days or weeks.
But that's not the point. The CPU reference path is like a glass box:
everything is visible, no shortcuts. You'll use this to learn the
mechanics and to verify that your GPU runs match the same results.

If you want to speed things up slightly, you can:

\begin{itemize}
\item
  Increase OpenMP threads:

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{OMP\_NUM\_THREADS}\OperatorTok{=}\NormalTok{8 }\ExtensionTok{./train\_gpt2}
\end{Highlighting}
\end{Shaded}
\item
  Use a smaller dataset (Tiny Shakespeare) to see faster progress.
\item
  Reduce model size by changing config values (fewer layers, smaller
  channels).
\end{itemize}

\subsubsection{When to Move On}\label{when-to-move-on}

Once you're comfortable with how training looks on CPU-loss values going
down, checkpoints being written, logs appearing-you'll be ready to
graduate to the GPU version (\texttt{train\_gpt2.cu}). That's where
performance and scaling come in, but the CPU run gives you the
conceptual foundation.

The takeaway: Running \texttt{train\_gpt2.c} is your first hands-on
encounter with GPT-2 training in \emph{llm.c}. It's slow, transparent,
and designed for learning. You'll see every piece of the model at work,
one step at a time, before diving into the complexity of CUDA.

\subsection{\texorpdfstring{5. Quickstart: 1-GPU Legacy Path
(\texttt{train\_gpt2\_fp32.cu})}{5. Quickstart: 1-GPU Legacy Path (train\_gpt2\_fp32.cu)}}\label{quickstart-1-gpu-legacy-path-train_gpt2_fp32.cu}

Once you've seen the CPU trainer in action, the natural next step is to
try training on a GPU. The file \texttt{train\_gpt2\_fp32.cu} is the
simplest GPU entry point. It predates the more advanced mixed-precision
trainer (\texttt{train\_gpt2.cu}), and it runs everything in full 32-bit
floating point (FP32) precision. That makes it easier to follow and
debug, even though it's slower than modern approaches. Think of it as
the ``training wheels'' for GPU training in \emph{llm.c}.

\subsubsection{Why This Path Exists}\label{why-this-path-exists}

Modern GPU training almost always uses mixed precision (FP16/BF16 for
speed and memory savings, FP32 for stability). But mixed precision
introduces extra complexity: scaling losses, maintaining master weights,
checking for overflows. For beginners, all that can be distracting.

The FP32 path avoids those complications:

\begin{itemize}
\tightlist
\item
  Every tensor (activations, weights, gradients) is stored as 32-bit
  floats.
\item
  No special handling of loss scaling is needed.
\item
  Debugging mismatches with PyTorch is straightforward.
\end{itemize}

The trade-off is performance-this version runs significantly slower and
uses more memory.

\subsubsection{Building the FP32 CUDA
Trainer}\label{building-the-fp32-cuda-trainer}

From the root of the repository:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make}\NormalTok{ train\_gpt2\_fp32}
\end{Highlighting}
\end{Shaded}

This invokes \texttt{nvcc} (the NVIDIA CUDA compiler) and links against
cuBLAS for matrix multiplications. The output is an executable named
\texttt{train\_gpt2\_fp32}.

\subsubsection{Running It}\label{running-it}

Just like the CPU version, make sure you've downloaded the starter pack
first:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{./dev/download\_starter\_pack.sh}
\end{Highlighting}
\end{Shaded}

Then launch training on your GPU:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{./train\_gpt2\_fp32}
\end{Highlighting}
\end{Shaded}

If CUDA is installed correctly, the program will detect your GPU and
start training. You'll see logs that look similar to the CPU trainer's,
but with much shorter step times. For example, a training step that took
\textasciitilde2 seconds on CPU might take \textasciitilde50
milliseconds on GPU.

\subsubsection{Under the Hood}\label{under-the-hood}

Although the training loop looks the same on the surface, a lot changes
under the hood when running on GPU:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Tensors are allocated in GPU memory (not system RAM).
\item
  Matrix multiplications (the core of attention and MLP layers) are
  executed by cuBLAS, NVIDIA's high-performance linear algebra library.
\item
  Kernels for elementwise operations (like adding residuals, applying
  softmax, or normalizing) are written in CUDA or use built-in
  primitives.
\item
  Gradients and optimizer states are updated entirely on the device,
  with minimal CPU↔GPU transfers.
\end{enumerate}

This makes training dramatically faster, but the structure of the code
is still recognizable compared to the CPU version.

\subsubsection{When to Use FP32 vs Mixed
Precision}\label{when-to-use-fp32-vs-mixed-precision}

\begin{itemize}
\item
  Use FP32 (this path) when:

  \begin{itemize}
  \tightlist
  \item
    You're learning how GPU training works step by step.
  \item
    You want a clean comparison with the CPU trainer.
  \item
    You're debugging correctness issues without worrying about loss
    scaling.
  \end{itemize}
\item
  Use Mixed Precision (\texttt{train\_gpt2.cu}) when:

  \begin{itemize}
  \tightlist
  \item
    You want real performance (2--4× faster training).
  \item
    You're training larger models (774M, 1.6B parameters) where memory
    efficiency matters.
  \item
    You're aiming to reproduce published GPT-2 runs on modern GPUs.
  \end{itemize}
\end{itemize}

\subsubsection{Common Pitfalls}\label{common-pitfalls}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  CUDA not installed → If \texttt{nvcc} isn't found, the Makefile will
  fail. You'll need the CUDA Toolkit installed.
\item
  Driver mismatch → Your NVIDIA driver must match the CUDA version.
\item
  Out of memory errors → FP32 uses more GPU memory, so you may need to
  lower batch size if you're on a smaller GPU.
\end{enumerate}

\subsubsection{Why This Step Matters}\label{why-this-step-matters}

The FP32 trainer is like a bridge:

\begin{itemize}
\tightlist
\item
  On one side is the CPU reference path, slow but crystal-clear.
\item
  On the other is the mixed-precision CUDA path, fast but more complex.
\end{itemize}

By walking across this bridge, you learn how GPU acceleration works
without being overwhelmed by optimizations.

The takeaway: \texttt{train\_gpt2\_fp32.cu} is your first taste of real
GPU training in \emph{llm.c}. It skips advanced tricks and shows you a
clean, one-GPU, full-precision implementation. It's not the fastest, but
it's the friendliest way to understand how training moves from CPU to
GPU.

\subsection{\texorpdfstring{6. Quickstart: Modern CUDA Path
(\texttt{train\_gpt2.cu})}{6. Quickstart: Modern CUDA Path (train\_gpt2.cu)}}\label{quickstart-modern-cuda-path-train_gpt2.cu}

This is the high-performance trainer most people use day to day. It runs
on a single NVIDIA GPU (and also forms the basis for multi-GPU), uses
mixed precision (FP16/BF16 where safe, FP32 where needed), and can
optionally enable cuDNN FlashAttention for fast attention. Compared to
the FP32 legacy path, it's significantly faster and uses less memory,
while keeping the training loop easy to follow.

\subsubsection{What ``mixed precision'' means (in plain
words)}\label{what-mixed-precision-means-in-plain-words}

\begin{itemize}
\tightlist
\item
  Weights \& activations: stored/processed in FP16 or BF16 for speed and
  lower memory.
\item
  Master weights: a FP32 copy of parameters kept for stable updates.
\item
  Loss scaling: multiply the loss before backward to avoid underflow;
  unscale the grads before the optimizer step.
\item
  Autocast-like behavior: the code picks safe dtypes for each op (GEMMs
  in tensor cores, reductions in FP32, etc.).
\end{itemize}

You get 2--4× speedups on many GPUs, and the same final accuracy when
configured properly.

\subsubsection{Build the modern CUDA
trainer}\label{build-the-modern-cuda-trainer}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make}\NormalTok{ train\_gpt2cu}
\end{Highlighting}
\end{Shaded}

Common variants:

\begin{itemize}
\item
  With cuDNN FlashAttention (if available):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make}\NormalTok{ train\_gpt2cu USE\_CUDNN=1}
\end{Highlighting}
\end{Shaded}
\item
  With debug symbols (slower, but easier to step through):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make}\NormalTok{ train\_gpt2cu DEBUG=1}
\end{Highlighting}
\end{Shaded}
\end{itemize}

This produces an executable named \texttt{train\_gpt2cu}.

\subsubsection{One-time data \&
artifacts}\label{one-time-data-artifacts}

If you haven't already:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{./dev/download\_starter\_pack.sh}
\end{Highlighting}
\end{Shaded}

This fetches the tokenizer and a small dataset so you can run
immediately.

\subsubsection{Run your first GPU training
session}\label{run-your-first-gpu-training-session}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{./train\_gpt2cu}
\end{Highlighting}
\end{Shaded}

You should see a config header (model dims, vocab, sequence length)
followed by step-by-step loss prints. Step times will be much shorter
than CPU and noticeably faster than FP32, especially on tensor-core GPUs
(Turing and newer).

Speed tips right away:

\begin{itemize}
\item
  Use a larger global batch if memory allows-it improves GPU
  utilization.
\item
  Set environment threads for any CPU preprocessing:

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{OMP\_NUM\_THREADS}\OperatorTok{=}\NormalTok{8 }\ExtensionTok{./train\_gpt2cu}
\end{Highlighting}
\end{Shaded}
\end{itemize}

\subsubsection{What's different under the hood
vs.~FP32}\label{whats-different-under-the-hood-vs.-fp32}

\begin{itemize}
\tightlist
\item
  Tensor cores: GEMMs run in FP16/BF16 paths via cuBLAS/cuBLASLt for big
  throughput.
\item
  Scaled loss \& unscale pass: Forward computes the loss, multiplies it
  by a scale factor; backward divides gradients by the same factor
  before updates.
\item
  Master FP32 copy: Optimizer (AdamW) updates this copy, then casts back
  to low precision for the next forward.
\item
  Fused/fast attention (optional): With \texttt{USE\_CUDNN=1}, attention
  may route through cuDNN FlashAttention backends.
\end{itemize}

You still recognize the same loop: load batch → forward → loss →
backward → AdamW step → log.

\subsubsection{Choosing FP16 vs.~BF16}\label{choosing-fp16-vs.-bf16}

\begin{itemize}
\tightlist
\item
  FP16: best speed, needs loss scaling; widely supported.
\item
  BF16: more numerically forgiving (often needs little/no scaling),
  requires hardware support (Ampere+); slightly larger memory than FP16
  but often simpler.
\end{itemize}

The trainer picks what your GPU supports or what the code defaults to;
you can expose a flag later if you want to force one.

\subsubsection{Common command patterns}\label{common-command-patterns}

\begin{itemize}
\item
  Small GPU (less VRAM):

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{./train\_gpt2cu} \AttributeTok{{-}{-}batch\_size}\NormalTok{ 4 }\AttributeTok{{-}{-}micro\_batch\_size}\NormalTok{ 1 }\AttributeTok{{-}{-}seq\_len}\NormalTok{ 512}
\end{Highlighting}
\end{Shaded}
\item
  Faster warmup with cosine schedule:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{./train\_gpt2cu} \AttributeTok{{-}{-}warmup\_steps}\NormalTok{ 1000 }\AttributeTok{{-}{-}lr}\NormalTok{ 6e{-}4 }\AttributeTok{{-}{-}scheduler}\NormalTok{ cosine}
\end{Highlighting}
\end{Shaded}
\item
  Periodic eval to sanity-check:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{./train\_gpt2cu} \AttributeTok{{-}{-}eval\_interval}\NormalTok{ 200 }\AttributeTok{{-}{-}eval\_batches}\NormalTok{ 50}
\end{Highlighting}
\end{Shaded}
\end{itemize}

(Flag names above mirror typical patterns; adjust to match the binary's
printed help.)

\subsubsection{Validating correctness (highly
recommended)}\label{validating-correctness-highly-recommended}

\begin{itemize}
\item
  Run the CUDA test binary to compare against the PyTorch reference on
  small batches:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make}\NormalTok{ test\_gpt2cu}
\ExtensionTok{./test\_gpt2cu}
\end{Highlighting}
\end{Shaded}
\item
  Check that logits/loss match within a small tolerance. If mismatches
  happen, recompile without optimizations or disable cuDNN fast paths
  (\texttt{USE\_CUDNN=0}) to isolate the issue.
\end{itemize}

\subsubsection{Enabling FlashAttention (when
available)}\label{enabling-flashattention-when-available}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make}\NormalTok{ train\_gpt2cu USE\_CUDNN=1}
\ExtensionTok{./train\_gpt2cu}
\end{Highlighting}
\end{Shaded}

Good signs: faster attention time and lower step latency. If you hit
build/runtime errors, ensure your CUDA, cuDNN, and driver versions are
compatible; fall back to \texttt{USE\_CUDNN=0} while you sort it out.

\subsubsection{Memory \& performance tuning
checklist}\label{memory-performance-tuning-checklist}

\begin{itemize}
\tightlist
\item
  Batching: Increase \texttt{micro\_batch\_size} until you reach
  \textasciitilde90\% GPU utilization without OOM.
\item
  Sequence length: Longer sequences increase compute quadratically in
  attention; reduce \texttt{-\/-seq\_len} if memory is tight.
\item
  Grad accumulation: Keep global batch size large by accumulating over
  multiple micro-batches.
\item
  Pinned host memory \& async copies: Already used where sensible; keep
  CPU↔GPU transfers minimal.
\item
  Profiler: Once it runs, profile hotspots to confirm GEMMs dominate (as
  expected) and attention isn't a bottleneck unless FlashAttention is
  off.
\end{itemize}

\subsubsection{Troubleshooting}\label{troubleshooting}

\begin{itemize}
\tightlist
\item
  \texttt{cudaErrorNoKernelImageForDevice}: Toolkit too new/old for your
  GPU; rebuild with proper \texttt{-arch=} or update drivers.
\item
  \texttt{CUBLAS\_STATUS\_ALLOC\_FAILED} / OOM: Lower batch size,
  sequence length, or switch to BF16 if supported.
\item
  Diverging loss with FP16: Increase loss scale (if configurable) or try
  BF16; confirm master-weight updates are in FP32.
\item
  cuDNN errors: Rebuild without \texttt{USE\_CUDNN} to verify the base
  path works, then revisit versions/paths.
\end{itemize}

The takeaway: \texttt{train\_gpt2.cu} is the practical, fast trainer:
mixed precision, optional FlashAttention, and ready to scale. You keep
the same readable training loop while tapping your GPU's tensor cores
for large speedups and much better memory efficiency.

\subsection{\texorpdfstring{7. Starter Artifacts \& Data Prep
(\texttt{dev/download\_starter\_pack.sh},
\texttt{dev/data/})}{7. Starter Artifacts \& Data Prep (dev/download\_starter\_pack.sh, dev/data/)}}\label{starter-artifacts-data-prep-devdownload_starter_pack.sh-devdata}

Before you can actually train or test a model in \emph{llm.c}, you need
a few essential artifacts: the tokenizer, a dataset, and a config. These
files aren't stored in the repo directly (they're too large and often
under different licenses), so the project provides scripts to fetch or
generate them. This is where the \texttt{dev/} folder comes into play.

\subsubsection{The Starter Pack Script}\label{the-starter-pack-script}

The easiest way to get going is with:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{./dev/download\_starter\_pack.sh}
\end{Highlighting}
\end{Shaded}

This script pulls down a ready-made bundle containing:

\begin{itemize}
\tightlist
\item
  \texttt{gpt2\_tokenizer.bin} - The GPT-2 byte-pair encoding (BPE)
  tokenizer in binary format.
\item
  \texttt{train.bin} / \texttt{val.bin} - Pre-tokenized training and
  validation datasets, often based on OpenWebText or Tiny Shakespeare
  for demos.
\item
  Model configs - A JSON or header file that sets hyperparameters like
  layers, hidden size, and number of heads for GPT-2 124M.
\end{itemize}

Think of this as your ``starter kit'': it contains just enough to run a
demo and see training loss decreasing without setting up a full-scale
dataset pipeline yourself.

\subsubsection{\texorpdfstring{The Tokenizer File
(\texttt{gpt2\_tokenizer.bin})}{The Tokenizer File (gpt2\_tokenizer.bin)}}\label{the-tokenizer-file-gpt2_tokenizer.bin}

This is a binary representation of GPT-2's tokenizer vocabulary. It maps
raw text (like \texttt{"Hello\ world"}) into integer token IDs, which
are the actual inputs to the model.

\begin{itemize}
\tightlist
\item
  Why binary? It's faster to load in C than parsing a text-based
  vocabulary.
\item
  Size? \textasciitilde500 KB, representing \textasciitilde50,000
  tokens.
\item
  Role in training? Used in both the dataloader (to prepare inputs) and
  the sampler (to decode outputs).
\end{itemize}

Without this file, the model can't understand text at all-it would just
be manipulating meaningless numbers.

\subsubsection{\texorpdfstring{Dataset Files (\texttt{train.bin},
\texttt{val.bin})}{Dataset Files (train.bin, val.bin)}}\label{dataset-files-train.bin-val.bin}

Each dataset file is a binary blob containing:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A header (about 1 KB) describing sequence length, vocab size, and
  other metadata.
\item
  A stream of token IDs (\texttt{uint16}), representing the text corpus
  already tokenized.
\end{enumerate}

This design means the C dataloader can simply \texttt{fread()} chunks of
tokens into memory, without needing to tokenize text on the fly. It's
fast and memory-efficient, perfect for a lean project like \emph{llm.c}.

The script usually fetches two versions:

\begin{itemize}
\tightlist
\item
  Training set (\texttt{train.bin})
\item
  Validation set (\texttt{val.bin})
\end{itemize}

That way, the training loop can occasionally switch to validation mode
and report a validation loss, helping you track overfitting.

\subsubsection{\texorpdfstring{The \texttt{dev/data/}
Folder}{The dev/data/ Folder}}\label{the-devdata-folder}

If you want to generate your own datasets, this is where you'll find the
tools:

\begin{itemize}
\tightlist
\item
  Scripts for Tiny Shakespeare, OpenWebText, or other corpora.
\item
  Utilities to tokenize text using the GPT-2 tokenizer and write out the
  \texttt{.bin} format.
\item
  Small Python snippets to check dataset statistics (like number of
  tokens or average sequence length).
\end{itemize}

For example, if you wanted to try fine-tuning GPT-2 on your own text
files, you'd:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run a preprocessing script in \texttt{dev/data/} to tokenize and save
  your corpus.
\item
  Point \texttt{train\_gpt2.c} or \texttt{train\_gpt2.cu} to your new
  \texttt{train.bin} and \texttt{val.bin}.
\item
  Kick off training as usual.
\end{enumerate}

\subsubsection{Why Preprocessing
Matters}\label{why-preprocessing-matters}

Tokenization and dataset preparation can be surprisingly heavy in
Python, especially for large corpora. By precomputing everything into
compact \texttt{.bin} files, \emph{llm.c} keeps the runtime training
loop as simple as possible-just reading arrays of integers and feeding
them into the model.

This separation of concerns (preprocessing vs.~training) is what makes
the training code clean and focused.

\subsubsection{Quick Sanity Check}\label{quick-sanity-check}

After running \texttt{download\_starter\_pack.sh}, you should see these
files in your working directory:

\begin{verbatim}
gpt2_tokenizer.bin
train.bin
val.bin
\end{verbatim}

If any are missing, re-run the script. Without them, the trainer will
exit with a file-not-found error.

The takeaway: The starter pack is your ticket to running \emph{llm.c}
right away. It gives you a tokenizer and datasets in exactly the format
the C code expects. Later, when you're ready to train on your own text
or scale up, the \texttt{dev/data/} folder shows you how to prepare
custom datasets the same way.

\subsection{\texorpdfstring{8. Debugging Tips \& IDE Stepping
(\texttt{-g})}{8. Debugging Tips \& IDE Stepping (-g)}}\label{debugging-tips-ide-stepping--g}

Even though \emph{llm.c} is designed to be small and readable, training
a transformer model is still a big program with lots of moving parts.
When something goes wrong-whether it's a segmentation fault,
\texttt{NaN} losses, or unexpected results-you'll want to be able to
debug effectively. That's where debug builds and IDE stepping come in.

\subsubsection{Why Debug Mode Exists}\label{why-debug-mode-exists}

By default, the Makefile compiles with heavy optimization
(\texttt{-O3}). That makes the code run fast, but it also makes
debugging harder:

\begin{itemize}
\tightlist
\item
  Variables may be optimized away.
\item
  Functions might get inlined so you can't step through them clearly.
\item
  The debugger may jump around unpredictably.
\end{itemize}

Adding the \texttt{-g} flag (enabled with \texttt{DEBUG=1}) tells the
compiler to include extra information in the binary so you can see
exactly what the code is doing at runtime.

\subsubsection{Building a Debug Binary}\label{building-a-debug-binary}

To build with debug info:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make}\NormalTok{ train\_gpt2 DEBUG=1}
\end{Highlighting}
\end{Shaded}

This produces a slower executable, but one that works seamlessly with
tools like:

\begin{itemize}
\tightlist
\item
  gdb - the classic GNU debugger.
\item
  lldb - default on macOS.
\item
  VS Code / CLion / Xcode - IDEs with integrated debuggers and GUI
  interfaces.
\end{itemize}

\subsubsection{Using gdb on Linux/macOS}\label{using-gdb-on-linuxmacos}

Start your program under gdb:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gdb}\NormalTok{ ./train\_gpt2}
\end{Highlighting}
\end{Shaded}

Inside gdb:

\begin{itemize}
\tightlist
\item
  Run the program: \texttt{run}
\item
  Set a breakpoint at \texttt{main}: \texttt{break\ main}
\item
  Step through line by line: \texttt{step} or \texttt{next}
\item
  Inspect variables: \texttt{print\ loss}, \texttt{print\ i}
\item
  Quit: \texttt{quit}
\end{itemize}

This is the fastest way to see exactly where a crash happens.

\subsubsection{Using an IDE}\label{using-an-ide}

If command-line debugging feels intimidating, you can use an IDE like VS
Code or CLion:

\begin{itemize}
\tightlist
\item
  Open the project folder.
\item
  Configure the debugger (choose \texttt{gdb} or \texttt{lldb} backend).
\item
  Add breakpoints by clicking next to line numbers.
\item
  Run the debug build (\texttt{train\_gpt2} with \texttt{DEBUG=1}).
\item
  Step through forward pass, backward pass, or optimizer updates.
\end{itemize}

This way, you can visually watch variables update with each step.

\subsubsection{Debugging CUDA Code}\label{debugging-cuda-code}

CUDA debugging is a bit trickier, but still possible:

\begin{itemize}
\tightlist
\item
  \texttt{cuda-gdb} - NVIDIA's GPU debugger, works like gdb but supports
  stepping into kernels.
\item
  Nsight Systems / Nsight Compute - graphical profilers/debuggers that
  let you trace kernel launches, memory transfers, and GPU utilization.
\end{itemize}

If your CUDA code crashes with cryptic messages like
\texttt{illegal\ memory\ access}, \texttt{cuda-gdb} can help pinpoint
the kernel and even the exact line.

\subsubsection{Debugging Common Issues in
llm.c}\label{debugging-common-issues-in-llm.c}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  File not found → Make sure \texttt{gpt2\_tokenizer.bin},
  \texttt{train.bin}, and \texttt{val.bin} are downloaded.
\item
  Segfault at malloc/fread → Check file paths and dataset sizes.
\item
  Loss becomes NaN →

  \begin{itemize}
  \tightlist
  \item
    On CPU: check for division by zero in normalization.
  \item
    On GPU: check loss scaling (mixed precision) or try FP32 path for
    comparison.
  \end{itemize}
\item
  Mismatch with PyTorch tests → Run \texttt{test\_gpt2} or
  \texttt{test\_gpt2cu} and compare outputs; this usually isolates
  whether the bug is in forward pass, backward pass, or optimizer.
\end{enumerate}

\subsubsection{Logging \& Sanity Checks}\label{logging-sanity-checks}

When debugging, it helps to add extra logging. The repo already has a
lightweight logger, but you can also sprinkle \texttt{printf}s (on CPU)
or \texttt{cudaDeviceSynchronize();\ printf(...)} (on GPU) to track
values. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{printf}\OperatorTok{(}\StringTok{"Step }\SpecialCharTok{\%d}\StringTok{: loss=}\SpecialCharTok{\%f\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ step}\OperatorTok{,}\NormalTok{ loss}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

Sometimes the quickest fix is just to print and see what's going on.

\subsubsection{Best Practices for
Beginners}\label{best-practices-for-beginners}

\begin{itemize}
\tightlist
\item
  Start with the CPU build when learning-it's easier to debug than CUDA.
\item
  Always keep a small dataset (like Tiny Shakespeare) for fast
  iteration.
\item
  Compare against the PyTorch reference for the same batch to catch
  subtle errors.
\item
  Use \texttt{DEBUG=1} whenever you hit strange behavior-you'll trade
  speed for clarity, which is usually worth it when learning.
\end{itemize}

The takeaway: Debug builds (\texttt{-g}) turn \emph{llm.c} from a black
box into a step-through learning tool. With gdb, lldb, or an IDE, you
can pause at any line, inspect variables, and understand exactly how
GPT-2 training works inside C or CUDA. It's slower, but it's the
clearest way to learn and fix issues.

\subsection{9. Project Constraints \& Readability
Contract}\label{project-constraints-readability-contract}

The \emph{llm.c} project isn't trying to be the fastest or most
feature-rich GPT-2 trainer. Instead, it has a very deliberate set of
constraints-rules the author imposes on the codebase to keep it
approachable and educational. You can think of these as the ``contract''
between the code and the reader: certain things are kept simple on
purpose, even if they cost some performance.

\subsubsection{Minimalism over
Optimizations}\label{minimalism-over-optimizations}

\begin{itemize}
\tightlist
\item
  No processor-specific intrinsics: You won't see AVX, NEON, or other
  hardware-tuned assembly calls in the CPU path.
\item
  No fancy template metaprogramming: Unlike in C++ frameworks, here you
  get plain C structs and functions.
\item
  No exotic libraries: Aside from cuBLAS/cuDNN for GPU acceleration,
  most functionality is implemented directly.
\end{itemize}

This means the code runs almost anywhere, and you don't need to
understand deep compiler tricks to follow what's going on.

\subsubsection{Transparency over
Abstraction}\label{transparency-over-abstraction}

\begin{itemize}
\tightlist
\item
  Every operation is visible in the source. For example, instead of
  calling a framework function like \texttt{nn.CrossEntropyLoss}, you'll
  find an explicit forward and backward pass coded in C.
\item
  Data loading, tokenization, optimizer steps, and schedulers are all
  implemented as separate, small modules in \texttt{llmc/}.
\item
  You don't need to guess what's happening-if you're curious, you can
  open the corresponding \texttt{.h} file and see the exact code.
\end{itemize}

The guiding idea: if something is central to training GPT-2, you should
be able to read and understand it.

\subsubsection{Performance Where It Matters (but No
More)}\label{performance-where-it-matters-but-no-more}

\begin{itemize}
\tightlist
\item
  OpenMP pragmas are allowed in CPU builds, because they give large
  speedups with minimal extra code.
\item
  cuBLAS/cuDNN are used for GPU matmuls and attention, because
  re-implementing them would be a distraction and would make the project
  impossibly large.
\item
  But the project avoids unnecessary complexity-no kernel fusion, no
  elaborate caching layers, no half-implemented ``framework''
  abstractions.
\end{itemize}

This balance ensures you can still run experiments at a reasonable
speed, but the code never sacrifices readability.

\subsubsection{Educational First}\label{educational-first}

The code is written to teach, not to win benchmarks. That means:

\begin{itemize}
\tightlist
\item
  Variable names are descriptive, not cryptic.
\item
  Comments explain not just \emph{what} happens, but also \emph{why}.
\item
  Files are kept small and focused, rather than sprawling across dozens
  of layers of abstraction.
\item
  There's a matching PyTorch reference implementation so you can always
  check your understanding against a familiar baseline.
\end{itemize}

\subsubsection{Limitations You Should
Expect}\label{limitations-you-should-expect}

\begin{itemize}
\tightlist
\item
  Training is slower than PyTorch/XLA/JAX or DeepSpeed-tuned runs.
\item
  Multi-GPU scaling is functional but not heavily optimized.
\item
  Only GPT-2 architectures are covered-don't expect GPT-3 or transformer
  variants.
\item
  Features like dataset streaming, checkpoint sharding, or advanced
  distributed tricks are intentionally left out.
\end{itemize}

These are not bugs-they're conscious trade-offs to keep the codebase
small, sharp, and didactic.

\subsubsection{Why This Matters for You}\label{why-this-matters-for-you}

If you're learning how transformers work, this contract is a gift:

\begin{itemize}
\tightlist
\item
  You won't get lost in performance hacks.
\item
  You won't fight through an abstraction jungle.
\item
  You'll always know that what you're reading is close to the ``pure''
  algorithmic idea.
\end{itemize}

On the flip side, if you're aiming for production-grade speed, you'll
need to layer more on top. But that's outside the mission of
\emph{llm.c}.

The takeaway: \emph{llm.c} is bound by a readability contract: clarity
over raw speed, transparency over abstraction, minimalism over
complexity. These constraints keep the project small enough to fit in
your head, while still powerful enough to reproduce GPT-2 training. It's
a teaching lab, not a racing car-and that's exactly why it's valuable.

\subsection{10. Community, Discussions, and Learning
Path}\label{community-discussions-and-learning-path}

The last piece of the quickstart isn't about code at all-it's about the
people and resources around the project. \emph{llm.c} has grown into
more than just a single repository; it has become a meeting point for
learners, tinkerers, and researchers who want to strip large language
models down to their essentials. Understanding this community layer is
just as important as understanding the code itself.

\subsubsection{Discussions and Issues on
GitHub}\label{discussions-and-issues-on-github}

The project's Discussions tab is full of valuable context:

\begin{itemize}
\tightlist
\item
  Developers asking about build errors on different platforms (Linux,
  macOS, Windows).
\item
  Explorations of how to extend \emph{llm.c} to train larger GPT-2
  models (355M, 774M, 1.6B).
\item
  Reports on multi-GPU and MPI runs, including troubleshooting NCCL
  hangs and performance bottlenecks.
\item
  Debates on mixed precision vs FP32 vs BF16 stability.
\end{itemize}

Reading these threads is like looking over the shoulders of hundreds of
other learners. You'll see not only the official answers but also the
thought process of people solving problems in real time.

\subsubsection{Roadmap and
Contributions}\label{roadmap-and-contributions}

The README and issues sometimes hint at where the project might grow:

\begin{itemize}
\tightlist
\item
  Making the CUDA kernels more modular in \texttt{dev/cuda/}.
\item
  Simplifying multi-GPU startup for clusters.
\item
  Adding small tutorial-style docs (like the LayerNorm walkthrough).
\end{itemize}

The project is open to contributions, but it follows the same minimalist
philosophy. If you're thinking of contributing, remember: the goal is
clarity first, performance second.

\subsubsection{External Learning
Resources}\label{external-learning-resources}

While \emph{llm.c} is self-contained, it pairs nicely with outside
material:

\begin{itemize}
\tightlist
\item
  The PyTorch reference implementation in \texttt{train\_gpt2.py} is
  your canonical oracle for correctness.
\item
  The GPT-2 paper gives the architecture background.
\item
  CUDA and cuBLAS/cuDNN docs explain the GPU APIs that the project calls
  into.
\item
  Community blog posts often walk through specific sections of the code
  in plain English, making it easier to digest.
\end{itemize}

By combining the code, the paper, and these resources, you can
triangulate a much deeper understanding.

\subsubsection{A Suggested Learning
Path}\label{a-suggested-learning-path}

If you're coming to \emph{llm.c} as a beginner, here's a natural
progression:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run the CPU trainer (\texttt{train\_gpt2.c}) on Tiny Shakespeare.
  Watch the loss decrease.
\item
  Step through the code with \texttt{DEBUG=1}, confirming that you
  understand forward, backward, and optimizer steps.
\item
  Move to the FP32 CUDA trainer to see how the same loop runs on GPU.
\item
  Switch to the modern CUDA trainer (\texttt{train\_gpt2.cu}) and learn
  how mixed precision works.
\item
  Experiment with dataset scripts in \texttt{dev/data/}-try your own
  text corpus.
\item
  Read the LayerNorm doc in \texttt{doc/} to deepen your theory-practice
  connection.
\item
  Explore multi-GPU runs with MPI/NCCL if you have access to multiple
  GPUs.
\item
  Follow GitHub Discussions for real-world debugging and scaling
  stories.
\end{enumerate}

\subsubsection{Why This Matters}\label{why-this-matters}

Code alone is not enough. The community context, the discussions, and
the learning path make \emph{llm.c} a living project. By engaging with
them, you avoid the feeling of learning in isolation. You'll see others
wrestling with the same challenges, and you'll have a clearer sense of
what to try next.

The takeaway: Beyond the files and scripts, \emph{llm.c} is a
community-driven learning environment. GitHub issues, discussions,
reference docs, and external tutorials all form part of the ``extended
classroom.'' If the code is the lab bench, the community is the set of
lab partners who help you figure things out along the way.

\section{Chapter 2. Data, Tokenization, and
Loaders}\label{chapter-2.-data-tokenization-and-loaders}

\subsection{\texorpdfstring{11. GPT-2 Tokenizer Artifacts
(\texttt{gpt2\_tokenizer.bin})}{11. GPT-2 Tokenizer Artifacts (gpt2\_tokenizer.bin)}}\label{gpt-2-tokenizer-artifacts-gpt2_tokenizer.bin}

A language model like GPT-2 doesn't directly understand English,
Vietnamese, or any other natural language. Instead, it understands
numbers. These numbers are called tokens. A tokenizer is the tool that
translates between human text and tokens. In \emph{llm.c}, the GPT-2
tokenizer is stored in a small file called \texttt{gpt2\_tokenizer.bin}.
This file is the key that lets the model read input text and produce
output text that we can understand.

\subsubsection{What This File Contains}\label{what-this-file-contains}

The file \texttt{gpt2\_tokenizer.bin} is a binary version of GPT-2's
tokenizer. It includes:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.2385}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.7615}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Byte vocabulary (0--255) & Makes sure every possible character can be
represented. \\
Merge rules (BPE) & Combines frequent sequences like ``ing'' or '' the''
into single tokens for efficiency. \\
Vocabulary size (\textasciitilde50,257) & Defines how many distinct
tokens GPT-2 can work with. \\
Mapping IDs ↔ text & Lets the program turn model outputs (numbers) back
into human-readable strings. \\
\end{longtable}

Instead of being written as JSON or text, the tokenizer is stored in
binary form. This allows \emph{llm.c} to load it very quickly using a
simple file read, which keeps the code clean and fast.

\subsubsection{Where It Comes From}\label{where-it-comes-from}

You don't need to build this file by hand. The repository provides a
script to download it, along with small training and validation
datasets:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{./dev/download\_starter\_pack.sh}
\end{Highlighting}
\end{Shaded}

After running the script, you should see \texttt{gpt2\_tokenizer.bin},
\texttt{train.bin}, and \texttt{val.bin} in your working directory. If
the tokenizer is missing, the program cannot run because it won't know
how to interpret text.

\subsubsection{How the Code Uses It}\label{how-the-code-uses-it}

During training, the tokenizer is not active because the datasets
(\texttt{train.bin} and \texttt{val.bin}) are already pre-tokenized into
integers. This keeps the training loop fast and simple.

During sampling or evaluation, the tokenizer becomes important again.
After the model predicts a sequence of token IDs, the tokenizer
translates those numbers back into text that you can read on your
screen.

The C API for the tokenizer, defined in \texttt{llmc/tokenizer.h},
provides just three main functions:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{int}\NormalTok{ tokenizer\_init}\OperatorTok{(}\NormalTok{Tokenizer }\OperatorTok{*}\NormalTok{t}\OperatorTok{,} \DataTypeTok{const} \DataTypeTok{char} \OperatorTok{*}\NormalTok{filename}\OperatorTok{);}
\DataTypeTok{int}\NormalTok{ tokenizer\_decode}\OperatorTok{(}\NormalTok{Tokenizer }\OperatorTok{*}\NormalTok{t}\OperatorTok{,} \DataTypeTok{const} \DataTypeTok{int} \OperatorTok{*}\NormalTok{ids}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ n}\OperatorTok{,} \DataTypeTok{char} \OperatorTok{*}\NormalTok{out}\OperatorTok{);}
\DataTypeTok{void}\NormalTok{ tokenizer\_free}\OperatorTok{(}\NormalTok{Tokenizer }\OperatorTok{*}\NormalTok{t}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

This is all you need: initialize the tokenizer from the file, decode
tokens into text, and free memory when done.

\subsubsection{Example Workflow in
Practice}\label{example-workflow-in-practice}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Initialize the tokenizer:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tokenizer t}\OperatorTok{;}
\NormalTok{tokenizer\_init}\OperatorTok{(\&}\NormalTok{t}\OperatorTok{,} \StringTok{"gpt2\_tokenizer.bin"}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}
\item
  Decode a sequence of tokens back to text:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{char}\NormalTok{ buf}\OperatorTok{[}\DecValTok{512}\OperatorTok{];}
\NormalTok{tokenizer\_decode}\OperatorTok{(\&}\NormalTok{t}\OperatorTok{,}\NormalTok{ tokens}\OperatorTok{,}\NormalTok{ ntokens}\OperatorTok{,}\NormalTok{ buf}\OperatorTok{);}
\NormalTok{printf}\OperatorTok{(}\StringTok{"}\SpecialCharTok{\%s\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ buf}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}
\item
  Clean up memory when you no longer need it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tokenizer\_free}\OperatorTok{(\&}\NormalTok{t}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

This small cycle is enough to turn model outputs into readable
sentences.

\subsubsection{Why It Matters}\label{why-it-matters}

Without the tokenizer, the model cannot communicate. The tokenizer is
like a shared dictionary between humans and the neural network. If you
give the model text, the tokenizer converts it into numbers the model
understands. When the model responds, the tokenizer converts its numbers
back into text. If the tokenizer does not match the dataset, the model's
predictions will come out as gibberish. Keeping the tokenizer and
dataset in sync is essential for correct training and evaluation.

\subsubsection{Try It Yourself}\label{try-it-yourself}

Here are a few small exercises you can do to understand the tokenizer
better:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Check that the file exists: After running the starter pack script,
  verify that \texttt{gpt2\_tokenizer.bin} is in your directory. Try
  running the trainer without it and observe the error message.
\item
  Inspect vocab size: Run the trainer and look for the line that prints
  \texttt{vocab\_size:\ 50257}. Compare this with
  \texttt{padded\_vocab\_size:\ 50304}. Why do you think padding helps
  GPUs?
\item
  Decode a sequence manually: Write a short C program that loads the
  tokenizer and decodes a fixed list of token IDs (for example
  \texttt{{[}464,\ 3290,\ 318{]}}). Observe what text you get.
\item
  Mismatch experiment: If you build your own dataset with a different
  tokenizer (say, a custom vocabulary), try decoding it with
  \texttt{gpt2\_tokenizer.bin}. Notice how the output becomes
  meaningless, showing why consistency matters.
\item
  Dataset + tokenizer link: Open \texttt{train.bin} in a hex viewer.
  You'll see it's just numbers. Use the tokenizer to decode the first
  few hundred tokens and see real text emerge.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway}

\texttt{gpt2\_tokenizer.bin} is a tiny but vital file. It is the bridge
that allows the model and humans to speak the same language. Training is
efficient because all data is pre-tokenized, and when you want to see
what the model has written, the tokenizer turns raw numbers back into
words. Without it, the entire system would be silent.

\subsection{\texorpdfstring{12. Binary Dataset Format
(\texttt{train.bin} and
\texttt{val.bin})}{12. Binary Dataset Format (train.bin and val.bin)}}\label{binary-dataset-format-train.bin-and-val.bin}

Just like the tokenizer turns text into numbers, the datasets in
\emph{llm.c} are stored as numbers too. Instead of reading plain text
files like \texttt{.txt}, the training and validation data are kept in
simple binary files: \texttt{train.bin} and \texttt{val.bin}. These
files are the fuel for the training loop.

\subsubsection{What These Files Look
Like}\label{what-these-files-look-like}

At first glance, \texttt{train.bin} and \texttt{val.bin} look like
unreadable blobs if you open them in a text editor. That's because they
are not meant to be human-readable. They contain:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.3137}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.6863}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Part
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
A tiny header (about 1 KB) & Stores metadata such as sequence length and
vocab size. \\
A stream of token IDs (\texttt{uint16}) & Every tokenized word piece
from the dataset, saved as 16-bit integers. \\
\end{longtable}

Each integer represents one token from the tokenizer's vocabulary. Since
GPT-2 has a vocabulary of about 50,000 tokens, 16-bit integers
(\texttt{uint16\_t}) are enough to store them all.

\subsubsection{Why Binary Format?}\label{why-binary-format}

\begin{itemize}
\tightlist
\item
  Efficiency: Instead of re-tokenizing text every time, the data is
  pre-tokenized once and stored as numbers. The trainer just reads them
  directly.
\item
  Speed: Reading integers from a file is faster than parsing and
  processing raw text.
\item
  Simplicity: The training loop only has to deal with arrays of
  integers-no string handling, no parsing, no surprises.
\end{itemize}

This choice makes the training code in \emph{llm.c} much cleaner and
faster.

\subsubsection{How the Dataloader Uses
Them}\label{how-the-dataloader-uses-them}

When training starts, the dataloader reads chunks of numbers from
\texttt{train.bin}. Each chunk corresponds to one batch of size B × T:

\begin{itemize}
\tightlist
\item
  B = batch size (number of examples in a batch).
\item
  T = sequence length (number of tokens per example).
\end{itemize}

For example, if \texttt{B\ =\ 8} and \texttt{T\ =\ 1024}, the dataloader
will read \texttt{8\ ×\ 1024\ =\ 8192} token IDs from the file, reshape
them into sequences, and feed them to the model.

The validation file (\texttt{val.bin}) works the same way but is only
used occasionally during training to measure validation loss. This helps
detect overfitting.

\subsubsection{Workflow in Code}\label{workflow-in-code}

Inside the repo, you'll see functions like these in
\texttt{llmc/dataloader.h}:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{int}\NormalTok{ dataloader\_init}\OperatorTok{(}\NormalTok{Dataloader }\OperatorTok{*}\NormalTok{loader}\OperatorTok{,} \DataTypeTok{const} \DataTypeTok{char} \OperatorTok{*}\NormalTok{filename}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ B}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ T}\OperatorTok{);}
\DataTypeTok{int}\NormalTok{ dataloader\_next\_batch}\OperatorTok{(}\NormalTok{Dataloader }\OperatorTok{*}\NormalTok{loader}\OperatorTok{,} \DataTypeTok{int} \OperatorTok{*}\NormalTok{inputs}\OperatorTok{,} \DataTypeTok{int} \OperatorTok{*}\NormalTok{targets}\OperatorTok{);}
\DataTypeTok{void}\NormalTok{ dataloader\_reset}\OperatorTok{(}\NormalTok{Dataloader }\OperatorTok{*}\NormalTok{loader}\OperatorTok{);}
\DataTypeTok{void}\NormalTok{ dataloader\_free}\OperatorTok{(}\NormalTok{Dataloader }\OperatorTok{*}\NormalTok{loader}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

Here's what happens step by step:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize with the binary file and batch/sequence sizes.
\item
  Next batch reads the next B × T tokens into an input array and a
  target array.
\item
  Reset allows re-reading from the beginning when you start a new epoch.
\item
  Free cleans up resources when training ends.
\end{enumerate}

The target array is simply the same sequence shifted by one
token-because language modeling predicts the \emph{next} token.

\subsubsection{Why It Matters}\label{why-it-matters-1}

The dataset format is what makes \emph{llm.c} practical. Without it, the
code would need to handle messy text, encodings, and tokenization during
every training step. By storing clean arrays of token IDs, the training
loop becomes very short and easy to follow. It's a design decision that
keeps the project minimal yet faithful to real training pipelines.

\subsubsection{Try It Yourself}\label{try-it-yourself-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Check file size: Run \texttt{ls\ -lh\ train.bin} and notice how large
  it is compared to a plain \texttt{.txt} file. Why is it smaller or
  larger?
\item
  Peek inside: Use a hex viewer
  (\texttt{xxd\ train.bin\ \textbar{}\ head}) to see raw numbers. They
  won't look like text, but they are the tokens the model trains on.
\item
  Count tokens: Write a short Python or C script to count how many token
  IDs are stored in \texttt{train.bin}. This gives you a sense of
  dataset size.
\item
  Mini-dataset: Try generating your own dataset from a small
  \texttt{.txt} file using the scripts in \texttt{dev/data/}. See how
  the \texttt{.bin} file is created.
\item
  Validation experiment: During training, reduce the validation set to
  only a few batches and observe how the validation loss stabilizes or
  fluctuates compared to training loss.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-1}

\texttt{train.bin} and \texttt{val.bin} may look like gibberish, but
they are carefully prepared binary files containing token IDs. They make
training faster, simpler, and more reproducible. The dataloader in
\emph{llm.c} reads these numbers in neat chunks and serves them directly
to the model, letting you focus on learning how transformers work
instead of wrestling with raw text parsing.

\subsection{\texorpdfstring{13. Dataset Scripts in
\texttt{dev/data/}}{13. Dataset Scripts in dev/data/}}\label{dataset-scripts-in-devdata}

The repository doesn't just give you ready-made binary datasets like
\texttt{train.bin} and \texttt{val.bin}. It also provides scripts inside
the \texttt{dev/data/} folder that show you how to create your own.
These scripts are important because they demonstrate how raw text gets
transformed into the binary format that the dataloader in \emph{llm.c}
expects.

\subsubsection{\texorpdfstring{What's Inside
\texttt{dev/data/}}{What's Inside dev/data/}}\label{whats-inside-devdata}

This folder contains small Python scripts that:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.2474}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.7526}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Script
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{prepare\_shakespeare.py} & Turns the Tiny Shakespeare dataset
into \texttt{train.bin} and \texttt{val.bin}. \\
\texttt{prepare\_openwebtext.py} & Prepares a large-scale dataset
similar to the one GPT-2 was trained on. \\
Other helpers & Tokenize raw \texttt{.txt} files, split them into
train/val, and save to binary. \\
\end{longtable}

Each script follows the same basic recipe:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Read raw text from a source file.
\item
  Apply the GPT-2 tokenizer to turn text into token IDs.
\item
  Split the tokens into training and validation portions.
\item
  Write the IDs into binary files that \emph{llm.c} can read directly.
\end{enumerate}

\subsubsection{Why Preprocessing Happens Outside
C}\label{why-preprocessing-happens-outside-c}

In C, handling text files with Unicode, punctuation, and different
encodings is messy. Instead, preprocessing is done once in Python, where
tokenizers are easier to use. The results are saved in a simple binary
format (\texttt{uint16} IDs). From then on, C only has to deal with
arrays of integers-clean and efficient.

This design keeps the training loop minimal: no text parsing, no string
handling, just numbers.

\subsubsection{Example: Tiny
Shakespeare}\label{example-tiny-shakespeare}

One of the simplest datasets is Tiny Shakespeare, about 1 MB of text
from Shakespeare's plays. The script \texttt{prepare\_shakespeare.py}
will:

\begin{itemize}
\tightlist
\item
  Read \texttt{input.txt} (the raw text).
\item
  Use the GPT-2 tokenizer (\texttt{gpt2\_tokenizer.bin}) to turn every
  word and symbol into token IDs.
\item
  Split 90\% of the data into \texttt{train.bin} and 10\% into
  \texttt{val.bin}.
\end{itemize}

After running the script, you'll have small binary files that let you
train GPT-2 from scratch in minutes on CPU or GPU.

\subsubsection{Example: OpenWebText}\label{example-openwebtext}

The script \texttt{prepare\_openwebtext.py} shows how to tokenize a much
larger dataset, closer to what GPT-2 was originally trained on. This is
heavier and requires more disk space, but it's useful if you want to try
scaling up training to bigger models.

\subsubsection{Why It Matters}\label{why-it-matters-2}

These scripts are more than convenience tools-they are examples of how
to adapt llm.c to your own data. If you have a collection of emails,
poems, or programming code, you can:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Put them into a single \texttt{.txt} file.
\item
  Modify one of the scripts in \texttt{dev/data/}.
\item
  Generate new \texttt{train.bin} and \texttt{val.bin} files.
\item
  Train GPT-2 on your own text.
\end{enumerate}

By separating dataset creation from training, \emph{llm.c} keeps the C
code small and makes experimentation flexible.

\subsubsection{Try It Yourself}\label{try-it-yourself-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Run the Shakespeare script:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{python}\NormalTok{ dev/data/prepare\_shakespeare.py}
\end{Highlighting}
\end{Shaded}

  Then check that \texttt{train.bin} and \texttt{val.bin} were created.
\item
  Open the binary files with a hex viewer and confirm that they contain
  only numbers.
\item
  Modify the script to tokenize a different text file (for example, your
  own writing).
\item
  Compare dataset sizes: Tiny Shakespeare is tiny (MBs), OpenWebText is
  huge (GBs). Observe how training speed changes depending on dataset
  size.
\item
  Re-run training with your custom dataset and watch how the model
  starts generating text in your style.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-2}

The \texttt{dev/data/} scripts are the bridge between raw human text and
the binary datasets used in training. They let you prepare small demo
datasets or scale up to larger corpora. By experimenting with these
scripts, you learn how to bring your own data into \emph{llm.c} and
train a GPT-style model on anything you like.

\subsection{14. DataLoader Design (Batching, Strides,
Epochs)}\label{dataloader-design-batching-strides-epochs}

Now that the datasets are prepared as \texttt{.bin} files, we need a way
to feed them into the model during training. This is the job of the
DataLoader in \emph{llm.c}. You'll find its interface in
\texttt{llmc/dataloader.h}, and its purpose is very simple: take a big
stream of token IDs from \texttt{train.bin} or \texttt{val.bin}, cut it
into manageable chunks, and serve those chunks to the training loop as
batches.

\subsubsection{The Core Idea}\label{the-core-idea}

Training a language model requires two arrays for every batch:

\begin{itemize}
\tightlist
\item
  Inputs: a sequence of token IDs, like
  \texttt{{[}The,\ cat,\ sat,\ on{]}}
\item
  Targets: the same sequence shifted by one, like
  \texttt{{[}cat,\ sat,\ on,\ the{]}}
\end{itemize}

The model learns to predict each next token in the sequence. The
DataLoader automates slicing these arrays out of the giant dataset file.

\subsubsection{The Interface}\label{the-interface}

In the code you'll see function declarations like these:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{int}\NormalTok{ dataloader\_init}\OperatorTok{(}\NormalTok{Dataloader }\OperatorTok{*}\NormalTok{loader}\OperatorTok{,} \DataTypeTok{const} \DataTypeTok{char} \OperatorTok{*}\NormalTok{filename}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ B}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ T}\OperatorTok{);}
\DataTypeTok{int}\NormalTok{ dataloader\_next\_batch}\OperatorTok{(}\NormalTok{Dataloader }\OperatorTok{*}\NormalTok{loader}\OperatorTok{,} \DataTypeTok{int} \OperatorTok{*}\NormalTok{inputs}\OperatorTok{,} \DataTypeTok{int} \OperatorTok{*}\NormalTok{targets}\OperatorTok{);}
\DataTypeTok{void}\NormalTok{ dataloader\_reset}\OperatorTok{(}\NormalTok{Dataloader }\OperatorTok{*}\NormalTok{loader}\OperatorTok{);}
\DataTypeTok{void}\NormalTok{ dataloader\_free}\OperatorTok{(}\NormalTok{Dataloader }\OperatorTok{*}\NormalTok{loader}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

Here's what each does:

\begin{itemize}
\tightlist
\item
  \texttt{dataloader\_init}: opens the dataset file, remembers batch
  size \texttt{B} and sequence length \texttt{T}.
\item
  \texttt{dataloader\_next\_batch}: returns the next chunk of
  \texttt{B\ ×\ T} tokens (inputs) and their shifted version (targets).
\item
  \texttt{dataloader\_reset}: rewinds to the start of the file when an
  epoch ends.
\item
  \texttt{dataloader\_free}: closes the file and releases memory.
\end{itemize}

This design keeps the training loop clean: just call
\texttt{next\_batch} and you get the data ready for forward/backward
passes.

\subsubsection{B × T Explained}\label{b-t-explained}

The two most important parameters are:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Symbol & Meaning & Example \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
B & Batch size (how many sequences per step) & 16 \\
T & Sequence length (how many tokens per sequence) & 1024 \\
\end{longtable}

So one batch contains \texttt{B\ ×\ T} tokens. For example, with
\texttt{B\ =\ 16} and \texttt{T\ =\ 1024}, each batch holds 16,384
tokens. The DataLoader simply reads that many numbers from the binary
file and arranges them in memory.

\subsubsection{Strides Through the
Dataset}\label{strides-through-the-dataset}

As you call \texttt{dataloader\_next\_batch}, the loader moves forward
through the dataset by \texttt{B\ ×\ T} tokens each time. When it
reaches the end of the dataset file, it either:

\begin{itemize}
\tightlist
\item
  Resets back to the beginning (\texttt{dataloader\_reset}), or
\item
  Switches from training to validation, depending on the training loop's
  needs.
\end{itemize}

This stride-based reading is efficient: no random access, just
sequential reads from a file.

\subsubsection{Epochs and Shuffling}\label{epochs-and-shuffling}

In deep learning, an epoch means one full pass through the dataset. The
DataLoader in \emph{llm.c} is simple: it goes linearly from start to
finish. It doesn't shuffle data like PyTorch's \texttt{DataLoader}. Why?
Because language data is already very diverse, and the project values
minimal code over extra features. If you want shuffling, you can
preprocess the dataset differently before creating \texttt{.bin} files.

\subsubsection{Why It Matters}\label{why-it-matters-3}

The DataLoader is the quiet workhorse of training. It ensures that every
step sees a fresh batch of token sequences, always with matching inputs
and targets. By separating dataset reading from the training loop, the
code stays clean and focused. This design also makes it easy to swap
datasets-once you generate a \texttt{.bin} file, the loader doesn't care
where it came from.

\subsubsection{Try It Yourself}\label{try-it-yourself-3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Print the first batch: Modify the code to print the first 20 input
  tokens and their targets. See how each input token aligns with the
  next target token.
\item
  Experiment with B and T: Set \texttt{B\ =\ 2} and \texttt{T\ =\ 8} and
  observe how the loader slices the dataset into tiny chunks. Then try
  larger values and see how memory usage changes.
\item
  Check epoch length: Write a small loop to count how many batches you
  get before \texttt{dataloader\_reset} is called. Does this match the
  total tokens divided by \texttt{B\ ×\ T}?
\item
  Validation check: Observe how often the training loop switches to
  \texttt{val.bin}. How does validation loss compare to training loss
  over time?
\item
  Custom stride: Modify the code so the DataLoader skips some tokens
  between batches. What effect does this have on training?
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-3}

The DataLoader in \emph{llm.c} is intentionally simple. It streams token
IDs in fixed-sized batches, moves forward stride by stride, and resets
when done. This straightforward design avoids complexity and keeps the
focus on the model itself, while still teaching you the essential
mechanics of batching and sequence handling in language model training.

\subsection{15. EvalLoader and Validation
Workflow}\label{evalloader-and-validation-workflow}

Training a model isn't just about watching the training loss go down. To
know whether the model is actually learning patterns that generalize-and
not just memorizing the training data-you need to run validation. In
\emph{llm.c}, validation is handled by a component called the
EvalLoader, which works just like the DataLoader but reads from the
validation dataset (\texttt{val.bin}) instead of the training dataset
(\texttt{train.bin}).

\subsubsection{Why We Need Validation}\label{why-we-need-validation}

Imagine teaching a student only by drilling them with the same math
problems over and over. They might get really good at those problems,
but fail completely when given new ones. Validation is like giving the
student a pop quiz with unseen questions. If they do well, you know
they've actually learned the concepts.

For language models, validation helps detect overfitting: when the
training loss keeps improving but the validation loss stays flat or even
gets worse.

\subsubsection{How EvalLoader Works}\label{how-evalloader-works}

EvalLoader lives in the same code file as the DataLoader
(\texttt{llmc/dataloader.h}), but it points to a different dataset file.
Its workflow is nearly identical:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Open \texttt{val.bin} and prepare for reading.
\item
  Serve up batches of size \texttt{B\ ×\ T} (batch size × sequence
  length).
\item
  Provide inputs and targets the same way as the training DataLoader.
\item
  Reset after one full pass through the file.
\end{enumerate}

The training loop typically calls the EvalLoader at intervals-for
example, every few hundred steps-so you get a snapshot of validation
loss during training.

\subsubsection{What Happens During
Validation}\label{what-happens-during-validation}

When validation is triggered:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The current model parameters are frozen (no gradient updates).
\item
  A few batches are read from \texttt{val.bin}.
\item
  The model runs forward passes only, computing the loss on each batch.
\item
  The losses are averaged and reported as validation loss.
\end{enumerate}

This doesn't take long because it usually samples just a subset of the
validation dataset, not the entire file.

\subsubsection{Training Loop with
Validation}\label{training-loop-with-validation}

In pseudocode, the loop looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for} \OperatorTok{(}\NormalTok{step }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ step }\OperatorTok{\textless{}}\NormalTok{ max\_steps}\OperatorTok{;}\NormalTok{ step}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{    dataloader\_next\_batch}\OperatorTok{(\&}\NormalTok{train\_loader}\OperatorTok{,}\NormalTok{ inputs}\OperatorTok{,}\NormalTok{ targets}\OperatorTok{);}
\NormalTok{    forward\_backward\_update}\OperatorTok{(}\NormalTok{model}\OperatorTok{,}\NormalTok{ inputs}\OperatorTok{,}\NormalTok{ targets}\OperatorTok{);}
    
    \ControlFlowTok{if} \OperatorTok{(}\NormalTok{step }\OperatorTok{\%}\NormalTok{ eval\_interval }\OperatorTok{==} \DecValTok{0}\OperatorTok{)} \OperatorTok{\{}
        \DataTypeTok{float}\NormalTok{ val\_loss }\OperatorTok{=} \FloatTok{0.0}\BuiltInTok{f}\OperatorTok{;}
        \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}}\NormalTok{ eval\_batches}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{            evalloader\_next\_batch}\OperatorTok{(\&}\NormalTok{val\_loader}\OperatorTok{,}\NormalTok{ inputs}\OperatorTok{,}\NormalTok{ targets}\OperatorTok{);}
\NormalTok{            val\_loss }\OperatorTok{+=}\NormalTok{ forward\_only}\OperatorTok{(}\NormalTok{model}\OperatorTok{,}\NormalTok{ inputs}\OperatorTok{,}\NormalTok{ targets}\OperatorTok{);}
        \OperatorTok{\}}
\NormalTok{        val\_loss }\OperatorTok{/=}\NormalTok{ eval\_batches}\OperatorTok{;}
\NormalTok{        printf}\OperatorTok{(}\StringTok{"step }\SpecialCharTok{\%d}\StringTok{: val loss }\SpecialCharTok{\%.4f\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ step}\OperatorTok{,}\NormalTok{ val\_loss}\OperatorTok{);}
    \OperatorTok{\}}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

This is simplified, but it shows the idea: the validation loop is nested
inside the training loop, running occasionally instead of every step.

\subsubsection{Why It Matters}\label{why-it-matters-4}

Validation is the reality check of training. Without it, you could train
forever and celebrate low training losses, only to discover that your
model produces nonsense on new text. By tracking validation loss, you
can:

\begin{itemize}
\tightlist
\item
  Detect overfitting early.
\item
  Adjust hyperparameters (like learning rate or batch size).
\item
  Know when training has plateaued and it's time to stop.
\end{itemize}

In professional setups, validation curves are often plotted live, but in
\emph{llm.c}, the minimalist approach is to just print numbers to the
console.

\subsubsection{Try It Yourself}\label{try-it-yourself-4}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Watch val loss: Run training and note how validation loss compares to
  training loss. Do they both decrease together?
\item
  Overfitting demo: Train on a very tiny dataset (like 10 KB of text).
  Notice how training loss plummets but validation loss stalls or rises.
\item
  Change eval interval: Reduce \texttt{eval\_interval} so validation
  runs every step. How much slower does training feel?
\item
  Change eval batches: Set \texttt{eval\_batches} to 1 vs 100. What
  difference does this make in the stability of the reported validation
  loss?
\item
  Validation as stopping rule: Stop training when validation loss stops
  improving for many intervals. How does this affect final performance?
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-4}

The EvalLoader is a twin of the DataLoader, but for validation. It feeds
the model data it has never seen during training, and the resulting
validation loss tells you whether your model is learning useful patterns
or just memorizing. It's the simplest safeguard against wasted compute,
and it's an essential part of every training loop-even in the
stripped-down world of \emph{llm.c}.

\subsection{16. Sequence Length and Memory
Budgeting}\label{sequence-length-and-memory-budgeting}

When training GPT-2 in \emph{llm.c}, one of the most important decisions
you make is choosing the sequence length (often called T). This value
determines how many tokens the model processes in a single forward pass.
It might sound like just another parameter, but sequence length has a
huge impact on what the model can learn, how much memory it uses, and
how fast training runs.

\subsubsection{What Sequence Length
Means}\label{what-sequence-length-means}

Sequence length is simply the number of tokens per training example. If
\texttt{T\ =\ 1024}, the model reads 1,024 tokens in a row (like words
or subwords) and tries to predict the next token at each position.

Think of it like this: if you give the model a paragraph of text,
sequence length is how much of that paragraph it sees at once. Shorter
lengths give the model less context, while longer lengths allow it to
capture bigger patterns, like whole paragraphs or even multiple pages.

\subsubsection{Where It Appears in the
Code}\label{where-it-appears-in-the-code}

In the logs, you'll often see lines like:

\begin{verbatim}
max_seq_len: 1024
\end{verbatim}

This number is defined in the GPT-2 configuration and passed into the
DataLoader. The DataLoader slices chunks of exactly \texttt{T} tokens
from \texttt{train.bin} and \texttt{val.bin}. The model itself has fixed
positional embeddings of size \texttt{T}, so it cannot process sequences
longer than this maximum.

\subsubsection{Memory Costs of Longer
Sequences}\label{memory-costs-of-longer-sequences}

Transformers are powerful but expensive. The attention mechanism
compares every token to every other token in the sequence. This means
memory and compute scale with the square of sequence length:

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Sequence Length (T) & Relative Attention Cost \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
256 & 1× \\
512 & 4× \\
1024 & 16× \\
2048 & 64× \\
\end{longtable}

So doubling \texttt{T} doesn't just double the cost-it multiplies it by
four. That's why training at long context lengths requires a lot of GPU
memory.

\subsubsection{Trade-offs}\label{trade-offs}

\begin{itemize}
\tightlist
\item
  Shorter sequences: Faster, less memory, but limited context. Good for
  quick experiments or tiny datasets like Tiny Shakespeare.
\item
  Longer sequences: More memory, slower, but the model can understand
  larger spans of text. Required for large-scale GPT-2 training.
\end{itemize}

You can think of sequence length as a dial: turning it up increases the
model's ability to ``remember,'' but it also makes training much
heavier.

\subsubsection{\texorpdfstring{Practical Choices in
\emph{llm.c}}{Practical Choices in llm.c}}\label{practical-choices-in-llm.c}

\begin{itemize}
\tightlist
\item
  Tiny Shakespeare example: often trained with \texttt{T\ =\ 64} or
  \texttt{128} for speed.
\item
  GPT-2 small (124M): typically uses \texttt{T\ =\ 1024}, the same as
  the original paper.
\item
  If your GPU has limited memory, you might need to shrink \texttt{T}
  and/or batch size \texttt{B}.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-5}

Choosing sequence length is about balancing learning power against
hardware limits. A too-small sequence length can prevent the model from
capturing long-term dependencies. A too-large one can make training
impossible on your hardware. Every run of \emph{llm.c} is a negotiation
between what you'd like the model to see and what your system can
handle.

\subsubsection{Try It Yourself}\label{try-it-yourself-5}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Short vs long: Train Tiny Shakespeare with \texttt{T\ =\ 64} and then
  \texttt{T\ =\ 256}. Compare both the speed and the coherence of
  generated text.
\item
  Memory test: Increase \texttt{T} step by step until you hit an
  out-of-memory (OOM) error. Note the maximum your GPU can handle.
\item
  Batch trade-off: Try reducing batch size \texttt{B} while increasing
  \texttt{T}. Can you keep GPU memory stable while giving the model more
  context?
\item
  Validation impact: Run with different \texttt{T} values and watch how
  validation loss behaves. Does longer context always help?
\item
  Inspect embeddings: Print out the shape of the positional embeddings.
  Notice how they are always tied to \texttt{T}.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-5}

Sequence length (\texttt{T}) controls how much context the model sees.
It directly determines the size of the positional embeddings, the
structure of batches, and the memory required for attention. In
\emph{llm.c}, adjusting \texttt{T} is one of the fastest ways to explore
the trade-offs between speed, memory, and model capability.

\subsection{17. Reproducibility and Seeding Across
Runs}\label{reproducibility-and-seeding-across-runs}

When training machine learning models, it's common to notice that two
runs-using the same code and the same dataset-don't produce exactly the
same results. This happens because many parts of training involve
randomness. In \emph{llm.c}, reproducibility is controlled by random
seeds. A seed is a starting point for a random number generator. If you
always start from the same seed, the sequence of ``random'' numbers will
be identical, and so will the training run.

\subsubsection{Where Randomness Appears}\label{where-randomness-appears}

Even in a small project like \emph{llm.c}, randomness shows up in
several places:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.1984}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.8016}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Random Role
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Weight initialization & The model's parameters (like attention matrices)
are set randomly at the start. \\
Optimizer states & Some optimizers use random noise (though AdamW is
mostly deterministic). \\
Sampling outputs & When generating text, randomness decides which token
to pick if probabilities are close. \\
Parallelism & On GPU, threads may execute in slightly different orders,
sometimes introducing small nondeterminism. \\
\end{longtable}

Without a fixed seed, every training run can drift apart, even if all
settings look the same.

\subsubsection{\texorpdfstring{How \emph{llm.c} Handles
Seeds}{How llm.c Handles Seeds}}\label{how-llm.c-handles-seeds}

The repository provides a small random utilities module:
\texttt{llmc/rand.h}. Inside you'll find functions such as:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{void}\NormalTok{ manual\_seed}\OperatorTok{(}\DataTypeTok{uint64\_t}\NormalTok{ seed}\OperatorTok{);}
\DataTypeTok{float}\NormalTok{ normal\_}\OperatorTok{(}\DataTypeTok{float}\NormalTok{ mean}\OperatorTok{,} \DataTypeTok{float}\NormalTok{ std}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{manual\_seed} sets the seed for the internal random number
  generator, ensuring reproducibility.
\item
  \texttt{normal\_} is used for initializing weights with Gaussian
  noise, similar to PyTorch's \texttt{torch.nn.init.normal\_}.
\end{itemize}

When you call \texttt{manual\_seed(1337);}, the model weights will be
initialized the same way every time.

\subsubsection{Why Seeds Don't Guarantee Perfect
Reproducibility}\label{why-seeds-dont-guarantee-perfect-reproducibility}

Even with a fixed seed, you may still see small differences:

\begin{itemize}
\tightlist
\item
  GPU kernels sometimes use parallel algorithms that are not bitwise
  deterministic.
\item
  Floating-point math can produce slightly different rounding on
  different hardware.
\item
  Multi-GPU runs (via NCCL/MPI) may introduce nondeterministic reduce
  operations.
\end{itemize}

These differences are usually tiny-validation loss might vary by
0.001-but they exist. For most educational purposes, \emph{llm.c} seeds
are enough to make experiments repeatable.

\subsubsection{Typical Defaults}\label{typical-defaults}

In many examples, you'll see:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{manual\_seed}\OperatorTok{(}\DecValTok{1337}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

This ``magic number'' 1337 is just a convention. You can change it to
any integer. Using the same seed across runs guarantees the same
starting weights, which helps when comparing hyperparameters.

\subsubsection{Why It Matters}\label{why-it-matters-6}

Reproducibility is crucial in machine learning because it lets you:

\begin{itemize}
\tightlist
\item
  Debug effectively: If a bug appears, you want it to appear
  consistently.
\item
  Compare settings: You can test learning rates or batch sizes fairly by
  keeping everything else the same.
\item
  Share results: Other people can run your exact setup and see the same
  outcomes.
\end{itemize}

Without seeds, it becomes hard to tell whether a difference came from
your hyperparameter change or just random luck.

\subsubsection{Try It Yourself}\label{try-it-yourself-6}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run twice with same seed: Train GPT-2 with \texttt{manual\_seed(1337)}
  set. Do you get identical training loss curves?
\item
  Change the seed: Try \texttt{manual\_seed(42)} and compare the loss
  curve. How similar are they? Do they converge to about the same final
  validation loss?
\item
  Remove seeding: Comment out the seed line and run again. Notice how
  runs diverge.
\item
  Sampling experiment: With a fixed seed, generate text multiple times.
  Then change the seed and generate again. See how outputs change.
\item
  Multi-GPU test: If you have more than one GPU, run the same seed
  across devices. Do results stay exactly the same or only
  approximately?
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-6}

Reproducibility in \emph{llm.c} comes from setting seeds for random
number generators. While floating-point quirks mean you can't always get
perfect bit-for-bit matches, seeds let you control the biggest source of
randomness: weight initialization and sampling. With seeding, you can
debug, compare, and share results confidently.

\subsection{18. Error Surfaces from Bad Data (Bounds,
Asserts)}\label{error-surfaces-from-bad-data-bounds-asserts}

When training a model in \emph{llm.c}, everything depends on the quality
and correctness of the data you feed in. If the dataset or batches
contain mistakes, the training process can go off track
quickly-sometimes by crashing outright, other times by producing strange
loss values like \texttt{NaN}. To guard against this, the code uses
bounds checks and asserts that catch problems early.

\subsubsection{What Can Go Wrong with
Data}\label{what-can-go-wrong-with-data}

There are several common data issues:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.1257}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.8743}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Problem
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
What Happens
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Token ID out of range & The model expects IDs between 0 and
\texttt{vocab\_size-1}. A wrong ID can cause array indexing errors. \\
Empty or short dataset & The DataLoader may run out of tokens before
filling a batch. \\
Mismatched tokenizer & If you build a dataset with a different
tokenizer, IDs may not correspond to the GPT-2 tokenizer in
\texttt{gpt2\_tokenizer.bin}. This produces nonsense outputs. \\
Corrupt \texttt{.bin} files & If files are incomplete or written
incorrectly, the DataLoader might read garbage values. \\
\end{longtable}

These errors show up as segfaults, invalid memory access, or exploding
losses during training.

\subsubsection{\texorpdfstring{How \emph{llm.c} Defends Against Bad
Data}{How llm.c Defends Against Bad Data}}\label{how-llm.c-defends-against-bad-data}

The repository makes heavy use of asserts-simple checks that stop the
program immediately if something unexpected happens. For example, in
\texttt{llmc/utils.h}, functions like \texttt{freadCheck} and
\texttt{mallocCheck} ensure that file reads and memory allocations
succeed. If not, they print an error message and abort instead of
silently failing.

Inside the DataLoader, token IDs are often validated to make sure they
fall inside the expected vocabulary range. If you try to access an
invalid index in the embedding table, the program will crash quickly,
which is better than continuing with corrupted values.

\subsubsection{Example: Vocab Range
Check}\label{example-vocab-range-check}

During training, every input token is used to look up a row in the
embedding matrix. If a token ID is too large, you'd access memory
outside the matrix. This is why checking
\texttt{0\ \textless{}=\ id\ \textless{}\ vocab\_size} is essential. In
C, asserts provide this safety net.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{assert}\OperatorTok{(}\NormalTok{id }\OperatorTok{\textgreater{}=} \DecValTok{0} \OperatorTok{\&\&}\NormalTok{ id }\OperatorTok{\textless{}}\NormalTok{ vocab\_size}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

This kind of check may look simple, but it saves hours of debugging
mysterious crashes.

\subsubsection{Error Surfaces in Loss}\label{error-surfaces-in-loss}

Even if your program doesn't crash, bad data can create ``error
surfaces'' in the loss function:

\begin{itemize}
\tightlist
\item
  NaNs: Appear when invalid values propagate through softmax, layernorm,
  or division operations.
\item
  Flat loss: If the dataset is empty or repetitive, the model never
  improves.
\item
  Mismatch behavior: Training loss decreases but validation loss stays
  high if training and validation sets use inconsistent tokenization.
\end{itemize}

These are signs that something is wrong with the dataset or
preprocessing.

\subsubsection{Why It Matters}\label{why-it-matters-7}

C is a low-level language with very little safety by default. One
out-of-range index can corrupt memory and cause unpredictable bugs. By
aggressively checking assumptions (file sizes, vocab bounds, token IDs),
\emph{llm.c} turns hard-to-find errors into immediate, clear failures.
For learners, this makes it much easier to understand what went wrong.

\subsubsection{Try It Yourself}\label{try-it-yourself-7}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Corrupt a dataset: Open \texttt{train.bin} and delete a few bytes. Run
  training and see what error appears. Notice how quickly asserts catch
  it.
\item
  Force a bad ID: Modify the DataLoader to add \texttt{+100000} to a
  token. Does the model crash with an assertion?
\item
  Skip asserts: Temporarily disable checks and rerun. Compare how much
  harder it is to figure out what went wrong.
\item
  Validation mismatch: Tokenize a file with a different tokenizer and
  save it as \texttt{val.bin}. Watch how the validation loss behaves
  compared to training loss.
\item
  Print debug info: Add logging to display the first 20 tokens of each
  batch. Can you spot bad data before it crashes?
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-7}

Bad data can silently sabotage training, but \emph{llm.c} uses asserts
and bounds checks to make errors loud and clear. This design choice
helps learners focus on the real logic of transformers instead of
chasing hidden bugs caused by corrupted or mismatched datasets. In
machine learning, good data hygiene and strict validation are as
important as the model itself.

\subsection{19. Tokenization Edge Cases (UNKs, EOS,
BOS)}\label{tokenization-edge-cases-unks-eos-bos}

Tokenization looks simple at first: take text, split it into tokens, and
assign each token an ID. But in practice, there are always tricky
situations. \emph{llm.c} inherits the quirks of the GPT-2 tokenizer,
which is byte-level BPE (Byte Pair Encoding). This design mostly avoids
``unknown'' tokens, but it still has details you need to understand when
preparing datasets or interpreting outputs.

\subsubsection{No True ``UNK'' in GPT-2}\label{no-true-unk-in-gpt-2}

Some tokenizers, like those used in earlier NLP systems, include a
special \texttt{UNK} (unknown) token for words that aren't in the
vocabulary. GPT-2 avoids this problem by working at the byte level:

\begin{itemize}
\tightlist
\item
  Every possible byte (0--255) is in the base vocabulary.
\item
  If the tokenizer doesn't know how to split a character or word, it
  just falls back to raw bytes.
\end{itemize}

That means you will never see an \texttt{UNK} token in \emph{llm.c}. Any
input text is always representable. This is one of the main reasons
GPT-2's tokenizer is so robust.

\subsubsection{Special Tokens: EOS and
BOS}\label{special-tokens-eos-and-bos}

Even though GPT-2 doesn't use \texttt{UNK}, it does use other special
tokens:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2095}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1419}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6486}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Token
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
EOS (End of Sequence) & 50256 & Marks the end of a text segment. Used
during training and sampling. \\
BOS (Beginning of Sequence) & Not explicit in GPT-2 & GPT-2 doesn't use
a fixed BOS token. Instead, the model assumes generation starts at
position 0. \\
\end{longtable}

In \emph{llm.c}, you'll often see \texttt{EOS} at the end of training
sequences or when sampling text. If you generate text and see strange
endings, it's usually because the model predicted \texttt{EOS}.

\subsubsection{Whitespace Quirks}\label{whitespace-quirks}

The tokenizer also handles whitespace in a slightly unusual way. For
example, the word ``hello'' and the word '' hello'' (with a leading
space) map to different tokens. This is why generated text sometimes
starts with a space-it's part of the token definition.

Example:

\begin{itemize}
\tightlist
\item
  \texttt{"hello"} → token ID 31373
\item
  \texttt{"\ hello"} → token ID 15496
\end{itemize}

This is normal behavior for GPT-2. It helps the model capture spacing
and punctuation consistently.

\subsubsection{Unicode and Rare
Characters}\label{unicode-and-rare-characters}

Because it's byte-level, GPT-2 can encode emojis, accented characters,
or even binary junk data. But the BPE merges are optimized for English,
so rare characters often get split into multiple byte tokens. That means
sequences with lots of rare symbols (like Chinese or emojis) will use
more tokens than plain English text.

\subsubsection{Why It Matters}\label{why-it-matters-8}

Edge cases in tokenization affect both dataset preparation and model
outputs. If you see weird spacing or early \texttt{EOS} tokens, it's not
a bug-it's just how the tokenizer works. Understanding these quirks
helps you debug outputs and prepare datasets without surprises.

\subsubsection{Try It Yourself}\label{try-it-yourself-8}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  EOS inspection: Open \texttt{val.bin} with a hex viewer and look for
  token ID \texttt{50256}. These mark the ends of text segments.
\item
  Whitespace check: Use the tokenizer to encode \texttt{"hello"} and
  \texttt{"\ hello"}. Compare the token IDs.
\item
  Emoji test: Encode a string with emojis (e.g., \texttt{"🙂🙂🙂"}) and
  see how many tokens it becomes.
\item
  Rare character dataset: Create a small \texttt{.txt} file with
  accented characters and tokenize it. How many bytes does each
  character consume?
\item
  Sampling experiment: Generate text until you see the EOS token appear.
  Notice how the model ``knows'' to stop.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-8}

Tokenization in GPT-2 is robust, but it has quirks. There are no unknown
tokens thanks to byte-level encoding, but whitespace and special tokens
like \texttt{EOS} play important roles. By experimenting with these edge
cases, you'll develop an intuition for how raw text is mapped into the
numbers that drive training and generation in \emph{llm.c}.

\subsection{20. Data Hygiene and
Logging}\label{data-hygiene-and-logging}

When training with \emph{llm.c}, having clean data is just as important
as having the right model code. If the dataset contains errors,
duplicates, or formatting issues, the model may waste capacity
memorizing noise instead of learning useful patterns. This is where data
hygiene comes in-making sure your training and validation sets are
prepared properly. Alongside this, logging ensures you can monitor
what's happening during training and catch problems early.

\subsubsection{What Data Hygiene Means}\label{what-data-hygiene-means}

Data hygiene is about making sure your dataset is both valid and useful.
For language models, this includes:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.2286}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.7714}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Check
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Why It Matters
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Correct tokenization & Must match the tokenizer
(\texttt{gpt2\_tokenizer.bin}), otherwise IDs won't line up. \\
No corrupt files & Binary \texttt{.bin} files must be complete; partial
writes cause crashes. \\
Balanced splits & Training and validation sets should come from the same
distribution. \\
Reasonable size & Too small → overfitting. Too large → slow or
infeasible. \\
Deduplication & Repeated passages (e.g., web scrapes) make models
memorize instead of generalize. \\
\end{longtable}

The scripts in \texttt{dev/data/} handle basic hygiene by tokenizing
consistently and splitting into train/val sets. But if you bring your
own dataset, you are responsible for cleaning it first.

\subsubsection{Logging During Training}\label{logging-during-training}

Once training starts, logging becomes your window into what's happening.
\emph{llm.c} uses a minimal logging system (\texttt{llmc/logger.h}) to
print progress to the console. Typical logs include:

\begin{verbatim}
step 0: train loss 5.19, val loss 5.32
step 100: train loss 4.87, val loss 5.01
step 200: train loss 4.62, val loss 4.88
\end{verbatim}

These numbers let you track:

\begin{itemize}
\tightlist
\item
  Training loss: Is the model fitting the data?
\item
  Validation loss: Is it generalizing, or overfitting?
\item
  Step timing: How long each batch takes, useful for profiling.
\end{itemize}

Even in such a small project, this logging loop gives you most of what
you need to debug runs.

\subsubsection{Why Hygiene and Logging Go
Together}\label{why-hygiene-and-logging-go-together}

Bad data often reveals itself in the logs. For example:

\begin{itemize}
\tightlist
\item
  If validation loss is much higher than training loss, your validation
  set may be mismatched.
\item
  If loss suddenly becomes \texttt{NaN}, your dataset might contain
  corrupt tokens.
\item
  If loss plateaus at a high value, you may have too little data or poor
  preprocessing.
\end{itemize}

By keeping your data clean and watching logs closely, you can detect
these issues early instead of wasting hours of compute.

\subsubsection{Try It Yourself}\label{try-it-yourself-9}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Dirty dataset test: Take a \texttt{.txt} file, add random symbols or
  binary junk, and prepare a \texttt{.bin} dataset. What happens to
  training loss?
\item
  Duplicate passages: Copy the same paragraph 100 times into a training
  file. Does validation loss improve, or does the model just memorize?
\item
  Log frequency: Modify the code to log every step instead of every N
  steps. How noisy are the results?
\item
  Custom logger: Extend the logger to also print gradient norms or
  learning rate values. Does this help you understand training dynamics
  better?
\item
  Compare splits: Build two datasets with different train/val splits.
  Which one gives more stable validation losses?
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-9}

Data hygiene ensures the model learns from clean, consistent input,
while logging ensures you can see whether learning is actually
happening. Together, they form the foundation of reliable experiments in
\emph{llm.c}. If you clean your data carefully and pay attention to the
logs, you'll catch most problems before they become serious.

\section{Chapter 3. Model Definition and
Weights}\label{chapter-3.-model-definition-and-weights}

\subsection{21. GPT-2 Config: Vocab, Layers, Heads,
Channels}\label{gpt-2-config-vocab-layers-heads-channels}

Every GPT-2 model, no matter how large or small, is defined by a handful
of configuration numbers. These numbers decide how big the model is, how
much memory it needs, and how powerful it can become. In \emph{llm.c},
these settings are stored in a simple config struct and printed at the
start of training. They describe the ``blueprint'' of the transformer.

\subsubsection{The Core Parameters}\label{the-core-parameters}

Here are the most important values you'll see in the logs:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2150}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5888}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1963}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example (GPT-2 Small)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{vocab\_size} & Number of distinct tokens (from tokenizer). &
50,257 \\
\texttt{padded\_vocab\_size} & Vocab size rounded up to nearest multiple
(for GPU efficiency). & 50,304 \\
\texttt{max\_seq\_len} & Longest sequence of tokens the model can
handle. & 1,024 \\
\texttt{num\_layers} & Number of transformer blocks stacked on top of
each other. & 12 \\
\texttt{num\_heads} & Number of attention heads per block. & 12 \\
\texttt{channels} & Width of hidden states (embedding dimension). &
768 \\
\texttt{num\_parameters} & Total trainable weights in the model. &
\textasciitilde124M \\
\end{longtable}

Together, these values define both the structure and the capacity of the
model.

\subsubsection{What They Control}\label{what-they-control}

\begin{itemize}
\tightlist
\item
  Vocabulary size connects the model to the tokenizer. Every input token
  ID must be less than \texttt{vocab\_size}. The padded version makes
  GPU matrix multiplications easier.
\item
  Max sequence length fixes the size of the positional embeddings. If
  you set this to 1024, the model can't read beyond 1024 tokens in one
  pass.
\item
  Layers control model depth. Each layer contains an attention block and
  an MLP. More layers = more representational power.
\item
  Heads divide attention into parallel ``subspaces.'' With 12 heads, the
  model can track different types of relationships in the text at the
  same time.
\item
  Channels set the dimensionality of embeddings and hidden vectors.
  Larger channels mean more expressive representations but also more
  computation.
\item
  Parameters are the sum of it all. This number tells you how heavy the
  model is to train and how much memory it will consume.
\end{itemize}

\subsubsection{Configs Across GPT-2
Sizes}\label{configs-across-gpt-2-sizes}

The original GPT-2 models come in several sizes:

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Model & Layers & Heads & Channels & Parameters \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
GPT-2 Small & 12 & 12 & 768 & 124M \\
GPT-2 Medium & 24 & 16 & 1024 & 355M \\
GPT-2 Large & 36 & 20 & 1280 & 774M \\
GPT-2 XL & 48 & 25 & 1600 & 1.6B \\
\end{longtable}

\emph{llm.c} can scale between these by just changing a few numbers in
the config struct.

\subsubsection{Where Config Appears in the
Code}\label{where-config-appears-in-the-code}

In \texttt{train\_gpt2.c} and \texttt{train\_gpt2.cu}, you'll see
something like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GPT2Config config }\OperatorTok{=} \OperatorTok{\{}
    \OperatorTok{.}\NormalTok{vocab\_size }\OperatorTok{=} \DecValTok{50257}\OperatorTok{,}
    \OperatorTok{.}\NormalTok{max\_seq\_len }\OperatorTok{=} \DecValTok{1024}\OperatorTok{,}
    \OperatorTok{.}\NormalTok{num\_layers }\OperatorTok{=} \DecValTok{12}\OperatorTok{,}
    \OperatorTok{.}\NormalTok{num\_heads }\OperatorTok{=} \DecValTok{12}\OperatorTok{,}
    \OperatorTok{.}\NormalTok{channels }\OperatorTok{=} \DecValTok{768}\OperatorTok{,}
\OperatorTok{\};}
\end{Highlighting}
\end{Shaded}

Later, the model is initialized using this struct, and the log prints
all the derived information (like \texttt{num\_parameters}).

\subsubsection{Why It Matters}\label{why-it-matters-9}

The config is the contract between your dataset and your model.

\begin{itemize}
\tightlist
\item
  If \texttt{vocab\_size} doesn't match your tokenizer, you'll get
  crashes.
\item
  If \texttt{max\_seq\_len} is too small, you'll lose context.
\item
  If \texttt{num\_layers} or \texttt{channels} are too large for your
  GPU, you'll run out of memory.
\end{itemize}

By tweaking the config, you decide whether you want a tiny model for
learning or a massive one closer to GPT-2 XL.

\subsubsection{Try It Yourself}\label{try-it-yourself-10}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Print config: Run the trainer and note the printed values. Compare
  them with the GPT-2 sizes in the table.
\item
  Shrink the model: Change \texttt{num\_layers\ =\ 4},
  \texttt{num\_heads\ =\ 4}, and \texttt{channels\ =\ 256}. Train on
  Tiny Shakespeare and see how fast it runs.
\item
  Increase sequence length: Try setting \texttt{max\_seq\_len\ =\ 2048}.
  Does your GPU still handle it, or do you get out-of-memory errors?
\item
  Parameter count check: Compute how many parameters your custom config
  has. Compare it to the reported \texttt{num\_parameters}.
\item
  Tokenizer mismatch test: Intentionally set
  \texttt{vocab\_size\ =\ 30000} and watch what error appears when
  loading the tokenizer.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-10}

The GPT-2 config struct in \emph{llm.c} is small but powerful. It
defines everything about the model's architecture: vocabulary, sequence
length, depth, width, and total parameters. By adjusting just a few
integers, you can scale from a toy model that runs on CPU to a
billion-parameter giant (if your hardware allows it). Understanding
these numbers is the first step to understanding how transformer
capacity is controlled.

\subsection{22. Parameter Tensors and Memory
Layout}\label{parameter-tensors-and-memory-layout}

Once the GPT-2 configuration is set, the next big step is to allocate
the parameters of the model. These are the trainable numbers-weights and
biases-that define how the model processes input tokens. In
\emph{llm.c}, parameters are stored in flat arrays of floats rather than
in deeply nested objects like in PyTorch. This choice makes the code
easier to read and keeps memory access predictable.

\subsubsection{What Are Parameters?}\label{what-are-parameters}

Every part of the transformer has its own trainable weights:

\begin{itemize}
\tightlist
\item
  Embedding tables: one for tokens and one for positions.
\item
  Attention layers: query, key, value, and output projections.
\item
  MLP layers: two linear layers plus their biases.
\item
  LayerNorms: scale (\texttt{gamma}) and shift (\texttt{beta}) values.
\item
  Final projection: maps hidden states back to vocab size for logits.
\end{itemize}

Together, these add up to hundreds of millions of numbers, even for
GPT-2 Small.

\subsubsection{\texorpdfstring{Flat Memory Design in
\emph{llm.c}}{Flat Memory Design in llm.c}}\label{flat-memory-design-in-llm.c}

Instead of allocating each parameter separately, \emph{llm.c} stores all
parameters in one contiguous block of memory. Each layer is given a
slice of this big array.

This has two benefits:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simplicity: You only need one malloc (or cudaMalloc) for all
  parameters.
\item
  Performance: Contiguous memory access is faster on both CPU and GPU.
\end{enumerate}

To keep track of where each layer's weights live inside the block, the
code uses offsets.

\subsubsection{Example in Code}\label{example-in-code}

In \texttt{train\_gpt2.c}, parameters are packed into a single array:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{float}\OperatorTok{*}\NormalTok{ params }\OperatorTok{=} \OperatorTok{(}\DataTypeTok{float}\OperatorTok{*)}\NormalTok{mallocCheck}\OperatorTok{(}\NormalTok{config}\OperatorTok{.}\NormalTok{num\_parameters }\OperatorTok{*} \KeywordTok{sizeof}\OperatorTok{(}\DataTypeTok{float}\OperatorTok{));}
\end{Highlighting}
\end{Shaded}

Later, helper functions compute pointers into this array for each
sub-module. For example, the token embedding weights are just the first
slice:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{float}\OperatorTok{*}\NormalTok{ token\_embedding\_table }\OperatorTok{=}\NormalTok{ params}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

Then the program moves forward, assigning chunks to positional
embeddings, attention weights, and so on.

\subsubsection{Shapes of the Tensors}\label{shapes-of-the-tensors}

Even though parameters are stored in 1D memory, they conceptually form
2D or 3D tensors. For example:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2564}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4530}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2906}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Shape
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Token embeddings & \texttt{{[}vocab\_size,\ channels{]}} & Maps token
IDs to vectors. \\
Positional embeddings & \texttt{{[}max\_seq\_len,\ channels{]}} & Adds
position info. \\
Attention weights (Q, K, V, O) & \texttt{{[}channels,\ channels{]}} &
Project hidden states. \\
MLP layers & \texttt{{[}channels,\ 4×channels{]}} and
\texttt{{[}4×channels,\ channels{]}} & Expand and contract hidden
states. \\
LayerNorm scale/shift & \texttt{{[}channels{]}} & Normalize and rescale
features. \\
\end{longtable}

When you look at the code, remember: these shapes are ``virtual.''
They're just views into slices of the big 1D array.

\subsubsection{Why This Layout Works
Well}\label{why-this-layout-works-well}

PyTorch or TensorFlow manage parameter tensors with lots of
abstractions. \emph{llm.c} strips this away: you see the raw memory, the
exact number of parameters, and the order they're laid out in. This
makes it clear how large the model really is and why it uses so much RAM
or VRAM.

It also means you can easily save and load checkpoints by writing or
reading the flat array directly to disk. No need for complicated
serialization formats.

\subsubsection{Why It Matters}\label{why-it-matters-10}

Understanding parameter layout helps you:

\begin{itemize}
\tightlist
\item
  See how the model's size explodes as you increase layers, heads, or
  channels.
\item
  Debug memory issues by checking how big each slice is.
\item
  Realize how much of training is just linear algebra on big arrays of
  floats.
\end{itemize}

This perspective is powerful because it demystifies deep learning: at
its core, GPT-2 is just multiplying slices of one giant float array
again and again.

\subsubsection{Try It Yourself}\label{try-it-yourself-11}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Print parameter count: Add a line in the code to print
  \texttt{config.num\_parameters}. Compare it with the table for GPT-2
  Small/Medium/Large.
\item
  Inspect a slice: Print the first 10 numbers of the embedding table.
  They'll look random (from initialization).
\item
  Change precision: Modify the code to allocate \texttt{half} (FP16)
  instead of \texttt{float}. How much memory do you save?
\item
  Checkpoint peek: Save a checkpoint, then open it in a hex viewer. It's
  just raw floats-proof that parameters are stored flat.
\item
  Parameter scaling: Double the number of layers and see how
  \texttt{num\_parameters} changes. Can you predict the increase?
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-11}

In \emph{llm.c}, parameters are not hidden inside classes or objects.
They live in one flat block of memory, sliced up by convention into
embeddings, attention matrices, MLP weights, and norms. This design
makes the relationship between model architecture and memory crystal
clear-and reminds you that even a billion-parameter transformer is
``just'' a giant array of numbers.

\subsection{23. Embedding Tables: Token +
Positional}\label{embedding-tables-token-positional}

Before a transformer can reason about text, it first needs to turn
tokens into vectors. In \emph{llm.c}, this job is handled by the
embedding tables: one for tokens, one for positions. These tables are
the very first layer of GPT-2, and they transform plain integer IDs into
continuous values that the neural network can process.

\subsubsection{Token Embedding Table}\label{token-embedding-table}

When you feed in a batch of token IDs, the model looks up their
corresponding vectors in the token embedding table.

\begin{itemize}
\item
  Shape: \texttt{{[}vocab\_size,\ channels{]}}

  \begin{itemize}
  \tightlist
  \item
    \texttt{vocab\_size\ ≈\ 50,257} (for GPT-2)
  \item
    \texttt{channels\ =\ hidden\ size} (768 for GPT-2 Small)
  \end{itemize}
\item
  Each row corresponds to one token in the vocabulary.
\item
  Each row is a dense vector of size \texttt{channels}.
\end{itemize}

So if your input batch has size \texttt{(B,\ T)}, looking up embeddings
gives you a tensor of shape \texttt{(B,\ T,\ channels)}.

In the code, this is implemented as an array slice from the flat
parameter block:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{float}\OperatorTok{*}\NormalTok{ token\_embedding\_table }\OperatorTok{=}\NormalTok{ params}\OperatorTok{;}  \CommentTok{// first slice of parameters}
\end{Highlighting}
\end{Shaded}

At runtime, token IDs index directly into this table.

\subsubsection{Positional Embedding
Table}\label{positional-embedding-table}

Transformers don't inherently know about word order. That's what
positional embeddings are for.

\begin{itemize}
\item
  Shape: \texttt{{[}max\_seq\_len,\ channels{]}}

  \begin{itemize}
  \tightlist
  \item
    \texttt{max\_seq\_len\ =\ 1024} in GPT-2 Small
  \item
    Same channel dimension as token embeddings
  \end{itemize}
\item
  Each position (0, 1, 2, \ldots, 1023) has its own vector.
\end{itemize}

During training, when the model sees token \texttt{i} at position
\texttt{j}, it takes the token embedding vector and adds the positional
embedding vector for \texttt{j}. This gives the model both word identity
and word position.

In \emph{llm.c}, positional embeddings immediately follow the token
embeddings in the flat parameter array.

\subsubsection{Adding Them Together}\label{adding-them-together}

The embedding layer's forward pass is simple:

\begin{verbatim}
embedding_out[token, pos] = token_embedding[token] + positional_embedding[pos]
\end{verbatim}

This results in a \texttt{(B,\ T,\ channels)} tensor that becomes the
input to the first transformer block.

\subsubsection{Why This Matters}\label{why-this-matters-1}

Embeddings are the bridge between discrete tokens and continuous math.
Without them, the model couldn't use linear algebra to learn patterns.
By adding positional embeddings, GPT-2 knows the difference between:

\begin{itemize}
\tightlist
\item
  ``dog bites man'' → \texttt{dog} comes first, \texttt{man} comes last
\item
  ``man bites dog'' → same tokens, but swapped positions change the
  meaning
\end{itemize}

This small step is essential: order and identity must both be captured
before attention can begin.

\subsubsection{Try It Yourself}\label{try-it-yourself-12}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Inspect shapes: Print the sizes of the token and positional embedding
  tables during initialization. Confirm they match
  \texttt{{[}vocab\_size,\ channels{]}} and
  \texttt{{[}max\_seq\_len,\ channels{]}}.
\item
  Look at first rows: Print the first 5 vectors of the token embedding
  table. They should look like small random floats from initialization.
\item
  Change max\_seq\_len: Double \texttt{max\_seq\_len} in the config. How
  does this change the size of the positional table? Does training still
  work?
\item
  Overwrite embeddings: Try setting the token embedding table to all
  zeros. What happens to training loss?
\item
  Sampling experiment: After training a few steps, decode outputs
  without adding positional embeddings. Do the results become
  nonsensical or repetitive?
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-12}

The embedding tables are the foundation of GPT-2. Token embeddings give
meaning to symbols, while positional embeddings give structure to
sequences. In \emph{llm.c}, they are just two slices of the flat
parameter array, added together at the very start of the forward
pass-but without them, the transformer would be blind to both words and
order.

\subsection{24. Attention Stack: QKV Projections and
Geometry}\label{attention-stack-qkv-projections-and-geometry}

After embeddings, the real magic of transformers begins: the attention
mechanism. In GPT-2, every transformer block contains an attention
stack. This is where the model learns how each token relates to others
in the sequence-whether it's paying attention to the previous word, the
beginning of a sentence, or even punctuation marks far away.

\subsubsection{What Attention Does}\label{what-attention-does}

Attention lets the model answer the question:

\begin{quote}
``Given the current word, which other words in the context should I care
about, and how much?''
\end{quote}

Instead of treating words independently, the model uses attention to
build connections across the sequence.

\subsubsection{The Q, K, V Projections}\label{the-q-k-v-projections}

Each attention block starts with three linear projections:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1529}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2588}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5882}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Shape
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Q (Query) & \texttt{{[}channels,\ channels{]}} & Represents what each
token is \emph{asking} about. \\
K (Key) & \texttt{{[}channels,\ channels{]}} & Represents how each token
can be \emph{recognized}. \\
V (Value) & \texttt{{[}channels,\ channels{]}} & Represents the actual
\emph{information} to pass along. \\
\end{longtable}

Here's the flow:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each input vector (from embeddings or previous block) is multiplied by
  these three matrices to produce Q, K, and V vectors.
\item
  Attention scores are computed by comparing Qs with Ks.
\item
  These scores are used to weight the Vs, mixing information from other
  tokens into the current one.
\end{enumerate}

\subsubsection{Geometry of Attention}\label{geometry-of-attention}

\begin{itemize}
\tightlist
\item
  Q and K define a similarity score: how well does this token match
  another one?
\item
  V carries the actual features (like meaning, grammar cues).
\item
  The result is a weighted sum: tokens borrow information from others
  based on attention scores.
\end{itemize}

In equations:

\begin{verbatim}
scores = Q × K^T / sqrt(d_k)
weights = softmax(scores + mask)
output  = weights × V
\end{verbatim}

The division by \texttt{sqrt(d\_k)} normalizes scores so they don't blow
up as dimensions grow.

\subsubsection{Multi-Head Attention}\label{multi-head-attention}

GPT-2 doesn't use just one attention projection-it uses many in
parallel, called heads. Each head learns to focus on different types of
relationships:

\begin{itemize}
\tightlist
\item
  One head might track subject--verb agreement.
\item
  Another might watch punctuation and quotes.
\item
  Another might connect pronouns to their referents.
\end{itemize}

For GPT-2 Small:

\begin{itemize}
\tightlist
\item
  12 heads per layer
\item
  Each head works on a reduced dimension
  (\texttt{channels\ /\ num\_heads})
\item
  Outputs are concatenated and projected back to \texttt{channels}
\end{itemize}

This setup is what gives transformers their flexibility.

\subsubsection{\texorpdfstring{Implementation in
\emph{llm.c}}{Implementation in llm.c}}\label{implementation-in-llm.c}

In the parameter array, each transformer block has slices for Q, K, V,
and output projection (O). During forward pass:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Multiply input by Q, K, V matrices.
\item
  Reshape into heads.
\item
  Compute attention scores (masked to prevent looking forward).
\item
  Apply softmax.
\item
  Multiply by V to get weighted values.
\item
  Concatenate heads and apply the O projection.
\end{enumerate}

All of this is done with plain matrix multiplications and softmax
calls-no magic beyond linear algebra.

\subsubsection{Why It Matters}\label{why-it-matters-11}

Attention is the beating heart of GPT-2. It's how the model captures
dependencies across text, from short-term grammar to long-range
coherence. Without QKV, embeddings would stay isolated, and the model
could never build context-aware representations.

\subsubsection{Try It Yourself}\label{try-it-yourself-13}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Print shapes: Log the shapes of Q, K, V matrices in one layer. Confirm
  they match \texttt{{[}channels,\ channels{]}}.
\item
  Visualize scores: After a forward pass, print the attention weights
  for one head. Do they concentrate on recent tokens or spread across
  the sequence?
\item
  Reduce heads: Change \texttt{num\_heads} from 12 to 4. What happens to
  validation loss?
\item
  Break symmetry: Initialize all Q, K, V matrices with zeros. Does
  training loss decrease at all?
\item
  Mask experiment: Disable the causal mask (allow looking ahead). Does
  the model ``cheat'' by predicting future tokens perfectly?
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-13}

The attention stack is where tokens stop being isolated and start
talking to each other. Q, K, and V projections turn context into
weighted relationships, and multi-head attention lets the model juggle
many types of dependencies at once. In \emph{llm.c}, this is implemented
with straightforward linear algebra, making the most powerful idea in
modern NLP visible and accessible.

\subsection{25. MLP Block: Linear Layers +
Activation}\label{mlp-block-linear-layers-activation}

After attention mixes information across tokens, GPT-2 applies a second
transformation inside each block: the MLP (Multi-Layer Perceptron). This
part doesn't look at other tokens-it processes each position
independently. But it's just as important because it gives the model
extra capacity to transform and refine the hidden features before
passing them to the next layer.

\subsubsection{What the MLP Looks Like}\label{what-the-mlp-looks-like}

Every transformer block contains an MLP with two linear layers and a
nonlinear activation in between:

\begin{verbatim}
hidden = Linear1(x)
hidden = GELU(hidden)
out    = Linear2(hidden)
\end{verbatim}

This structure expands the feature dimension and then compresses it back
down, which lets the network learn richer representations.

\subsubsection{Shapes of the Layers}\label{shapes-of-the-layers}

If the hidden size (channels) is \texttt{d\_model}, the MLP works as
follows:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1392}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3038}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5570}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Shape
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Input & \texttt{{[}B,\ T,\ d\_model{]}} & Output of attention for each
token. \\
Linear1 & \texttt{{[}d\_model,\ 4\ ×\ d\_model{]}} & Expands features 4×
wider. \\
GELU & elementwise & Introduces nonlinearity. \\
Linear2 & \texttt{{[}4\ ×\ d\_model,\ d\_model{]}} & Projects back to
original size. \\
Output & \texttt{{[}B,\ T,\ d\_model{]}} & Same shape as input, ready
for residual add. \\
\end{longtable}

For GPT-2 Small (\texttt{d\_model\ =\ 768}), Linear1 expands to 3072
channels, then Linear2 reduces back to 768.

\subsubsection{Activation: GELU}\label{activation-gelu}

The activation function in GPT-2 is GELU (Gaussian Error Linear Unit).
It's smoother than ReLU, giving the model a more nuanced way of handling
values around zero. In code, GELU looks like:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{float}\NormalTok{ gelu}\OperatorTok{(}\DataTypeTok{float}\NormalTok{ x}\OperatorTok{)} \OperatorTok{\{}
    \ControlFlowTok{return} \FloatTok{0.5}\BuiltInTok{f} \OperatorTok{*}\NormalTok{ x }\OperatorTok{*} \OperatorTok{(}\FloatTok{1.0}\BuiltInTok{f} \OperatorTok{+}\NormalTok{ tanhf}\OperatorTok{(}\FloatTok{0.79788456}\BuiltInTok{f} \OperatorTok{*} \OperatorTok{(}\NormalTok{x }\OperatorTok{+} \FloatTok{0.044715}\BuiltInTok{f} \OperatorTok{*}\NormalTok{ x }\OperatorTok{*}\NormalTok{ x }\OperatorTok{*}\NormalTok{ x}\OperatorTok{)));}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

This formula may look complicated, but the idea is simple: it smoothly
squashes negative values toward zero and keeps positive values flowing
through.

\subsubsection{Why Expand and Shrink?}\label{why-expand-and-shrink}

The expansion to \texttt{4\ ×\ d\_model} may seem wasteful, but it's
deliberate:

\begin{itemize}
\tightlist
\item
  Expanding gives the model more capacity to represent patterns at each
  token.
\item
  Shrinking keeps the overall parameter count manageable.
\item
  Together, they act like a bottleneck layer that forces the model to
  transform information more effectively.
\end{itemize}

This ``expand → activate → shrink'' design is one of the main reasons
transformers scale so well.

\subsubsection{\texorpdfstring{Implementation in
\emph{llm.c}}{Implementation in llm.c}}\label{implementation-in-llm.c-1}

Just like attention, the MLP parameters live in the flat array of
floats. Each block stores two weight matrices and two bias vectors.
During forward pass:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Multiply input by \texttt{Linear1} weights, add bias.
\item
  Apply GELU elementwise.
\item
  Multiply by \texttt{Linear2} weights, add bias.
\item
  Pass result through residual connection.
\end{enumerate}

Because each position is processed independently, the MLP is easy to
parallelize across tokens.

\subsubsection{Why It Matters}\label{why-it-matters-12}

The MLP is the nonlinear refiner of transformer blocks. Attention
spreads information, but MLPs transform it in-place, giving the model
more expressive power. Without the MLP, the network would be mostly
linear, limiting its ability to capture complex patterns in text.

\subsubsection{Try It Yourself}\label{try-it-yourself-14}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Print shapes: Log the dimensions of Linear1 and Linear2 weights in one
  block. Do they match \texttt{{[}768,\ 3072{]}} and
  \texttt{{[}3072,\ 768{]}} for GPT-2 Small?
\item
  Swap activation: Replace GELU with ReLU in the code. Does training
  still work? How does validation loss compare?
\item
  Reduce expansion: Change expansion from 4× to 2×
  (\texttt{{[}768,\ 1536{]}}). What effect does this have on parameter
  count and performance?
\item
  Zero out MLP: Set MLP weights to zero. Does the model still learn
  anything, or does performance collapse?
\item
  Compare speed: Measure training step time with and without the MLP
  enabled. How much slower is it?
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-14}

The MLP block in GPT-2 is a simple two-layer network with GELU
activation, applied independently to each token. It expands, activates,
and compresses features, giving the model nonlinear power to reshape
hidden states. In \emph{llm.c}, it's implemented with basic matrix
multiplications and a smooth GELU function, proving that even small
building blocks can have a big impact on the model's ability to learn
language.

\subsection{\texorpdfstring{26. LayerNorm: Theory and Implementation
(\texttt{doc/layernorm})}{26. LayerNorm: Theory and Implementation (doc/layernorm)}}\label{layernorm-theory-and-implementation-doclayernorm}

Deep neural networks often suffer from unstable training if activations
drift too high or too low. To stabilize this, GPT-2 uses Layer
Normalization (LayerNorm) inside every transformer block. In
\emph{llm.c}, LayerNorm is implemented directly in C, and there's even a
detailed explanation in the repo's \texttt{doc/layernorm} file to help
learners understand how it works.

\subsubsection{The Idea of
Normalization}\label{the-idea-of-normalization}

When you pass vectors through many layers, their values can become
unbalanced-some features dominate while others shrink. Normalization
fixes this by:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Centering: subtracting the mean of the vector.
\item
  Scaling: dividing by the standard deviation.
\end{enumerate}

This makes every feature vector have mean 0 and variance 1, improving
stability.

\subsubsection{Why ``Layer'' Norm?}\label{why-layer-norm}

There are different kinds of normalization (BatchNorm, InstanceNorm,
etc.). LayerNorm is special because:

\begin{itemize}
\tightlist
\item
  It normalizes across the features of a single token (the ``layer''),
  not across the batch.
\item
  This makes it independent of batch size, which is important for NLP
  where batch sizes can vary.
\end{itemize}

So if a hidden vector has 768 channels, LayerNorm computes the mean and
variance over those 768 numbers for each token.

\subsubsection{Trainable Parameters}\label{trainable-parameters}

LayerNorm isn't just normalization-it also has two trainable vectors:

\begin{itemize}
\tightlist
\item
  γ (gamma): scales each feature after normalization.
\item
  β (beta): shifts each feature after normalization.
\end{itemize}

These allow the network to ``undo'' normalization when necessary, giving
it flexibility.

\subsubsection{Formula}\label{formula}

For each input vector \texttt{x} of size \texttt{d}:

\begin{verbatim}
mean = (1/d) * Σ x_i
var  = (1/d) * Σ (x_i - mean)^2
x_norm = (x - mean) / sqrt(var + eps)
y = γ * x_norm + β
\end{verbatim}

Where \texttt{eps} is a tiny constant (like \texttt{1e-5}) to avoid
dividing by zero.

\subsubsection{\texorpdfstring{Implementation in
\emph{llm.c}}{Implementation in llm.c}}\label{implementation-in-llm.c-2}

In the code, LayerNorm is implemented as a simple function that loops
over features, computes mean and variance, and applies the formula
above. It's not hidden inside a framework-it's right there in C, so you
can step through it line by line.

For example, the forward pass looks like this (simplified):

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{void}\NormalTok{ layernorm\_forward}\OperatorTok{(}\DataTypeTok{float}\OperatorTok{*}\NormalTok{ out}\OperatorTok{,} \DataTypeTok{float}\OperatorTok{*}\NormalTok{ inp}\OperatorTok{,} \DataTypeTok{float}\OperatorTok{*}\NormalTok{ weight}\OperatorTok{,} \DataTypeTok{float}\OperatorTok{*}\NormalTok{ bias}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ N}\OperatorTok{)} \OperatorTok{\{}
    \DataTypeTok{float}\NormalTok{ mean }\OperatorTok{=} \FloatTok{0.0}\BuiltInTok{f}\OperatorTok{,}\NormalTok{ var }\OperatorTok{=} \FloatTok{0.0}\BuiltInTok{f}\OperatorTok{;}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}}\NormalTok{ N}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)}\NormalTok{ mean }\OperatorTok{+=}\NormalTok{ inp}\OperatorTok{[}\NormalTok{i}\OperatorTok{];}
\NormalTok{    mean }\OperatorTok{/=}\NormalTok{ N}\OperatorTok{;}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}}\NormalTok{ N}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)}\NormalTok{ var }\OperatorTok{+=} \OperatorTok{(}\NormalTok{inp}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{{-}}\NormalTok{ mean}\OperatorTok{)} \OperatorTok{*} \OperatorTok{(}\NormalTok{inp}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{{-}}\NormalTok{ mean}\OperatorTok{);}
\NormalTok{    var }\OperatorTok{/=}\NormalTok{ N}\OperatorTok{;}
    \DataTypeTok{float}\NormalTok{ inv\_std }\OperatorTok{=} \FloatTok{1.0}\BuiltInTok{f} \OperatorTok{/}\NormalTok{ sqrtf}\OperatorTok{(}\NormalTok{var }\OperatorTok{+} \FloatTok{1e{-}5}\BuiltInTok{f}\OperatorTok{);}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}}\NormalTok{ N}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{        out}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{=} \OperatorTok{(}\NormalTok{inp}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{{-}}\NormalTok{ mean}\OperatorTok{)} \OperatorTok{*}\NormalTok{ inv\_std }\OperatorTok{*}\NormalTok{ weight}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{+}\NormalTok{ bias}\OperatorTok{[}\NormalTok{i}\OperatorTok{];}
    \OperatorTok{\}}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

This is the kind of clear, low-level implementation that makes
\emph{llm.c} educational.

\subsubsection{Where It Fits in GPT-2}\label{where-it-fits-in-gpt-2}

Each transformer block contains two LayerNorms:

\begin{itemize}
\tightlist
\item
  One before attention.
\item
  One before the MLP.
\end{itemize}

GPT-2 uses Pre-LN architecture: inputs are normalized before each
sublayer. This makes training more stable and gradients flow better.

\subsubsection{Why It Matters}\label{why-it-matters-13}

LayerNorm may look like a small detail, but without it, GPT-2 would fail
to train reliably. It smooths out the flow of activations so attention
and MLP layers can do their job. In practice, this is one of the
critical ``glue'' components that makes deep transformers trainable at
scale.

\subsubsection{Try It Yourself}\label{try-it-yourself-15}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Print statistics: After applying LayerNorm, print the mean and
  variance of the output. Do they stay close to 0 and 1?
\item
  Remove γ and β: Force gamma to 1 and beta to 0. Does the model still
  train? Compare losses.
\item
  Disable normalization: Comment out LayerNorm and train. How unstable
  does training become?
\item
  Compare positions: Try switching to Post-LN (apply normalization after
  attention/MLP). Does this change convergence speed?
\item
  Vary epsilon: Change \texttt{1e-5} to \texttt{1e-2} or \texttt{1e-8}.
  How sensitive is training?
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-15}

LayerNorm is the quiet stabilizer of GPT-2. It makes sure each token's
features stay balanced, while γ and β keep flexibility. In \emph{llm.c},
it's implemented directly with clear C code, letting you see exactly how
normalization is calculated. It's a small but indispensable piece of the
transformer puzzle.

\subsection{27. Residual Connections: Keeping the Signal
Flowing}\label{residual-connections-keeping-the-signal-flowing}

Transformers like GPT-2 don't just stack layers on top of each other
blindly. They use residual connections-a trick that allows the input of
a layer to be added back to its output. This simple addition helps
signals flow through the network without vanishing or exploding, and it
makes training deep models possible.

\subsubsection{The Basic Idea}\label{the-basic-idea}

Imagine you have a function \texttt{F(x)} representing some
transformation (like attention or an MLP). Instead of just computing:

\begin{verbatim}
y = F(x)
\end{verbatim}

the transformer does:

\begin{verbatim}
y = F(x) + x
\end{verbatim}

This means the layer learns only the \emph{difference} it needs to add
to the input, instead of replacing it entirely.

\subsubsection{Why This Helps}\label{why-this-helps}

Residuals solve two big problems in deep networks:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Gradient flow: During backpropagation, gradients can get smaller and
  smaller as they pass through many layers. Adding the input back
  ensures gradients always have a path straight through.
\item
  Information preservation: Even if \texttt{F(x)} distorts the signal,
  the original \texttt{x} is still there. This prevents the model from
  ``forgetting'' important information.
\item
  Faster training: The network doesn't have to re-learn identity
  mappings-it can just pass them through the skip connection.
\end{enumerate}

\subsubsection{\texorpdfstring{Implementation in
\emph{llm.c}}{Implementation in llm.c}}\label{implementation-in-llm.c-3}

Residuals in \emph{llm.c} are implemented as a straightforward
elementwise addition:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{void}\NormalTok{ residual\_forward}\OperatorTok{(}\DataTypeTok{float}\OperatorTok{*}\NormalTok{ out}\OperatorTok{,} \DataTypeTok{float}\OperatorTok{*}\NormalTok{ inp1}\OperatorTok{,} \DataTypeTok{float}\OperatorTok{*}\NormalTok{ inp2}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ N}\OperatorTok{)} \OperatorTok{\{}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}}\NormalTok{ N}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{        out}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{=}\NormalTok{ inp1}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{+}\NormalTok{ inp2}\OperatorTok{[}\NormalTok{i}\OperatorTok{];}
    \OperatorTok{\}}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

Here:

\begin{itemize}
\tightlist
\item
  \texttt{inp1} is the output of the layer (like attention).
\item
  \texttt{inp2} is the original input.
\item
  \texttt{out} is the combined result.
\end{itemize}

This is done for every token position and feature channel.

\subsubsection{Where Residuals Are Used}\label{where-residuals-are-used}

In GPT-2, every transformer block has two residuals:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Attention residual: Adds the input of the attention layer to its
  output.
\item
  MLP residual: Adds the input of the MLP to its output.
\end{enumerate}

So the data flowing through the network always carries both the new
transformation and the original signal.

\subsubsection{Why It Matters}\label{why-it-matters-14}

Without residual connections, stacking 12--48 transformer blocks would
be nearly impossible to train. Gradients would vanish, and the model
would either stop learning or take forever to converge. Residuals let
deep transformers scale smoothly.

They also add an intuitive interpretation: each block is like a
``refinement step'' rather than a full rewrite of the representation.

\subsubsection{Try It Yourself}\label{try-it-yourself-16}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Remove residuals: Comment out the addition in the code. Does training
  collapse?
\item
  Scale residuals: Multiply the input by 0.5 before adding. Does this
  slow convergence?
\item
  Check loss curves: Compare training with and without residuals for the
  first 500 steps.
\item
  Inspect outputs: Print the norms of \texttt{inp1}, \texttt{inp2}, and
  \texttt{out}. Are the scales balanced?
\item
  Deeper models: Increase the number of layers from 12 to 24. Does the
  importance of residuals become more obvious?
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-16}

Residual connections are the ``lifeline'' of deep transformers. By
simply adding inputs back into outputs, they make it possible to train
very deep networks without losing gradients or information. In
\emph{llm.c}, the implementation is as simple as looping over arrays and
adding them-but the effect is profound: residuals are what let GPT-2 go
deep and still work.

\subsection{28. Attention Masking: Enforcing
Causality}\label{attention-masking-enforcing-causality}

One of the defining traits of GPT-2 is that it's a causal language
model. That means it predicts the \emph{next} token given all the tokens
before it, but never cheats by looking ahead. To enforce this, GPT-2
applies an attention mask inside every attention layer.

\subsubsection{Why a Mask Is Needed}\label{why-a-mask-is-needed}

Without a mask, attention is free to connect any token to any other,
including future ones. For example:

\begin{itemize}
\tightlist
\item
  Input: ``The cat sat on the''
\item
  Target: ``mat''
\end{itemize}

If the model could peek at ``mat'' while computing attention, the task
would be trivial-it could just copy the next word. That would break the
training objective.

The mask forces the model to only use tokens at or before the current
position when making predictions.

\subsubsection{How the Mask Works}\label{how-the-mask-works}

When computing attention scores (\texttt{Q\ ×\ K\^{}T\ /\ sqrt(d\_k)}),
the result is a matrix of size \texttt{{[}T,\ T{]}} where each row
corresponds to one token attending to all others.

The mask modifies this matrix:

\begin{itemize}
\tightlist
\item
  Allowed positions (past and present): keep scores as is.
\item
  Disallowed positions (future): set scores to \texttt{-inf}.
\end{itemize}

After applying softmax, those \texttt{-inf} entries become zero
probability, effectively blocking attention to the future.

\subsubsection{\texorpdfstring{Implementation in
\emph{llm.c}}{Implementation in llm.c}}\label{implementation-in-llm.c-4}

The causal mask is applied during the attention forward pass. The code
uses a loop to zero out invalid positions:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ t }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ t }\OperatorTok{\textless{}}\NormalTok{ T}\OperatorTok{;}\NormalTok{ t}\OperatorTok{++)} \OperatorTok{\{}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ u }\OperatorTok{=}\NormalTok{ t }\OperatorTok{+} \DecValTok{1}\OperatorTok{;}\NormalTok{ u }\OperatorTok{\textless{}}\NormalTok{ T}\OperatorTok{;}\NormalTok{ u}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{        scores}\OperatorTok{[}\NormalTok{t}\OperatorTok{][}\NormalTok{u}\OperatorTok{]} \OperatorTok{=} \OperatorTok{{-}}\FloatTok{1e9}\OperatorTok{;} \CommentTok{// block future positions}
    \OperatorTok{\}}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

Here \texttt{T} is the sequence length. This ensures that token
\texttt{t} can only attend to itself and earlier tokens.

\subsubsection{Visualizing the Mask}\label{visualizing-the-mask}

Think of the mask as a triangular matrix:

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
& 0 & 1 & 2 & 3 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & ✓ & & & \\
1 & ✓ & ✓ & & \\
2 & ✓ & ✓ & ✓ & \\
3 & ✓ & ✓ & ✓ & ✓ \\
\end{longtable}

Each row shows which past tokens a given position can look at. Future
positions remain blank.

\subsubsection{Why It Matters}\label{why-it-matters-15}

The mask is what makes GPT-2 a predictive model instead of a
bidirectional encoder like BERT. Without it, the model could ``cheat''
and the training objective would no longer match how it's used at
inference time (generating text step by step).

This small detail-just filling part of a matrix with \texttt{-inf}-is
critical to making autoregressive text generation possible.

\subsubsection{Try It Yourself}\label{try-it-yourself-17}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Disable the mask: Comment out the masking code. Watch validation loss
  drop unrealistically, then notice that text generation produces
  garbage.
\item
  Reverse the mask: Block the past and allow the future. Does the model
  still train? What does it predict?
\item
  Partial mask: Only allow attention to the previous 5 tokens (a sliding
  window). How does this affect learning long-range structure?
\item
  Print scores: Before and after masking, log a row of attention scores.
  Notice how future positions become huge negatives.
\item
  Visualize: Write a small script to plot the attention mask as a
  matrix. It should look strictly lower-triangular.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-17}

Attention masking is a simple but essential trick. By filling future
positions with \texttt{-inf} before softmax, GPT-2 ensures that each
token can only attend to its past. In \emph{llm.c}, this is implemented
with just a couple of loops-but it's what turns a generic transformer
into a true causal language model.

\subsection{29. Output Head: From Hidden States to
Vocabulary}\label{output-head-from-hidden-states-to-vocabulary}

After tokens pass through embeddings, attention, MLPs, LayerNorm, and
residuals, we end up with hidden states for every position in the
sequence. But GPT-2's final job is not to output vectors-it must predict
the next token from the vocabulary. This is handled by the output head,
the last stage of the model.

\subsubsection{What the Output Head
Does}\label{what-the-output-head-does}

The output head maps hidden states of shape \texttt{(B,\ T,\ channels)}
into logits of shape \texttt{(B,\ T,\ vocab\_size)}. Each logit
represents the model's ``raw score'' for how likely a particular token
is at the next step.

The pipeline looks like this:

\begin{verbatim}
hidden states → Linear projection → Logits → Softmax → Probabilities
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Logits: real numbers, one per token in the vocabulary.
\item
  Softmax: converts logits into probabilities that sum to 1.
\item
  Predicted token: the token with the highest probability (or sampled
  from the distribution).
\end{itemize}

\subsubsection{Tied Weights with
Embeddings}\label{tied-weights-with-embeddings}

In GPT-2, the token embedding table and the output head share weights.
This means the same matrix is used both for:

\begin{itemize}
\tightlist
\item
  Mapping tokens to vectors at the start (embedding lookup).
\item
  Mapping vectors back to tokens at the end (output head).
\end{itemize}

Mathematically, this improves efficiency and helps align input and
output representations.

In \emph{llm.c}, this is done by simply pointing both embedding and
output head to the same parameter slice.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{// token embedding table}
\DataTypeTok{float}\OperatorTok{*}\NormalTok{ token\_embedding\_table }\OperatorTok{=}\NormalTok{ params}\OperatorTok{;}
\CommentTok{// output head reuses the same memory}
\DataTypeTok{float}\OperatorTok{*}\NormalTok{ output\_head }\OperatorTok{=}\NormalTok{ token\_embedding\_table}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

When the model projects hidden states back to vocab space, it does a
matrix multiply with this shared matrix.

\subsubsection{Shapes in Action}\label{shapes-in-action}

For GPT-2 Small:

\begin{itemize}
\tightlist
\item
  Hidden states: \texttt{{[}B,\ T,\ 768{]}}
\item
  Output projection (embedding transpose): \texttt{{[}768,\ 50257{]}}
\item
  Logits: \texttt{{[}B,\ T,\ 50257{]}}
\end{itemize}

That's more than 50k scores per position, one for each token in the
vocabulary.

\subsubsection{Why Weight Tying Helps}\label{why-weight-tying-helps}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Memory efficiency: You don't need a separate giant matrix for the
  output head.
\item
  Better learning: The same vectors that represent tokens going in also
  represent them going out, which reinforces consistency.
\item
  Simpler code: Just reuse the same parameter slice.
\end{enumerate}

This trick is why GPT-2 can scale vocab sizes without blowing up
parameter counts too much.

\subsubsection{Why It Matters}\label{why-it-matters-16}

The output head is where everything comes together. For each position,
the model collapses its hidden representation into a distribution over
possible next tokens. This is how GPT-2 generates text one step at a
time. Without this step, you'd only have abstract hidden states-useful
internally, but not something you can read.

\subsubsection{Try It Yourself}\label{try-it-yourself-18}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Print logits: After a forward pass, print the logits for the last
  token. Do they look like random floats at initialization?
\item
  Check probability sum: Apply softmax to logits and verify the
  probabilities sum to 1.
\item
  Untie weights: Make the output head its own matrix instead of reusing
  embeddings. Does training still work? How does the parameter count
  change?
\item
  Top-k sampling: Modify sampling to keep only the top 5 logits before
  softmax. What kind of text does this produce?
\item
  Greedy vs random: Compare greedy decoding (argmax) vs random sampling
  from probabilities. Which one gives more interesting outputs?
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-18}

The output head is the final bridge between hidden vectors and actual
words. By reusing the token embedding matrix, GPT-2 projects hidden
states back into vocabulary space and produces logits for every possible
token. In \emph{llm.c}, this step is just another matrix
multiplication-but it's the one that turns internal math into real text
predictions.

\subsection{30. Loss Function: Cross-Entropy over
Vocabulary}\label{loss-function-cross-entropy-over-vocabulary}

Training GPT-2 means teaching it to predict the next token in a
sequence. To measure how well it's doing, we need a loss function that
compares the model's predicted probabilities with the true token IDs. In
\emph{llm.c}, this is done with the cross-entropy loss-a standard choice
for classification tasks.

\subsubsection{From Logits to
Probabilities}\label{from-logits-to-probabilities}

After the output head, we have logits of shape
\texttt{(B,\ T,\ vocab\_size)}. These are raw scores. To turn them into
probabilities:

\begin{verbatim}
probs = softmax(logits)
\end{verbatim}

Softmax ensures two things:

\begin{itemize}
\tightlist
\item
  All values are between 0 and 1.
\item
  They sum to 1 across the vocabulary.
\end{itemize}

So for each position, you get a probability distribution over all
possible next tokens.

\subsubsection{Cross-Entropy Definition}\label{cross-entropy-definition}

Cross-entropy compares the predicted distribution \texttt{p} with the
true distribution \texttt{q}. For language modeling:

\begin{itemize}
\tightlist
\item
  \texttt{q} is a one-hot vector (all zeros, except 1 at the true token
  index).
\item
  \texttt{p} is the probability vector from softmax.
\end{itemize}

The formula for one token:

\begin{verbatim}
loss = -log(p[true_token])
\end{verbatim}

For a batch, you average across all tokens in all sequences.

\subsubsection{\texorpdfstring{Implementation in
\emph{llm.c}}{Implementation in llm.c}}\label{implementation-in-llm.c-5}

In C, this boils down to:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{float}\NormalTok{ loss }\OperatorTok{=} \FloatTok{0.0}\BuiltInTok{f}\OperatorTok{;}
\ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ b }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ b }\OperatorTok{\textless{}}\NormalTok{ B}\OperatorTok{;}\NormalTok{ b}\OperatorTok{++)} \OperatorTok{\{}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ t }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ t }\OperatorTok{\textless{}}\NormalTok{ T}\OperatorTok{;}\NormalTok{ t}\OperatorTok{++)} \OperatorTok{\{}
        \DataTypeTok{int}\NormalTok{ target }\OperatorTok{=}\NormalTok{ targets}\OperatorTok{[}\NormalTok{b}\OperatorTok{*}\NormalTok{T }\OperatorTok{+}\NormalTok{ t}\OperatorTok{];}
        \DataTypeTok{float}\NormalTok{ logit\_max }\OperatorTok{=} \OperatorTok{{-}}\FloatTok{1e9}\OperatorTok{;}
        \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ v }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ v }\OperatorTok{\textless{}}\NormalTok{ vocab\_size}\OperatorTok{;}\NormalTok{ v}\OperatorTok{++)} \OperatorTok{\{}
            \ControlFlowTok{if} \OperatorTok{(}\NormalTok{logits}\OperatorTok{[}\NormalTok{b}\OperatorTok{*}\NormalTok{T}\OperatorTok{*}\NormalTok{vocab\_size }\OperatorTok{+}\NormalTok{ t}\OperatorTok{*}\NormalTok{vocab\_size }\OperatorTok{+}\NormalTok{ v}\OperatorTok{]} \OperatorTok{\textgreater{}}\NormalTok{ logit\_max}\OperatorTok{)} \OperatorTok{\{}
\NormalTok{                logit\_max }\OperatorTok{=}\NormalTok{ logits}\OperatorTok{[}\NormalTok{b}\OperatorTok{*}\NormalTok{T}\OperatorTok{*}\NormalTok{vocab\_size }\OperatorTok{+}\NormalTok{ t}\OperatorTok{*}\NormalTok{vocab\_size }\OperatorTok{+}\NormalTok{ v}\OperatorTok{];}
            \OperatorTok{\}}
        \OperatorTok{\}}
        \CommentTok{// compute softmax denominator}
        \DataTypeTok{float}\NormalTok{ sum }\OperatorTok{=} \FloatTok{0.0}\BuiltInTok{f}\OperatorTok{;}
        \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ v }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ v }\OperatorTok{\textless{}}\NormalTok{ vocab\_size}\OperatorTok{;}\NormalTok{ v}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{            sum }\OperatorTok{+=}\NormalTok{ expf}\OperatorTok{(}\NormalTok{logits}\OperatorTok{[}\NormalTok{b}\OperatorTok{*}\NormalTok{T}\OperatorTok{*}\NormalTok{vocab\_size }\OperatorTok{+}\NormalTok{ t}\OperatorTok{*}\NormalTok{vocab\_size }\OperatorTok{+}\NormalTok{ v}\OperatorTok{]} \OperatorTok{{-}}\NormalTok{ logit\_max}\OperatorTok{);}
        \OperatorTok{\}}
        \DataTypeTok{float}\NormalTok{ logprob }\OperatorTok{=}\NormalTok{ logits}\OperatorTok{[}\NormalTok{b}\OperatorTok{*}\NormalTok{T}\OperatorTok{*}\NormalTok{vocab\_size }\OperatorTok{+}\NormalTok{ t}\OperatorTok{*}\NormalTok{vocab\_size }\OperatorTok{+}\NormalTok{ target}\OperatorTok{]} \OperatorTok{{-}}\NormalTok{ logit\_max }\OperatorTok{{-}}\NormalTok{ logf}\OperatorTok{(}\NormalTok{sum}\OperatorTok{);}
\NormalTok{        loss }\OperatorTok{+=} \OperatorTok{{-}}\NormalTok{logprob}\OperatorTok{;}
    \OperatorTok{\}}
\OperatorTok{\}}
\NormalTok{loss }\OperatorTok{/=} \OperatorTok{(}\NormalTok{B }\OperatorTok{*}\NormalTok{ T}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

This snippet shows how \emph{llm.c} explicitly computes softmax and
cross-entropy in loops. No black boxes-just raw math.

\subsubsection{Intuition}\label{intuition}

\begin{itemize}
\tightlist
\item
  If the model assigns high probability to the correct token → loss is
  small.
\item
  If the model assigns low probability to the correct token → loss is
  large.
\item
  Minimizing loss means pushing probability mass toward the right
  answers.
\end{itemize}

\subsubsection{Why Cross-Entropy Works for
Language}\label{why-cross-entropy-works-for-language}

Language modeling is essentially a huge multi-class classification
problem: at each step, which word comes next? Cross-entropy is perfect
here because it directly penalizes wrong predictions proportional to how
confident the model was.

\subsubsection{Why It Matters}\label{why-it-matters-17}

The loss function is the only signal the model gets about how well it's
doing. Everything else-parameter updates, weight tuning, learning
dynamics-flows from this single number. A well-implemented cross-entropy
ensures training is stable and meaningful.

\subsubsection{Try It Yourself}\label{try-it-yourself-19}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Check values: Print the loss after the first few steps. It should be
  close to \texttt{log(vocab\_size)} (≈10.8 for 50k vocab) before
  training.
\item
  Overfit tiny batch: Train on just one sequence. Does the loss go near
  0 after enough steps?
\item
  Change target: Replace the true token with a random one. Does the loss
  increase immediately?
\item
  Compare vocab sizes: Train with a smaller vocabulary (e.g., 100
  tokens). Does initial loss drop to \texttt{log(100)\ ≈\ 4.6}?
\item
  Inspect probabilities: For one token, print the top 5 predicted
  probabilities. Does the true token climb to the top as training
  progresses?
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-19}

The cross-entropy loss is the compass guiding GPT-2 during training. It
turns raw logits into probabilities and measures how well the model
predicts the correct next token. In \emph{llm.c}, it's implemented with
explicit loops and math, letting you see exactly how probabilities and
losses are computed. Without this step, the model would have no way to
learn from its mistakes.

\section{Chapter 4. CPU Inference (Forward
Only)}\label{chapter-4.-cpu-inference-forward-only}

\subsection{31. Forward Pass
Walkthrough}\label{forward-pass-walkthrough}

When we talk about the \emph{forward pass} in GPT-2, we mean the process
of turning an input sentence (like ``The cat sat on the'') into
predictions for the next word. In simple terms, it's how the model
``thinks'' before giving an answer. In \texttt{train\_gpt2.c}, this
happens inside the function \texttt{gpt2\_forward}. Let's walk through
it slowly, step by step, so you can see how numbers flow through the
model and transform along the way.

\subsubsection{1. From Words to Numbers}\label{from-words-to-numbers}

Computers don't understand words like \emph{cat} or \emph{sat}. They
only understand numbers. Before the forward pass starts, text is already
tokenized into IDs (integers). For example:

\begin{verbatim}
"The cat sat" → [464, 3290, 616]
\end{verbatim}

Each number is a token ID. The model doesn't yet know what ``464'' means
in plain English-it just knows it's a number that points into a table.

\subsubsection{2. Embedding: Giving Words
Meaning}\label{embedding-giving-words-meaning}

The first real step in the forward pass is embedding lookup. Imagine we
have a huge dictionary, but instead of definitions in English, each word
ID points to a long vector of numbers (say, 768 numbers for GPT-2
small).

\begin{itemize}
\tightlist
\item
  Word embeddings (\texttt{wte}): Each token ID becomes a vector that
  captures the meaning of the word.
\item
  Position embeddings (\texttt{wpe}): Each token also gets a vector for
  its position: first word, second word, third word, etc.
\end{itemize}

The model adds these two vectors together. This way, it knows not just
what the word is, but also where it is in the sentence.

For example:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0575}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0575}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2989}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.3448}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2414}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Token
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Word
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Word Embedding (shortened)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Position Embedding (shortened)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Combined Vector
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
464 & ``The'' & {[}0.2, -0.5, 0.1, \ldots{]} & {[}0.0, 0.1, -0.3,
\ldots{]} & {[}0.2, -0.4, -0.2, \ldots{]} \\
3290 & ``cat'' & {[}0.9, -0.2, 0.4, \ldots{]} & {[}0.1, -0.1, -0.2,
\ldots{]} & {[}1.0, -0.3, 0.2, \ldots{]} \\
\end{longtable}

Now every token is a vector with both meaning and position built in.

\subsubsection{3. Transformer Layers: The Thinking
Steps}\label{transformer-layers-the-thinking-steps}

GPT-2 has multiple identical layers stacked on top of each other. Each
layer has two big parts: attention and MLP (feed-forward network).

Attention (looking around):

\begin{itemize}
\tightlist
\item
  Each word asks: ``Which other words should I pay attention to right
  now?''
\item
  For ``sat,'' attention might focus heavily on ``cat,'' because those
  words are related.
\item
  The code computes \emph{queries}, \emph{keys}, and \emph{values} for
  every word, then does dot-products, softmax, and weighted sums to mix
  information.
\end{itemize}

MLP (processing deeply):

\begin{itemize}
\tightlist
\item
  After attention, each token passes through a mini neural network (two
  matrix multiplications with a nonlinear GELU function in between).
\item
  This helps each word refine its understanding, even if it doesn't
  directly attend to another word.
\end{itemize}

Both blocks have residual connections: the input is added back to the
output, like keeping the original notes while adding new insights. This
prevents information loss.

\subsubsection{4. Normalization: Keeping Numbers
Stable}\label{normalization-keeping-numbers-stable}

At many points, the model normalizes vectors so they don't explode in
size or shrink too small. This is called LayerNorm. It ensures training
is stable, like making sure your cooking pot doesn't boil over or dry
out.

\subsubsection{5. The Final Prediction
Layer}\label{the-final-prediction-layer}

After all layers, the model produces a final vector for each position.
Then:

\begin{itemize}
\tightlist
\item
  It multiplies those vectors by the embedding table again (but
  transposed).
\item
  This gives logits: raw scores for each word in the vocabulary (about
  50k options).
\end{itemize}

Example: for the last token ``on the,'' the logits might be:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Word & Logit & Probability (after softmax) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
``mat'' & 7.2 & 0.85 \\
``dog'' & 5.1 & 0.10 \\
``car'' & 3.0 & 0.05 \\
\end{longtable}

The highest probability is ``mat.''

\subsubsection{6. Softmax: Turning Scores into
Probabilities}\label{softmax-turning-scores-into-probabilities}

The logits are big numbers, but they don't mean much until we apply
softmax. Softmax makes them into probabilities that sum to 1. This way,
we can interpret them as chances: ``There's an 85\% chance the next word
is \emph{mat}.''

\subsubsection{7. Cross-Entropy Loss: Measuring
Mistakes}\label{cross-entropy-loss-measuring-mistakes}

If we're training, we also give the model the correct next word. The
model checks how much probability it gave to that word. If it gave it
high probability, the loss is low. If it gave it low probability, the
loss is high.

\begin{itemize}
\tightlist
\item
  Correct: ``mat'' (probability 0.85 → loss ≈ 0.16, small).
\item
  Wrong: ``car'' (probability 0.05 → loss ≈ 3.0, large).
\end{itemize}

This loss is averaged across all tokens, and it's the signal that tells
the backward pass how to update the model.

\subsubsection{8. Why It Matters}\label{why-it-matters-18}

The forward pass is the part of GPT-2 that generates predictions.
Without it, the model can't ``think'' or make sense of input. It's like
the brain processing sensory input before deciding what to do. In
\texttt{train\_gpt2.c}, the forward pass is written with plain C loops,
which makes the math crystal clear instead of hidden inside deep
learning libraries.

\subsubsection{9. Try It Yourself}\label{try-it-yourself-20}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Print embeddings: Modify the code to print the vector for the first
  token. See how it's just numbers, but those numbers are the
  ``meaning'' of the word.
\item
  Inspect probabilities: After the forward pass, print the softmax
  probabilities for one position. They should sum to 1.0.
\item
  Change sequence length: Increase \texttt{T} from 64 to 128. Notice how
  validation slows down, because attention compares all tokens with all
  others (\texttt{T²} scaling).
\item
  Baseline loss: Before training, measure the loss. It should be around
  \texttt{log(vocab\_size)} (≈10.8 for GPT-2 small). That's the loss of
  random guessing.
\item
  Mask experiment: Temporarily remove the causal mask in attention. The
  model will ``cheat'' by looking ahead, and loss will drop
  unrealistically.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-20}

The forward pass is like the thought process of GPT-2. Input words
become vectors, vectors mix through attention and MLPs, everything gets
normalized, and finally the model produces probabilities for the next
word. It's a carefully choreographed dance of math operations, all coded
in plain C loops in \texttt{train\_gpt2.c}. Once you understand this
flow, you can follow exactly how GPT-2 turns raw tokens into intelligent
predictions.

\subsection{32. Token and Positional Embedding
Lookup}\label{token-and-positional-embedding-lookup}

Before GPT-2 can do anything intelligent with text, it needs to turn raw
numbers (token IDs) into vectors that capture meaning and context. This
is the role of embeddings. In \texttt{train\_gpt2.c}, this step is
handled by the function \texttt{encoder\_forward}. Let's take a closer
look at how it works and why it matters.

\subsubsection{Tokens Are Just Numbers}\label{tokens-are-just-numbers}

Suppose you type:

\begin{verbatim}
"The cat sat on the mat."
\end{verbatim}

After tokenization, this sentence might look like:

\begin{verbatim}
[464, 3290, 616, 319, 262, 1142, 13]
\end{verbatim}

These are just IDs. The model doesn't inherently know that \texttt{3290}
means ``cat.'' It only knows it needs to use these numbers to fetch
vectors from a table.

\subsubsection{The Embedding Tables}\label{the-embedding-tables}

The model has two important tables stored in memory:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Word Token Embeddings (\texttt{wte})

  \begin{itemize}
  \tightlist
  \item
    Size: \texttt{(V,\ C)} where \texttt{V} is vocab size
    (\textasciitilde50,000 for GPT-2 small) and \texttt{C} is channels
    (768).
  \item
    Each row corresponds to a token ID.
  \item
    Example: row 3290 might be \texttt{{[}0.12,\ -0.45,\ 0.88,\ …{]}}.
  \end{itemize}
\item
  Positional Embeddings (\texttt{wpe})

  \begin{itemize}
  \tightlist
  \item
    Size: \texttt{(maxT,\ C)} where \texttt{maxT} is the maximum
    sequence length (e.g.~1024).
  \item
    Each row corresponds to a position index: 0 for the first token, 1
    for the second, etc.
  \item
    Example: position 2 might be \texttt{{[}0.07,\ 0.31,\ -0.22,\ …{]}}.
  \end{itemize}
\end{enumerate}

Both tables are filled with trainable values. At the start, they're
random. As training progresses, the optimizer updates them so they
encode useful patterns.

\subsubsection{Adding Them Together}\label{adding-them-together-1}

For each token at position \texttt{t}:

\begin{itemize}
\tightlist
\item
  Look up its word vector from \texttt{wte}.
\item
  Look up its position vector from \texttt{wpe}.
\item
  Add them elementwise.
\end{itemize}

This gives a final vector of size \texttt{C} that represents what the
token is and where it is.

Example with simplified numbers:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1270}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0635}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2698}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2698}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2698}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Token ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Word
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Word Embedding
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Position
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Combined
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
464 & The & {[}0.1, -0.2, 0.3{]} & {[}0.2, 0.0, -0.1{]} & {[}0.3, -0.2,
0.2{]} \\
3290 & cat & {[}0.4, 0.5, -0.3{]} & {[}0.0, 0.1, 0.2{]} & {[}0.4, 0.6,
-0.1{]} \\
\end{longtable}

Now the vector doesn't just mean ``cat,'' it means ``cat at position
1.''

\subsubsection{Why Position Matters}\label{why-position-matters}

Without positions, the model would treat:

\begin{itemize}
\tightlist
\item
  ``The cat sat''
\item
  ``Sat cat the''
\end{itemize}

as identical, because they use the same tokens. But word order is
essential in language. By adding positional embeddings, GPT-2 knows the
difference between ``dog bites man'' and ``man bites dog.''

\subsubsection{Inside the Code}\label{inside-the-code}

The embedding lookup is written explicitly with loops in C:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ b }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ b }\OperatorTok{\textless{}}\NormalTok{ B}\OperatorTok{;}\NormalTok{ b}\OperatorTok{++)} \OperatorTok{\{}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ t }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ t }\OperatorTok{\textless{}}\NormalTok{ T}\OperatorTok{;}\NormalTok{ t}\OperatorTok{++)} \OperatorTok{\{}
        \DataTypeTok{float}\OperatorTok{*}\NormalTok{ out\_bt }\OperatorTok{=}\NormalTok{ out }\OperatorTok{+}\NormalTok{ b }\OperatorTok{*}\NormalTok{ T }\OperatorTok{*}\NormalTok{ C }\OperatorTok{+}\NormalTok{ t }\OperatorTok{*}\NormalTok{ C}\OperatorTok{;}
        \DataTypeTok{int}\NormalTok{ ix }\OperatorTok{=}\NormalTok{ inp}\OperatorTok{[}\NormalTok{b }\OperatorTok{*}\NormalTok{ T }\OperatorTok{+}\NormalTok{ t}\OperatorTok{];}
        \DataTypeTok{float}\OperatorTok{*}\NormalTok{ wte\_ix }\OperatorTok{=}\NormalTok{ wte }\OperatorTok{+}\NormalTok{ ix }\OperatorTok{*}\NormalTok{ C}\OperatorTok{;}
        \DataTypeTok{float}\OperatorTok{*}\NormalTok{ wpe\_t }\OperatorTok{=}\NormalTok{ wpe }\OperatorTok{+}\NormalTok{ t }\OperatorTok{*}\NormalTok{ C}\OperatorTok{;}
        \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}}\NormalTok{ C}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{            out\_bt}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{=}\NormalTok{ wte\_ix}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{+}\NormalTok{ wpe\_t}\OperatorTok{[}\NormalTok{i}\OperatorTok{];}
        \OperatorTok{\}}
    \OperatorTok{\}}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

What's happening here:

\begin{itemize}
\tightlist
\item
  Loop over batches (\texttt{b}) and sequence positions (\texttt{t}).
\item
  Find the token ID \texttt{ix}.
\item
  Fetch its embedding \texttt{wte\_ix}.
\item
  Fetch its position embedding \texttt{wpe\_t}.
\item
  Add them element by element.
\end{itemize}

The result, \texttt{out\_bt}, is the vector for this token at this
position.

\subsubsection{Analogy}\label{analogy}

Think of it like name tags at a conference:

\begin{itemize}
\tightlist
\item
  The word embedding is your name: ``Alice.''
\item
  The position embedding is your table number: ``Table 7.''
\item
  Together, they tell the conference staff who you are and where you are
  seated.
\end{itemize}

Without the table number, they might know who you are but not where to
find you. Without your name, they just know there's someone at Table 7
but not who. Both are needed for proper context.

\subsubsection{Why It Matters}\label{why-it-matters-19}

Embeddings are the foundation of the whole model. If this step is wrong,
everything else collapses. They transform meaningless IDs into rich
vectors that carry semantic and positional information. This is the
entry point where language starts becoming something a neural network
can reason about.

\subsubsection{Try It Yourself}\label{try-it-yourself-21}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Print a token embedding: Modify the code to print out \texttt{wte\_ix}
  for a specific token ID like ``cat.'' You'll see a vector of floats,
  the learned representation.
\item
  Print a position embedding: Do the same for \texttt{wpe\_t} at
  position 0, 1, 2\ldots{} Notice how positions have unique but
  consistent patterns.
\item
  Check the sum: Verify that
  \texttt{out\_bt{[}i{]}\ =\ wte\_ix{[}i{]}\ +\ wpe\_t{[}i{]}}. This is
  literally how word and position are fused.
\item
  Shuffle words: Try feeding ``cat sat'' vs.~``sat cat.'' The embeddings
  will differ because the position vectors change, even though the words
  are the same.
\item
  Observe growth during training: After some training steps, dump the
  embeddings again. You'll notice they stop being random and start
  showing structure.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-21}

The embedding lookup is the very first step of the forward pass. It
takes raw numbers and makes them meaningful by combining token identity
and position. This prepares the input for the deeper transformer layers.
Even though the C code looks simple-a few nested loops-it's doing the
crucial work of giving words a mathematical shape the model can
understand.

\subsection{33. Attention: Matmuls, Masking, and Softmax on
CPU}\label{attention-matmuls-masking-and-softmax-on-cpu}

The attention mechanism is the heart of GPT-2. It's where each word in
the input sequence decides which other words to look at when forming its
representation. In \texttt{train\_gpt2.c}, this happens inside the
\texttt{attention\_forward} function, which implements multi-head
self-attention using plain C loops and matrix multiplications. Let's
break it down carefully, step by step, so even an absolute beginner can
follow the flow.

\subsubsection{The Big Idea of
Attention}\label{the-big-idea-of-attention}

Imagine you're reading:

\begin{quote}
``The cat sat on the mat.''
\end{quote}

When the model is trying to understand the word \emph{sat}, it doesn't
just look at \emph{sat} by itself. It wants to consider other words like
\emph{cat} (the subject) and \emph{mat} (likely the object). Attention
gives each token a way to ``consult'' earlier tokens and decide how
important they are.

This is done mathematically by projecting each token into three roles:
Query (Q), Key (K), and Value (V).

\begin{itemize}
\tightlist
\item
  Query (Q): ``What am I looking for?''
\item
  Key (K): ``What do I offer?''
\item
  Value (V): ``What information do I carry?''
\end{itemize}

\subsubsection{Step 1: Creating Q, K, and
V}\label{step-1-creating-q-k-and-v}

For every input vector of size \texttt{C} (e.g., 768), the code performs
three separate linear projections (matrix multiplications). These
produce Q, K, and V vectors of smaller size, divided among attention
heads.

In the code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{matmul\_forward}\OperatorTok{(}\NormalTok{acts}\OperatorTok{.}\NormalTok{q}\OperatorTok{,}\NormalTok{ acts}\OperatorTok{.}\NormalTok{ln1}\OperatorTok{,}\NormalTok{ params}\OperatorTok{.}\NormalTok{wq}\OperatorTok{,}\NormalTok{ params}\OperatorTok{.}\NormalTok{bq}\OperatorTok{,}\NormalTok{ B}\OperatorTok{,}\NormalTok{ T}\OperatorTok{,}\NormalTok{ C}\OperatorTok{,}\NormalTok{ C}\OperatorTok{);}
\NormalTok{matmul\_forward}\OperatorTok{(}\NormalTok{acts}\OperatorTok{.}\NormalTok{k}\OperatorTok{,}\NormalTok{ acts}\OperatorTok{.}\NormalTok{ln1}\OperatorTok{,}\NormalTok{ params}\OperatorTok{.}\NormalTok{wk}\OperatorTok{,}\NormalTok{ params}\OperatorTok{.}\NormalTok{bk}\OperatorTok{,}\NormalTok{ B}\OperatorTok{,}\NormalTok{ T}\OperatorTok{,}\NormalTok{ C}\OperatorTok{,}\NormalTok{ C}\OperatorTok{);}
\NormalTok{matmul\_forward}\OperatorTok{(}\NormalTok{acts}\OperatorTok{.}\NormalTok{v}\OperatorTok{,}\NormalTok{ acts}\OperatorTok{.}\NormalTok{ln1}\OperatorTok{,}\NormalTok{ params}\OperatorTok{.}\NormalTok{wv}\OperatorTok{,}\NormalTok{ params}\OperatorTok{.}\NormalTok{bv}\OperatorTok{,}\NormalTok{ B}\OperatorTok{,}\NormalTok{ T}\OperatorTok{,}\NormalTok{ C}\OperatorTok{,}\NormalTok{ C}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

Here:

\begin{itemize}
\tightlist
\item
  \texttt{acts.ln1} is the normalized input from the previous step.
\item
  \texttt{params.wq}, \texttt{params.wk}, \texttt{params.wv} are the
  weight matrices.
\item
  The output shapes are \texttt{(B,\ T,\ C)}.
\end{itemize}

So each token now has three new representations: Q, K, and V.

\subsubsection{Step 2: Computing Attention
Scores}\label{step-2-computing-attention-scores}

For each token at position \texttt{t}, we want to know how much it
should pay attention to every earlier token (including itself). This is
done with a dot product between its Query and all Keys.

Mathematically:

\begin{verbatim}
score[t][u] = (Q[t] ⋅ K[u]) / sqrt(dk)
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \texttt{t} = current token.
\item
  \texttt{u} = another token at or before \texttt{t}.
\item
  \texttt{sqrt(dk)} is a scaling factor (dk = size of each head) to keep
  values stable.
\end{itemize}

In the code, these dot products are done explicitly in loops.

\subsubsection{Step 3: Applying the Causal
Mask}\label{step-3-applying-the-causal-mask}

GPT-2 is an autoregressive model, meaning it only predicts the future
from the past, not the other way around. To enforce this, the attention
matrix is masked:

\begin{itemize}
\tightlist
\item
  Token at position \texttt{t} can only look at positions \texttt{≤\ t}.
\item
  Anything beyond \texttt{t} is set to a very negative value
  (\texttt{-1e9}), which becomes effectively zero after softmax.
\end{itemize}

This ensures, for example, that when predicting the 3rd word, the model
doesn't cheat by looking at the 4th.

\subsubsection{Step 4: Turning Scores into
Probabilities}\label{step-4-turning-scores-into-probabilities}

The scores are raw numbers that can be large and unstable. To convert
them into meaningful weights, the code applies softmax:

\begin{verbatim}
attention_weights[t][u] = exp(score[t][u]) / Σ exp(score[t][v])
\end{verbatim}

This makes all weights positive and ensures they sum to 1. Now each
token has a probability distribution over earlier tokens.

Example for the word \emph{sat}:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Attended Token & Raw Score & After Softmax \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
The & 1.2 & 0.10 \\
cat & 3.4 & 0.80 \\
sat (itself) & 0.7 & 0.10 \\
on & masked & 0.00 \\
\end{longtable}

Clearly, \emph{sat} focuses most strongly on \emph{cat}.

\subsubsection{Step 5: Weighted Sum of
Values}\label{step-5-weighted-sum-of-values}

Once the attention weights are computed, the model uses them to take a
weighted sum of the Value vectors:

\begin{verbatim}
output[t] = Σ attention_weights[t][u] * V[u]
\end{verbatim}

This produces a new representation for each token that blends in
information from others.

For \emph{sat}, its new vector will be mostly influenced by \emph{cat},
but also a little by \emph{The} and itself.

\subsubsection{Step 6: Multi-Head
Attention}\label{step-6-multi-head-attention}

In practice, attention is split into multiple heads (12 for GPT-2
small). Each head works on smaller chunks of the vector (C/heads).

\begin{itemize}
\tightlist
\item
  Head 1 might focus on subject--verb relationships.
\item
  Head 2 might track distances (like ``how far back was this token?'').
\item
  Head 3 might specialize in punctuation.
\end{itemize}

After all heads compute their outputs, the results are concatenated and
projected back into size \texttt{C} with another matrix multiply.

\subsubsection{Step 7: Residual
Connection}\label{step-7-residual-connection}

Finally, the output of the attention block is added back to the original
input (residual connection). This keeps the original signal flowing,
even if the attention introduces distortions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{residual\_forward}\OperatorTok{(}\NormalTok{acts}\OperatorTok{.}\NormalTok{residual2}\OperatorTok{,}\NormalTok{ acts}\OperatorTok{.}\NormalTok{ln1}\OperatorTok{,}\NormalTok{ acts}\OperatorTok{.}\NormalTok{att}\OperatorTok{,}\NormalTok{ B}\OperatorTok{*}\NormalTok{T}\OperatorTok{,}\NormalTok{ C}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

This ensures information isn't lost and gradients flow smoothly during
training.

\subsubsection{Why It Matters}\label{why-it-matters-20}

Attention is the mechanism that lets GPT-2 capture relationships between
words. Without it, the model would treat each token independently,
losing context. By explicitly computing ``who should I look at?'' for
every token, GPT-2 learns patterns like subject--verb agreement,
long-distance dependencies, and even stylistic nuances.

\subsubsection{Try It Yourself}\label{try-it-yourself-22}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Inspect the attention mask: Print out the scores before and after
  masking. Notice how future tokens are set to huge negative values.
\item
  Visualize weights: Run attention on a short sentence and plot the
  weights. You'll see which words attend to which.
\item
  Change sequence length: Try increasing \texttt{T} and observe how
  computation grows quadratically (\texttt{T²}). Attention is expensive!
\item
  Experiment with heads: Force the model to use only 1 head instead of
  12. See how this limits the diversity of patterns it can capture.
\item
  Check sum of weights: For one token, verify that all attention weights
  add up to 1.0 after softmax.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-22}

Attention is what makes transformers powerful. It allows each word to
dynamically decide which other words matter for understanding its role
in a sentence. In \texttt{train\_gpt2.c}, this process is spelled out
with explicit loops and matrix multiplications, so you can follow every
step of the math. Understanding this section gives you the key to why
GPT-2-and all modern LLMs-work so well.

\subsection{34. MLP: GEMMs and Activation
Functions}\label{mlp-gemms-and-activation-functions}

After the attention block lets tokens ``talk to each other,'' GPT-2
applies a second kind of transformation called the MLP block
(multi-layer perceptron). Unlike attention, which mixes information
between tokens, the MLP processes each token independently, enriching
its internal representation. Even though it looks simpler than
attention, the MLP is essential for capturing complex relationships in
language.

\subsubsection{What the MLP Does}\label{what-the-mlp-does}

Every token's vector (size \texttt{C}, e.g., 768 for GPT-2 small) goes
through:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Linear expansion: project from size \texttt{C} to size \texttt{4C}
  (3072 in GPT-2 small).
\item
  Nonlinear activation: apply the GELU function, which adds flexibility.
\item
  Linear projection back: reduce size from \texttt{4C} back to
  \texttt{C}.
\item
  Residual connection: add the input vector back to the output, keeping
  the original signal intact.
\end{enumerate}

This allows the model to not only share information between tokens (via
attention) but also refine how each token represents itself.

\subsubsection{Step 1: Expanding with a Matrix
Multiply}\label{step-1-expanding-with-a-matrix-multiply}

The first step is to expand each token's vector from 768 to 3072
dimensions. This is done with a general matrix multiply (GEMM):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{matmul\_forward}\OperatorTok{(}\NormalTok{acts}\OperatorTok{.}\NormalTok{mlp\_in}\OperatorTok{,}\NormalTok{ acts}\OperatorTok{.}\NormalTok{ln2}\OperatorTok{,}\NormalTok{ params}\OperatorTok{.}\NormalTok{wfc}\OperatorTok{,}\NormalTok{ params}\OperatorTok{.}\NormalTok{bfc}\OperatorTok{,}\NormalTok{ B}\OperatorTok{,}\NormalTok{ T}\OperatorTok{,}\NormalTok{ C}\OperatorTok{,} \DecValTok{4}\OperatorTok{*}\NormalTok{C}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{acts.ln2}: the normalized input from the previous residual.
\item
  \texttt{params.wfc}: the weight matrix of size \texttt{(C,\ 4C)}.
\item
  \texttt{params.bfc}: bias vector of size \texttt{(4C)}.
\item
  \texttt{acts.mlp\_in}: the result, shape \texttt{(B,\ T,\ 4C)}.
\end{itemize}

Think of it like stretching a rubber band-suddenly, the token has much
more room to express richer features.

\subsubsection{Step 2: GELU Activation}\label{step-2-gelu-activation}

After expansion, each number passes through GELU (Gaussian Error Linear
Unit).

The formula:

\begin{verbatim}
GELU(x) = 0.5 * x * (1 + tanh(√(2/π) * (x + 0.044715 * x^3)))
\end{verbatim}

This looks complicated, but the key idea is:

\begin{itemize}
\tightlist
\item
  For small negative numbers, output ≈ 0 (ignore weak signals).
\item
  For large positive numbers, output ≈ x (pass strong signals).
\item
  For numbers in between, it smoothly blends.
\end{itemize}

Unlike ReLU, which just chops off negatives, GELU lets small signals
through in a probabilistic way. This makes it better for language, where
even small hints matter.

Analogy: Imagine you're grading homework. If an answer is completely
wrong, you give 0 points (ReLU style). If it's perfect, you give full
credit. But if it's partially right, you give partial credit. GELU
behaves like that-soft, nuanced grading.

\subsubsection{Step 3: Projecting Back
Down}\label{step-3-projecting-back-down}

Once the token vector has been expanded and passed through GELU, it's
projected back to the original size \texttt{C}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{matmul\_forward}\OperatorTok{(}\NormalTok{acts}\OperatorTok{.}\NormalTok{mlp\_out}\OperatorTok{,}\NormalTok{ acts}\OperatorTok{.}\NormalTok{mlp\_in\_gelu}\OperatorTok{,}\NormalTok{ params}\OperatorTok{.}\NormalTok{wproj}\OperatorTok{,}\NormalTok{ params}\OperatorTok{.}\NormalTok{bproj}\OperatorTok{,}\NormalTok{ B}\OperatorTok{,}\NormalTok{ T}\OperatorTok{,} \DecValTok{4}\OperatorTok{*}\NormalTok{C}\OperatorTok{,}\NormalTok{ C}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{params.wproj}: the projection weights, size \texttt{(4C,\ C)}.
\item
  \texttt{params.bproj}: bias, size \texttt{(C)}.
\item
  \texttt{acts.mlp\_out}: result, shape \texttt{(B,\ T,\ C)}.
\end{itemize}

Now each token has gone through a non-linear ``thinking step,'' mixing
and reshaping features.

\subsubsection{Step 4: Residual
Connection}\label{step-4-residual-connection}

Just like with attention, the MLP output is added back to the input:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{residual\_forward}\OperatorTok{(}\NormalTok{acts}\OperatorTok{.}\NormalTok{residual3}\OperatorTok{,}\NormalTok{ acts}\OperatorTok{.}\NormalTok{residual2}\OperatorTok{,}\NormalTok{ acts}\OperatorTok{.}\NormalTok{mlp\_out}\OperatorTok{,}\NormalTok{ B}\OperatorTok{*}\NormalTok{T}\OperatorTok{,}\NormalTok{ C}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

This means the token keeps its old representation while adding the new
refinements. If the MLP makes a mistake early in training, the residual
ensures the token doesn't lose all its meaning.

\subsubsection{Inside the Code:
Simplicity}\label{inside-the-code-simplicity}

Even though MLPs in deep learning libraries like PyTorch are one-liners
(\texttt{nn.Linear} + \texttt{nn.GELU} + \texttt{nn.Linear}), here in C
you see every step spelled out:

\begin{itemize}
\tightlist
\item
  First GEMM expands to 4C.
\item
  Loop applies GELU element by element.
\item
  Second GEMM projects back to C.
\item
  Residual adds input and output.
\end{itemize}

It's like watching a magician reveal the trick instead of just seeing
the final illusion.

\subsubsection{Why the Expansion
Matters}\label{why-the-expansion-matters}

You might ask: why expand to 4C and then shrink back? Why not just keep
the size the same?

The expansion allows the model to capture more complicated combinations
of features. By spreading information out, applying a nonlinear
transformation, and then compressing it again, the model can discover
patterns that wouldn't fit in the smaller space.

Think of it like brainstorming on a huge whiteboard. You spread out all
your ideas (4C), reorganize them, and then condense the best ones into a
neat summary (C).

\subsubsection{Example Walkthrough}\label{example-walkthrough}

Let's say we're processing the token ``cat'' in the sentence \emph{The
cat sat}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Input vector (size 768): \texttt{{[}0.12,\ -0.08,\ 0.33,\ …{]}}
\item
  After first matrix multiply: expanded to
  \texttt{{[}1.2,\ -0.9,\ 0.5,\ …{]}} (size 3072).
\item
  After GELU: \texttt{{[}1.1,\ -0.0,\ 0.4,\ …{]}} (smooth nonlinearity).
\item
  After projection: back to \texttt{{[}0.15,\ -0.02,\ 0.27,\ …{]}} (size
  768).
\item
  Add back original input: \texttt{{[}0.27,\ -0.10,\ 0.60,\ …{]}}.
\end{enumerate}

Now ``cat'' has been enriched with new internal features that help the
model predict what comes next.

\subsubsection{Why It Matters}\label{why-it-matters-21}

The MLP is the part of GPT-2 that lets each token refine itself.
Attention gives context from neighbors, but the MLP deepens the
representation of the token itself. Without it, the model would lack the
ability to detect fine-grained patterns.

\subsubsection{Try It Yourself}\label{try-it-yourself-23}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Print intermediate sizes: Add debug prints to see how token vectors
  grow to 4C and shrink back to C.
\item
  Swap activation: Replace GELU with ReLU in the code and train. Compare
  losses-you'll notice GPT-2 prefers GELU.
\item
  Disable residual: Temporarily remove the residual add. Watch how the
  model struggles to learn, because it can't preserve information.
\item
  Visualize values: Track how many values are near 0 before and after
  GELU. You'll see GELU softly zeroes out weak signals.
\item
  Smaller expansion: Try changing 4C to 2C in the code. You'll save
  memory but lose accuracy, since the MLP has less expressive power.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-23}

The MLP block is a token's personal deep thinker. It stretches the
representation wide, filters it through GELU, compresses it again, and
then adds it back to the original. While attention handles the
conversation between words, the MLP ensures each word processes and
refines its own role. Together, they create the layered reasoning
ability that makes GPT-2 so powerful.

\subsection{35. LayerNorm on CPU
(Step-by-Step)}\label{layernorm-on-cpu-step-by-step}

One of the most important but often overlooked ingredients in GPT-2 is
Layer Normalization, or LayerNorm for short. While attention and MLPs
are the big stars, LayerNorm is like the stage crew keeping everything
running smoothly behind the scenes. It ensures the numbers flowing
through the network stay stable and balanced, preventing explosions or
collapses that could make training impossible. In
\texttt{train\_gpt2.c}, LayerNorm is implemented with explicit loops so
you can see every calculation. Let's walk through it carefully.

\subsubsection{Why Do We Need
Normalization?}\label{why-do-we-need-normalization}

Imagine a classroom where every student talks at different volumes. Some
whisper, some shout. If you try to listen to all of them at once, the
loud voices drown out the quiet ones.

Neural networks face a similar problem. The outputs of layers can have
wildly different scales. If one dimension of a vector is much larger
than the others, it dominates. Training becomes unstable, and gradients
may vanish or explode.

LayerNorm fixes this by ensuring that, for each token at each layer, the
vector has:

\begin{itemize}
\tightlist
\item
  Mean = 0 (centered around zero)
\item
  Variance = 1 (consistent spread of values)
\end{itemize}

After that, trainable parameters scale and shift the result so the model
can still learn flexible transformations.

\subsubsection{The Math Behind
LayerNorm}\label{the-math-behind-layernorm}

For a given token vector \texttt{x} of size \texttt{C} (e.g., 768):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute mean:

\begin{verbatim}
μ = (1/C) * Σ x[i]
\end{verbatim}
\item
  Compute variance:

\begin{verbatim}
σ² = (1/C) * Σ (x[i] - μ)²
\end{verbatim}
\item
  Normalize:

\begin{verbatim}
x_norm[i] = (x[i] - μ) / sqrt(σ² + ε)
\end{verbatim}

  where \texttt{ε} is a tiny constant (like 1e-5) to avoid division by
  zero.
\item
  Scale and shift with trainable weights \texttt{g} (gamma) and
  \texttt{b} (beta):

\begin{verbatim}
y[i] = g[i] * x_norm[i] + b[i]
\end{verbatim}
\end{enumerate}

So the final output has controlled statistics, but still enough
flexibility for the model to adjust.

\subsubsection{\texorpdfstring{The Code in
\texttt{train\_gpt2.c}}{The Code in train\_gpt2.c}}\label{the-code-in-train_gpt2.c}

Here's a simplified version from the repository:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{void}\NormalTok{ layernorm\_forward}\OperatorTok{(}\DataTypeTok{float}\OperatorTok{*}\NormalTok{ out}\OperatorTok{,} \DataTypeTok{float}\OperatorTok{*}\NormalTok{ inp}\OperatorTok{,} \DataTypeTok{float}\OperatorTok{*}\NormalTok{ weight}\OperatorTok{,} \DataTypeTok{float}\OperatorTok{*}\NormalTok{ bias}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ B}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ T}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ C}\OperatorTok{)} \OperatorTok{\{}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ b }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ b }\OperatorTok{\textless{}}\NormalTok{ B}\OperatorTok{;}\NormalTok{ b}\OperatorTok{++)} \OperatorTok{\{}
        \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ t }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ t }\OperatorTok{\textless{}}\NormalTok{ T}\OperatorTok{;}\NormalTok{ t}\OperatorTok{++)} \OperatorTok{\{}
            \DataTypeTok{float}\OperatorTok{*}\NormalTok{ x }\OperatorTok{=}\NormalTok{ inp }\OperatorTok{+}\NormalTok{ b}\OperatorTok{*}\NormalTok{T}\OperatorTok{*}\NormalTok{C }\OperatorTok{+}\NormalTok{ t}\OperatorTok{*}\NormalTok{C}\OperatorTok{;}
            \DataTypeTok{float}\OperatorTok{*}\NormalTok{ o }\OperatorTok{=}\NormalTok{ out }\OperatorTok{+}\NormalTok{ b}\OperatorTok{*}\NormalTok{T}\OperatorTok{*}\NormalTok{C }\OperatorTok{+}\NormalTok{ t}\OperatorTok{*}\NormalTok{C}\OperatorTok{;}

            \CommentTok{// mean}
            \DataTypeTok{float}\NormalTok{ mean }\OperatorTok{=} \FloatTok{0.0}\BuiltInTok{f}\OperatorTok{;}
            \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}}\NormalTok{ C}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)}\NormalTok{ mean }\OperatorTok{+=}\NormalTok{ x}\OperatorTok{[}\NormalTok{i}\OperatorTok{];}
\NormalTok{            mean }\OperatorTok{/=}\NormalTok{ C}\OperatorTok{;}

            \CommentTok{// variance}
            \DataTypeTok{float}\NormalTok{ var }\OperatorTok{=} \FloatTok{0.0}\BuiltInTok{f}\OperatorTok{;}
            \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}}\NormalTok{ C}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)} \OperatorTok{\{}
                \DataTypeTok{float}\NormalTok{ diff }\OperatorTok{=}\NormalTok{ x}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{{-}}\NormalTok{ mean}\OperatorTok{;}
\NormalTok{                var }\OperatorTok{+=}\NormalTok{ diff }\OperatorTok{*}\NormalTok{ diff}\OperatorTok{;}
            \OperatorTok{\}}
\NormalTok{            var }\OperatorTok{/=}\NormalTok{ C}\OperatorTok{;}

            \CommentTok{// normalize, scale, shift}
            \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}}\NormalTok{ C}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)} \OperatorTok{\{}
                \DataTypeTok{float}\NormalTok{ norm }\OperatorTok{=} \OperatorTok{(}\NormalTok{x}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{{-}}\NormalTok{ mean}\OperatorTok{)} \OperatorTok{/}\NormalTok{ sqrtf}\OperatorTok{(}\NormalTok{var }\OperatorTok{+} \FloatTok{1e{-}5}\BuiltInTok{f}\OperatorTok{);}
\NormalTok{                o}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{=}\NormalTok{ norm }\OperatorTok{*}\NormalTok{ weight}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{+}\NormalTok{ bias}\OperatorTok{[}\NormalTok{i}\OperatorTok{];}
            \OperatorTok{\}}
        \OperatorTok{\}}
    \OperatorTok{\}}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

Notice the structure:

\begin{itemize}
\tightlist
\item
  Outer loops over batch \texttt{B} and sequence length \texttt{T}.
\item
  Inner loops compute mean, variance, and then apply normalization per
  token vector of length \texttt{C}.
\item
  \texttt{weight} and \texttt{bias} are the learnable gamma and beta.
\end{itemize}

This is exactly what LayerNorm means: normalize \emph{each layer's
inputs per token}.

\subsubsection{Example Walkthrough}\label{example-walkthrough-1}

Suppose we have a single token vector (C=4) =
\texttt{{[}2.0,\ -1.0,\ 3.0,\ 0.0{]}}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Mean: \texttt{(2\ -\ 1\ +\ 3\ +\ 0)/4\ =\ 1.0}
\item
  Variance:
  \texttt{((2-1)²\ +\ (-1-1)²\ +\ (3-1)²\ +\ (0-1)²)/4\ =\ (1\ +\ 4\ +\ 4\ +\ 1)/4\ =\ 2.5}
\item
  Normalize: subtract mean and divide by sqrt(2.5):

\begin{verbatim}
[ (2-1)/1.58, (-1-1)/1.58, (3-1)/1.58, (0-1)/1.58 ]
= [0.63, -1.26, 1.26, -0.63]
\end{verbatim}
\item
  Scale and shift (say weight={[}1,1,1,1{]}, bias={[}0,0,0,0{]}):

\begin{verbatim}
[0.63, -1.26, 1.26, -0.63]
\end{verbatim}
\end{enumerate}

Now the vector has mean 0, variance 1, and is ready for the next layer.

\subsubsection{Analogy}\label{analogy-1}

Think of LayerNorm like a baking recipe. If one ingredient is way too
strong (like adding five times too much salt), the whole dish is ruined.
LayerNorm tastes the mixture, balances all the flavors, and then lets
you adjust the seasoning with learnable gamma (scale) and beta (shift).

\subsubsection{Why It Matters}\label{why-it-matters-22}

Without LayerNorm, the model would quickly become unstable:

\begin{itemize}
\tightlist
\item
  Some tokens would dominate, while others fade.
\item
  Gradients could explode, making loss jump wildly.
\item
  Training would be inconsistent between batches.
\end{itemize}

With LayerNorm, each layer works with clean, normalized inputs. This
allows deeper stacks of attention and MLP blocks to learn reliably.

\subsubsection{Try It Yourself}\label{try-it-yourself-24}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Print statistics: Add debug code to check the mean and variance before
  and after LayerNorm. Before: mean ≠ 0, variance ≠ 1. After: mean ≈ 0,
  variance ≈ 1.
\item
  Remove LayerNorm: Comment out LayerNorm in the code. Watch training
  collapse-loss will not decrease properly.
\item
  Change epsilon: Try making \texttt{ε\ =\ 1e-1} or
  \texttt{ε\ =\ 1e-12}. See how too large or too small values can break
  stability.
\item
  Observe gamma and beta: Initialize gamma=1, beta=0. During training,
  watch how these parameters drift, fine-tuning normalization.
\item
  Experiment with batch norm: Replace LayerNorm with BatchNorm (not
  typical for transformers). You'll see it doesn't work well, because
  transformers process variable-length sequences where per-batch
  statistics vary too much.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-24}

LayerNorm is the quiet but critical stabilizer in GPT-2. It ensures
every token vector is balanced, centered, and scaled before moving into
attention or MLP. In \texttt{train\_gpt2.c}, you see exactly how it
works: compute mean, compute variance, normalize, then scale and shift.
Even though it's just a few lines of C code, it's one of the main
reasons deep transformers can stack dozens of layers without breaking.

\subsection{36. Residual Adds and Signal
Flow}\label{residual-adds-and-signal-flow}

Once embeddings, attention, and MLP blocks are computed, there's still
one piece left to keep the whole network stable and effective: residual
connections. In \texttt{train\_gpt2.c}, these appear in functions like
\texttt{residual\_forward}, where outputs of a layer are added back to
their inputs. This simple-looking step is one of the key reasons GPT-2
and other deep transformer models can stack many layers without
collapsing.

\subsubsection{The Core Idea}\label{the-core-idea-1}

A residual connection says:

\begin{verbatim}
output = input + transformation(input)
\end{verbatim}

Instead of replacing the old representation with the new one, the model
adds them together. That way, the original signal always survives, even
if the new transformation is noisy or imperfect.

Think of it like taking lecture notes. Each time the teacher explains
more, you don't throw away your old notes. You add new details next to
them. That way, you preserve everything learned so far, while layering
new insights on top.

\subsubsection{Why Residuals Are
Crucial}\label{why-residuals-are-crucial}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Preventing information loss: If you only applied transformations, some
  features might vanish forever. Adding the input back ensures no
  information is lost.
\item
  Helping gradients flow: During backpropagation, gradients must travel
  backward through many layers. Without shortcuts, they can vanish or
  explode. Residuals create direct paths for gradients, making learning
  stable.
\item
  Improving training speed: With residuals, deeper networks converge
  faster because the model can ``skip'' bad transformations while still
  using the identity mapping.
\end{enumerate}

\subsubsection{\texorpdfstring{The Code in
\texttt{train\_gpt2.c}}{The Code in train\_gpt2.c}}\label{the-code-in-train_gpt2.c-1}

Here's the implementation of residual addition:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{void}\NormalTok{ residual\_forward}\OperatorTok{(}\DataTypeTok{float}\OperatorTok{*}\NormalTok{ out}\OperatorTok{,} \DataTypeTok{float}\OperatorTok{*}\NormalTok{ inp1}\OperatorTok{,} \DataTypeTok{float}\OperatorTok{*}\NormalTok{ inp2}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ N}\OperatorTok{)} \OperatorTok{\{}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}}\NormalTok{ N}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{        out}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{=}\NormalTok{ inp1}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{+}\NormalTok{ inp2}\OperatorTok{[}\NormalTok{i}\OperatorTok{];}
    \OperatorTok{\}}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

It's deceptively simple:

\begin{itemize}
\tightlist
\item
  \texttt{inp1} is the original input.
\item
  \texttt{inp2} is the new transformation (from attention or MLP).
\item
  \texttt{out} stores the sum.
\item
  \texttt{N} is the total number of floats (\texttt{B\ *\ T\ *\ C}).
\end{itemize}

Even though it's just a single line inside a loop, this is what makes
stacking 12+ transformer blocks possible.

\subsubsection{Example Walkthrough}\label{example-walkthrough-2}

Suppose we're processing the token ``cat.''

\begin{itemize}
\tightlist
\item
  Input vector (simplified, size=3): \texttt{{[}0.5,\ -0.3,\ 0.7{]}}
\item
  After attention block: \texttt{{[}0.2,\ 0.1,\ -0.4{]}}
\end{itemize}

Residual addition:

\begin{verbatim}
[0.5 + 0.2, -0.3 + 0.1, 0.7 - 0.4] = [0.7, -0.2, 0.3]
\end{verbatim}

Now the representation of ``cat'' contains both the original signal and
the contextual information from attention.

Later, after the MLP:

\begin{itemize}
\tightlist
\item
  Input again: \texttt{{[}0.7,\ -0.2,\ 0.3{]}}
\item
  MLP output: \texttt{{[}0.1,\ -0.5,\ 0.4{]}}
\item
  Residual: \texttt{{[}0.8,\ -0.7,\ 0.7{]}}
\end{itemize}

Step by step, the vector grows richer without losing its foundation.

\subsubsection{Analogy}\label{analogy-2}

Residual connections are like building layers in Photoshop. Each new
layer adds adjustments, but you always keep the original photo
underneath. If a new adjustment is bad, you can still see the original.
This makes the final composition stronger and safer to experiment with.

\subsubsection{Residuals Across the
Model}\label{residuals-across-the-model}

In GPT-2's forward pass, residuals appear in two main places inside each
transformer block:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  After attention:

\begin{verbatim}
x = x + Attention(x)
\end{verbatim}
\item
  After MLP:

\begin{verbatim}
x = x + MLP(x)
\end{verbatim}
\end{enumerate}

Together with LayerNorm before each block, these form the backbone of
the transformer architecture:

\begin{verbatim}
x → LayerNorm → Attention → Residual Add → LayerNorm → MLP → Residual Add
\end{verbatim}

\subsubsection{Why It Matters}\label{why-it-matters-23}

Without residual connections, GPT-2 would struggle to train past a few
layers. Deeper stacks would lose track of the original signal, gradients
would vanish, and performance would stall. Residuals are the glue that
holds the whole architecture together, enabling models with billions of
parameters to train effectively.

\subsubsection{Try It Yourself}\label{try-it-yourself-25}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Remove residuals: Temporarily comment out the
  \texttt{residual\_forward} calls. Training will quickly fail-the loss
  won't decrease properly.
\item
  Print before/after: Inspect a token's vector before and after residual
  add. Notice how the numbers change smoothly rather than being
  overwritten.
\item
  Experiment with scaling: Try replacing
  \texttt{out{[}i{]}\ =\ inp1{[}i{]}\ +\ inp2{[}i{]};} with
  \texttt{out{[}i{]}\ =\ inp1{[}i{]}\ +\ 0.1\ *\ inp2{[}i{]};}. This
  reduces the impact of the new transformation-sometimes used in
  advanced architectures.
\item
  Compare to skip-less RNNs: Research how older recurrent networks
  without residuals had trouble scaling deep. You'll see why residuals
  are a game changer.
\item
  Chain of signals: Track how a single token's vector evolves across all
  12 layers. You'll notice it keeps its core identity while absorbing
  new context.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-25}

Residual connections may look like a simple addition, but they're the
key to deep learning's success in transformers. They preserve
information, stabilize training, and allow GPT-2 to stack many layers
without falling apart. In \texttt{train\_gpt2.c}, this idea is laid
bare: a few lines of C code implementing one of the most powerful tricks
in modern neural networks.

\subsection{37. Cross-Entropy Loss on
CPU}\label{cross-entropy-loss-on-cpu}

After embeddings, attention, MLPs, LayerNorm, and residuals have done
their job, the model produces logits-raw scores for every word in the
vocabulary at every position in the sequence. But logits alone don't
tell us if the model is ``good'' or ``bad'' at predicting the right
word. To measure performance and guide learning, GPT-2 uses the
cross-entropy loss function. In \texttt{train\_gpt2.c}, this is
implemented in the function \texttt{crossentropy\_forward}.

\subsubsection{What Are Logits?}\label{what-are-logits}

At the final stage of the forward pass, each token position has a vector
of length \texttt{V} (vocabulary size, \textasciitilde50k). For example,
the model might produce these logits for a tiny vocabulary of 3 words:

\begin{verbatim}
logits = [5.2, 1.1, -2.7]
\end{verbatim}

Logits are just numbers-bigger means ``more likely,'' smaller means
``less likely''-but they aren't probabilities yet.

\subsubsection{Step 1: Softmax -- Turning Scores Into
Probabilities}\label{step-1-softmax-turning-scores-into-probabilities}

To compare predictions with the true target, we first convert logits
into probabilities. The tool for this is the softmax function:

\begin{verbatim}
p[i] = exp(logits[i]) / Σ exp(logits[j])
\end{verbatim}

Softmax has two important effects:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It makes all values positive.
\item
  It normalizes them so they sum to 1, forming a probability
  distribution.
\end{enumerate}

Example:

\begin{itemize}
\tightlist
\item
  Logits: \texttt{{[}5.2,\ 1.1,\ -2.7{]}}
\item
  Subtract max (5.2) for stability → \texttt{{[}0.0,\ -4.1,\ -7.9{]}}
\item
  Exponentiate → \texttt{{[}1.0,\ 0.017,\ 0.0004{]}}
\item
  Normalize → \texttt{{[}0.98,\ 0.017,\ 0.0004{]}}
\end{itemize}

Now the model is saying:

\begin{itemize}
\tightlist
\item
  Word 0: 98\% chance
\item
  Word 1: 1.7\% chance
\item
  Word 2: 0.04\% chance
\end{itemize}

\subsubsection{Step 2: Cross-Entropy -- Measuring
Mistakes}\label{step-2-cross-entropy-measuring-mistakes}

Cross-entropy compares the predicted probability for the correct word
against the ideal case (probability = 1).

Formula:

\begin{verbatim}
loss = -log(probability_of_correct_word)
\end{verbatim}

\begin{itemize}
\tightlist
\item
  If the model assigns high probability to the correct word, loss is
  small.
\item
  If the model assigns low probability, loss is large.
\end{itemize}

Example:

\begin{itemize}
\tightlist
\item
  Correct word = Word 0, probability = 0.98 → loss = -log(0.98) ≈ 0.02
  (excellent).
\item
  Correct word = Word 1, probability = 0.017 → loss = -log(0.017) ≈ 4.1
  (bad).
\end{itemize}

\subsubsection{Step 3: Averaging Over the
Batch}\label{step-3-averaging-over-the-batch}

In practice, we don't train on just one word, but a batch of sequences.
The code loops over every token in every batch, collects their losses,
and averages them.

From \texttt{train\_gpt2.c}:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{float}\NormalTok{ loss }\OperatorTok{=} \FloatTok{0.0}\BuiltInTok{f}\OperatorTok{;}
\ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}}\NormalTok{ B}\OperatorTok{*}\NormalTok{T}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)} \OperatorTok{\{}
    \DataTypeTok{int}\NormalTok{ target }\OperatorTok{=}\NormalTok{ targets}\OperatorTok{[}\NormalTok{i}\OperatorTok{];}
    \DataTypeTok{float}\NormalTok{ logit\_max }\OperatorTok{=} \OperatorTok{{-}}\FloatTok{1e9}\OperatorTok{;}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ j }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ j }\OperatorTok{\textless{}}\NormalTok{ Vp}\OperatorTok{;}\NormalTok{ j}\OperatorTok{++)} \OperatorTok{\{}
        \ControlFlowTok{if} \OperatorTok{(}\NormalTok{logits}\OperatorTok{[}\NormalTok{i}\OperatorTok{*}\NormalTok{Vp }\OperatorTok{+}\NormalTok{ j}\OperatorTok{]} \OperatorTok{\textgreater{}}\NormalTok{ logit\_max}\OperatorTok{)}\NormalTok{ logit\_max }\OperatorTok{=}\NormalTok{ logits}\OperatorTok{[}\NormalTok{i}\OperatorTok{*}\NormalTok{Vp }\OperatorTok{+}\NormalTok{ j}\OperatorTok{];}
    \OperatorTok{\}}
    \DataTypeTok{float}\NormalTok{ sum }\OperatorTok{=} \FloatTok{0.0}\BuiltInTok{f}\OperatorTok{;}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ j }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ j }\OperatorTok{\textless{}}\NormalTok{ Vp}\OperatorTok{;}\NormalTok{ j}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{        sum }\OperatorTok{+=}\NormalTok{ expf}\OperatorTok{(}\NormalTok{logits}\OperatorTok{[}\NormalTok{i}\OperatorTok{*}\NormalTok{Vp }\OperatorTok{+}\NormalTok{ j}\OperatorTok{]} \OperatorTok{{-}}\NormalTok{ logit\_max}\OperatorTok{);}
    \OperatorTok{\}}
    \DataTypeTok{float}\NormalTok{ log\_sum }\OperatorTok{=}\NormalTok{ logf}\OperatorTok{(}\NormalTok{sum}\OperatorTok{);}
    \DataTypeTok{float}\NormalTok{ correct\_logit }\OperatorTok{=}\NormalTok{ logits}\OperatorTok{[}\NormalTok{i}\OperatorTok{*}\NormalTok{Vp }\OperatorTok{+}\NormalTok{ target}\OperatorTok{];}
\NormalTok{    loss }\OperatorTok{+=} \OperatorTok{(}\NormalTok{log\_sum }\OperatorTok{+}\NormalTok{ logit\_max }\OperatorTok{{-}}\NormalTok{ correct\_logit}\OperatorTok{);}
\OperatorTok{\}}
\NormalTok{loss }\OperatorTok{/=} \OperatorTok{(}\NormalTok{B}\OperatorTok{*}\NormalTok{T}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

What's happening here:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For each token, find the max logit (\texttt{logit\_max}) to improve
  numerical stability.
\item
  Compute softmax denominator (\texttt{sum}).
\item
  Calculate log probability of the correct token.
\item
  Accumulate losses across all tokens.
\item
  Divide by total tokens (\texttt{B*T}) to get the average.
\end{enumerate}

\subsubsection{Numerical Stability
Tricks}\label{numerical-stability-tricks}

Without subtracting \texttt{logit\_max}, \texttt{exp(logits)} can
overflow. For example, \texttt{exp(1000)} is infinite. By subtracting
the max, the largest logit becomes 0, so its exponential is 1, and all
others are ≤ 1. This keeps numbers manageable while preserving the
probability ratios.

\subsubsection{Example With a Sentence}\label{example-with-a-sentence}

Sentence: \emph{The cat sat on the mat.}

Suppose the model predicts probabilities for the last token:

\begin{itemize}
\tightlist
\item
  ``mat'': 0.85
\item
  ``dog'': 0.10
\item
  ``car'': 0.05
\end{itemize}

Correct word = ``mat.''

Loss = \texttt{-log(0.85)\ ≈\ 0.16}.

If instead the model guessed ``dog'' with 0.10, loss =
\texttt{-log(0.10)\ ≈\ 2.3}. Higher penalty for being wrong.

\subsubsection{Analogy}\label{analogy-3}

Cross-entropy is like grading multiple-choice exams. If the student
picks the right answer confidently (high probability), they lose almost
no points. If they're hesitant or wrong, they lose more points. Over
many questions (tokens), you calculate their average score-the training
loss.

\subsubsection{Why It Matters}\label{why-it-matters-24}

Cross-entropy loss is the guiding signal for the entire training
process. It tells the optimizer:

\begin{itemize}
\tightlist
\item
  ``Increase probability for the right words.''
\item
  ``Decrease probability for the wrong words.''
\end{itemize}

Without it, GPT-2 would have no way of knowing whether its predictions
are improving.

\subsubsection{Try It Yourself}\label{try-it-yourself-26}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Check baseline loss: Before training, print the loss. It should be
  close to \texttt{log(vocab\_size)} (\textasciitilde10.8 for GPT-2
  small), which corresponds to random guessing.
\item
  Inspect softmax sums: For one token, sum all probabilities. It should
  equal \textasciitilde1.0.
\item
  Force the wrong answer: Temporarily change the target to an incorrect
  word. Watch how the loss shoots up.
\item
  Observe loss during training: Print loss every step. It should
  steadily decrease as the model learns.
\item
  Compare with accuracy: Track how often the model's top prediction
  matches the target. Loss and accuracy will move together, but loss is
  smoother and more informative.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-26}

Cross-entropy loss turns raw model scores into a clear training signal.
It penalizes wrong predictions, rewards confident correct ones, and
ensures the optimizer knows exactly how to adjust weights. In
\texttt{train\_gpt2.c}, you see this implemented explicitly, without any
library shortcuts-just loops, exponentials, and logs. Understanding this
section is key to understanding how GPT-2 learns from its mistakes.

\subsection{\texorpdfstring{38. Putting It All Together: The
\texttt{gpt2\_forward}
Function}{38. Putting It All Together: The gpt2\_forward Function}}\label{putting-it-all-together-the-gpt2_forward-function}

Up to this point, we've explored the forward pass piece by piece -
embeddings, attention, feed-forward layers, layer normalization,
residual connections, and finally the loss. But a model doesn't live as
disconnected pieces; they all come together in a single function that
drives inference: \texttt{gpt2\_forward}. This function is where the
code actually executes the story we've been telling. Let's walk through
it carefully so you can see how every building block plugs into the
whole picture.

\subsubsection{\texorpdfstring{The role of
\texttt{gpt2\_forward}}{The role of gpt2\_forward}}\label{the-role-of-gpt2_forward}

Think of \texttt{gpt2\_forward} as the director of the play. The actors
(embeddings, attention, MLP, layernorm, etc.) already know their roles.
The director calls them on stage in the right order and makes sure they
hand the script off smoothly to the next actor. In our case:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Tokens come in as integers (word IDs).
\item
  They're turned into embeddings (token + position).
\item
  Each transformer block processes the sequence through attention, MLP,
  layernorm, and residuals.
\item
  The final hidden states are mapped back into vocabulary space.
\item
  If labels are provided, a loss is computed.
\end{enumerate}

\subsubsection{Code skeleton}\label{code-skeleton}

Here's a simplified excerpt of the real function from
\texttt{train\_gpt2.c} (slightly shortened for readability):

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{void}\NormalTok{ gpt2\_forward}\OperatorTok{(}\NormalTok{GPT2 }\OperatorTok{*}\NormalTok{model}\OperatorTok{,} \DataTypeTok{int} \OperatorTok{*}\NormalTok{tokens}\OperatorTok{,} \DataTypeTok{int} \OperatorTok{*}\NormalTok{labels}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ B}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ T}\OperatorTok{)} \OperatorTok{\{}
    \CommentTok{// Step 1: Embedding lookup}
\NormalTok{    embedding\_forward}\OperatorTok{(}\NormalTok{model}\OperatorTok{{-}\textgreater{}}\NormalTok{token\_embedding}\OperatorTok{,}\NormalTok{ tokens}\OperatorTok{,}\NormalTok{ B}\OperatorTok{,}\NormalTok{ T}\OperatorTok{);}
\NormalTok{    embedding\_forward}\OperatorTok{(}\NormalTok{model}\OperatorTok{{-}\textgreater{}}\NormalTok{position\_embedding}\OperatorTok{,}\NormalTok{ positions}\OperatorTok{,}\NormalTok{ B}\OperatorTok{,}\NormalTok{ T}\OperatorTok{);}

    \CommentTok{// Step 2: Transformer blocks}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ l }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ l }\OperatorTok{\textless{}}\NormalTok{ model}\OperatorTok{{-}\textgreater{}}\NormalTok{config}\OperatorTok{{-}\textgreater{}}\NormalTok{n\_layer}\OperatorTok{;}\NormalTok{ l}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{        attention\_forward}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{{-}\textgreater{}}\NormalTok{blocks}\OperatorTok{[}\NormalTok{l}\OperatorTok{].}\NormalTok{attn}\OperatorTok{,} \OperatorTok{...);}
\NormalTok{        mlp\_forward}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{{-}\textgreater{}}\NormalTok{blocks}\OperatorTok{[}\NormalTok{l}\OperatorTok{].}\NormalTok{mlp}\OperatorTok{,} \OperatorTok{...);}
\NormalTok{        layernorm\_forward}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{{-}\textgreater{}}\NormalTok{blocks}\OperatorTok{[}\NormalTok{l}\OperatorTok{].}\NormalTok{ln}\OperatorTok{,} \OperatorTok{...);}
\NormalTok{        residual\_forward}\OperatorTok{(...);}
    \OperatorTok{\}}

    \CommentTok{// Step 3: Final normalization + logits}
\NormalTok{    layernorm\_forward}\OperatorTok{(}\NormalTok{model}\OperatorTok{{-}\textgreater{}}\NormalTok{final\_ln}\OperatorTok{,} \OperatorTok{...);}
\NormalTok{    matmul\_forward}\OperatorTok{(}\NormalTok{model}\OperatorTok{{-}\textgreater{}}\NormalTok{lm\_head}\OperatorTok{,} \OperatorTok{...);} \CommentTok{// project to vocab}

    \CommentTok{// Step 4: Loss (optional)}
    \ControlFlowTok{if} \OperatorTok{(}\NormalTok{labels }\OperatorTok{!=}\NormalTok{ NULL}\OperatorTok{)} \OperatorTok{\{}
\NormalTok{        crossentropy\_forward}\OperatorTok{(...);}
    \OperatorTok{\}}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

Don't worry if this looks intimidating - we'll decode each part in plain
language.

\subsubsection{Step 1: Embedding lookup}\label{step-1-embedding-lookup}

Before the model can reason about words, it has to map token IDs into
continuous vectors. That's where embedding tables come in:

\begin{itemize}
\tightlist
\item
  \texttt{token\_embedding} converts each integer token ID into a dense
  vector of size \texttt{C} (the channel dimension).
\item
  \texttt{position\_embedding} does the same for positions (0, 1, 2,
  \ldots, T-1).
\item
  These two are added together, giving each token both a meaning (word
  identity) and a place in the sentence.
\end{itemize}

\subsubsection{Step 2: Transformer
blocks}\label{step-2-transformer-blocks}

Each block is like a mini-pipeline that processes the sequence and
passes it forward. Inside the loop:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Attention: compares tokens with each other, weighted by learned Q/K/V
  projections.
\item
  MLP: expands each token vector, applies a nonlinear GELU activation,
  then projects back down.
\item
  LayerNorm: normalizes values for stable training and inference.
\item
  Residual: adds the input of the block back to its output to keep
  information flowing.
\end{enumerate}

This loop runs \texttt{n\_layer} times - for GPT-2 124M, that's 12
blocks.

\subsubsection{Step 3: Final normalization and
logits}\label{step-3-final-normalization-and-logits}

After the last block, the sequence of token representations goes through
a final layer normalization. Then, a large matrix multiplication
(\texttt{lm\_head}) projects each token's hidden state into the size of
the vocabulary (≈50,000 for GPT-2). The result is a tensor of shape
\texttt{(B,\ T,\ vocab\_size)} containing the raw prediction scores for
each next token.

\subsubsection{Step 4: Optional loss
computation}\label{step-4-optional-loss-computation}

If you pass \texttt{labels} (the correct next tokens) into
\texttt{gpt2\_forward}, the function calls
\texttt{crossentropy\_forward}. This compares the predicted scores with
the true tokens and outputs a single number: the loss. The loss tells
you ``how wrong'' the model was, which is critical during training. But
if you're only doing inference, you don't need this step.

\subsubsection{How the pieces connect}\label{how-the-pieces-connect}

Here's a table that maps our earlier sections to the parts of
\texttt{gpt2\_forward}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1644}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5205}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3151}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Code Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Section Covered Earlier
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Embeddings & token + positional vectors & 32 \\
Attention & QKV projections, masking, softmax & 33 \\
MLP & feed-forward expansion and compression & 34 \\
LayerNorm & normalization for stability & 35 \\
Residual & skip connections for signal flow & 36 \\
CrossEntropy & comparing predictions with labels & 37 \\
\end{longtable}

So \texttt{gpt2\_forward} is really just a neat orchestration of
everything you've already learned.

\subsubsection{Why it matters}\label{why-it-matters-25}

Understanding \texttt{gpt2\_forward} gives you the complete mental
picture of inference. It shows how embeddings, attention, MLP,
normalization, and residuals work together in code to turn a batch of
tokens into predictions. Without this integration step, the model would
just be a collection of disconnected parts.

\subsubsection{Try it yourself}\label{try-it-yourself-27}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Print shapes: Add \texttt{printf} statements inside
  \texttt{gpt2\_forward} to print tensor shapes after embeddings, after
  each block, and after logits. This helps you see the data flow.
\item
  Use a single block: Change the loop to run only 1 transformer block
  instead of all 12. Watch how the outputs degrade - the model loses
  depth of reasoning.
\item
  Disable position embeddings: Comment out the line that adds
  \texttt{position\_embedding}. Try running inference. You'll notice the
  model becomes worse at handling word order.
\item
  Loss vs no loss: Call \texttt{gpt2\_forward} with and without labels.
  Compare the difference - with labels you get a scalar loss, without
  labels you just get logits.
\item
  Smaller vocab: Try using a toy tokenizer with a small vocabulary and
  rerun the projection step. You'll see the logits shrink to
  \texttt{(B,\ T,\ tiny\_vocab\_size)}.
\end{enumerate}

\subsubsection{The takeaway}\label{the-takeaway-27}

\texttt{gpt2\_forward} is where GPT-2 inference really happens. It ties
together every concept - embeddings, attention, feed-forward layers,
normalization, residuals, and the final projection into vocabulary
space. Once you understand this function, you don't just know the pieces
of GPT-2, you know how they actually work together to produce
predictions. It's the ``main stage'' of inference, and mastering it
means you can confidently say you understand how a transformer runs
forward on CPU.

\subsection{39. OpenMP Pragmas for Parallel
Loops}\label{openmp-pragmas-for-parallel-loops}

CPU training in \texttt{train\_gpt2.c} is intentionally ``plain C,'' but
it still squeezes out a lot of speed by adding a few OpenMP pragmas
(\texttt{\#pragma\ omp\ …}) in the hottest loops. OpenMP lets the
compiler split a loop's iterations across multiple CPU cores-no threads
to create by hand, no locks to manage. If you compile without OpenMP
support, these pragmas are simply ignored and the code still runs (just
slower).

Below we'll (1) show exactly where OpenMP is used, (2) explain why those
loops are good candidates, and (3) offer practical tips to get solid
speedups on your machine.

\subsubsection{OpenMP in this file: where it appears and
why}\label{openmp-in-this-file-where-it-appears-and-why}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1130}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1652}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2391}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4826}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Location / Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Pragma used
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
What's parallelized
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Why it's a great fit
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{matmul\_forward\_naive} &
\texttt{\#pragma\ omp\ parallel\ for\ collapse(2)} & Outer loops over
\texttt{b} (batch) and \texttt{t} (time) & Each \texttt{(b,t)} row
computes an independent output vector; no write conflicts. Large,
regular work = easy scaling. \\
\texttt{matmul\_forward} (tiled) & \texttt{\#pragma\ omp\ parallel\ for}
& Collapsed \texttt{B*T} loop in tiles of \texttt{LOOP\_UNROLL} &
Heaviest compute in the model; tiling + per-thread tiles keep caches
warm. \\
\texttt{matmul\_backward} (part 1) &
\texttt{\#pragma\ omp\ parallel\ for\ collapse(2)} & Backprop into
\texttt{inp} over \texttt{(b,t)} & Each \texttt{(b,t)} reads weights and
\texttt{dout}, writes a private slice of \texttt{dinp} → no overlap. \\
\texttt{matmul\_backward} (part 2) &
\texttt{\#pragma\ omp\ parallel\ for} & Backprop into
\texttt{weight}/\texttt{bias} over \texttt{o} (output channel) & Each
thread owns one output channel's gradient row, avoiding atomics. \\
\texttt{softmax\_forward} &
\texttt{\#pragma\ omp\ parallel\ for\ collapse(2)} & Over \texttt{(b,t)}
positions & Each softmax is independent; perfect ``embarrassingly
parallel'' loop. \\
\texttt{attention\_forward} &
\texttt{\#pragma\ omp\ parallel\ for\ collapse(3)} & Over
\texttt{(b,\ t,\ h)} = batch, time, head & Per \texttt{(b,t,h)} head's
work is independent; big 3-D grid parallelizes extremely well. \\
\end{longtable}

A few key patterns to notice:

\begin{itemize}
\tightlist
\item
  Collapse clauses (\texttt{collapse(2)} / \texttt{collapse(3)}) fuse
  nested loops into one big iteration space so the scheduler can
  distribute more, smaller chunks-great for load-balancing when
  \texttt{B}, \texttt{T}, or \texttt{NH} are modest.
\item
  Parallelizing along independent dimensions avoids race conditions. For
  example, in \texttt{matmul\_backward} the pass that writes
  \texttt{dinp{[}b,t,:{]}} is parallelized over \texttt{(b,t)} so no two
  threads update the same memory.
\item
  Own-your-row strategy: when accumulating \texttt{dweight}, the loop
  goes over \texttt{o} (output channels) so each thread writes its own
  gradient row \texttt{dweight{[}o,:{]}}. No atomics needed.
\end{itemize}

\subsubsection{Quick refresher: what OpenMP is
doing}\label{quick-refresher-what-openmp-is-doing}

A typical pattern looks like:

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{\#pragma omp parallel for collapse(2)}
\ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ b }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ b }\OperatorTok{\textless{}}\NormalTok{ B}\OperatorTok{;}\NormalTok{ b}\OperatorTok{++)} \OperatorTok{\{}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ t }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ t }\OperatorTok{\textless{}}\NormalTok{ T}\OperatorTok{;}\NormalTok{ t}\OperatorTok{++)} \OperatorTok{\{}
        \CommentTok{// compute outputs for (b, t) independently}
    \OperatorTok{\}}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

When compiled with OpenMP, the compiler creates a team of threads and
divides the iteration space (\texttt{B*T} in this example) among them.
Each thread executes its assigned iterations; when the loop finishes,
threads sync at an implicit barrier.

Because each \texttt{(b,t)} (or \texttt{(b,t,h)}) writes to a disjoint
slice of the output arrays, there's no need for locks or atomics. This
is why these loops scale cleanly across cores.

\subsubsection{Enabling OpenMP, safely}\label{enabling-openmp-safely}

\begin{itemize}
\item
  The source guards the OpenMP header with:

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{\#ifdef OMP}
\PreprocessorTok{\#include }\ImportTok{\textless{}omp.h\textgreater{}}
\PreprocessorTok{\#endif}
\end{Highlighting}
\end{Shaded}

  so you can define \texttt{OMP} in your build and add your compiler
  switch. Example (GCC/Clang):

\begin{verbatim}
-D OMP -fopenmp
\end{verbatim}
\item
  If you forget \texttt{-fopenmp} (or your platform's equivalent), the
  pragmas are ignored and the program runs single-threaded.
\item
  You can control threads at runtime:

\begin{verbatim}
export OMP_NUM_THREADS=8
\end{verbatim}

  A good rule of thumb is to start with the number of physical cores on
  your CPU.
\end{itemize}

\subsubsection{Why these loops benefit the
most}\label{why-these-loops-benefit-the-most}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Matrix multiplies dominate runtime.
  \texttt{matmul\_forward}/\texttt{matmul\_backward} consume the bulk of
  CPU time. Parallelizing them yields the largest end-to-end speedups.
\item
  Softmax is independent per position. Each \texttt{(b,t)} softmax
  computes a max, then exponentials and a sum-no cross-talk between
  positions.
\item
  Attention splits across batch/time/head. The triple loop over
  \texttt{(b,t,h)} has lots of work per iteration (Q·K, softmax,
  weighted sum), making thread overhead negligible compared to useful
  compute.
\item
  Minimal synchronization and no atomics. By choosing iteration spaces
  that own exclusive output slices, we avoid costly synchronization.
\end{enumerate}

\subsubsection{Practical tips for better
scaling}\label{practical-tips-for-better-scaling}

\begin{itemize}
\tightlist
\item
  Set \texttt{OMP\_NUM\_THREADS} to your CPU. Too many threads can hurt
  (oversubscription). Start with physical cores, then experiment.
\item
  Pin threads (optional, advanced). Some OpenMP runtimes support
  \texttt{OMP\_PROC\_BIND=close} to improve cache locality.
\item
  Mind memory bandwidth. On wide CPUs, GEMMs may become bandwidth-bound.
  Bigger \texttt{B}/\texttt{T} improves arithmetic intensity; tiny
  batches underutilize cores.
\item
  Warm caches with tiling. The ``tiled'' \texttt{matmul\_forward} keeps
  small accumulators in registers and reuses loaded weights across
  \texttt{LOOP\_UNROLL} inner iterations.
\item
  Avoid hidden sharing. If you add new parallel loops, ensure each
  thread writes to unique memory regions. If you must accumulate to the
  same place, restructure (like ``own-your-row'') or use per-thread
  scratch buffers then reduce.
\end{itemize}

\subsubsection{\texorpdfstring{Micro-walkthrough: why \texttt{collapse}
helps}{Micro-walkthrough: why collapse helps}}\label{micro-walkthrough-why-collapse-helps}

Consider \texttt{softmax\_forward}:

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{\#pragma omp parallel for collapse(2)}
\ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ b }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ b }\OperatorTok{\textless{}}\NormalTok{ B}\OperatorTok{;}\NormalTok{ b}\OperatorTok{++)} \OperatorTok{\{}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ t }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ t }\OperatorTok{\textless{}}\NormalTok{ T}\OperatorTok{;}\NormalTok{ t}\OperatorTok{++)} \OperatorTok{\{}
        \CommentTok{// 1) find max over V}
        \CommentTok{// 2) exp/log{-}sum}
        \CommentTok{// 3) normalize first V entries; zero out padded [V..Vp)}
    \OperatorTok{\}}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

If \texttt{B=4}, \texttt{T=64}, that's 256 independent softmaxes. With
\texttt{collapse(2)}, OpenMP sees a single loop of 256 iterations to
distribute evenly; without \texttt{collapse}, it might chunk by
\texttt{b} first (only 4 big chunks), which can load-imbalance.

\subsubsection{Common pitfalls (and how this code avoids
them)}\label{common-pitfalls-and-how-this-code-avoids-them}

\begin{itemize}
\item
  Race conditions: Two threads writing the same \texttt{out{[}i{]}}.
  \emph{Avoided by design:} each parallel loop writes distinct slices
  (e.g., per \texttt{(b,t)} or per \texttt{o}).
\item
  False sharing: Threads write adjacent memory locations on the same
  cache line. It's minimized by the large, contiguous slices per thread
  (entire rows/tiles), but if you extend the code with fine-grained
  parallelism, keep this in mind.
\item
  Tiny loops: Overhead can exceed work. The file parallelizes only
  large, hot loops (GEMMs, attention, softmax), not small scalar ops.
\end{itemize}

\subsubsection{Try it yourself}\label{try-it-yourself-28}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change thread count: Run with \texttt{OMP\_NUM\_THREADS=1,2,4,8,…} and
  log step time. Plot speedup vs.~threads.
\item
  Toggle a pragma: Comment out \texttt{\#pragma\ omp} in
  \texttt{matmul\_forward} only. Measure the slowdown; you'll see where
  most time goes.
\item
  Experiment with \texttt{collapse}: Remove \texttt{collapse(2)} in
  \texttt{softmax\_forward}. On small \texttt{B}, you'll likely see
  worse scaling.
\item
  Per-layer profiling: Print elapsed time around
  \texttt{matmul\_forward}, \texttt{attention\_forward}, and
  \texttt{softmax\_forward} to see which benefits most on your CPU.
\item
  Schedule policy (advanced): Try
  \texttt{\#pragma\ omp\ parallel\ for\ schedule(static)}
  vs.~\texttt{dynamic} on a heavy loop to see if it changes load balance
  (defaults are usually fine here).
\end{enumerate}

\subsubsection{The takeaway}\label{the-takeaway-28}

A handful of well-placed OpenMP pragmas deliver big wins on CPU by
parallelizing the most expensive loops (GEMMs, attention, softmax)
across cores-without complicating the code. The design ensures each
thread works on independent slices, so there's no locking, no atomics,
and very little overhead. If you compile with OpenMP enabled, you get
fast, multi-core training; if not, you still have a clean, readable
reference implementation.

\subsection{40. CPU Memory Footprint and
Performance}\label{cpu-memory-footprint-and-performance}

When you train GPT-2 on your CPU using \texttt{train\_gpt2.c}, two big
questions usually pop up almost immediately: \emph{how much memory is
this going to take?} and \emph{how fast will it run?} Let's walk through
both of these in a beginner-friendly way, so you understand not just
what happens in the code, but why it behaves the way it does.

\subsubsection{Memory: where does it all
go?}\label{memory-where-does-it-all-go}

Imagine training GPT-2 is like cooking a big meal in a small kitchen.
You need space for ingredients, bowls for mixing, and counter space for
preparing. Memory on your CPU is that kitchen. GPT-2 needs several
``bowls'' to hold different parts of the computation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Parameters (the weights of the model). These are the ``fixed recipe''
  - the actual numbers the network learns. They come from the checkpoint
  file you load at the start. For GPT-2 124M, this is about 124 million
  floating-point numbers. Each one takes 4 bytes, so just the weights
  are around 500 MB.
\item
  Optimizer state (AdamW). Training doesn't just adjust weights blindly;
  it keeps track of two extra moving averages for each weight, called
  \emph{m} and \emph{v}. That means for every single parameter, you
  store three numbers: the weight, m, and v. So memory for optimizer
  state is often double the size of the weights themselves. For GPT-2
  124M, that's about 1 GB more.
\item
  Gradients. Every time we run a backward pass, we store how much each
  weight should change. That's another buffer roughly the same size as
  the weights - another 500 MB.
\item
  Activations (intermediate results). This is the sneaky one. Every
  forward pass produces temporary tensors like embeddings, attention
  maps, and feed-forward outputs. Their size depends on batch size (B)
  and sequence length (T). If B=4 and T=64, activations are a few
  hundred MB. If B=32 and T=1024, they can balloon to many gigabytes.
\end{enumerate}

Here's a rough mental budget for GPT-2 124M with a small setup (B=4,
T=64):

\begin{itemize}
\tightlist
\item
  Parameters: \textasciitilde500 MB
\item
  Optimizer state: \textasciitilde1 GB
\item
  Gradients: \textasciitilde500 MB
\item
  Activations: \textasciitilde200--300 MB Total: \textasciitilde2--2.5
  GB
\end{itemize}

Even for the ``tiny'' GPT-2, you already need a couple gigabytes of RAM
to train. On a laptop, this can quickly push you to the limit.

\subsubsection{Performance: where does time
go?}\label{performance-where-does-time-go}

Now let's talk speed. When you run \texttt{train\_gpt2.c} on CPU, you'll
see lines like:

\begin{verbatim}
step 1: train loss 5.191576 (took 1927.230000 ms)
\end{verbatim}

That ``took X ms'' tells you how long one step took. Why is it slow?
Three main reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Matrix multiplications (matmuls). These are the heart of neural
  networks. Every attention head and every MLP layer does them. On CPU,
  most of your step time is spent here. That's why the code uses OpenMP
  pragmas (\texttt{\#pragma\ omp}) to parallelize loops across cores.
\item
  Attention softmax. Attention compares every token in a sequence with
  every other token. If your sequence length is 1024, that's over a
  million comparisons per head per layer. On CPU, this quadratic growth
  is painful.
\item
  Memory bandwidth. CPUs can only move numbers from RAM to cores so
  fast. Even if you had infinite FLOPs, you'd still be slowed down by
  how quickly you can fetch and store these huge tensors.
\end{enumerate}

\subsubsection{A simple experiment}\label{a-simple-experiment}

You can see these effects yourself:

\begin{itemize}
\tightlist
\item
  Change batch size (B). Run with B=1, then with B=8. Notice how memory
  usage and step time scale up.
\item
  Change sequence length (T). Try T=16, then T=256. You'll see attention
  costs grow dramatically.
\item
  Change threads. Set \texttt{OMP\_NUM\_THREADS=1} versus
  \texttt{OMP\_NUM\_THREADS=8}. With more threads, you'll often see
  speedups, but only up to the number of physical cores your CPU has.
\end{itemize}

\subsubsection{Why this matters}\label{why-this-matters-2}

For beginners, CPU runs are perfect for learning:

\begin{itemize}
\tightlist
\item
  You can debug with small batches and short sequences.
\item
  You can step into functions with a debugger and watch tensors being
  created.
\item
  You don't need a GPU just to understand how training works.
\end{itemize}

But when it comes to \emph{serious} training - larger GPT-2 models or
even long sequences - CPU is simply too slow. What takes seconds on GPU
may take minutes on CPU. That's why in practice, people use CPUs for
learning and testing, and GPUs for large-scale training.

\subsubsection{The takeaway}\label{the-takeaway-29}

Training GPT-2 on CPU is like practicing piano on a small keyboard. It's
slower, limited, and you can't play the biggest pieces, but it's great
for learning the fundamentals. Memory usage comes from weights,
optimizer state, gradients, and activations, and performance is
dominated by matmuls and attention. Once you understand where the
resources go, you can adjust batch size, sequence length, and threads to
find the sweet spot for your machine.

\section{Chapter 5. Training Loop (CPU
Path)}\label{chapter-5.-training-loop-cpu-path}

\subsection{41. Backward Pass
Walkthrough}\label{backward-pass-walkthrough}

Up until now, we've spent all our time looking at the forward pass.
That's the part of the model that takes tokens, pushes them through
embeddings, attention, feed-forward layers, and finally produces logits
or a loss. For inference, forward pass alone is enough. But if you want
to train a model, forward is only half the story.

Training means adjusting the weights of the model so that its
predictions become better over time. To do this, we need a way to figure
out how wrong each weight was and in what direction it should move to
reduce the loss. That's the job of the backward pass.

The backward pass is also called backpropagation. It's the algorithm
that moves information in reverse through the network: from the loss,
back through the final logits, through every transformer block, down to
the embeddings. Along the way, it calculates gradients - small numbers
that tell us how much each weight contributed to the error.

\subsubsection{The big idea: chain rule in
action}\label{the-big-idea-chain-rule-in-action}

At the heart of backpropagation is something very familiar from
calculus: the chain rule. If the output of the network depends on many
functions stacked together (embedding → attention → MLP → \ldots{} →
loss), then the derivative of the loss with respect to an early
parameter is a product of partial derivatives through the entire chain.

Instead of writing long formulas, the code in \texttt{train\_gpt2.c}
simply calls each layer's backward function in reverse order. The
gradient flows backward, step by step, and each layer computes its own
contribution using local rules.

Think of it like a relay race, but run backwards: the loss hands a
``blame baton'' to the output head, which hands it back to the last
transformer block, and so on, until it reaches the very first embedding
table.

\subsubsection{\texorpdfstring{Walking through
\texttt{gpt2\_backward}}{Walking through gpt2\_backward}}\label{walking-through-gpt2_backward}

Here's a simplified sketch of how the backward function looks in the
code (names shortened for readability):

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{void}\NormalTok{ gpt2\_backward}\OperatorTok{(}\NormalTok{GPT2 }\OperatorTok{*}\NormalTok{model}\OperatorTok{,} \DataTypeTok{int} \OperatorTok{*}\NormalTok{tokens}\OperatorTok{,} \DataTypeTok{int} \OperatorTok{*}\NormalTok{labels}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ B}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ T}\OperatorTok{)} \OperatorTok{\{}
    \CommentTok{// Step 1: loss gradient}
\NormalTok{    crossentropy\_backward}\OperatorTok{(...);}

    \CommentTok{// Step 2: final projection (lm\_head)}
\NormalTok{    matmul\_backward}\OperatorTok{(}\NormalTok{model}\OperatorTok{{-}\textgreater{}}\NormalTok{lm\_head}\OperatorTok{,} \OperatorTok{...);}

    \CommentTok{// Step 3: transformer blocks in reverse}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ l }\OperatorTok{=}\NormalTok{ model}\OperatorTok{{-}\textgreater{}}\NormalTok{config}\OperatorTok{{-}\textgreater{}}\NormalTok{n\_layer }\OperatorTok{{-}} \DecValTok{1}\OperatorTok{;}\NormalTok{ l }\OperatorTok{\textgreater{}=} \DecValTok{0}\OperatorTok{;}\NormalTok{ l}\OperatorTok{{-}{-})} \OperatorTok{\{}
\NormalTok{        residual\_backward}\OperatorTok{(...);}
\NormalTok{        layernorm\_backward}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{{-}\textgreater{}}\NormalTok{blocks}\OperatorTok{[}\NormalTok{l}\OperatorTok{].}\NormalTok{ln}\OperatorTok{,} \OperatorTok{...);}
\NormalTok{        mlp\_backward}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{{-}\textgreater{}}\NormalTok{blocks}\OperatorTok{[}\NormalTok{l}\OperatorTok{].}\NormalTok{mlp}\OperatorTok{,} \OperatorTok{...);}
\NormalTok{        attention\_backward}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{{-}\textgreater{}}\NormalTok{blocks}\OperatorTok{[}\NormalTok{l}\OperatorTok{].}\NormalTok{attn}\OperatorTok{,} \OperatorTok{...);}
    \OperatorTok{\}}

    \CommentTok{// Step 4: embeddings}
\NormalTok{    embedding\_backward}\OperatorTok{(}\NormalTok{model}\OperatorTok{{-}\textgreater{}}\NormalTok{token\_embedding}\OperatorTok{,}\NormalTok{ tokens}\OperatorTok{,} \OperatorTok{...);}
\NormalTok{    embedding\_backward}\OperatorTok{(}\NormalTok{model}\OperatorTok{{-}\textgreater{}}\NormalTok{position\_embedding}\OperatorTok{,}\NormalTok{ positions}\OperatorTok{,} \OperatorTok{...);}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

Let's unpack this line by line.

\subsubsection{Step 1: Starting from the
loss}\label{step-1-starting-from-the-loss}

The journey begins with the loss function. In training, the most common
loss is cross-entropy. Its backward function compares the predicted
probabilities with the true labels and produces a gradient for the
logits.

\begin{itemize}
\tightlist
\item
  If the model predicted ``cat'' with high confidence and the true label
  was ``dog,'' the gradient will push the logits away from ``cat'' and
  toward ``dog.''
\item
  This gradient is the starting signal that propagates backward through
  the entire network.
\end{itemize}

\subsubsection{Step 2: Back through the output
head}\label{step-2-back-through-the-output-head}

After the loss, the next stop is the final linear projection
(\texttt{lm\_head}). This is just a big matrix multiply that turns
hidden states into vocabulary logits. Its backward function computes two
things:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The gradient with respect to the weights of \texttt{lm\_head}.
\item
  The gradient with respect to the hidden states that fed into it.
\end{enumerate}

This hidden-state gradient is then passed back to the last transformer
block.

\subsubsection{Step 3: Transformer blocks in
reverse}\label{step-3-transformer-blocks-in-reverse}

Here comes the heavy lifting. Each block has multiple components, and
their backward functions are called in the exact opposite order of the
forward pass.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Residual backward: the skip connection splits the gradient into two
  paths - one flowing back into the transformed output, one flowing back
  into the original input.
\item
  LayerNorm backward: computes gradients with respect to its scale
  (\texttt{gamma}) and shift (\texttt{beta}), and also passes gradients
  back to the normalized input.
\item
  MLP backward: applies the chain rule to the two linear layers and the
  GELU activation. The code reuses temporary values from the forward
  pass (like activations) to make this efficient.
\item
  Attention backward: this is the trickiest. It computes gradients for
  Q, K, and V projections, as well as for the softmaxed attention
  weights. It has to apply the causal mask again to ensure no illegal
  gradient flows.
\end{enumerate}

This loop continues until all transformer blocks have been processed.

\subsubsection{Step 4: Back to
embeddings}\label{step-4-back-to-embeddings}

Finally, the gradient reaches the embedding tables. This is where the
model first looked up vectors for tokens and positions. Now it
calculates how much each embedding contributed to the error. These
gradients are added into the embedding matrices, telling the optimizer
how to update them.

\subsubsection{Why this matters}\label{why-this-matters-3}

The backward pass is what makes learning possible. Without it, the model
would forever output the same predictions, never improving. By flowing
``blame'' backwards, each parameter learns how to nudge itself so that
the next forward pass is a little bit better.

Even though the code looks like a lot of function calls, the principle
is simple: start from the loss, step backward through each layer, apply
the chain rule locally, and collect gradients.

\subsubsection{Try it yourself}\label{try-it-yourself-29}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Print gradient norms: Add a \texttt{printf} to see the average
  gradient magnitude at each layer. Notice how they change - sometimes
  exploding, sometimes vanishing.
\item
  Freeze a layer: Comment out \texttt{mlp\_backward} for one block and
  see how the model fails to update properly.
\item
  Inspect embeddings: After training a few steps, dump a few rows of the
  token embedding matrix. You'll see the numbers changing because of
  gradient updates.
\item
  Tiny dataset experiment: Train on a very small dataset (like a 10-word
  corpus) and watch how the backward pass quickly pushes embeddings to
  memorize it.
\item
  Check symmetry: Compare the order of calls in \texttt{gpt2\_forward}
  with \texttt{gpt2\_backward}. They're exact opposites - forward
  builds, backward unbuilds.
\end{enumerate}

\subsubsection{The takeaway}\label{the-takeaway-30}

Backpropagation is the learning engine of neural networks. In
\texttt{llm.c}, the backward pass is written out explicitly, showing how
gradients flow from the loss, through the output head, back through
every transformer block, and finally into embeddings. Once you
understand this flow, you can see how training stitches forward and
backward together to slowly shape a random model into a working language
model.

\subsection{42. Skeleton of Training
Loop}\label{skeleton-of-training-loop}

The backward pass gave us gradients, but gradients by themselves don't
train a model. Training requires a loop: a cycle that repeatedly runs
forward, backward, and update steps over and over until the model
improves. This cycle is called the training loop, and it is the
heartbeat of every deep learning program. In \texttt{train\_gpt2.c}, the
loop is written explicitly in C, which means you can see every piece
instead of it being hidden away in a framework.

\subsubsection{The basic rhythm}\label{the-basic-rhythm}

Every training step follows the same rhythm:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Get a batch of data (input tokens and their labels).
\item
  Run the forward pass to compute predictions and loss.
\item
  Run the backward pass to compute gradients.
\item
  Update weights using an optimizer like AdamW.
\item
  Log progress and, occasionally, validate.
\end{enumerate}

This rhythm repeats thousands or millions of times. With each
repetition, the weights shift slightly, nudging the model toward lower
loss and better predictions.

\subsubsection{How the loop looks in
code}\label{how-the-loop-looks-in-code}

Here's a simplified sketch from \texttt{train\_gpt2.c} (with some
details omitted for clarity):

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ step }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ step }\OperatorTok{\textless{}}\NormalTok{ max\_steps}\OperatorTok{;}\NormalTok{ step}\OperatorTok{++)} \OperatorTok{\{}
    \CommentTok{// 1. Load batch of tokens and labels}
\NormalTok{    dataloader\_next\_batch}\OperatorTok{(\&}\NormalTok{train\_loader}\OperatorTok{,}\NormalTok{ tokens}\OperatorTok{,}\NormalTok{ labels}\OperatorTok{,}\NormalTok{ B}\OperatorTok{,}\NormalTok{ T}\OperatorTok{);}

    \CommentTok{// 2. Forward pass}
\NormalTok{    gpt2\_forward}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{,}\NormalTok{ tokens}\OperatorTok{,}\NormalTok{ labels}\OperatorTok{,}\NormalTok{ B}\OperatorTok{,}\NormalTok{ T}\OperatorTok{);}

    \CommentTok{// 3. Zero gradients}
\NormalTok{    gpt2\_zero\_grad}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{);}

    \CommentTok{// 4. Backward pass}
\NormalTok{    gpt2\_backward}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{,}\NormalTok{ tokens}\OperatorTok{,}\NormalTok{ labels}\OperatorTok{,}\NormalTok{ B}\OperatorTok{,}\NormalTok{ T}\OperatorTok{);}

    \CommentTok{// 5. Optimizer step (AdamW)}
\NormalTok{    adamw\_update}\OperatorTok{(\&}\NormalTok{opt}\OperatorTok{,} \OperatorTok{\&}\NormalTok{model}\OperatorTok{,}\NormalTok{ learning\_rate}\OperatorTok{);}

    \CommentTok{// 6. Logging and validation}
    \ControlFlowTok{if} \OperatorTok{(}\NormalTok{step }\OperatorTok{\%}\NormalTok{ log\_interval }\OperatorTok{==} \DecValTok{0}\OperatorTok{)} \OperatorTok{\{}\NormalTok{ print\_loss}\OperatorTok{(}\NormalTok{step}\OperatorTok{,}\NormalTok{ model}\OperatorTok{.}\NormalTok{loss}\OperatorTok{);} \OperatorTok{\}}
    \ControlFlowTok{if} \OperatorTok{(}\NormalTok{step }\OperatorTok{\%}\NormalTok{ val\_interval }\OperatorTok{==} \DecValTok{0}\OperatorTok{)} \OperatorTok{\{}\NormalTok{ run\_validation}\OperatorTok{(...);} \OperatorTok{\}}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

This loop captures the full training lifecycle: data, forward, backward,
update, and monitoring.

\subsubsection{Step 1: Batching data}\label{step-1-batching-data}

The dataloader feeds the loop with small chunks of tokens. Instead of
sending the whole dataset at once, it breaks it down into batches of
size \texttt{B} (number of sequences per batch) and length \texttt{T}
(number of tokens per sequence).

\begin{itemize}
\tightlist
\item
  Example: if \texttt{B=4} and \texttt{T=128}, each batch is 512 tokens
  long.
\item
  Each sequence has a matching set of labels, which are simply the same
  tokens shifted one position ahead (so the model always predicts the
  \emph{next} word).
\end{itemize}

This batching keeps memory use manageable and helps the model see many
small samples instead of a few giant ones.

\subsubsection{Step 2: Forward pass}\label{step-2-forward-pass}

The forward pass computes predictions for all tokens in the batch and
calculates the loss. This is the ``evaluation'' step - how well did the
model do on this batch? The result is stored in \texttt{model.loss}.

\subsubsection{Step 3: Zeroing
gradients}\label{step-3-zeroing-gradients}

Before calculating new gradients, the old ones must be cleared out. If
you skip this step, gradients from previous batches would accumulate and
corrupt the update. In frameworks like PyTorch you'd call
\texttt{optimizer.zero\_grad()}. Here it's a plain C function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gpt2\_zero\_grad}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

It walks through all parameters and resets their gradient buffers to
zero.

\subsubsection{Step 4: Backward pass}\label{step-4-backward-pass}

Now the backward function is called. It pushes gradients back through
the network, computing how each weight influenced the error. At this
point, every parameter has an associated gradient stored in memory.

\subsubsection{Step 5: Optimizer update}\label{step-5-optimizer-update}

With gradients ready, the optimizer (AdamW in this code) updates each
parameter:

\begin{verbatim}
new_weight = old_weight - learning_rate * gradient (with AdamW tweaks)
\end{verbatim}

This step is what actually changes the model. Without it, the model
would never learn - the forward and backward passes would just repeat
the same results forever.

\subsubsection{Step 6: Logging and
validation}\label{step-6-logging-and-validation}

Every few steps, the loop prints out useful numbers: current step, loss,
time taken, and sometimes throughput (tokens per second). This feedback
is important to check whether training is actually working.

Every few hundred or thousand steps, the loop also runs a validation
pass on held-out data. This tells you whether the model is just
memorizing training data or genuinely learning patterns that generalize.

\subsubsection{Why the training loop
matters}\label{why-the-training-loop-matters}

The training loop is deceptively simple, but it is the engine room of
machine learning. Every improvement in model performance happens because
this loop runs many times. By writing it explicitly in C, \texttt{llm.c}
exposes details that high-level frameworks usually hide: zeroing
gradients, passing pointers to arrays, calling backward and optimizer
functions directly.

This makes it a perfect learning tool. You can see clearly:

\begin{itemize}
\tightlist
\item
  Where the data comes in,
\item
  Where predictions are made,
\item
  Where gradients are calculated,
\item
  And where learning actually happens.
\end{itemize}

\subsubsection{Try it yourself}\label{try-it-yourself-30}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Print the loss curve: Add a \texttt{printf} inside the loop and write
  the loss to a file. Plot it - you should see it decrease over time.
\item
  Change batch size: Set \texttt{B=1} vs.~\texttt{B=8}. Notice how the
  loop becomes noisier with smaller batches but smoother with larger
  ones.
\item
  Skip backward: Comment out \texttt{gpt2\_backward} and optimizer
  update. Run the loop. You'll see the loss never decreases - a clear
  demonstration that forward alone doesn't train.
\item
  Experiment with steps: Try \texttt{max\_steps=10}
  vs.~\texttt{max\_steps=1000}. Short runs show no improvement; longer
  runs start to reduce the loss.
\item
  Slow it down: Insert a \texttt{sleep(1);} inside the loop. This makes
  the rhythm visible step by step, so you can literally watch the model
  ``breathe'' as it trains.
\end{enumerate}

\subsubsection{The takeaway}\label{the-takeaway-31}

The skeleton of the training loop is the core cycle of learning. It
feeds data into the model, computes predictions, finds errors, sends
them backward, updates weights, and logs progress. Everything else -
optimizers, schedulers, distributed training, mixed precision - is just
an enhancement of this basic loop. If you understand how this loop works
in \texttt{llm.c}, you understand the beating heart of deep learning
training.

\subsection{43. AdamW Implementation in
C}\label{adamw-implementation-in-c}

Training a neural network is about adjusting millions of parameters so
that the model gradually becomes better at predicting text. The function
\texttt{gpt2\_update} in \texttt{train\_gpt2.c} is responsible for this
adjustment. It implements the AdamW optimizer, one of the most widely
used algorithms in deep learning. Let's walk through both the theory and
the actual implementation.

\subsubsection{From Gradient Descent to
AdamW}\label{from-gradient-descent-to-adamw}

The most basic optimizer is gradient descent:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_param = old\_param {-} learning\_rate * gradient}
\end{Highlighting}
\end{Shaded}

This approach works, but it has weaknesses. The step size (learning
rate) must be tuned carefully: too small and training is slow, too large
and training diverges. Moreover, all parameters use the same step size,
even though some may need gentler updates.

AdamW improves this by keeping track of moving averages of gradients and
scaling updates adaptively. It also introduces weight decay, which
prevents parameters from growing too large and helps regularize the
model.

\subsubsection{How AdamW Works}\label{how-adamw-works}

AdamW combines several techniques into a single update rule. First, it
uses momentum: instead of relying only on the current gradient, it
averages recent gradients. This smooths noisy updates. Second, it
maintains a running estimate of the squared gradient, which scales down
steps in directions where gradients are consistently large. These are
sometimes called the first and second moments.

Since both running averages start at zero, the algorithm applies bias
correction during the first few steps. Without this, the early updates
would be too small. Finally, AdamW applies weight decay directly in the
update, shrinking parameter values slightly each step.

Putting it together, each parameter update looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m\_t = β1 * m\_(t{-}1) + (1 {-} β1) * g\_t}
\NormalTok{v\_t = β2 * v\_(t{-}1) + (1 {-} β2) * g\_t²}
\NormalTok{m̂\_t = m\_t / (1 {-} β1\^{}t)}
\NormalTok{v̂\_t = v\_t / (1 {-} β2\^{}t)}

\NormalTok{new\_param = old\_param {-} lr * ( m̂\_t / (sqrt(v̂\_t) + ε) + λ * old\_param )}
\end{Highlighting}
\end{Shaded}

Here \texttt{m} is momentum, \texttt{v} is variance, \texttt{lr} is
learning rate, \texttt{ε} is a small constant for stability, and
\texttt{λ} is the weight decay factor.

\subsubsection{\texorpdfstring{The Implementation in
\texttt{train\_gpt2.c}}{The Implementation in train\_gpt2.c}}\label{the-implementation-in-train_gpt2.c}

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{void}\NormalTok{ gpt2\_update}\OperatorTok{(}\NormalTok{GPT2 }\OperatorTok{*}\NormalTok{model}\OperatorTok{,} \DataTypeTok{float}\NormalTok{ learning\_rate}\OperatorTok{,} \DataTypeTok{float}\NormalTok{ beta1}\OperatorTok{,} \DataTypeTok{float}\NormalTok{ beta2}\OperatorTok{,}
                 \DataTypeTok{float}\NormalTok{ eps}\OperatorTok{,} \DataTypeTok{float}\NormalTok{ weight\_decay}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ t}\OperatorTok{)} \OperatorTok{\{}
    \ControlFlowTok{if} \OperatorTok{(}\NormalTok{model}\OperatorTok{{-}\textgreater{}}\NormalTok{m\_memory }\OperatorTok{==}\NormalTok{ NULL}\OperatorTok{)} \OperatorTok{\{}
\NormalTok{        model}\OperatorTok{{-}\textgreater{}}\NormalTok{m\_memory }\OperatorTok{=} \OperatorTok{(}\DataTypeTok{float}\OperatorTok{*)}\NormalTok{calloc}\OperatorTok{(}\NormalTok{model}\OperatorTok{{-}\textgreater{}}\NormalTok{num\_parameters}\OperatorTok{,} \KeywordTok{sizeof}\OperatorTok{(}\DataTypeTok{float}\OperatorTok{));}
\NormalTok{        model}\OperatorTok{{-}\textgreater{}}\NormalTok{v\_memory }\OperatorTok{=} \OperatorTok{(}\DataTypeTok{float}\OperatorTok{*)}\NormalTok{calloc}\OperatorTok{(}\NormalTok{model}\OperatorTok{{-}\textgreater{}}\NormalTok{num\_parameters}\OperatorTok{,} \KeywordTok{sizeof}\OperatorTok{(}\DataTypeTok{float}\OperatorTok{));}
    \OperatorTok{\}}

    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{size\_t}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}}\NormalTok{ model}\OperatorTok{{-}\textgreater{}}\NormalTok{num\_parameters}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)} \OperatorTok{\{}
        \DataTypeTok{float}\NormalTok{ param }\OperatorTok{=}\NormalTok{ model}\OperatorTok{{-}\textgreater{}}\NormalTok{params\_memory}\OperatorTok{[}\NormalTok{i}\OperatorTok{];}
        \DataTypeTok{float}\NormalTok{ grad }\OperatorTok{=}\NormalTok{ model}\OperatorTok{{-}\textgreater{}}\NormalTok{grads\_memory}\OperatorTok{[}\NormalTok{i}\OperatorTok{];}

        \DataTypeTok{float}\NormalTok{ m }\OperatorTok{=}\NormalTok{ beta1 }\OperatorTok{*}\NormalTok{ model}\OperatorTok{{-}\textgreater{}}\NormalTok{m\_memory}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{+} \OperatorTok{(}\FloatTok{1.0}\BuiltInTok{f} \OperatorTok{{-}}\NormalTok{ beta1}\OperatorTok{)} \OperatorTok{*}\NormalTok{ grad}\OperatorTok{;}
        \DataTypeTok{float}\NormalTok{ v }\OperatorTok{=}\NormalTok{ beta2 }\OperatorTok{*}\NormalTok{ model}\OperatorTok{{-}\textgreater{}}\NormalTok{v\_memory}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{+} \OperatorTok{(}\FloatTok{1.0}\BuiltInTok{f} \OperatorTok{{-}}\NormalTok{ beta2}\OperatorTok{)} \OperatorTok{*}\NormalTok{ grad }\OperatorTok{*}\NormalTok{ grad}\OperatorTok{;}

        \DataTypeTok{float}\NormalTok{ m\_hat }\OperatorTok{=}\NormalTok{ m }\OperatorTok{/} \OperatorTok{(}\FloatTok{1.0}\BuiltInTok{f} \OperatorTok{{-}}\NormalTok{ powf}\OperatorTok{(}\NormalTok{beta1}\OperatorTok{,}\NormalTok{ t}\OperatorTok{));}
        \DataTypeTok{float}\NormalTok{ v\_hat }\OperatorTok{=}\NormalTok{ v }\OperatorTok{/} \OperatorTok{(}\FloatTok{1.0}\BuiltInTok{f} \OperatorTok{{-}}\NormalTok{ powf}\OperatorTok{(}\NormalTok{beta2}\OperatorTok{,}\NormalTok{ t}\OperatorTok{));}

\NormalTok{        model}\OperatorTok{{-}\textgreater{}}\NormalTok{m\_memory}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{=}\NormalTok{ m}\OperatorTok{;}
\NormalTok{        model}\OperatorTok{{-}\textgreater{}}\NormalTok{v\_memory}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{=}\NormalTok{ v}\OperatorTok{;}
\NormalTok{        model}\OperatorTok{{-}\textgreater{}}\NormalTok{params\_memory}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{{-}=}\NormalTok{ learning\_rate }\OperatorTok{*}
            \OperatorTok{(}\NormalTok{m\_hat }\OperatorTok{/} \OperatorTok{(}\NormalTok{sqrtf}\OperatorTok{(}\NormalTok{v\_hat}\OperatorTok{)} \OperatorTok{+}\NormalTok{ eps}\OperatorTok{)} \OperatorTok{+}\NormalTok{ weight\_decay }\OperatorTok{*}\NormalTok{ param}\OperatorTok{);}
    \OperatorTok{\}}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

The first \texttt{if} block allocates memory for the moving averages
\texttt{m} and \texttt{v} the first time the optimizer runs. Then, for
each parameter, the code computes the new averages, applies bias
correction, and finally updates the parameter with the AdamW formula.

\subsubsection{Example Walkthrough}\label{example-walkthrough-3}

Suppose we have a parameter \texttt{w\ =\ 0.5} with gradient
\texttt{g\ =\ 0.2} on the first training step. Using β1 = 0.9 and β2 =
0.999:

\begin{itemize}
\item
  Momentum:

\begin{verbatim}
m = 0.9 * 0 + 0.1 * 0.2 = 0.02
\end{verbatim}
\item
  Variance:

\begin{verbatim}
v = 0.999 * 0 + 0.001 * 0.04 = 0.00004
\end{verbatim}
\item
  Bias correction:

\begin{verbatim}
m̂ = 0.02 / (1 - 0.9) = 0.2
v̂ = 0.00004 / (1 - 0.999) = 0.04
\end{verbatim}
\item
  Final update (lr = 0.001, weight\_decay = 0.01):

\begin{verbatim}
update = 0.001 * (0.2 / sqrt(0.04) + 0.01 * 0.5)
       = 0.001 * (1.0 + 0.005)
       = 0.001005
\end{verbatim}
\end{itemize}

So the parameter becomes \texttt{w\ =\ 0.498995}.

\subsubsection{Intuition}\label{intuition-1}

Think of a ball rolling down a slope. The gradient is the slope itself.
Momentum makes the ball keep rolling even if the slope flattens briefly.
The variance term makes the ball slow down on rocky ground where the
slope changes rapidly. Bias correction ensures the ball doesn't move too
timidly at the start. Weight decay adds friction so the ball doesn't
roll out of control.

\subsubsection{Why It Matters}\label{why-it-matters-26}

Optimizers are the difference between a model that trains smoothly and
one that diverges or gets stuck. AdamW became popular because it
combines stability with efficiency. It automatically adapts to each
parameter's scale, reduces the need for manual learning rate tuning, and
includes weight decay in a principled way. For GPT-style models with
hundreds of millions of parameters, these qualities make training
feasible.

\subsubsection{Try It Yourself}\label{try-it-yourself-31}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change the learning rate from \texttt{0.001} to \texttt{0.01} in the
  code and see how quickly the model diverges.
\item
  Set \texttt{weight\_decay\ =\ 0} and compare validation loss after a
  few epochs. The model might overfit more quickly.
\item
  Print out the first 10 values of \texttt{m\_memory} and
  \texttt{v\_memory} during training to watch how they evolve over
  steps.
\item
  Replace AdamW with plain SGD (just \texttt{param\ -=\ lr\ *\ grad})
  and compare training speed and stability.
\item
  Experiment with β1 = 0 (no momentum) or β2 = 0 (no variance smoothing)
  and see how noisy updates become.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-32}

AdamW provides a balance of speed, stability, and generalization. In
practice, it allows models like GPT-2 to train much more reliably than
with vanilla gradient descent. The C implementation in \texttt{llm.c}
demonstrates that beneath the math, it's just a simple loop applying a
few arithmetic operations for each parameter.

\subsection{44. Gradient Accumulation and
Micro-Batching}\label{gradient-accumulation-and-micro-batching}

Modern language models are enormous, and so are the batches of text we
would like to feed them during training. But real hardware has limits: a
single GPU or CPU may not have enough memory to process a large batch in
one go. To solve this, training code often uses gradient accumulation
and micro-batching. Both ideas allow us to simulate training with larger
batches without requiring more memory than our hardware can provide.

\subsubsection{What Problem Are We
Solving?}\label{what-problem-are-we-solving}

When you process a batch of data, you run forward and backward passes to
calculate gradients. If your batch size is very large, you get smoother
gradients (less noisy), which often helps the model converge better. But
large batches may not fit in memory.

Imagine trying to train with a batch of 1024 sequences on a GPU that can
only handle 128 sequences at once. Without tricks, you would be forced
to use the smaller batch size and give up the benefits of larger
batches. Gradient accumulation fixes this problem by letting you split
the big batch into smaller micro-batches, process them one at a time,
and accumulate the results as if you had processed the big batch all at
once.

\subsubsection{How It Works in Practice}\label{how-it-works-in-practice}

Let's say we want an effective batch size of 1024, but our hardware only
supports 128. We split the big batch into 8 micro-batches of 128 each:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run forward + backward on micro-batch 1, store the gradients.
\item
  Run forward + backward on micro-batch 2, add its gradients to the
  stored ones.
\item
  Repeat until all 8 micro-batches are processed.
\item
  Once gradients for all 8 are accumulated, perform the optimizer
  update.
\end{enumerate}

The important part is step 4: we only update the parameters once per
effective batch, not after each micro-batch. This preserves the effect
of training with a large batch.

\subsubsection{Pseudocode Example}\label{pseudocode-example}

Here's how this might look in simplified pseudocode:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{int}\NormalTok{ accumulation\_steps }\OperatorTok{=} \DecValTok{8}\OperatorTok{;}
\ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ step }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ step }\OperatorTok{\textless{}}\NormalTok{ total\_steps}\OperatorTok{;}\NormalTok{ step}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{    zero\_grad}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{);}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}}\NormalTok{ accumulation\_steps}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{        dataloader\_next\_batch}\OperatorTok{(\&}\NormalTok{train\_loader}\OperatorTok{,}\NormalTok{ tokens}\OperatorTok{,}\NormalTok{ labels}\OperatorTok{,}\NormalTok{ B}\OperatorTok{,}\NormalTok{ T}\OperatorTok{);}
\NormalTok{        forward}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{,}\NormalTok{ tokens}\OperatorTok{,}\NormalTok{ labels}\OperatorTok{,}\NormalTok{ B}\OperatorTok{,}\NormalTok{ T}\OperatorTok{);}
\NormalTok{        backward}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{,}\NormalTok{ tokens}\OperatorTok{,}\NormalTok{ labels}\OperatorTok{,}\NormalTok{ B}\OperatorTok{,}\NormalTok{ T}\OperatorTok{);}
        \CommentTok{// do NOT call optimizer update yet}
    \OperatorTok{\}}
\NormalTok{    adamw\_update}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{,}\NormalTok{ lr}\OperatorTok{,}\NormalTok{ beta1}\OperatorTok{,}\NormalTok{ beta2}\OperatorTok{,}\NormalTok{ eps}\OperatorTok{,}\NormalTok{ weight\_decay}\OperatorTok{,}\NormalTok{ step}\OperatorTok{+}\DecValTok{1}\OperatorTok{);}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

Notice how the optimizer only runs once per outer loop iteration, even
though gradients were accumulated across multiple micro-batches.

\subsubsection{Why Gradient Accumulation
Helps}\label{why-gradient-accumulation-helps}

\begin{itemize}
\tightlist
\item
  Memory efficiency: You can train with larger effective batch sizes
  without needing more hardware.
\item
  Training stability: Larger batches reduce the variance of gradients,
  making training less noisy.
\item
  Flexibility: You can scale effective batch size up or down depending
  on your needs without changing hardware.
\end{itemize}

\subsubsection{Micro-Batching
vs.~Accumulation}\label{micro-batching-vs.-accumulation}

Micro-batching refers to the act of splitting a batch into smaller
parts. Gradient accumulation is what you do after micro-batching: sum up
the gradients across those parts. Together, they allow you to simulate
training with any batch size you want, within memory constraints.

\subsubsection{Why It Matters}\label{why-it-matters-27}

The quality of training often depends on batch size. If you can't fit a
large batch directly, gradient accumulation ensures you still reap the
benefits. It's one of those ``engineering hacks'' that makes training
state-of-the-art models possible on limited resources.

\subsubsection{Try It Yourself}\label{try-it-yourself-32}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run training with batch size = 16 and no accumulation. Watch how noisy
  the loss curve looks.
\item
  Now set micro-batch size = 4 and accumulation\_steps = 4. This
  simulates batch size = 16, but in smaller chunks. Compare the loss
  curve.
\item
  Increase accumulation\_steps to simulate batch size = 32. Observe if
  training becomes smoother.
\item
  Experiment with turning accumulation off and on while keeping the same
  effective batch size. Notice how optimizer updates per epoch differ.
\item
  Print out how many times the optimizer is called. With accumulation,
  it should be fewer than the number of micro-batches.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-33}

Gradient accumulation and micro-batching are techniques that let you
train with large effective batch sizes while staying within the limits
of your hardware. They preserve the benefits of large batches-stability
and smoother gradients-without demanding extra memory. In
\texttt{llm.c}, the simplicity of the training loop means you can
clearly see where accumulation fits: gradients are summed across
micro-batches, and only then does the optimizer step in. This is a small
adjustment in code but a huge enabler in practice.

\subsection{45. Logging and Progress
Reporting}\label{logging-and-progress-reporting}

Every training loop needs a way to show what's happening under the hood.
Without logs, you wouldn't know if the model is improving, if the code
is running efficiently, or if something has silently gone wrong. In
\texttt{train\_gpt2.c}, logging is intentionally minimal but highly
informative: each training step prints the step number, the current
training loss, and how long that step took to run.

\subsubsection{The Real Code for
Logging}\label{the-real-code-for-logging}

Here's the relevant snippet from \texttt{train\_gpt2.c}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{// do a training step}
\NormalTok{clock\_gettime}\OperatorTok{(}\NormalTok{CLOCK\_MONOTONIC}\OperatorTok{,} \OperatorTok{\&}\NormalTok{start}\OperatorTok{);}
\NormalTok{dataloader\_next\_batch}\OperatorTok{(\&}\NormalTok{train\_loader}\OperatorTok{);}
\NormalTok{gpt2\_forward}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{,}\NormalTok{ train\_loader}\OperatorTok{.}\NormalTok{inputs}\OperatorTok{,}\NormalTok{ train\_loader}\OperatorTok{.}\NormalTok{targets}\OperatorTok{,}\NormalTok{ B}\OperatorTok{,}\NormalTok{ T}\OperatorTok{);}
\NormalTok{gpt2\_zero\_grad}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{);}
\NormalTok{gpt2\_backward}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{);}
\NormalTok{gpt2\_update}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{,} \FloatTok{1e{-}4}\BuiltInTok{f}\OperatorTok{,} \FloatTok{0.9}\BuiltInTok{f}\OperatorTok{,} \FloatTok{0.999}\BuiltInTok{f}\OperatorTok{,} \FloatTok{1e{-}8}\BuiltInTok{f}\OperatorTok{,} \FloatTok{0.0}\BuiltInTok{f}\OperatorTok{,}\NormalTok{ step}\OperatorTok{+}\DecValTok{1}\OperatorTok{);}
\NormalTok{clock\_gettime}\OperatorTok{(}\NormalTok{CLOCK\_MONOTONIC}\OperatorTok{,} \OperatorTok{\&}\NormalTok{end}\OperatorTok{);}

\DataTypeTok{double}\NormalTok{ time\_elapsed\_s }\OperatorTok{=} \OperatorTok{(}\NormalTok{end}\OperatorTok{.}\NormalTok{tv\_sec }\OperatorTok{{-}}\NormalTok{ start}\OperatorTok{.}\NormalTok{tv\_sec}\OperatorTok{)} \OperatorTok{+}
                        \OperatorTok{(}\NormalTok{end}\OperatorTok{.}\NormalTok{tv\_nsec }\OperatorTok{{-}}\NormalTok{ start}\OperatorTok{.}\NormalTok{tv\_nsec}\OperatorTok{)} \OperatorTok{/} \FloatTok{1e9}\OperatorTok{;}
\NormalTok{printf}\OperatorTok{(}\StringTok{"step }\SpecialCharTok{\%d}\StringTok{: train loss }\SpecialCharTok{\%f}\StringTok{ (took }\SpecialCharTok{\%f}\StringTok{ ms)}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\OperatorTok{,}
\NormalTok{       step}\OperatorTok{,}\NormalTok{ model}\OperatorTok{.}\NormalTok{mean\_loss}\OperatorTok{,}\NormalTok{ time\_elapsed\_s }\OperatorTok{*} \DecValTok{1000}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

This small block accomplishes two things:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It measures how long the training step took using
  \texttt{clock\_gettime}.
\item
  It reports the step number, the loss, and the elapsed time in
  milliseconds.
\end{enumerate}

The output looks like this when training:

\begin{verbatim}
step 0: train loss 4.677779 (took 1987.546 ms)
step 1: train loss 5.191576 (took 1927.230 ms)
step 2: train loss 4.438685 (took 1902.987 ms)
\end{verbatim}

\subsubsection{Understanding What's
Reported}\label{understanding-whats-reported}

\begin{itemize}
\item
  Step number (\texttt{step}) Tells you where you are in training. Since
  deep learning often runs for thousands of steps, this acts like a
  progress bar.
\item
  Training loss (\texttt{model.mean\_loss}) Shows how well the model is
  fitting the training batch. A lower value generally means better
  predictions. Watching this number decrease over time is the main
  signal that learning is happening.
\item
  Step duration (\texttt{time\_elapsed\_s\ *\ 1000}) Measures
  performance. If one step takes 2000 ms, then 5000 steps would take
  about 3 hours. Monitoring this helps you estimate total training time
  and spot performance regressions (e.g., if a new change suddenly
  doubles the step time).
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-28}

Logs are your window into the training process. If the loss goes down
smoothly, training is healthy. If it suddenly spikes or stays flat,
something is wrong-maybe the learning rate is too high, or the model has
run out of capacity. Timing information also matters: you need to know
whether the code is running efficiently or wasting cycles.

\subsubsection{Try It Yourself}\label{try-it-yourself-33}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Change the learning rate from \texttt{1e-4} to \texttt{1e-2} and watch
  how the loss behaves. If it jumps or becomes unstable, you'll see it
  directly in the logs.
\item
  Add validation logging by running the model on a held-out dataset
  every 100 steps and printing \texttt{val\_loss}. Compare it to
  \texttt{train\_loss}.
\item
  Record the log output to a file with:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{./train\_gpt2} \OperatorTok{\textgreater{}}\NormalTok{ log.txt}
\end{Highlighting}
\end{Shaded}

  Then plot \texttt{train\_loss} over steps in Python or Excel to
  visualize the curve.
\item
  Add throughput reporting: divide the batch size times sequence length
  (\texttt{B*T}) by the step time to print tokens per second. This gives
  a clearer sense of efficiency.
\item
  Try disabling \texttt{clock\_gettime} and only print loss. Notice how
  much harder it becomes to judge performance without timing
  information.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-34}

Even the simplest logs can tell you a lot. With just a single line-step,
loss, and duration-you know how fast training is, whether it's
converging, and how long it will take. In larger frameworks, this kind
of information is often hidden behind dashboards and monitoring tools,
but the core idea is the same: training is only useful if you can see
and interpret its progress.

\subsection{46. Validation Runs in the Training
Loop}\label{validation-runs-in-the-training-loop}

When you train a model, it is not enough to look only at how well it
does on the training data. The real test is whether the model has
learned patterns that apply to new, unseen data. This is where
validation comes in. Validation is like a quiz the model takes from time
to time during training. It does not count toward learning-it is just a
check to see how much the model has really understood.

In \texttt{train\_gpt2.c}, validation is built right into the training
loop. Every so often, instead of updating weights, the program pauses
and runs the model on a set of tokens it has never trained on. It then
prints out the average validation loss. This number tells you if the
model is actually generalizing, not just memorizing.

\subsubsection{How the validation code
looks}\label{how-the-validation-code-looks}

Here is the actual block of code that handles validation:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if} \OperatorTok{(}\NormalTok{step }\OperatorTok{\%} \DecValTok{10} \OperatorTok{==} \DecValTok{0}\OperatorTok{)} \OperatorTok{\{}
    \DataTypeTok{float}\NormalTok{ val\_loss }\OperatorTok{=} \FloatTok{0.0}\BuiltInTok{f}\OperatorTok{;}
\NormalTok{    dataloader\_reset}\OperatorTok{(\&}\NormalTok{val\_loader}\OperatorTok{);}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}}\NormalTok{ val\_num\_batches}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{        dataloader\_next\_batch}\OperatorTok{(\&}\NormalTok{val\_loader}\OperatorTok{);}
\NormalTok{        gpt2\_forward}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{,}\NormalTok{ val\_loader}\OperatorTok{.}\NormalTok{inputs}\OperatorTok{,}\NormalTok{ val\_loader}\OperatorTok{.}\NormalTok{targets}\OperatorTok{,}\NormalTok{ B}\OperatorTok{,}\NormalTok{ T}\OperatorTok{);}
\NormalTok{        val\_loss }\OperatorTok{+=}\NormalTok{ model}\OperatorTok{.}\NormalTok{mean\_loss}\OperatorTok{;}
    \OperatorTok{\}}
\NormalTok{    val\_loss }\OperatorTok{/=}\NormalTok{ val\_num\_batches}\OperatorTok{;}
\NormalTok{    printf}\OperatorTok{(}\StringTok{"val loss }\SpecialCharTok{\%f\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ val\_loss}\OperatorTok{);}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

At first glance, this might look like just a few lines of C code. But
behind it are several important ideas about how machine learning models
are tested while they learn. Let's go through this step by step.

\subsubsection{Step-by-step explanation}\label{step-by-step-explanation}

The first line checks whether it is time to run validation:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if} \OperatorTok{(}\NormalTok{step }\OperatorTok{\%} \DecValTok{10} \OperatorTok{==} \DecValTok{0}\OperatorTok{)} \OperatorTok{\{}
\end{Highlighting}
\end{Shaded}

This means that validation happens every 10 steps. The \texttt{\%}
operator is ``modulo,'' which returns the remainder of a division. If
the step number is divisible by 10 (like 0, 10, 20, 30), then the block
runs. By spacing it out this way, validation does not slow training too
much but still gives you regular updates.

Next, the code sets up a place to store the running total of the
validation loss:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{float}\NormalTok{ val\_loss }\OperatorTok{=} \FloatTok{0.0}\BuiltInTok{f}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

Then it resets the validation dataloader:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataloader\_reset}\OperatorTok{(\&}\NormalTok{val\_loader}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

This makes sure the validation dataset starts from the beginning each
time. That way, the results are consistent-you're always checking the
model on the same set of text, rather than starting from a random place.

Now comes the loop over validation batches:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}}\NormalTok{ val\_num\_batches}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)} \OperatorTok{\{}
\NormalTok{    dataloader\_next\_batch}\OperatorTok{(\&}\NormalTok{val\_loader}\OperatorTok{);}
\NormalTok{    gpt2\_forward}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{,}\NormalTok{ val\_loader}\OperatorTok{.}\NormalTok{inputs}\OperatorTok{,}\NormalTok{ val\_loader}\OperatorTok{.}\NormalTok{targets}\OperatorTok{,}\NormalTok{ B}\OperatorTok{,}\NormalTok{ T}\OperatorTok{);}
\NormalTok{    val\_loss }\OperatorTok{+=}\NormalTok{ model}\OperatorTok{.}\NormalTok{mean\_loss}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

Here's what's happening inside:

\begin{itemize}
\tightlist
\item
  \texttt{dataloader\_next\_batch} fetches the next chunk of tokens and
  labels from the validation set.
\item
  \texttt{gpt2\_forward} runs the model forward on those tokens,
  predicting the next word for each one, and computes the loss against
  the true labels.
\item
  The loss from that batch is added to \texttt{val\_loss}.
\end{itemize}

Notice that there is no call to \texttt{gpt2\_zero\_grad}, no
\texttt{gpt2\_backward}, and no \texttt{gpt2\_update}. That is because
validation does not train the model. It only measures performance.

Finally, the program averages the loss across the number of batches:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val\_loss }\OperatorTok{/=}\NormalTok{ val\_num\_batches}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

And prints the result:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{printf}\OperatorTok{(}\StringTok{"val loss }\SpecialCharTok{\%f\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ val\_loss}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

This gives you a single number that summarizes how well the model is
performing on unseen data at this point in training.

\subsubsection{How to read validation
loss}\label{how-to-read-validation-loss}

Imagine you are training and see logs like this:

\begin{verbatim}
step 0: train loss 4.677779 (took 1987.546 ms)
val loss 4.901234
step 1: train loss 5.191576 (took 1927.230 ms)
step 2: train loss 4.438685 (took 1902.987 ms)
...
step 10: train loss 3.912342 (took 1890.321 ms)
val loss 4.100321
\end{verbatim}

The training loss is printed every step, while the validation loss
appears every 10 steps. If both numbers are going down, that is a sign
the model is genuinely learning. If training loss drops but validation
loss stays the same or starts going up, the model is probably memorizing
the training set-this is called overfitting.

\subsubsection{Why validation is
important}\label{why-validation-is-important}

Without validation, you could be tricked into thinking the model is
improving just because the training loss is going down. But that might
only mean it has memorized the training data. Validation checks prevent
this by showing you whether the model can handle data it has not seen
before. It is like a student practicing with old exam papers (training)
versus being tested with new problems (validation).

\subsubsection{Small details that
matter}\label{small-details-that-matter}

The code averages validation loss over \texttt{val\_num\_batches}, which
is set earlier to 5. That means it only checks 5 batches, not the entire
validation dataset. This is a shortcut-it makes validation much faster,
at the cost of some accuracy in the measurement. But for training
feedback, this is usually enough.

The batch size \texttt{B} and sequence length \texttt{T} for validation
are the same as training. This keeps the loss comparable between
training and validation.

\subsubsection{Try it yourself}\label{try-it-yourself-34}

You can experiment with the validation process to understand it better.
Here are some ideas:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change the frequency from every 10 steps to every 5 or even every
  step. You'll see more validation updates, but training will slow down.
\item
  Increase \texttt{val\_num\_batches} to 20. The validation loss will
  become less noisy, but each check will take longer.
\item
  Comment out the validation block and train again. Notice how you lose
  a sense of whether the model is really generalizing.
\item
  Save validation loss values to a file and plot them. Compare the curve
  against the training loss curve. You'll see how they move together or
  diverge.
\item
  Try using a very small validation dataset. Watch how the loss jumps
  around more compared to a larger, more stable dataset.
\end{enumerate}

\subsubsection{The takeaway}\label{the-takeaway-35}

Validation runs are short forward-only tests that give you confidence
the model is learning patterns that apply to new text. They are easy to
implement-a few lines of code in \texttt{train\_gpt2.c}-but they are one
of the most important tools for monitoring training. By checking
validation loss regularly, you make sure your model is not just
memorizing but actually becoming better at language modeling.

\subsection{47. Checkpointing Parameters and Optimizer
State}\label{checkpointing-parameters-and-optimizer-state}

Training a model can take hours, days, or even weeks. If you stop the
program halfway-whether by accident (a crash, a power cut) or on purpose
(pausing to save compute)-you don't want to start over from scratch.
Checkpointing solves this problem by saving the model's parameters and
optimizer state to disk so you can resume training later.

\subsubsection{What a checkpoint
contains}\label{what-a-checkpoint-contains}

A checkpoint is like a ``save game'' for machine learning. At a minimum,
it needs:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Model parameters -- the actual weights of the neural network, stored
  as floating-point numbers in memory. These define what the model has
  learned so far.
\item
  Optimizer state -- for AdamW, this includes the running averages of
  gradients (\texttt{m\_memory}) and squared gradients
  (\texttt{v\_memory}). Without these, the optimizer would lose its
  ``memory'' of past updates, which could destabilize resumed training.
\item
  Step counter -- the number of steps completed so far. This matters for
  bias correction in AdamW and for scheduling the learning rate.
\end{enumerate}

Together, these three things capture the full training state.

\subsubsection{Saving a checkpoint}\label{saving-a-checkpoint}

Although \texttt{train\_gpt2.c} is kept minimal and does not include
full checkpointing code, the idea is straightforward. You allocate a
file, write all parameters, optimizer buffers, and metadata, then close
the file. In pseudocode, it looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{FILE} \OperatorTok{*}\NormalTok{f }\OperatorTok{=}\NormalTok{ fopen}\OperatorTok{(}\StringTok{"checkpoint.bin"}\OperatorTok{,} \StringTok{"wb"}\OperatorTok{);}
\NormalTok{fwrite}\OperatorTok{(}\NormalTok{model}\OperatorTok{.}\NormalTok{params\_memory}\OperatorTok{,} \KeywordTok{sizeof}\OperatorTok{(}\DataTypeTok{float}\OperatorTok{),}\NormalTok{ model}\OperatorTok{.}\NormalTok{num\_parameters}\OperatorTok{,}\NormalTok{ f}\OperatorTok{);}
\NormalTok{fwrite}\OperatorTok{(}\NormalTok{model}\OperatorTok{.}\NormalTok{m\_memory}\OperatorTok{,} \KeywordTok{sizeof}\OperatorTok{(}\DataTypeTok{float}\OperatorTok{),}\NormalTok{ model}\OperatorTok{.}\NormalTok{num\_parameters}\OperatorTok{,}\NormalTok{ f}\OperatorTok{);}
\NormalTok{fwrite}\OperatorTok{(}\NormalTok{model}\OperatorTok{.}\NormalTok{v\_memory}\OperatorTok{,} \KeywordTok{sizeof}\OperatorTok{(}\DataTypeTok{float}\OperatorTok{),}\NormalTok{ model}\OperatorTok{.}\NormalTok{num\_parameters}\OperatorTok{,}\NormalTok{ f}\OperatorTok{);}
\NormalTok{fwrite}\OperatorTok{(\&}\NormalTok{step}\OperatorTok{,} \KeywordTok{sizeof}\OperatorTok{(}\DataTypeTok{int}\OperatorTok{),} \DecValTok{1}\OperatorTok{,}\NormalTok{ f}\OperatorTok{);}
\NormalTok{fclose}\OperatorTok{(}\NormalTok{f}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

This is a binary dump of the model and optimizer. Later, you can load
the file back with \texttt{fread} calls into the same memory locations.

\subsubsection{Loading a checkpoint}\label{loading-a-checkpoint}

Loading is the reverse:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{FILE} \OperatorTok{*}\NormalTok{f }\OperatorTok{=}\NormalTok{ fopen}\OperatorTok{(}\StringTok{"checkpoint.bin"}\OperatorTok{,} \StringTok{"rb"}\OperatorTok{);}
\NormalTok{fread}\OperatorTok{(}\NormalTok{model}\OperatorTok{.}\NormalTok{params\_memory}\OperatorTok{,} \KeywordTok{sizeof}\OperatorTok{(}\DataTypeTok{float}\OperatorTok{),}\NormalTok{ model}\OperatorTok{.}\NormalTok{num\_parameters}\OperatorTok{,}\NormalTok{ f}\OperatorTok{);}
\NormalTok{fread}\OperatorTok{(}\NormalTok{model}\OperatorTok{.}\NormalTok{m\_memory}\OperatorTok{,} \KeywordTok{sizeof}\OperatorTok{(}\DataTypeTok{float}\OperatorTok{),}\NormalTok{ model}\OperatorTok{.}\NormalTok{num\_parameters}\OperatorTok{,}\NormalTok{ f}\OperatorTok{);}
\NormalTok{fread}\OperatorTok{(}\NormalTok{model}\OperatorTok{.}\NormalTok{v\_memory}\OperatorTok{,} \KeywordTok{sizeof}\OperatorTok{(}\DataTypeTok{float}\OperatorTok{),}\NormalTok{ model}\OperatorTok{.}\NormalTok{num\_parameters}\OperatorTok{,}\NormalTok{ f}\OperatorTok{);}
\NormalTok{fread}\OperatorTok{(\&}\NormalTok{step}\OperatorTok{,} \KeywordTok{sizeof}\OperatorTok{(}\DataTypeTok{int}\OperatorTok{),} \DecValTok{1}\OperatorTok{,}\NormalTok{ f}\OperatorTok{);}
\NormalTok{fclose}\OperatorTok{(}\NormalTok{f}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

Once loaded, training can continue exactly where it left off.

\subsubsection{Why optimizer state
matters}\label{why-optimizer-state-matters}

It might seem enough to save only the model's parameters. But AdamW
depends on moving averages of past gradients. If you throw those away
and restart with only the parameters, the optimizer will behave
differently. Learning may suddenly become unstable, or the effective
learning rate may feel wrong. That's why saving both the parameters and
optimizer state gives the most faithful restart.

\subsubsection{Why checkpointing is
essential}\label{why-checkpointing-is-essential}

Training is rarely smooth. Servers reboot, experiments are interrupted,
bugs are found. Without checkpoints, any interruption means wasted
compute and lost progress. With checkpoints, you can pause and resume at
will. They also let you archive important moments in training-for
example, saving the model when validation loss is lowest, not just at
the end.

\subsubsection{Try it yourself}\label{try-it-yourself-35}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a small function that saves the model's parameters after every
  100 steps. Then kill the program midway and reload from the saved
  file. Confirm that resumed training picks up where it left off.
\item
  Try saving only parameters but not optimizer state. Resume training
  and compare loss curves. You'll see that the run diverges from the
  original.
\item
  Save checkpoints at multiple steps and later reload them to compare
  model generations (does the model produce more fluent text after 10
  steps, 100 steps, 1000 steps?).
\item
  Intentionally corrupt part of a checkpoint file (flip a few bytes) and
  try reloading. This helps you understand why consistency checks or
  checksums are often added in real systems.
\item
  Store checkpoints in a versioned way (e.g.,
  \texttt{checkpoint\_step100.bin}, \texttt{checkpoint\_step200.bin}) so
  you can roll back if a later training phase degrades performance.
\end{enumerate}

\subsubsection{The takeaway}\label{the-takeaway-36}

Checkpointing is what makes long-running training practical. By saving
parameters, optimizer state, and the step counter, you preserve not just
what the model knows but how it is learning. In real projects,
checkpoints are the bridge between experiments and production: they let
you stop, resume, compare, and deploy models without ever starting from
scratch. Even though \texttt{llm.c} does not fully implement it, the
concept is simple and invaluable.

\subsection{48. Reproducibility and Small
Divergences}\label{reproducibility-and-small-divergences}

When training deep learning models, two runs that look identical on the
surface can still behave differently. One run might converge quickly,
another might take longer, and sometimes losses diverge even though you
used the same dataset and code. This happens because of the way
randomness and numerical precision interact during training.
Reproducibility is about controlling these factors so that results are
consistent and meaningful.

\subsubsection{Sources of randomness}\label{sources-of-randomness}

There are several places where randomness sneaks into training:

\begin{itemize}
\tightlist
\item
  Data order: if batches are shuffled differently, the model sees tokens
  in a new sequence. Early steps can influence the trajectory of
  training.
\item
  Weight initialization: initial parameters are usually set randomly.
  Different seeds lead to slightly different starting points.
\item
  Dropout and sampling: while \texttt{train\_gpt2.c} is minimal and
  doesn't include dropout layers, many neural networks do. Dropout
  randomly disables activations during training.
\item
  Floating-point arithmetic: on CPUs and GPUs, the order of summations
  or parallel reductions can cause tiny rounding differences. Over many
  steps, these small changes accumulate.
\end{itemize}

\subsubsection{How llm.c handles
reproducibility}\label{how-llm.c-handles-reproducibility}

The repository includes functions like \texttt{manual\_seed} and
\texttt{random\_f32} in \texttt{llmc/rand.h}. These are simple random
number generators that can be seeded with a fixed value. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{manual\_seed}\OperatorTok{(}\DecValTok{1337}\OperatorTok{);}  
\end{Highlighting}
\end{Shaded}

If you call this before training, the random number generator starts
from the same state every run. That means weight initialization and
sampling will be reproducible.

The dataloaders also have reproducibility options. When you initialize a
\texttt{DataLoader}, you can decide whether it shuffles batches or not.
Keeping this consistent ensures the model sees the same data order each
run.

\subsubsection{Why small divergences happen
anyway}\label{why-small-divergences-happen-anyway}

Even with fixed seeds, you might notice that two runs are not perfectly
identical. On CPUs, differences often come from OpenMP parallel
loops-threads may sum numbers in a different order, producing slightly
different results. On GPUs, parallelism and library implementations
(like cuBLAS or cuDNN) can do the same.

These differences are usually very small, but deep learning systems are
chaotic: tiny changes in the early steps can grow into visible
differences later. This doesn't mean the code is wrong-it just means
floating-point math has limits.

\subsubsection{Why reproducibility
matters}\label{why-reproducibility-matters}

Reproducibility isn't just about peace of mind. It has real uses:

\begin{itemize}
\tightlist
\item
  Debugging: if a bug appears, you want to reproduce the exact same run
  to diagnose it.
\item
  Comparisons: when testing new optimizers, schedulers, or
  architectures, you want fair comparisons on identical conditions.
\item
  Science: reproducible results are essential for research papers and
  benchmarks.
\end{itemize}

At the same time, absolute bit-for-bit reproducibility is often
unrealistic in large parallel systems. Instead, the goal is practical
reproducibility: ensuring that runs are \emph{similar enough} to reach
the same conclusions.

\subsubsection{Example experiment}\label{example-experiment}

Suppose you seed training with \texttt{manual\_seed(1337)} and use the
same dataset. You might get a loss curve like this:

\begin{verbatim}
Run A: step 1000 → val loss 3.42  
Run B: step 1000 → val loss 3.43  
\end{verbatim}

The numbers are not identical, but they are close. The important part is
that the model's learning trajectory is stable and results are
comparable.

If you remove the seed and allow full randomness, you might get:

\begin{verbatim}
Run A: step 1000 → val loss 3.42  
Run B: step 1000 → val loss 3.89  
\end{verbatim}

Both are valid, but harder to compare.

\subsubsection{Try it yourself}\label{try-it-yourself-36}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run training twice without setting a seed. Compare how training loss
  and validation loss differ at step 500.
\item
  Set a fixed seed with \texttt{manual\_seed(42)} before building the
  model. Run training twice and compare again. You should see closer
  numbers.
\item
  Enable OpenMP with multiple threads and then run with a single thread.
  Notice how results differ slightly due to floating-point summation
  order.
\item
  Save two checkpoints from runs with different seeds. Use the model to
  generate text and compare outputs. You'll see different wording, but
  both grammatically plausible.
\item
  Increase the dataset size and check if differences between runs
  shrink. With more data, randomness matters less.
\end{enumerate}

\subsubsection{The takeaway}\label{the-takeaway-37}

Reproducibility in training is about controlling randomness where
possible and accepting small divergences where not. In \texttt{llm.c},
reproducibility is made clear through simple seeding functions and
deterministic dataloader options. Perfect bit-level reproducibility
isn't the point-the goal is to ensure results are stable, comparable,
and scientifically sound, even if tiny numerical differences creep in.

\subsection{49. Command-Line Flags and
Defaults}\label{command-line-flags-and-defaults}

When you run a training program, you often want to change certain
settings without editing the source code. For example, you might want to
try a different batch size, adjust the learning rate, or train for more
steps. Command-line flags make this possible. In \texttt{train\_gpt2.c},
defaults are set inside the program, but it can also be compiled to
accept arguments, giving you flexibility while keeping the code minimal.

\subsubsection{Why flags exist}\label{why-flags-exist}

Deep learning experiments are highly sensitive to hyperparameters-values
like learning rate, batch size, sequence length, or number of steps. If
every change required modifying source code, recompiling, and rerunning,
experimentation would be slow and error-prone. Flags allow you to
configure these parameters quickly at runtime.

In many large frameworks (like PyTorch or TensorFlow), command-line
arguments are parsed with helper libraries. In \texttt{llm.c}, the
philosophy is simplicity: flags are either defined in code as constants,
or you can extend \texttt{main} with standard C argument parsing to
override defaults.

\subsubsection{\texorpdfstring{Defaults in
\texttt{train\_gpt2.c}}{Defaults in train\_gpt2.c}}\label{defaults-in-train_gpt2.c}

Looking at the code, here are the main defaults hardcoded in the
\texttt{main} function:

\begin{itemize}
\item
  Batch size (\texttt{B}):

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{int}\NormalTok{ B }\OperatorTok{=} \DecValTok{4}\OperatorTok{;} \CommentTok{// number of sequences per batch}
\end{Highlighting}
\end{Shaded}
\item
  Sequence length (\texttt{T}):

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{int}\NormalTok{ T }\OperatorTok{=} \DecValTok{64}\OperatorTok{;} \CommentTok{// tokens per sequence}
\end{Highlighting}
\end{Shaded}
\item
  Validation batches:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{int}\NormalTok{ val\_num\_batches }\OperatorTok{=} \DecValTok{5}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}
\item
  Training steps:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ step }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ step }\OperatorTok{\textless{}=} \DecValTok{40}\OperatorTok{;}\NormalTok{ step}\OperatorTok{++)} \OperatorTok{\{}
\end{Highlighting}
\end{Shaded}

  By default, only 40 steps are run in this example.
\item
  Optimizer hyperparameters (inside \texttt{gpt2\_update} call):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gpt2\_update}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{,} \FloatTok{1e{-}4}\BuiltInTok{f}\OperatorTok{,} \FloatTok{0.9}\BuiltInTok{f}\OperatorTok{,} \FloatTok{0.999}\BuiltInTok{f}\OperatorTok{,} \FloatTok{1e{-}8}\BuiltInTok{f}\OperatorTok{,} \FloatTok{0.0}\BuiltInTok{f}\OperatorTok{,}\NormalTok{ step}\OperatorTok{+}\DecValTok{1}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

  Here the learning rate is \texttt{1e-4}, beta values for AdamW are
  \texttt{0.9} and \texttt{0.999}, epsilon is \texttt{1e-8}, and weight
  decay is \texttt{0.0}.
\end{itemize}

These defaults are chosen to make the reference training loop run
quickly and predictably, especially on small datasets like Tiny
Shakespeare or Tiny Stories.

\subsubsection{How to add flags}\label{how-to-add-flags}

If you want flexibility, you can extend \texttt{main} with argument
parsing:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{int}\NormalTok{ main}\OperatorTok{(}\DataTypeTok{int}\NormalTok{ argc}\OperatorTok{,} \DataTypeTok{char}\NormalTok{ argv}\OperatorTok{)} \OperatorTok{\{}
    \DataTypeTok{int}\NormalTok{ B }\OperatorTok{=} \DecValTok{4}\OperatorTok{;}
    \DataTypeTok{int}\NormalTok{ T }\OperatorTok{=} \DecValTok{64}\OperatorTok{;}
    \DataTypeTok{int}\NormalTok{ max\_steps }\OperatorTok{=} \DecValTok{40}\OperatorTok{;}
    \ControlFlowTok{if} \OperatorTok{(}\NormalTok{argc }\OperatorTok{\textgreater{}} \DecValTok{1}\OperatorTok{)}\NormalTok{ B }\OperatorTok{=}\NormalTok{ atoi}\OperatorTok{(}\NormalTok{argv}\OperatorTok{[}\DecValTok{1}\OperatorTok{]);}
    \ControlFlowTok{if} \OperatorTok{(}\NormalTok{argc }\OperatorTok{\textgreater{}} \DecValTok{2}\OperatorTok{)}\NormalTok{ T }\OperatorTok{=}\NormalTok{ atoi}\OperatorTok{(}\NormalTok{argv}\OperatorTok{[}\DecValTok{2}\OperatorTok{]);}
    \ControlFlowTok{if} \OperatorTok{(}\NormalTok{argc }\OperatorTok{\textgreater{}} \DecValTok{3}\OperatorTok{)}\NormalTok{ max\_steps }\OperatorTok{=}\NormalTok{ atoi}\OperatorTok{(}\NormalTok{argv}\OperatorTok{[}\DecValTok{3}\OperatorTok{]);}
    \OperatorTok{...}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

Now you can run:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{./train\_gpt2}\NormalTok{ 8 128 100}
\end{Highlighting}
\end{Shaded}

This sets batch size to 8, sequence length to 128, and steps to 100,
without changing source code.

\subsubsection{Why it matters}\label{why-it-matters-29}

Command-line flags make experimentation far more efficient. You can try
multiple configurations in one day without recompiling or editing the
file repeatedly. This is especially useful when running jobs on clusters
where you want scripts that launch many experiments automatically with
different parameters.

Defaults are equally important: they give you a safe, predictable
starting point. Beginners can run the code without thinking about flags,
while advanced users can override values as needed.

\subsubsection{Try it yourself}\label{try-it-yourself-37}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Keep the default batch size of 4 and sequence length of 64. Run
  training and note the time per step.
\item
  Change batch size to 8 by editing the code. Observe how training speed
  changes and how memory usage increases.
\item
  Modify the loop to train for 200 steps instead of 40. Watch how loss
  decreases further.
\item
  Add argument parsing to accept learning rate as a flag. Experiment
  with \texttt{1e-3} vs.~\texttt{1e-5} and see how quickly training
  diverges or stalls.
\item
  Create a shell script that runs training multiple times with different
  values for \texttt{B} and \texttt{T}. Compare results.
\end{enumerate}

\subsubsection{The takeaway}\label{the-takeaway-38}

Command-line flags and defaults balance simplicity with flexibility.
Defaults make the code runnable out of the box, while flags let you
scale experiments without constantly editing the source. In
\texttt{train\_gpt2.c}, this design keeps the training loop minimal but
still adaptable, encouraging both clarity and experimentation.

\subsection{50. Example CPU Training Logs and
Outputs}\label{example-cpu-training-logs-and-outputs}

One of the best ways to understand what a training loop is doing is by
reading its logs. Logs are the program's way of telling you how training
is progressing: what the loss is, how fast it's running, and whether
validation checks are improving. In \texttt{train\_gpt2.c}, logging is
deliberately minimal so you can easily see the essentials without being
overwhelmed.

\subsubsection{What the logs look like}\label{what-the-logs-look-like}

Here's a snippet of output from running the CPU training loop on Tiny
Shakespeare:

\begin{verbatim}
train dataset num_batches: 1192
val dataset num_batches: 128
[GPT-2]
max_seq_len: 1024
vocab_size: 50257
padded_vocab_size: 50304
num_layers: 12
num_heads: 12
channels: 768
num_parameters: 124475904
num_activations: 73347840
val loss 5.325529
step 0: train loss 4.677779 (took 1987.546000 ms)
step 1: train loss 5.191576 (took 1927.230000 ms)
step 2: train loss 4.438685 (took 1902.987000 ms)
...
\end{verbatim}

Each part of this output has meaning:

\begin{itemize}
\tightlist
\item
  Dataset sizes: how many training and validation batches are available.
\item
  Model config: confirmation that the GPT-2 model was loaded correctly
  (sequence length, vocab size, number of layers, etc.).
\item
  Validation loss: an average measure of how well the model is doing on
  unseen data.
\item
  Training step logs: for each step, you see the training loss and how
  long the step took in milliseconds.
\end{itemize}

\subsubsection{Understanding loss
values}\label{understanding-loss-values}

Loss is the number that tells us how far the model's predictions are
from the correct answers. Lower is better.

\begin{itemize}
\tightlist
\item
  A loss around 5.3 means the model is essentially guessing.
\item
  As training progresses, you want to see this number slowly decrease.
\item
  If the number gets stuck, or goes up, it can indicate problems with
  the learning rate, dataset, or implementation.
\end{itemize}

Think of it like a report card: at the beginning, the model is failing
every test, but as it practices (trains), the grades (loss values)
improve.

\subsubsection{Speed measurements}\label{speed-measurements}

The ``took \ldots{} ms'' part shows how long each step took. On CPU,
this is usually slow, sometimes a couple of seconds per step. On GPU,
the same step might only take tens of milliseconds.

Timing logs are useful because they help you:

\begin{itemize}
\tightlist
\item
  Estimate how long full training will take.
\item
  Compare performance between machines.
\item
  Spot problems if training suddenly slows down.
\end{itemize}

\subsubsection{Occasional validation
checks}\label{occasional-validation-checks}

Every few steps, the code switches to validation data and prints a
\texttt{val\ loss}. This is crucial: training loss always goes down if
the model memorizes the training set, but validation loss tells you if
it is \emph{actually learning patterns} that generalize.

If training loss goes down but validation loss stays high, that's a sign
of overfitting.

\subsubsection{Generated samples}\label{generated-samples}

At certain steps, the code also prints generated text like this:

\begin{verbatim}
generating:

 The King had not
 that the Duke of Northumberland and the Duke of
...
\end{verbatim}

Even though the text might look strange at first, it's a powerful sign
that the model is learning. At the beginning, output is pure gibberish,
but as training continues, you start to see recognizable words and
patterns.

\subsubsection{Why it matters}\label{why-it-matters-30}

Logs are your window into the training process. Without them, training
would be a black box-you'd wait hours and have no idea if it was
working. By watching loss curves, step times, and sample outputs, you
can make informed adjustments and gain confidence that the model is on
the right track.

\subsubsection{Try it yourself}\label{try-it-yourself-38}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run the training loop as-is and save the console output. Mark how loss
  changes between step 0 and step 40.
\item
  Increase the number of steps to 200 and compare how the losses evolve.
\item
  Change the batch size from 4 to 8 and note both the training speed and
  the loss behavior.
\item
  Edit the code to print validation loss every step instead of every 10
  steps. Does the trend look smoother?
\item
  Save the generated samples at steps 20 and 40. Compare how the quality
  changes.
\end{enumerate}

\subsubsection{The takeaway}\label{the-takeaway-39}

Training logs are like a diary of the model's progress. They show you
how quickly the model is learning, how well it generalizes, and how fast
the computation runs. By reading and interpreting logs carefully, you
can guide experiments, detect problems early, and appreciate the
progress that's happening inside the model.

\section{Chapter 6. Testing and
Profiling}\label{chapter-6.-testing-and-profiling}

\subsection{51. Debug State Structs and Their
Role}\label{debug-state-structs-and-their-role}

When building and training a model as complex as GPT-2, you need ways to
peek inside and check whether the values being passed around make sense.
This is where debug state structs come in. In \emph{llm.c}, the code is
written in plain C, without the rich debugging utilities of frameworks
like PyTorch. That means the developers had to create their own
mechanism to store, inspect, and compare intermediate values.

A struct in C is just a container that groups related variables
together. For debugging, you can think of a struct as a little notebook
where the program writes down numbers as it computes them. These numbers
might include:

\begin{itemize}
\tightlist
\item
  The raw embeddings for tokens.
\item
  The attention scores before and after softmax.
\item
  The outputs of each MLP block.
\item
  The predicted probabilities for the next token.
\end{itemize}

By saving these into a structured format, the program can later compare
them to the outputs of a trusted reference implementation (usually
PyTorch).

\subsubsection{How it works in
practice}\label{how-it-works-in-practice-1}

Inside \emph{llm.c}, there are places where arrays of floats---like
hidden states or logits---are copied into a debug state struct. Once
stored, these values can be printed, dumped to a file, or checked
against ``golden'' results from PyTorch.

Imagine you're testing a tiny batch of input tokens. The forward pass
runs as usual, but at specific checkpoints (say, right after attention
or after the final linear projection), the program writes those arrays
into a struct. Later, when running a PyTorch model with the same inputs
and weights, the two outputs can be compared element by element.

This is essential for catching subtle errors:

\begin{itemize}
\tightlist
\item
  A misplaced transpose in matrix multiplication.
\item
  Forgetting to apply a mask in attention.
\item
  A floating-point precision mismatch in softmax.
\end{itemize}

Without the struct, you'd only know the model loss looks ``off.'' With
the struct, you know \emph{exactly} which step went wrong.

\subsubsection{Why it matters}\label{why-it-matters-31}

Debug structs bridge the gap between C and Python ecosystems. PyTorch
has a decade of battle-tested layers, so it's the gold standard for
correctness. By saving intermediate activations in C and comparing them
against PyTorch, developers ensure that every layer behaves identically.
This builds confidence that the \emph{llm.c} codebase isn't just
``roughly correct,'' but precisely reproduces GPT-2's math.

For anyone modifying the code---for example, writing a new activation
function or experimenting with quantization---the debug structs act like
a safety net. You can quickly see if your change accidentally altered
the outputs in a way that breaks parity with the original model.

\subsubsection{Try it yourself}\label{try-it-yourself-39}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Trace a forward pass: Run the CPU version with a tiny batch and enable
  debug dumps. Look at the embeddings, attention scores, and final
  logits.
\item
  Cross-check with PyTorch: Run the same input through Hugging Face
  GPT-2. Print the same tensors. Compare a few entries by hand---do they
  match closely?
\item
  Introduce a bug: Change the scaling factor in attention (e.g., remove
  the \texttt{1/√d} term). Run again and see how quickly the mismatch
  shows up in the debug struct.
\item
  Extend the struct: Add a new field for an intermediate step you care
  about, like LayerNorm outputs. Print it during debugging to see how
  normalization changes the activations.
\end{enumerate}

\subsubsection{The takeaway}\label{the-takeaway-40}

Debug state structs are the microscope of \emph{llm.c}. They allow you
to pause the flow of numbers, record them, and compare them against a
known-good model. Without them, development would feel like working
blindfolded. With them, you can track down errors precisely, ensure
parity with PyTorch, and confidently extend the system knowing you have
a reliable safety net.

\subsection{\texorpdfstring{52. \texttt{test\_gpt2.c}: CPU vs
PyTorch}{52. test\_gpt2.c: CPU vs PyTorch}}\label{test_gpt2.c-cpu-vs-pytorch}

Testing is one of the most important parts of the \emph{llm.c} project.
The file \texttt{test\_gpt2.c} exists specifically to check whether the
C implementation of GPT-2 produces the same outputs as PyTorch, which is
the trusted reference. Without this file, you would only know if the
final training loss looked reasonable. With it, you can verify that
every part of the forward pass matches.

At its core, \texttt{test\_gpt2.c} runs a very controlled experiment: it
loads a GPT-2 model checkpoint (exported from PyTorch), prepares a small
batch of input tokens, executes a forward pass in C, and compares the
outputs against the corresponding tensors from PyTorch. If everything
matches within a tight numerical tolerance, you know the C code is
correct. If not, you have a clear signal that something is wrong.

\subsubsection{How the test works}\label{how-the-test-works}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Load the checkpoint The test begins by reading a binary checkpoint
  file, such as \texttt{gpt2\_124M.bin}, which contains the weights of
  the GPT-2 model. These weights were originally trained in PyTorch and
  then exported into a binary format that \emph{llm.c} can understand.
\item
  Prepare the inputs The test uses a known sequence of token
  IDs---sometimes from a dataset like Tiny Shakespeare, sometimes just a
  few hand-picked tokens. This ensures the same inputs can be run
  through both PyTorch and C implementations.
\item
  Run the C forward pass The function \texttt{gpt2\_forward} is executed
  on the CPU. All embeddings, attention layers, MLPs, and final logits
  are computed exactly as they would be during real inference or
  training.
\item
  Compare with PyTorch For each major tensor (e.g., hidden states,
  attention outputs, final logits), the values are compared against
  saved outputs from PyTorch. The comparison usually allows for very
  small differences, since floating-point math can vary slightly between
  libraries. A tolerance like \texttt{1e-5} is common.
\item
  Report mismatches If any element deviates beyond the allowed
  tolerance, the test reports the difference. Developers can then
  investigate where the divergence started, often by adding more debug
  dumps of intermediate states.
\end{enumerate}

\subsubsection{Why this test is crucial}\label{why-this-test-is-crucial}

C code is low-level and unforgiving. A single indexing mistake, a wrong
stride, or a missing scaling factor in attention can make outputs
diverge wildly. Since GPT-2 has millions of parameters, such errors are
almost impossible to spot by hand. By tying the implementation back to
PyTorch, \texttt{test\_gpt2.c} provides a ground truth check.

This also ensures scientific reproducibility. If someone else downloads
\emph{llm.c} and runs \texttt{test\_gpt2.c}, they should see the same
level of agreement with PyTorch. That way, they can trust that training
runs, losses, and model outputs are not artifacts of a broken
implementation.

\subsubsection{Example in action}\label{example-in-action}

Imagine you've just modified the attention code to optimize the matrix
multiplication. You recompile and run \texttt{test\_gpt2.c}. If you see
an error like:

\begin{verbatim}
Mismatch at position [0, 12, 42]: C=0.1234, Torch=0.1235
\end{verbatim}

you know the two match within tolerance---everything is fine. But if you
see:

\begin{verbatim}
Mismatch at position [0, 12, 42]: C=0.3456, Torch=0.1235
\end{verbatim}

that's a red flag. It means the optimization introduced a bug. Without
the test, you might not notice until much later when training fails to
converge.

\subsubsection{Why it matters}\label{why-it-matters-32}

\texttt{test\_gpt2.c} is the guarantee that the C implementation is not
just ``close enough,'' but \emph{faithful}. It ensures that
improvements, optimizations, or even rewrites don't silently corrupt the
model's behavior. It's a direct bridge between the experimental world of
C internals and the well-established baseline of PyTorch.

\subsubsection{Try it yourself}\label{try-it-yourself-40}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run \texttt{make\ test\_gpt2} in the repository. Observe whether the
  outputs match PyTorch.
\item
  Deliberately change one line of code in \texttt{gpt2\_forward}---for
  example, remove the attention scaling factor. Run the test again and
  see how quickly it fails.
\item
  Add your own print statements to show which tensors are being
  compared. Watch how the numbers line up almost exactly.
\item
  Try running with different checkpoints (e.g., 124M vs 355M) to see if
  parity holds across scales.
\end{enumerate}

\subsubsection{The takeaway}\label{the-takeaway-41}

\texttt{test\_gpt2.c} is not just another file in the repository---it's
the truth meter. It reassures you that the complicated layers of GPT-2
have been implemented correctly in C and remain consistent with PyTorch.
This confidence is what allows further work---whether training,
profiling, or extending the model---to proceed on a solid foundation.

\subsection{53. Matching Outputs Within
Tolerances}\label{matching-outputs-within-tolerances}

Once you have a test like \texttt{test\_gpt2.c} set up, the next
challenge is figuring out how close the outputs need to be for the test
to pass. Computers don't always produce bit-for-bit identical results
when doing floating-point math. The order of operations, the precision
of instructions, and even the type of hardware (CPU vs GPU) can cause
tiny differences.

If you demanded exact matches, most tests would fail even when the
implementation is correct. That's why \emph{llm.c} uses tolerance-based
comparison. Instead of asking ``are these numbers exactly equal?'', the
code asks ``are these numbers \emph{close enough}?''

\subsubsection{Absolute vs relative
tolerance}\label{absolute-vs-relative-tolerance}

There are two common ways to define ``close enough'':

\begin{itemize}
\item
  Absolute tolerance: check that the difference between two numbers is
  smaller than a threshold. For example,

\begin{verbatim}
|0.123456 - 0.123455| = 0.000001 < 1e-5
\end{verbatim}

  This works well for values near zero.
\item
  Relative tolerance: check that the difference is small relative to the
  size of the numbers. For example,

\begin{verbatim}
|1000.0 - 1000.1| / 1000.0 = 0.0001
\end{verbatim}

  Even though the absolute difference is 0.1, that's tiny compared to
  the scale of 1000.
\end{itemize}

In practice, the code often combines both. It passes if the difference
is smaller than either the absolute tolerance or the relative tolerance.

\subsubsection{Why tolerances are
necessary}\label{why-tolerances-are-necessary}

Imagine you run the forward pass on CPU in C and in PyTorch. PyTorch
might use fused kernels or higher-precision accumulations. If you
compare the final logits, you may see values like:

\begin{itemize}
\tightlist
\item
  PyTorch: \texttt{-3.4521234}
\item
  C: \texttt{-3.4521255}
\end{itemize}

The difference is just \texttt{0.0000021}. For practical purposes,
they're the same. Without tolerance, this tiny difference would fail the
test. With tolerance, you can safely say both implementations agree.

\subsubsection{Example from debugging}\label{example-from-debugging}

Suppose you compare the probabilities after softmax. You might get:

\begin{itemize}
\tightlist
\item
  PyTorch: \texttt{0.3333333,\ 0.3333333,\ 0.3333333}
\item
  C: \texttt{0.3333334,\ 0.3333333,\ 0.3333333}
\end{itemize}

Here, the first value differs in the last decimal place. The tolerance
rule says that's fine, since the absolute error is smaller than
\texttt{1e-7}.

But if you saw something like:

\begin{itemize}
\tightlist
\item
  PyTorch: \texttt{0.3333333,\ 0.3333333,\ 0.3333333}
\item
  C: \texttt{0.5000000,\ 0.2500000,\ 0.2500000}
\end{itemize}

the mismatch is huge---no tolerance rule would allow it. That's a clear
bug.

\subsubsection{Why it matters}\label{why-it-matters-33}

Matching within tolerance isn't just a technical detail; it's about
trust. It lets you say with confidence that the implementation is
mathematically faithful to the reference. You don't waste time chasing
harmless decimal noise, but you also don't miss real mistakes.

This approach is also what makes cross-platform development possible.
The same \emph{llm.c} code can be run on Linux, macOS, or even inside
different compilers, and as long as results fall within tolerance, you
know the model behavior is preserved.

\subsubsection{Try it yourself}\label{try-it-yourself-41}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run \texttt{test\_gpt2.c} and look at the output logs. Notice how many
  decimal places match.
\item
  Change the tolerance threshold in the code from \texttt{1e-5} to
  something stricter like \texttt{1e-8}. See if the test starts failing
  due to harmless floating-point noise.
\item
  Add a deliberate bug---for example, skip dividing by \texttt{√d} in
  the attention code---and rerun. The mismatches will be far larger than
  the tolerance, proving the bug is real.
\item
  Compare CPU results with PyTorch, then recompile with different
  compiler flags (like \texttt{-O0} vs \texttt{-O3}) and check if
  results still fall within tolerance.
\end{enumerate}

\subsubsection{The takeaway}\label{the-takeaway-42}

Tolerance-based testing is what allows \emph{llm.c} to be both rigorous
and realistic. It ensures that differences are only flagged when they
matter, while ignoring the harmless quirks of floating-point math. This
makes the test suite a reliable tool for catching true errors without
overwhelming you with false alarms.

\subsection{\texorpdfstring{54. Profiling with
\texttt{profile\_gpt2.c}}{54. Profiling with profile\_gpt2.c}}\label{profiling-with-profile_gpt2.c}

After verifying that the outputs match PyTorch, the next big question
is: how fast is the code running? Correctness is essential, but
performance is what makes a minimal C implementation like \emph{llm.c}
worthwhile. That's where \texttt{profile\_gpt2.c} comes in. It is a
small program that runs controlled forward passes through GPT-2 and
measures the time they take, helping you understand where the
bottlenecks are.

\subsubsection{What profiling means}\label{what-profiling-means}

Profiling is the act of measuring the performance of a program, not just
its correctness. Instead of asking ``does this number match PyTorch?'',
profiling asks:

\begin{itemize}
\tightlist
\item
  How many milliseconds does one forward pass take?
\item
  Which part of the model consumes the most time?
\item
  Does using OpenMP threads actually speed things up?
\item
  How does batch size affect runtime?
\end{itemize}

By answering these, you can make informed decisions about optimization.

\subsubsection{\texorpdfstring{How \texttt{profile\_gpt2.c}
works}{How profile\_gpt2.c works}}\label{how-profile_gpt2.c-works}

The profiling program is structured like a simplified inference loop.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Model setup It loads a GPT-2 checkpoint (e.g.,
  \texttt{gpt2\_124M.bin}) into memory and allocates space for
  activations.
\item
  Dummy input Instead of using real text, it creates random token IDs.
  That way, the cost measured comes purely from the computation, not
  from data loading or tokenization.
\item
  Timing with clock functions Before and after each forward pass, it
  records timestamps with
  \texttt{clock\_gettime(CLOCK\_MONOTONIC,\ \&start)} and
  \texttt{\&end}. The difference gives the runtime in seconds, which is
  usually converted into milliseconds.
\item
  Looping for stability A single run can be noisy due to background
  processes on your computer. To smooth things out,
  \texttt{profile\_gpt2.c} runs the forward pass multiple times and
  averages the results.
\item
  Reporting results Finally, it prints the average time per forward
  pass. Sometimes it also estimates FLOPs (floating-point operations per
  second), giving you a rough idea of efficiency compared to the
  theoretical peak of your CPU.
\end{enumerate}

\subsubsection{What you can learn from
profiling}\label{what-you-can-learn-from-profiling}

Running \texttt{profile\_gpt2.c} on a CPU gives insights like:

\begin{itemize}
\tightlist
\item
  The attention blocks dominate runtime, because they involve large
  matrix multiplications.
\item
  Increasing batch size makes the runtime longer, but not
  proportionally---sometimes bigger batches use hardware more
  efficiently.
\item
  OpenMP can speed things up when there are multiple CPU cores
  available, but scaling may flatten out after a certain number of
  threads.
\end{itemize}

This helps decide where to spend effort. For example, if LayerNorm takes
2\% of the runtime but attention takes 70\%, you know optimization
should focus on the attention code.

\subsubsection{Why it matters}\label{why-it-matters-34}

Profiling isn't just about numbers. It's about guiding your development
choices. Without profiling, you might spend weeks hand-optimizing
LayerNorm, only to discover it barely affects overall runtime. With
profiling, you see immediately where the slowdowns are and can focus on
the real bottlenecks.

It also provides baseline performance numbers. If you change something
in the implementation, re-running \texttt{profile\_gpt2.c} will tell you
if it sped things up or slowed them down. This feedback loop is
essential for optimization work.

\subsubsection{Try it yourself}\label{try-it-yourself-42}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compile and run \texttt{profile\_gpt2.c} with a small model
  checkpoint. Note the reported runtime per forward pass.
\item
  Change the batch size \texttt{B} and sequence length \texttt{T}, then
  re-run. Watch how runtime scales with larger inputs.
\item
  Set \texttt{OMP\_NUM\_THREADS=1} to disable threading and compare it
  against \texttt{OMP\_NUM\_THREADS=4} or higher. How much faster is it
  with multiple cores?
\item
  Modify the code to time individual layers (embeddings, attention,
  MLP). This gives even more precise insight into which parts dominate
  computation.
\end{enumerate}

\subsubsection{The takeaway}\label{the-takeaway-43}

\texttt{profile\_gpt2.c} turns performance from a guess into hard data.
It tells you exactly how long a forward pass takes, how much threading
helps, and where the real bottlenecks are. With it, you can track
progress as you optimize the code, ensuring that your changes make the
model not only correct but also efficient.

\subsection{55. Measuring FLOPs and CPU
Performance}\label{measuring-flops-and-cpu-performance}

When talking about performance in deep learning, it's not enough to say
``this run took 200 milliseconds.'' To really understand efficiency, we
need a measure that's independent of hardware and input size. That's
where FLOPs come in: floating-point operations. A FLOP is a single
numerical calculation involving real numbers---like an addition,
multiplication, or division.

By counting how many FLOPs a model requires and comparing it to how many
the computer can perform per second, you can evaluate how close your
implementation is to the theoretical maximum speed of your CPU.

\subsubsection{What FLOPs mean in
practice}\label{what-flops-mean-in-practice}

Every layer of GPT-2---embeddings, attention, feed-forward---can be
broken down into a sequence of multiplications and additions. For
example:

\begin{itemize}
\tightlist
\item
  A matrix multiplication between two matrices of size \texttt{M\ ×\ K}
  and \texttt{K\ ×\ N} requires \texttt{2\ ×\ M\ ×\ K\ ×\ N} FLOPs.
\item
  A softmax across a vector of size \texttt{d} requires about
  \texttt{3d} FLOPs (exponentials, sums, and divisions).
\item
  An MLP block with hidden size \texttt{h} and intermediate size
  \texttt{4h} requires multiple matrix multiplications, adding up to
  billions of FLOPs per training step.
\end{itemize}

When you sum these across all layers, even a small GPT-2 model like 124M
parameters involves several gigaFLOPs (billions of operations) for one
forward pass.

\subsubsection{\texorpdfstring{How \texttt{profile\_gpt2.c} estimates
FLOPs}{How profile\_gpt2.c estimates FLOPs}}\label{how-profile_gpt2.c-estimates-flops}

The profiling code doesn't literally count every multiplication.
Instead, it uses formulas derived from matrix dimensions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The configuration struct (\texttt{model.config}) gives the number of
  layers, heads, embedding size, and sequence length.
\item
  For each block (attention + MLP), the code applies standard FLOPs
  formulas for matrix multiplication and softmax.
\item
  These counts are added up to estimate the total FLOPs for a forward
  pass.
\item
  Dividing the total FLOPs by the measured runtime (in seconds) gives
  FLOPs/second, also known as throughput.
\end{enumerate}

For example, if a forward pass takes 0.1 seconds and involves 20 billion
FLOPs, then throughput is about 200 GFLOPs/s.

\subsubsection{Why this is useful}\label{why-this-is-useful}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare hardware: You can test the same model on a laptop CPU and a
  server CPU, then compare FLOPs/s to see how much faster the server is.
\item
  Compare implementations: If you modify attention to use a different
  algorithm, the FLOPs count won't change, but if runtime decreases,
  throughput increases---showing the optimization worked.
\item
  Know your limits: CPUs often achieve only a fraction of their
  theoretical peak FLOPs due to memory bottlenecks. Profiling shows how
  close you're getting in practice.
\end{enumerate}

\subsubsection{Example}\label{example}

Let's say:

\begin{itemize}
\tightlist
\item
  Forward pass FLOPs: 15 billion
\item
  Runtime: 0.2 seconds
\item
  Throughput: 75 GFLOPs/s
\end{itemize}

If your CPU's datasheet says the peak is 200 GFLOPs/s, then you're at
about 37\% efficiency. That gap might be due to memory latency, cache
misses, or lack of vectorization.

\subsubsection{Try it yourself}\label{try-it-yourself-43}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run \texttt{profile\_gpt2.c} and note the reported FLOPs and runtime.
\item
  Change the sequence length \texttt{T} and observe how FLOPs scale
  linearly with it.
\item
  Increase the number of layers in the model configuration---watch FLOPs
  rise accordingly.
\item
  Compare your measured FLOPs/s with the theoretical maximum listed for
  your CPU. How close are you?
\end{enumerate}

\subsubsection{The takeaway}\label{the-takeaway-44}

FLOPs turn performance from ``this feels fast'' into hard numbers you
can compare across runs, machines, and implementations. By knowing both
the operation count and the achieved throughput, you gain a clear
picture of how efficient the \emph{llm.c} code really is and where
further optimizations might pay off.

\subsection{56. Capturing Memory Usage on
CPU}\label{capturing-memory-usage-on-cpu}

While FLOPs tell us how much raw computation a model needs, performance
isn't just about speed---it's also about memory usage. Modern language
models are enormous, and on CPUs with limited RAM, memory can become the
true bottleneck. That's why \emph{llm.c} also emphasizes monitoring how
much memory is used during inference and training.

\subsubsection{What consumes memory in
GPT-2}\label{what-consumes-memory-in-gpt-2}

There are several key components that take up space:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Parameters (weights): GPT-2 124M has about 124 million parameters.
  Each is stored as a 32-bit float (4 bytes). That alone is roughly 500
  MB.
\item
  Gradients: During training, gradients for each parameter are stored.
  That doubles the memory usage.
\item
  Optimizer states: AdamW requires two additional memory slots per
  parameter (\texttt{m} and \texttt{v}), which doubles it again. With
  parameters, gradients, and optimizer states combined, training can
  require 4× the parameter size in memory.
\item
  Activations: These are the intermediate outputs of each layer
  (attention scores, MLP results, normalized states). For
  backpropagation, activations from the forward pass must be kept until
  gradients are computed. Depending on batch size and sequence length,
  activations can rival parameter memory.
\end{enumerate}

\subsubsection{Measuring memory in
practice}\label{measuring-memory-in-practice}

On CPU, memory usage can be inspected in several ways:

\begin{itemize}
\tightlist
\item
  Operating system tools: \texttt{top}, \texttt{htop}, or Activity
  Monitor show total memory used by the program.
\item
  Manual accounting in code: \emph{llm.c} knows how many parameters,
  gradients, and optimizer states exist. By multiplying their counts by
  4 bytes, it can estimate usage precisely.
\item
  Instrumentation during profiling: you can add checkpoints that print
  memory usage at different stages of the forward or backward pass.
\end{itemize}

For example:

\begin{itemize}
\tightlist
\item
  Parameters only: \textasciitilde500 MB.
\item
  Parameters + gradients: \textasciitilde1 GB.
\item
  Parameters + gradients + optimizer: \textasciitilde2 GB.
\item
  Adding activations: 2.5--3 GB, depending on batch size and sequence
  length.
\end{itemize}

\subsubsection{Why memory matters on
CPU}\label{why-memory-matters-on-cpu}

On a CPU, you don't just care about ``can it fit in RAM?'' You also care
about cache efficiency. Modern CPUs have multiple levels of cache (L1,
L2, L3), which are much faster than main RAM. If activations or weights
don't fit well in cache, performance can suffer even if you technically
have enough RAM.

Memory footprint also limits experiment flexibility. For example,
increasing sequence length from 64 to 1024 multiplies activation storage
by 16. A run that fits at \texttt{T=64} may crash or swap at
\texttt{T=1024}.

\subsubsection{Example scenario}\label{example-scenario}

Suppose you run GPT-2 124M with:

\begin{itemize}
\tightlist
\item
  Batch size \texttt{B=4}
\item
  Sequence length \texttt{T=64}
\end{itemize}

This might use \textasciitilde2.5 GB of memory for training. If you
raise \texttt{T} to 512, suddenly activations balloon, and total usage
may exceed 10 GB. On a laptop with 8 GB RAM, this simply won't work.

By monitoring memory carefully, you can avoid mysterious crashes and
plan runs realistically.

\subsubsection{Try it yourself}\label{try-it-yourself-44}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run training with a small batch (\texttt{B=2}, \texttt{T=64}) and
  check memory usage with \texttt{htop}.
\item
  Increase \texttt{T} step by step (128, 256, 512) and record the
  growth. Watch how activations dominate beyond a certain length.
\item
  Calculate parameter memory manually:
  \texttt{num\_params\ ×\ 4\ bytes}. Compare it to what the OS reports.
  The difference comes from activations and optimizer states.
\item
  Modify the code to print memory allocations explicitly when arrays are
  created. This gives an internal log of usage at each step.
\end{enumerate}

\subsubsection{The takeaway}\label{the-takeaway-45}

Memory is the silent partner of FLOPs: you need both to train and run
models efficiently. Profiling without tracking memory is
incomplete---you might have a model that's fast but impossible to run on
your machine. By capturing and understanding memory usage, you gain the
ability to scale responsibly, balance batch size and sequence length,
and keep your experiments stable.

\subsection{57. Reproducing Known Loss Curves
(CPU-only)}\label{reproducing-known-loss-curves-cpu-only}

Once the model is correct and its performance is measured, the next
important step is to check whether it learns in the way we expect. In
deep learning, we usually monitor this with a loss curve---a graph that
shows how the training loss decreases over time as the model sees more
data.

For GPT-2 and other language models, the standard loss function is
cross-entropy, which measures how well the predicted probability
distribution over tokens matches the actual next token in the dataset.
If the implementation is right, the loss should fall in a predictable
way when trained on text like Tiny Shakespeare or Tiny Stories.

\subsubsection{What a ``known'' loss curve looks
like}\label{what-a-known-loss-curve-looks-like}

The community has already run countless GPT-2 experiments in PyTorch, so
we know roughly what the curve should look like:

\begin{itemize}
\tightlist
\item
  At the very beginning, the loss is high (around 5--6) because the
  model is basically guessing.
\item
  After a few hundred steps, the loss starts dropping steadily.
\item
  For Tiny Shakespeare, loss often goes down toward \textasciitilde2.0
  with a small GPT-2 model (124M parameters).
\item
  The exact numbers can vary, but the shape---a downward trend with
  small fluctuations---is consistent.
\end{itemize}

If the C implementation produces a similar curve, that's a strong sign
the forward pass, backward pass, and optimizer are all working
correctly.

\subsubsection{How reproduction is tested in
practice}\label{how-reproduction-is-tested-in-practice}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Train with a small dataset The test usually uses Tiny Shakespeare or
  Tiny Stories since they are small enough to run quickly on CPU.
\item
  Log the loss per step Each training step prints something like:

\begin{verbatim}
step 0: train loss 4.87
step 10: train loss 4.12
step 20: train loss 3.75
...
\end{verbatim}
\item
  Plot the curve Save the loss values and make a simple plot with step
  on the x-axis and loss on the y-axis.
\item
  Compare against PyTorch If you train the same model in PyTorch with
  the same hyperparameters, the loss curve should look almost identical.
  Small differences are normal due to random seeds or floating-point
  math.
\end{enumerate}

\subsubsection{Why this is important}\label{why-this-is-important}

Reproducing a known loss curve is more than just a sanity check. It
tells you:

\begin{itemize}
\tightlist
\item
  The math is right: gradients, optimizer updates, and scheduler logic
  are functioning.
\item
  The data pipeline is correct: tokens are being fed in properly and
  batches are consistent.
\item
  Nothing is silently broken: without this, a bug might go unnoticed
  until much later in training.
\end{itemize}

This is especially important on CPU, because training is slower and you
may only run a few hundred steps. If the curve starts to dip in the
expected way, you know you're on the right track.

\subsubsection{Example scenario}\label{example-scenario-1}

Suppose you train GPT-2 124M with \texttt{B=4} and \texttt{T=64} on Tiny
Shakespeare. The loss starts around 4.9, and by step 200 it falls to
around 3.2. If PyTorch shows a similar trajectory, then your
implementation is validated.

But if your loss stays flat, say at 5.0 for hundreds of steps, that's a
red flag. It could mean gradients are not flowing, the optimizer isn't
updating weights, or your data loader is feeding the same batch
repeatedly.

\subsubsection{Try it yourself}\label{try-it-yourself-45}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train for 200 steps on Tiny Shakespeare with the C code and save the
  printed losses.
\item
  Train the same setup in PyTorch. Plot both curves together and
  compare.
\item
  Intentionally break something---for example, comment out the optimizer
  update step---and observe how the loss no longer decreases.
\item
  Experiment with learning rates. Too high may cause the curve to bounce
  up and down, while too low will make it drop very slowly.
\end{enumerate}

\subsubsection{The takeaway}\label{the-takeaway-46}

Reproducing known loss curves is the ultimate integration test. It
proves that the entire pipeline---data, model, training loop,
optimizer---works together in harmony. When your loss curve matches the
reference, you can trust that your C implementation of GPT-2 is not only
correct in theory but also effective in practice.

\subsection{58. Debugging Numerical Stability (NaNs,
Infs)}\label{debugging-numerical-stability-nans-infs}

Even if the model produces correct outputs most of the time, there's a
hidden danger in deep learning: numerical instability. This happens when
floating-point numbers inside the computation blow up to infinity
(\texttt{Inf}) or collapse into ``not a number'' (\texttt{NaN}). When
this occurs, training usually grinds to a halt---loss becomes undefined,
gradients explode, and parameters no longer update meaningfully.

\subsubsection{Why NaNs and Infs happen}\label{why-nans-and-infs-happen}

Neural networks involve many multiplications, exponentials, and
divisions. On paper, all of these are fine. But computers store numbers
with limited precision (32-bit floats in this case). When values get too
large or too small, they can no longer be represented correctly.

Common sources include:

\begin{itemize}
\tightlist
\item
  Softmax overflow: computing \texttt{exp(x)} on large positive numbers
  leads to \texttt{Inf}.
\item
  Division by very small numbers: for example, dividing by
  \texttt{sqrt(v\_hat)\ +\ eps} in AdamW can produce instability if
  \texttt{eps} is too small.
\item
  Exploding gradients: during backpropagation, errors compound across
  many layers, producing extremely large values.
\item
  Improper initialization or learning rates: weights that are too large
  or step sizes that are too aggressive can push activations outside a
  stable range.
\end{itemize}

\subsubsection{\texorpdfstring{How to detect instability in
\emph{llm.c}}{How to detect instability in llm.c}}\label{how-to-detect-instability-in-llm.c}

Because the code is written in C without automatic checks, NaNs and Infs
can spread silently unless you look for them. Some useful strategies
include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Assertions: insert
  \texttt{assert(!isnan(value)\ \&\&\ !isinf(value));} inside loops to
  catch bad values immediately.
\item
  Debug prints: log sample values from activations or gradients each
  step to see if they drift toward extremely large numbers.
\item
  Check the loss: if the loss suddenly becomes \texttt{nan} or
  \texttt{inf}, that's a strong signal something went wrong upstream.
\item
  Small runs: testing on tiny sequences and batches makes it easier to
  inspect values directly.
\end{enumerate}

\subsubsection{How to fix instability}\label{how-to-fix-instability}

Several practical techniques help keep numbers stable:

\begin{itemize}
\tightlist
\item
  Add an epsilon: in divisions or square roots, add a small constant
  (like \texttt{1e-8}) to prevent division by zero.
\item
  Rescale before softmax: subtract the maximum value in the vector
  before computing exponentials. This keeps values in a safe range.
\item
  Gradient clipping: cap gradients so they cannot exceed a certain norm.
  This stops runaway updates.
\item
  Adjust learning rate: if training diverges, lowering the learning rate
  often restores stability.
\item
  Check data: corrupted inputs or unexpected tokens can inject extreme
  values into the model.
\end{itemize}

\subsubsection{Example scenario}\label{example-scenario-2}

Suppose you're training GPT-2 on Tiny Shakespeare. The first few steps
look fine:

\begin{verbatim}
step 0: train loss 4.95
step 1: train loss 4.72
step 2: train loss nan
\end{verbatim}

This sudden jump to \texttt{nan} suggests instability. Checking the
gradients reveals extremely large values in the attention weights. The
fix might be lowering the learning rate from \texttt{1e-4} to
\texttt{5e-5} or enabling gradient clipping.

\subsubsection{Try it yourself}\label{try-it-yourself-46}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train with a very high learning rate (\texttt{1e-2}) and watch how
  quickly NaNs appear.
\item
  Add a debug check inside \texttt{gpt2\_forward} that prints when any
  activation exceeds \texttt{1e6}. Run a few steps and observe if values
  explode.
\item
  Modify the softmax code to omit subtracting the max. Compare stability
  before and after.
\item
  Add a gradient clipping routine and measure whether it prevents loss
  from diverging.
\end{enumerate}

\subsubsection{The takeaway}\label{the-takeaway-47}

Numerical stability is the difference between a model that trains
smoothly and one that collapses after a few steps. By anticipating where
NaNs and Infs can arise, adding checks, and applying stabilizing tricks,
you make \emph{llm.c} robust. This ensures that experiments are reliable
and that debugging focuses on real algorithmic issues rather than
avoidable numerical traps.

\subsection{59. From Unit Test to Full Training
Readiness}\label{from-unit-test-to-full-training-readiness}

Unit tests are the first line of defense: they check whether small,
isolated parts of the code---like embeddings, attention, or
softmax---produce the correct outputs. But passing unit tests isn't the
same as being ready for full training. The transition from ``this layer
works'' to ``the whole system learns correctly over thousands of steps''
involves additional challenges.

\subsubsection{The gap between unit tests and
training}\label{the-gap-between-unit-tests-and-training}

\begin{itemize}
\tightlist
\item
  Unit tests check correctness: for example, verifying that
  \texttt{gpt2\_forward} produces the same logits as PyTorch on a single
  batch.
\item
  Training readiness checks robustness: making sure the model can run
  repeatedly for thousands of steps without crashing, diverging, or
  leaking memory.
\end{itemize}

Think of it like testing a car. Unit tests are like checking the brakes,
headlights, and steering individually. Training readiness is taking the
car on a 500-mile road trip and making sure nothing overheats, rattles
loose, or fails under stress.

\subsubsection{What needs to be validated for training
readiness}\label{what-needs-to-be-validated-for-training-readiness}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Loss curve behavior Run the training loop for several hundred steps.
  The training loss should steadily decrease, matching known curves from
  PyTorch. If it stagnates or spikes, something is wrong in gradients or
  the optimizer.
\item
  Validation runs Regularly measure validation loss during training. If
  it decreases at first and then stabilizes, that shows the model is
  generalizing. If it decreases too quickly and then shoots up, that
  suggests overfitting.
\item
  Memory stability Training uses more memory than inference because of
  gradients and optimizer states. A memory leak---forgetting to free
  arrays or reallocating without release---will cause the program to
  crash after many steps.
\item
  Optimizer state updates Check that AdamW accumulates \texttt{m} and
  \texttt{v} correctly over many iterations. If bias correction is
  missing, loss curves will diverge from expected baselines.
\item
  Reproducibility With the same random seed, two runs should produce
  nearly identical loss curves. Small differences are normal, but major
  deviations suggest nondeterministic bugs.
\end{enumerate}

\subsubsection{Why it matters}\label{why-it-matters-35}

Without this step, you might believe your implementation is complete
after unit tests, only to discover that training silently fails at step
500. Training readiness ensures the system is not only mathematically
correct in small pieces but also practically usable for long-running
experiments.

This is also where confidence in deploying the code comes from. Passing
training readiness means others can clone the repository, run the
scripts, and expect stable training without mysterious crashes.

\subsubsection{Example scenario}\label{example-scenario-3}

You run \texttt{test\_gpt2.c} and all outputs match PyTorch within
tolerance---great. Then you launch training for 5,000 steps. After step
600, the loss becomes \texttt{nan}. Investigation reveals that
\texttt{gpt2\_update} wasn't applying bias correction properly, so the
optimizer went unstable. That's a bug you'd never catch with a one-batch
unit test, but training readiness exposes it.

\subsubsection{Try it yourself}\label{try-it-yourself-47}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run training for 1,000 steps on Tiny Shakespeare and log the loss
  every 10 steps. Check that it decreases smoothly.
\item
  Add validation runs every 100 steps. Watch for the classic gap between
  train loss (lower) and validation loss (slightly higher).
\item
  Use \texttt{htop} or similar tools to monitor memory usage during
  training. Confirm that it stays steady rather than creeping upward.
\item
  Run the same training twice with the same seed. Compare the two loss
  curves---are they nearly identical?
\end{enumerate}

\subsubsection{The takeaway}\label{the-takeaway-48}

Unit tests prove the pieces are correct. Training readiness proves the
whole system works under real conditions. Both are necessary. Together,
they give you the confidence that \emph{llm.c} isn't just a collection
of working parts, but a functioning engine capable of training GPT-2
models end to end.

\subsection{60. Limitations of CPU
Testing}\label{limitations-of-cpu-testing}

Testing GPT-2 on CPU is invaluable for verifying correctness, but it
comes with clear limits. Understanding these limitations helps you
interpret the results properly and know when it's time to move on to
GPU-based experiments.

\subsubsection{Speed constraints}\label{speed-constraints}

The most obvious limitation is speed. CPUs are optimized for
general-purpose tasks, not the massive parallelism that neural networks
demand. A single forward and backward pass on GPT-2 124M can take
seconds or even minutes on CPU, while a GPU might handle it in
milliseconds. This makes:

\begin{itemize}
\tightlist
\item
  Full-scale training impractical: training GPT-2 124M to convergence
  could take weeks or months on CPU.
\item
  Experiment cycles slower: testing new optimizations or debugging is
  slowed because each run takes longer.
\end{itemize}

For this reason, CPU testing is best suited for small-scale sanity
checks, not full training runs.

\subsubsection{Memory overhead}\label{memory-overhead}

CPU memory is typically more abundant than GPU VRAM, but slower. The
bottleneck often isn't ``do we have enough RAM?'' but ``how quickly can
we move data in and out of memory?'' As sequence length \texttt{T}
grows, the activations balloon, and cache efficiency drops. This makes
even medium-sized runs sluggish.

\subsubsection{Limited realism}\label{limited-realism}

Although CPU runs confirm that the math is correct, they don't always
reflect the realities of GPU execution. For example:

\begin{itemize}
\tightlist
\item
  CUDA kernels have different numerical characteristics (fused
  operations, different rounding).
\item
  GPU memory layouts can expose bugs that CPU arrays hide.
\item
  Parallel execution may create timing or synchronization issues that
  never appear on CPU.
\end{itemize}

So while CPU parity with PyTorch is necessary, it isn't sufficient. You
must repeat testing once CUDA code is introduced.

\subsubsection{Loss of scale insights}\label{loss-of-scale-insights}

A CPU test can prove correctness for a few batches, but it doesn't tell
you how the code scales under heavy load. On GPU, you learn about kernel
efficiency, memory throughput, and distributed training. CPU tests
simply don't expose those concerns.

\subsubsection{Why it matters}\label{why-it-matters-36}

CPU testing is the foundation: it proves the algorithm is implemented
correctly, step by step, without relying on specialized hardware. But if
you stop there, you'll miss the bigger picture of performance and
scalability. CPU results should be treated as a green light to proceed,
not the final word on readiness.

\subsubsection{Example scenario}\label{example-scenario-4}

Suppose you run 500 training steps on Tiny Shakespeare. The loss curve
drops exactly as expected---success. But training on CPU is so slow that
finishing an epoch takes several hours. This validates correctness, but
makes it obvious that GPUs are required for meaningful experiments.

\subsubsection{Try it yourself}\label{try-it-yourself-48}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train GPT-2 124M on CPU for 100 steps and record the time per step.
  Extrapolate how long it would take to run 100k steps.
\item
  Increase sequence length from 64 to 512 and observe how memory access
  times affect throughput.
\item
  Compare your CPU loss curve with a GPU run from PyTorch. Notice they
  align in shape but differ dramatically in speed.
\item
  Use profiling tools (\texttt{perf}, \texttt{valgrind}, or
  \texttt{gprof}) to see which CPU functions dominate runtime.
\end{enumerate}

\subsubsection{The takeaway}\label{the-takeaway-49}

CPU testing is the safe laboratory where you validate correctness, catch
numerical errors, and reproduce known loss curves. But its
limitations---slow speed, reduced realism, and lack of scaling
insights---mean it's only a first step. Once CPU testing passes, the
journey continues with GPU testing, profiling, and multi-device scaling.

\section{\texorpdfstring{Chapter 7. CUDA Training
(\texttt{train\_gpt2.cu})}{Chapter 7. CUDA Training (train\_gpt2.cu)}}\label{chapter-7.-cuda-training-train_gpt2.cu}

\subsection{61. CUDA Architecture Overview (streams,
kernels)}\label{cuda-architecture-overview-streams-kernels}

When the CPU version of \emph{llm.c} runs, it executes instructions one
after another on your processor cores. This is fine for small models or
debugging, but deep learning workloads---especially Transformers like
GPT-2---demand an enormous number of floating-point operations. To
handle that, \emph{llm.c} also includes CUDA versions of the training
loop that shift computation to NVIDIA GPUs.

At a high level, CUDA is NVIDIA's programming model that lets developers
write code to run directly on the GPU. Unlike CPUs, which might have a
few cores optimized for general-purpose tasks, GPUs contain thousands of
simpler cores designed to process large batches of data in parallel.
CUDA provides the tools to organize work so that those cores can stay
busy.

\subsubsection{Kernels: Small Programs That Run on the
GPU}\label{kernels-small-programs-that-run-on-the-gpu}

In CUDA, a \emph{kernel} is a function that runs on the GPU. When you
launch a kernel, you don't call it once like a normal C function---you
launch thousands of copies at the same time. Each copy handles a
different piece of the data. For example, if you want to multiply two
vectors of a million elements, you can launch a million GPU threads,
each multiplying one pair of numbers.

In \emph{llm.c}, kernels are used for operations that can be expressed
in terms of lots of small, independent tasks. Examples include:

\begin{itemize}
\tightlist
\item
  Applying the GeLU activation function elementwise to a big tensor.
\item
  Adding residual connections across every dimension.
\item
  Normalizing values in LayerNorm.
\end{itemize}

For bigger, structured operations like matrix multiplications (GEMMs),
the CUDA code often relies on specialized libraries such as cuBLAS or
cuBLASLt, which are highly tuned for NVIDIA GPUs.

\subsubsection{Streams: Overlapping
Work}\label{streams-overlapping-work}

A GPU has the ability to handle multiple tasks at once. CUDA introduces
the idea of \emph{streams}, which are sequences of operations that run
in order relative to each other, but can overlap with operations in
other streams. This means:

\begin{itemize}
\tightlist
\item
  While one kernel is executing, another can start transferring data
  between CPU and GPU.
\item
  Computation and communication can overlap, reducing idle time.
\end{itemize}

In the training loop of \emph{llm.c}, streams let you schedule batches
of work so that data preparation and model computation can proceed side
by side. This is crucial for keeping the GPU saturated with useful work
instead of waiting on the CPU.

\subsubsection{The Memory Hierarchy}\label{the-memory-hierarchy}

CUDA programming is also shaped by the GPU memory hierarchy:

\begin{itemize}
\tightlist
\item
  Registers: Fastest, private to each thread.
\item
  Shared Memory: Small chunks of memory shared among threads in a block;
  much faster than global memory.
\item
  Global Memory: Large, but slower. This is where tensors like weights,
  activations, and gradients usually live.
\item
  Host Memory (CPU RAM): Separate from GPU memory; transferring between
  them can be slow and should be minimized.
\end{itemize}

For example, in the attention kernel, partial results might be stored in
registers or shared memory while processing a block of the sequence,
before writing the final result back to global memory.

\subsubsection{\texorpdfstring{How This Fits Into
\emph{llm.c}}{How This Fits Into llm.c}}\label{how-this-fits-into-llm.c}

In \texttt{train\_gpt2.cu}, most of the heavy lifting is done by calls
into cuBLAS/cuBLASLt and cuDNN for matrix multiplications and attention.
But understanding the CUDA model---kernels, streams, and memory---helps
explain:

\begin{itemize}
\tightlist
\item
  Why we batch operations the way we do.
\item
  Why minimizing data transfers between CPU and GPU is so important.
\item
  How GPU kernels map naturally to the kinds of tensor operations GPT-2
  requires.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-37}

Without CUDA, training GPT-2 would be painfully slow, even on a powerful
CPU. CUDA gives access to thousands of cores working in parallel, but it
also requires careful programming to avoid bottlenecks. Knowing about
kernels, streams, and memory hierarchy is the foundation for
understanding later sections where we dive into matrix multiplication,
attention, and optimization strategies.

\subsubsection{Try It Yourself}\label{try-it-yourself-49}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a simple CUDA kernel that adds two arrays elementwise. Compare
  its performance to a CPU loop.
\item
  Modify the kernel to use shared memory and see if it improves
  performance for larger arrays.
\item
  Create two CUDA streams: one for computing a kernel, and another for
  copying data. Measure whether the operations overlap in time.
\item
  Use \texttt{nvprof} or \texttt{nsys} to profile a CUDA program and
  observe how kernels and memory transfers appear on the timeline.
\item
  Think about how you would split a big matrix multiplication across
  thousands of threads---each thread computing one row, one column, or
  one element? What are the tradeoffs?
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-50}

CUDA is not just about writing code for GPUs---it's about rethinking
computation as thousands of small tasks that can run side by side.
Kernels handle the per-thread work, streams let you schedule and overlap
operations, and the memory hierarchy dictates how to organize data for
maximum speed. All of these ideas come together in \emph{llm.c}'s CUDA
implementation, making training feasible for models like GPT-2.

\subsection{62. Matrix Multiplication via
cuBLAS/cuBLASLt}\label{matrix-multiplication-via-cublascublaslt}

Matrix multiplication---often called GEMM (General Matrix-Matrix
Multiply)---is the beating heart of deep learning. In GPT-2, most of the
computation comes from multiplying large matrices: projecting embeddings
into query, key, and value vectors, applying attention weights, and
processing the MLP feed-forward layers. On the CPU, we saw this done
with nested loops and mild optimizations. On the GPU, however, we need
far more efficient approaches. That's where cuBLAS and cuBLASLt come in.

\subsubsection{Why Matrix Multiplication Is So
Central}\label{why-matrix-multiplication-is-so-central}

Almost every step of a Transformer involves multiplying two big
matrices:

\begin{itemize}
\tightlist
\item
  Embedding lookup can be seen as a matrix multiply between one-hot
  token vectors and the embedding table.
\item
  The attention mechanism computes dot products between queries and
  keys, followed by a weighted sum of values.
\item
  The MLP applies two fully connected layers, each of which is
  essentially a GEMM.
\end{itemize}

If you profile GPT-2, you'll find that GEMM operations dominate the
runtime. That's why NVIDIA's libraries devote enormous effort to making
these multiplications as fast as possible.

\subsubsection{cuBLAS: The Classic
Workhorse}\label{cublas-the-classic-workhorse}

cuBLAS is NVIDIA's GPU-accelerated version of the BLAS (Basic Linear
Algebra Subprograms) library. It provides highly optimized
implementations of GEMM and related routines. Under the hood, cuBLAS:

\begin{itemize}
\tightlist
\item
  Splits large matrices into tiles that fit into the GPU's shared
  memory.
\item
  Schedules thousands of threads to compute different tiles in parallel.
\item
  Uses fused multiply-add (FMA) instructions for high throughput.
\item
  Adapts to different GPU architectures to exploit Tensor Cores where
  available.
\end{itemize}

A typical call looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cublasHandle\_t handle}\OperatorTok{;}
\NormalTok{cublasCreate}\OperatorTok{(\&}\NormalTok{handle}\OperatorTok{);}

\DataTypeTok{float}\NormalTok{ alpha }\OperatorTok{=} \FloatTok{1.0}\BuiltInTok{f}\OperatorTok{,}\NormalTok{ beta }\OperatorTok{=} \FloatTok{0.0}\BuiltInTok{f}\OperatorTok{;}
\NormalTok{cublasSgemm}\OperatorTok{(}\NormalTok{handle}\OperatorTok{,}
\NormalTok{    CUBLAS\_OP\_N}\OperatorTok{,}\NormalTok{ CUBLAS\_OP\_N}\OperatorTok{,}
\NormalTok{    m}\OperatorTok{,}\NormalTok{ n}\OperatorTok{,}\NormalTok{ k}\OperatorTok{,}
    \OperatorTok{\&}\NormalTok{alpha}\OperatorTok{,}
\NormalTok{    A}\OperatorTok{,}\NormalTok{ m}\OperatorTok{,}
\NormalTok{    B}\OperatorTok{,}\NormalTok{ k}\OperatorTok{,}
    \OperatorTok{\&}\NormalTok{beta}\OperatorTok{,}
\NormalTok{    C}\OperatorTok{,}\NormalTok{ m}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

Here \texttt{A} and \texttt{B} are the input matrices, and \texttt{C} is
the result. The function handles all the low-level scheduling.

\subsubsection{cuBLASLt: The Flexible
Successor}\label{cublaslt-the-flexible-successor}

While cuBLAS is powerful, it's somewhat rigid. cuBLASLt (Lightweight
cuBLAS) is a newer API that adds:

\begin{itemize}
\tightlist
\item
  Better support for mixed precision (e.g., FP16 or BF16 inputs with
  FP32 accumulation).
\item
  More control over algorithm selection, so developers can tune for
  performance or memory usage.
\item
  Features like epilogues, which let you fuse additional operations
  (e.g., bias addition, activation functions) directly into the GEMM,
  reducing memory transfers.
\end{itemize}

In practice, cuBLASLt often outperforms cuBLAS because it can exploit
Tensor Cores more aggressively and fuse multiple steps into a single
kernel call.

\subsubsection{Precision and Tensor
Cores}\label{precision-and-tensor-cores}

On modern NVIDIA GPUs (Volta, Turing, Ampere, Hopper), Tensor Cores
accelerate matrix multiplications dramatically when using FP16, BF16, or
TF32. These special hardware units can perform
matrix-multiply-and-accumulate on small blocks of numbers in a single
instruction.

For example:

\begin{itemize}
\tightlist
\item
  On CPUs, multiplying two 16×16 matrices is done with many scalar
  multiplications.
\item
  On GPUs with Tensor Cores, the entire block can be computed in one
  fused operation.
\end{itemize}

In GPT-2 training, using FP16 with cuBLASLt enables much higher
throughput, while keeping master weights in FP32 to preserve numerical
stability.

\subsubsection{\texorpdfstring{Practical Example in
\emph{llm.c}}{Practical Example in llm.c}}\label{practical-example-in-llm.c}

In \texttt{train\_gpt2.cu}, most of the calls to perform linear
layers---such as projecting input activations into query, key, and value
matrices---are implemented with cuBLAS/cuBLASLt. For instance:

\begin{itemize}
\tightlist
\item
  Inputs \texttt{(B,\ T,\ C)} are multiplied by a weight matrix
  \texttt{(C,\ 3C)} to produce \texttt{(B,\ T,\ 3C)}.
\item
  Later, the output of attention \texttt{(B,\ T,\ C)} is multiplied by
  another projection matrix \texttt{(C,\ C)}.
\end{itemize}

Instead of writing custom kernels for each case, the code defers to
cuBLAS/cuBLASLt, ensuring maximum performance across GPU architectures.

\subsubsection{Why It Matters}\label{why-it-matters-38}

Matrix multiplications are so frequent and heavy that their performance
directly determines how fast you can train GPT-2. By leaning on
cuBLAS/cuBLASLt, \emph{llm.c} avoids reinventing the wheel and gets
near-peak GPU efficiency. This makes the code clean, maintainable, and
scalable to larger models.

\subsubsection{Try It Yourself}\label{try-it-yourself-50}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a small CUDA program that multiplies two matrices using naive
  kernels, and compare its performance to cuBLAS.
\item
  Experiment with FP32 versus FP16 inputs and observe the speedup when
  Tensor Cores are enabled.
\item
  Enable cuBLASLt's epilogues to fuse bias addition into GEMM, and
  measure memory savings.
\item
  Profile GPT-2 training with \texttt{nvprof} or \texttt{nsys} to see
  how much time is spent in GEMM calls.
\item
  Try scaling up matrix sizes to simulate bigger models and note how
  performance grows relative to CPU implementations.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-51}

Matrix multiplication is the computational engine of GPT-2, and on GPUs
it's powered by cuBLAS and cuBLASLt. These libraries harness the GPU's
architecture---tiling, Tensor Cores, mixed precision---to squeeze out
maximum efficiency. Understanding how they work gives insight into why
the GPU version of \emph{llm.c} runs so much faster than the CPU
version, and sets the stage for attention kernels and other
CUDA-accelerated components.

\subsection{63. Attention Kernels: cuDNN
FlashAttention}\label{attention-kernels-cudnn-flashattention}

The attention mechanism is at the core of every Transformer. It allows
the model to weigh different parts of the input sequence when producing
an output. For GPT-2, this means that when generating the next token,
the model doesn't just look at the last word---it considers the entire
sequence of words before it, adjusting how much each past token
contributes. But attention is expensive. Naively, it scales
quadratically with sequence length: for a sequence of 1024 tokens, you
need to compute a 1024×1024 attention matrix. That's more than a million
entries, and each must be computed, normalized, and multiplied back into
the value vectors.

On CPUs, we saw how this is implemented step by step: queries, keys, and
values are projected with matrix multiplications, dot products between
queries and keys are computed, softmax is applied, and the result is
multiplied by values. On GPUs, we want to do the same thing, but much
faster. That's where cuDNN's FlashAttention comes into play.

\subsubsection{What Is FlashAttention?}\label{what-is-flashattention}

FlashAttention is an algorithm that rethinks how attention is computed.
Instead of materializing the full attention matrix in memory, it
computes softmax and the weighted sum in a streaming fashion. This
reduces memory usage and improves cache efficiency.

Normally, attention involves these steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute scores = Q × Kᵀ (queries times keys transpose).
\item
  Apply softmax over scores to get attention weights.
\item
  Multiply weights × V (values) to get the output.
\end{enumerate}

The problem: step 1 produces a huge score matrix of size
\texttt{(sequence\_length\ ×\ sequence\_length)}. Storing and processing
this full matrix becomes the bottleneck.

FlashAttention avoids storing the full matrix by computing attention
block by block. It processes tiles of queries and keys, applies the
softmax incrementally, and accumulates results directly into the output.
This drastically cuts memory bandwidth requirements, which is critical
for GPUs.

\subsubsection{cuDNN FlashAttention in
Practice}\label{cudnn-flashattention-in-practice}

In \emph{llm.c}'s CUDA training code, when \texttt{USE\_CUDNN} is
enabled, the code can take advantage of cuDNN's implementation of
FlashAttention. This means:

\begin{itemize}
\tightlist
\item
  The library handles the tiling and streaming automatically.
\item
  It can leverage Tensor Cores for mixed-precision computation
  (FP16/BF16 inputs with FP32 accumulation).
\item
  It reduces memory use, which allows training longer sequences without
  running out of GPU memory.
\end{itemize}

From a developer's point of view, enabling cuDNN FlashAttention usually
involves passing specific descriptors and flags to cuDNN routines rather
than writing custom kernels. Instead of manually managing loops and
softmax stability tricks, you hand over the responsibility to cuDNN,
which has a heavily optimized kernel.

\subsubsection{Why This Is a
Game-Changer}\label{why-this-is-a-game-changer}

The quadratic cost of attention has long been a bottleneck in scaling
Transformers. With FlashAttention, the bottleneck shifts. The
computation is still O(N²), but because memory is handled so much more
efficiently, the GPU spends less time waiting on memory loads and more
time doing actual math. This means:

\begin{itemize}
\tightlist
\item
  Training can be faster even at the same sequence length.
\item
  You can push to larger sequence lengths (e.g., 2K or 4K tokens)
  without running out of GPU memory.
\item
  Energy efficiency improves because you avoid redundant reads/writes to
  global memory.
\end{itemize}

\subsubsection{Example: Why Memory Access
Matters}\label{example-why-memory-access-matters}

Let's imagine a toy example with 4 tokens. A naive implementation might
build a 4×4 attention matrix, compute softmax, and multiply by values.
That's fine for 4 tokens, but with 1024 tokens, you'd be juggling
matrices of a million entries. Even if each entry is just 2 bytes
(FP16), that's megabytes of temporary storage per step. On a real GPU,
constantly moving that in and out of global memory slows everything
down.

FlashAttention says: instead of storing the whole million entries,
compute them in chunks, normalize them on the fly, and immediately use
them to update the output. This way, only small temporary blocks live in
memory, and global memory pressure drops dramatically.

\subsubsection{How It Shows Up in GPT-2
Training}\label{how-it-shows-up-in-gpt-2-training}

When GPT-2 processes a batch of sequences, each block of the model
applies attention. In the CUDA version of \emph{llm.c}, these attention
calls can be routed through cuDNN FlashAttention. Practically, this
means that the inner loop of training---the part that would otherwise
grind on those giant attention matrices---becomes leaner and faster.

This matters even more as models grow. For GPT-2 124M (12 layers, 12
heads, 1024 sequence length), attention is already expensive. For GPT-2
1.5B or LLaMA-style models with longer contexts, FlashAttention can be
the difference between feasible training and ``out of memory'' errors.

\subsubsection{Why It Matters}\label{why-it-matters-39}

Attention is the defining operation of Transformers, but it's also their
Achilles heel. FlashAttention addresses the biggest
inefficiency---memory bandwidth---without changing the model's outputs.
By using cuDNN's optimized kernels, \emph{llm.c} ensures it runs close
to hardware peak performance while still producing correct results. For
anyone learning about deep learning systems, this is a perfect example
of how algorithmic innovations (streaming softmax) and hardware-level
optimizations (Tensor Cores, tiling) combine to make state-of-the-art
training practical.

\subsubsection{Try It Yourself}\label{try-it-yourself-51}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run GPT-2 training in \emph{llm.c} with \texttt{USE\_CUDNN=0} and then
  with \texttt{USE\_CUDNN=1}. Compare training speed and GPU memory
  usage.
\item
  Write a naive CUDA kernel that builds the full attention matrix, then
  benchmark it against cuDNN FlashAttention.
\item
  Vary sequence lengths (128, 512, 1024, 2048) and see how performance
  diverges between naive and FlashAttention implementations.
\item
  Examine how mixed precision interacts with FlashAttention---try FP16
  versus BF16.
\item
  Explore the FlashAttention paper and compare its algorithmic
  explanation with what you see in practice using \emph{llm.c}.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-52}

Attention is expensive, but it doesn't have to be crippling.
FlashAttention shows that clever algorithm design plus hardware-aware
implementation can shrink the memory bottleneck dramatically. By leaning
on cuDNN's implementation, \emph{llm.c} can train GPT-2 models more
efficiently, and learners get a real-world view of how deep learning
libraries squeeze performance out of GPUs.

\subsection{64. Mixed Precision: FP16/BF16 with Master FP32
Weights}\label{mixed-precision-fp16bf16-with-master-fp32-weights}

Training large models like GPT-2 involves multiplying and adding
enormous amounts of numbers---billions of operations in every training
step. GPUs can do this very quickly, but the type of numbers you use
matters a lot. Traditionally, training is done in 32-bit floating point
(FP32), which gives good precision but is heavy on memory and compute.
Modern GPUs offer special hardware---Tensor Cores---that run much faster
when using reduced precision, such as FP16 (half-precision floating
point) or BF16 (bfloat16). This technique is called mixed precision
training.

\subsubsection{Why Mixed Precision
Helps}\label{why-mixed-precision-helps}

Using FP16 or BF16 has two main benefits:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Speed: GPUs can perform more FP16/BF16 operations per clock cycle than
  FP32. For example, NVIDIA Tensor Cores are specifically designed to
  accelerate half-precision math, often delivering 2× or more
  throughput.
\item
  Memory: FP16/BF16 values take half the storage of FP32. That means you
  can fit larger batches or longer sequences into the same GPU memory,
  which is critical for scaling models.
\end{enumerate}

But reduced precision comes with a tradeoff: it's easier for numbers to
underflow (become zero) or overflow (become infinity), which can
destabilize training.

\subsubsection{Master Weights in FP32}\label{master-weights-in-fp32}

The trick used in \emph{llm.c} (and also in PyTorch and TensorFlow) is
to keep a master copy of weights in FP32. Here's the process:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  During the forward pass, weights are cast to FP16/BF16 so the GPU can
  run the math on Tensor Cores.
\item
  The gradients are computed in reduced precision as well.
\item
  When it's time to update parameters, the optimizer applies updates to
  the FP32 master copy.
\item
  The updated master weights are cast back to FP16/BF16 for the next
  forward pass.
\end{enumerate}

This way, you get the speed and memory savings of mixed precision
without fully losing the stability of FP32.

\subsubsection{FP16 vs.~BF16}\label{fp16-vs.-bf16}

Both FP16 and BF16 use 16 bits, but they split the bits differently:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0822}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1781}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1781}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0959}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.4658}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Format
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Exponent Bits
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mantissa Bits
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Range
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Precision
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
FP16 & 5 & 10 & Smaller & Higher precision for small numbers \\
BF16 & 8 & 7 & Wider & Rougher precision, better range \\
\end{longtable}

\begin{itemize}
\tightlist
\item
  FP16 has better precision near zero but a narrower range, so it's more
  prone to overflow.
\item
  BF16 has the same exponent size as FP32, giving it a much wider range
  but less precision.
\end{itemize}

Modern NVIDIA GPUs (Ampere, Hopper) support both, but BF16 is often
preferred for stability in very large models.

\subsubsection{Example in Practice}\label{example-in-practice}

Imagine training GPT-2 with a sequence length of 1024 and batch size of
32. With FP32, the activations might take \textasciitilde12 GB of GPU
memory. Switching to FP16 halves that to \textasciitilde6 GB, leaving
room for larger models or more sequences.

In \emph{llm.c}, enabling mixed precision means the forward pass can
look something like this:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Cast embeddings, weights, and activations to FP16/BF16.
\item
  Run matrix multiplications on Tensor Cores (very fast).
\item
  Compute gradients in reduced precision.
\item
  Convert gradients back to FP32 for stable updates.
\end{enumerate}

This flow is invisible to the high-level code but handled internally in
CUDA/cuBLAS/cuDNN calls.

\subsubsection{Common Challenges}\label{common-challenges}

Mixed precision introduces new wrinkles:

\begin{itemize}
\tightlist
\item
  Loss scaling: small gradients may underflow to zero in FP16. The
  solution is to multiply the loss by a large factor during
  backpropagation, then divide gradients back later. This preserves
  information.
\item
  Debugging: NaNs and Infs become more common when switching to FP16.
  Careful monitoring is required to catch these early.
\item
  Performance tuning: Not all operations benefit equally from FP16. For
  example, reductions (like summing a large array) may lose too much
  precision unless done in FP32.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-40}

Mixed precision is one of the key reasons modern Transformers can be
trained efficiently on today's hardware. Without it, many models would
require double the GPU memory and much more time to train. By combining
FP16/BF16 for speed and memory efficiency with FP32 master weights for
stability, \emph{llm.c} mirrors the strategy used in production
frameworks. This shows how even a minimalist codebase can teach the
cutting-edge tricks that power real-world large-scale training.

\subsubsection{Try It Yourself}\label{try-it-yourself-52}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train GPT-2 in \emph{llm.c} with FP32 only, then repeat with FP16.
  Compare memory usage (\texttt{nvidia-smi}) and runtime per step.
\item
  Experiment with FP16 vs.~BF16 if your GPU supports both. Observe
  whether one is more stable.
\item
  Intentionally remove the FP32 master weights (update parameters in
  FP16 only) and see how quickly training diverges.
\item
  Plot validation loss curves with FP32, FP16, and BF16 runs to see if
  the model quality differs.
\item
  Try scaling the batch size up with FP16 and note how much bigger a
  model you can fit into the same GPU.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-53}

Mixed precision combines the best of both worlds: the speed and memory
efficiency of FP16/BF16 with the stability of FP32. This technique has
become a standard in deep learning, and \emph{llm.c} demonstrates it in
a clear, accessible way. It's not just a neat optimization---it's what
makes training large language models on modern GPUs feasible at all.

\subsection{65. Loss Scaling in Mixed Precision
Training}\label{loss-scaling-in-mixed-precision-training}

When training in mixed precision (FP16 or BF16), one of the biggest
challenges is numerical underflow. Gradients can become so small that
they round down to zero when represented in 16-bit format. If that
happens too often, the optimizer stops receiving meaningful updates, and
training can stagnate or collapse. To address this, frameworks introduce
a technique called loss scaling.

\subsubsection{The Core Idea}\label{the-core-idea-2}

Loss scaling works by multiplying the loss value by a constant factor
(called the scale factor) before starting backpropagation. Since
gradients are proportional to the loss, this also multiplies all
gradients by the same factor, making them larger and less likely to
underflow when stored in FP16.

At the end of backpropagation, the gradients are divided by the same
scale factor, restoring their correct values before the optimizer step.

Mathematically:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{scaled\_loss\ =\ loss\ ×\ scale}
\item
  Compute gradients of \texttt{scaled\_loss} → produces
  \texttt{scaled\_gradients}
\item
  \texttt{true\_gradients\ =\ scaled\_gradients\ ÷\ scale}
\end{enumerate}

The optimizer then uses \texttt{true\_gradients} to update the weights.

\subsubsection{Static vs.~Dynamic
Scaling}\label{static-vs.-dynamic-scaling}

There are two common approaches:

\begin{itemize}
\tightlist
\item
  Static scaling: Use a fixed scale factor throughout training. For
  example, always multiply the loss by \texttt{1024}. This is simple but
  risky; if the scale is too high, gradients may overflow to infinity.
  If it's too low, underflow still happens.
\item
  Dynamic scaling: Adjust the scale factor on the fly. If overflows
  (NaNs or Infs) are detected, the scale factor is reduced. If training
  proceeds smoothly, the scale factor is gradually increased. This
  balances stability and efficiency.
\end{itemize}

In practice, dynamic scaling is the standard. Libraries like PyTorch's
\texttt{GradScaler} automatically handle this logic, so users don't have
to tweak values manually.

\subsubsection{\texorpdfstring{How It Appears in
\emph{llm.c}}{How It Appears in llm.c}}\label{how-it-appears-in-llm.c}

The minimal design of \emph{llm.c} doesn't yet include automatic loss
scaling, but the idea fits neatly into its training loop. Before calling
\texttt{gpt2\_backward}, you would scale the loss. After gradients are
computed, you would unscale them before \texttt{gpt2\_update}.
Conceptually:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{float}\NormalTok{ scale }\OperatorTok{=} \FloatTok{1024.0}\BuiltInTok{f}\OperatorTok{;} \CommentTok{// example scale factor}
\DataTypeTok{float}\NormalTok{ scaled\_loss }\OperatorTok{=}\NormalTok{ model}\OperatorTok{.}\NormalTok{mean\_loss }\OperatorTok{*}\NormalTok{ scale}\OperatorTok{;}
\NormalTok{gpt2\_backward\_scaled}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{,}\NormalTok{ scaled\_loss}\OperatorTok{);} \CommentTok{// backprop with scaled loss}
\NormalTok{unscale\_gradients}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{,}\NormalTok{ scale}\OperatorTok{);} \CommentTok{// divide gradients by scale}
\NormalTok{gpt2\_update}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{,}\NormalTok{ lr}\OperatorTok{,}\NormalTok{ beta1}\OperatorTok{,}\NormalTok{ beta2}\OperatorTok{,}\NormalTok{ eps}\OperatorTok{,}\NormalTok{ weight\_decay}\OperatorTok{,}\NormalTok{ step}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

This is not yet in the repository, but it's how one could extend
\emph{llm.c} to support stable FP16 training.

\subsubsection{Why It Matters}\label{why-it-matters-41}

Without loss scaling, mixed precision can fail silently. Training might
appear to run, but the gradients may be effectively zero for many
parameters. This wastes GPU time and produces poor results. With loss
scaling, FP16/BF16 training becomes both fast and reliable, combining
the hardware speedups with numerical stability.

\subsubsection{Example Scenario}\label{example-scenario-5}

Suppose you are training GPT-2 with FP16 and notice that the validation
loss barely decreases after several hundred steps. One possible reason
is gradient underflow. By enabling loss scaling with a scale factor of
512 or 1024, you might suddenly see the loss curve behave normally
again, matching the FP32 baseline.

\subsubsection{Try It Yourself}\label{try-it-yourself-53}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train with FP16 but without loss scaling. Monitor whether the loss
  decreases meaningfully.
\item
  Add a static scale factor (like 512) and rerun. Observe improvements
  in stability.
\item
  Implement a simple dynamic scaler: start with 128, double it if no
  NaNs appear for 100 steps, halve it if NaNs are detected.
\item
  Compare training curves (FP32 vs.~FP16 with and without scaling) to
  see the effect.
\item
  Experiment with very large scale factors to trigger overflow
  intentionally, then watch how dynamic scaling recovers.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-54}

Loss scaling is the hidden ingredient that makes mixed precision
training practical. By rescaling the loss and gradients, we protect tiny
numbers from disappearing in FP16 while still enjoying the massive
performance and memory benefits. Even in a minimal codebase like
\emph{llm.c}, understanding loss scaling bridges the gap between a model
that trains poorly and one that matches FP32 performance at half the
cost.

\subsection{66. Activation Checkpointing and Memory
Tradeoffs}\label{activation-checkpointing-and-memory-tradeoffs}

Training deep networks like GPT-2 involves storing a large number of
activations---the intermediate outputs produced at every layer during
the forward pass. These activations are needed later in the backward
pass to compute gradients. The problem is that they take up a huge
amount of GPU memory. For a 12-layer GPT-2 with long sequences and large
batch sizes, activations can consume more memory than the model weights
themselves.

\subsubsection{Why Activations Matter}\label{why-activations-matter}

Let's say you have a batch size of 8, sequence length of 1024, hidden
size of 768, and 12 layers. Each layer produces an activation tensor of
shape \texttt{(batch\_size,\ sequence\_length,\ hidden\_size)}, or
\texttt{8\ ×\ 1024\ ×\ 768}. That's about 6.3 million numbers per layer.
Multiply by 12 layers, and you have \textasciitilde75 million numbers.
At FP16, that's around 150 MB per forward pass just for storing
activations, and this grows with larger models.

If you scale up to GPT-2 Medium or GPT-2 XL, this number balloons
quickly into gigabytes, which may not fit in GPU memory.

\subsubsection{The Idea of
Checkpointing}\label{the-idea-of-checkpointing}

Activation checkpointing offers a tradeoff: instead of storing all
activations, you only keep a small subset (the checkpoints) and
recompute the rest during the backward pass.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  During forward pass: save only checkpoints (for example, the
  activations at the end of each transformer block).
\item
  During backward pass: when gradients for a layer are needed, recompute
  the missing activations by running part of the forward pass again.
\end{enumerate}

This saves memory at the cost of extra computation.

\subsubsection{How It Works in GPT-2}\label{how-it-works-in-gpt-2}

A GPT-2 block has multiple steps: embedding lookup, attention, MLP,
layer norm, residual connections. Normally, you'd store every output
tensor. With checkpointing, you might only store the input to each block
and discard the intermediate results. When backpropagation reaches that
block, you rerun the forward pass locally to regenerate those results,
then compute gradients.

This reduces memory usage almost linearly with the number of discarded
activations, at the cost of roughly 30--40\% more compute.

\subsubsection{\texorpdfstring{In \emph{llm.c}
Context}{In llm.c Context}}\label{in-llm.c-context}

\emph{llm.c} doesn't yet include activation checkpointing in its minimal
implementation, but it's a natural extension. In CUDA, this might be
implemented by wrapping blocks of code with a ``checkpoint'' function
that decides whether to save or discard activations. In PyTorch, the
equivalent is \texttt{torch.utils.checkpoint}.

If you train with longer sequences (e.g., 2048 tokens instead of 1024),
checkpointing could mean the difference between fitting in memory or
running into out-of-memory (OOM) errors.

\subsubsection{Why It Matters}\label{why-it-matters-42}

Modern GPUs have enormous compute capacity, but memory remains the
bottleneck. Checkpointing shifts the tradeoff: you spend a bit more
compute (re-running some forward passes) in exchange for freeing up
gigabytes of memory. This lets you:

\begin{itemize}
\tightlist
\item
  Train larger models on the same hardware.
\item
  Use longer sequence lengths for better context handling.
\item
  Increase batch size for more stable gradients.
\end{itemize}

In practice, this technique is used in nearly every large-scale
Transformer training run today.

\subsubsection{Example Analogy}\label{example-analogy}

Think of it like studying for an exam. You could take detailed notes on
every page of the textbook (storing all activations), but your notebook
would get huge. Alternatively, you could only mark the chapter headings
(checkpoints) and re-read sections when you need them during review
(recomputation). It takes more time, but saves notebook space.

\subsubsection{Try It Yourself}\label{try-it-yourself-54}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run training on a GPU with long sequences until you hit an
  out-of-memory error.
\item
  Implement a simple checkpointing scheme where you only store
  activations every other layer.
\item
  Measure how much memory usage decreases (using \texttt{nvidia-smi})
  and how much runtime increases per step.
\item
  Experiment with different checkpointing frequencies---every layer,
  every 2 layers, every 4 layers---and find the balance between memory
  savings and compute overhead.
\item
  Compare validation loss curves to confirm that checkpointing does not
  affect training quality (only runtime).
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-55}

Activation checkpointing is a clever strategy to bend the memory limits
of GPUs. By discarding and recomputing activations on demand, you can
fit models or sequence lengths that would otherwise be impossible. The
tradeoff is extra computation, but with today's hardware, compute is
usually cheaper than memory. This technique is one of the quiet enablers
behind scaling Transformers to billions of parameters.

\subsection{67. GPU Memory Planning: Parameters, Gradients,
States}\label{gpu-memory-planning-parameters-gradients-states}

When training a GPT-2 model on GPU, one of the most important practical
challenges is managing memory. Every tensor---the model's parameters,
gradients, optimizer state, and activations---must fit into limited GPU
RAM. Unlike CPUs, where you can often rely on swap space or large RAM
pools, GPU memory is tight and unforgiving. If you exceed the limit, the
program crashes with an out-of-memory error.

\subsubsection{Breaking Down What Takes
Memory}\label{breaking-down-what-takes-memory}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Parameters (Weights) These are the trainable values of the model:
  embeddings, attention projections, MLP weights, and so on. For GPT-2
  124M, there are about 124 million parameters.

  \begin{itemize}
  \tightlist
  \item
    In FP32, that's roughly 500 MB.
  \item
    In FP16/BF16, it's about 250 MB. Larger GPT-2 models (355M, 774M,
    1.5B) scale this up proportionally.
  \end{itemize}
\item
  Gradients For each parameter, the backward pass produces a gradient
  tensor of the same size. If parameters take 500 MB, gradients also
  take \textasciitilde500 MB in FP32. Mixed precision can halve this.
\item
  Optimizer States Optimizers like AdamW don't just store
  gradients---they also track moving averages (\texttt{m} and
  \texttt{v}). Each adds another full-sized tensor. With AdamW, you
  often end up with 3× the parameter size: weights + m + v.
\item
  Activations During the forward pass, every layer's intermediate
  outputs must be stored for the backward pass. This is often the
  largest single consumer of memory. For a 12-layer GPT-2 with sequence
  length 1024 and batch size 8, activations can easily exceed several
  GB. Checkpointing (as discussed earlier) helps reduce this.
\end{enumerate}

\subsubsection{A Simple Calculation
Example}\label{a-simple-calculation-example}

For GPT-2 124M in FP32:

\begin{itemize}
\tightlist
\item
  Parameters: \textasciitilde500 MB
\item
  Gradients: \textasciitilde500 MB
\item
  AdamW states: \textasciitilde1 GB (two copies)
\item
  Activations: 2--4 GB (depends on batch size and sequence length)
\end{itemize}

Total: \textasciitilde3--6 GB, which fits on most modern GPUs.

For GPT-2 774M in FP32:

\begin{itemize}
\tightlist
\item
  Parameters: \textasciitilde3 GB
\item
  Gradients: \textasciitilde3 GB
\item
  AdamW states: \textasciitilde6 GB
\item
  Activations: 8--12 GB
\end{itemize}

Total: \textasciitilde20+ GB---too large for many GPUs unless you use
tricks like FP16 and checkpointing.

\subsubsection{Strategies for Memory
Efficiency}\label{strategies-for-memory-efficiency}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Mixed Precision Using FP16/BF16 cuts parameter, gradient, and
  optimizer sizes in half. Instead of 20 GB, you may get by with
  \textasciitilde10 GB.
\item
  Activation Checkpointing Store fewer activations and recompute them
  during backpropagation. This often saves multiple GB.
\item
  Gradient Accumulation Instead of training with a huge batch at once,
  split it into smaller micro-batches and accumulate gradients across
  them. This reduces activation memory requirements.
\item
  Parameter Sharding (Advanced) In multi-GPU setups, parameters and
  optimizer states can be split across devices (e.g., ZeRO optimizer in
  DeepSpeed). While not in \emph{llm.c}, it's a common technique at
  scale.
\end{enumerate}

\subsubsection{\texorpdfstring{In
\emph{llm.c}}{In llm.c}}\label{in-llm.c}

The \texttt{GPT2} struct organizes memory into fields like
\texttt{params\_memory}, \texttt{grads\_memory}, \texttt{m\_memory}, and
\texttt{v\_memory}. These are allocated as flat arrays, making it easy
to calculate their size. This minimal design highlights the reality: for
every parameter, there's at least one matching gradient and potentially
two optimizer state values.

This structure mirrors how full frameworks like PyTorch allocate memory,
but \emph{llm.c} exposes it transparently so you can see exactly what's
taking up space.

\subsubsection{Why It Matters}\label{why-it-matters-43}

When training models, memory is usually the first limit you hit, not
compute. Even if your GPU is powerful enough to handle the math, if you
run out of memory, you can't proceed. Understanding how parameters,
gradients, optimizer states, and activations interact helps you design
training runs that actually fit.

\subsubsection{Try It Yourself}\label{try-it-yourself-55}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate memory usage for GPT-2 124M, 355M, and 774M using FP32 vs
  FP16. Compare your numbers to your GPU's memory size.
\item
  Run \emph{llm.c} with increasing batch sizes until you hit an
  out-of-memory error. Record the exact point where it breaks.
\item
  Enable mixed precision and checkpointing to see how much further you
  can push sequence length or batch size.
\item
  Write a script to print the sizes of \texttt{params\_memory},
  \texttt{grads\_memory}, and optimizer states in \emph{llm.c}. Compare
  this to \texttt{nvidia-smi} output during training.
\item
  Experiment with reducing optimizer states (e.g., try SGD instead of
  AdamW) to see the memory difference.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-56}

Training Transformers is not just about writing the forward and backward
passes---it's about planning memory carefully. Parameters, gradients,
optimizer states, and activations all compete for limited GPU RAM. By
understanding these categories and using techniques like mixed precision
and checkpointing, you can fit bigger models or longer contexts on the
same hardware. This balance between memory and compute is at the heart
of scaling modern deep learning.

\subsection{68. Kernel Launch Configurations and
Occupancy}\label{kernel-launch-configurations-and-occupancy}

When writing CUDA code for training models like GPT-2, one of the most
important yet subtle factors in performance is how kernels are launched.
A kernel is just a function that runs on the GPU, but the way you
configure it---the number of threads, blocks, and how work is
divided---can make the difference between a GPU that runs at 10\%
efficiency and one that's near peak utilization.

\subsubsection{Threads, Blocks, and
Grids}\label{threads-blocks-and-grids}

CUDA organizes computation hierarchically:

\begin{itemize}
\tightlist
\item
  Thread: the smallest unit of execution. Each thread runs the kernel
  code once.
\item
  Block: a collection of threads that share fast, on-chip memory (called
  shared memory).
\item
  Grid: a collection of blocks. Together, the grid represents the entire
  kernel launch.
\end{itemize}

When you launch a kernel, you decide:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_kernel}\OperatorTok{\textless{}\textless{}\textless{}}\NormalTok{num\_blocks}\OperatorTok{,}\NormalTok{ threads\_per\_block}\OperatorTok{\textgreater{}\textgreater{}\textgreater{}(...);}
\end{Highlighting}
\end{Shaded}

For example:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{int}\NormalTok{ threads }\OperatorTok{=} \DecValTok{256}\OperatorTok{;}
\DataTypeTok{int}\NormalTok{ blocks }\OperatorTok{=} \OperatorTok{(}\NormalTok{N }\OperatorTok{+}\NormalTok{ threads }\OperatorTok{{-}} \DecValTok{1}\OperatorTok{)} \OperatorTok{/}\NormalTok{ threads}\OperatorTok{;}
\NormalTok{my\_kernel}\OperatorTok{\textless{}\textless{}\textless{}}\NormalTok{blocks}\OperatorTok{,}\NormalTok{ threads}\OperatorTok{\textgreater{}\textgreater{}\textgreater{}(...);}
\end{Highlighting}
\end{Shaded}

Here, \texttt{N} might be the total number of elements to process. The
division ensures that all elements get covered.

\subsubsection{What Is Occupancy?}\label{what-is-occupancy}

Occupancy refers to how many threads are active on a GPU relative to the
maximum possible. Higher occupancy usually means the GPU is better
utilized, but it's not the only factor---memory access patterns and
instruction throughput also matter.

Each GPU has a fixed number of Streaming Multiprocessors (SMs), and each
SM can support only a certain number of threads and blocks at once. If
your kernel launch doesn't provide enough threads, the GPU will be
underutilized. If you launch too many, they may compete for shared
memory and registers, leading to inefficiency.

\subsubsection{Example: A Simple Vector
Add}\label{example-a-simple-vector-add}

Suppose you want to add two arrays of size \texttt{N\ =\ 1e6}.

\begin{itemize}
\tightlist
\item
  If you use \texttt{threads\_per\_block\ =\ 32}, you'll have many tiny
  blocks. This wastes parallelism because modern GPUs are designed to
  run hundreds of threads per SM.
\item
  If you use \texttt{threads\_per\_block\ =\ 1024}, you may hit the
  hardware limit but run very large blocks that restrict scheduling
  flexibility.
\item
  A good balance might be 256 or 512 threads per block, which lets the
  GPU overlap computation and memory access effectively.
\end{itemize}

\subsubsection{In Transformer Training}\label{in-transformer-training}

For GPT-2 in \emph{llm.c}, most heavy lifting is done by:

\begin{itemize}
\tightlist
\item
  Matrix multiplications (handled by cuBLAS/cuBLASLt). These libraries
  pick kernel launch parameters automatically.
\item
  Attention and normalization kernels (custom or cuDNN). When written by
  hand, launch configuration becomes crucial.
\end{itemize}

For example, in a softmax kernel over a sequence of 1024 tokens:

\begin{itemize}
\tightlist
\item
  You might launch one block per sequence row, with 1024 threads per
  block (one per token).
\item
  Alternatively, you might launch multiple blocks per row, each handling
  a tile of tokens, if shared memory limits require it.
\end{itemize}

Choosing wisely can double or triple performance.

\subsubsection{Balancing Factors}\label{balancing-factors}

When configuring kernels, you balance:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Occupancy: Are enough threads active to use the GPU fully?
\item
  Memory Coalescing: Do threads access memory in aligned, sequential
  chunks (which is fast) or scattered patterns (which is slow)?
\item
  Shared Memory and Registers: Each block has limited resources. If your
  kernel uses too much shared memory, fewer blocks can fit per SM,
  reducing occupancy.
\item
  Arithmetic Intensity: If the kernel does a lot of math per memory
  load, occupancy matters less; if it's memory-bound, occupancy matters
  more.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-44}

In GPU training, kernel launch decisions directly control how
efficiently hardware is used. Two kernels implementing the same math can
differ by 5--10× in runtime purely because of launch configuration.
cuBLAS and cuDNN automate much of this for matrix-heavy ops, but
understanding it is crucial when writing custom kernels.

\subsubsection{Try It Yourself}\label{try-it-yourself-56}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a simple CUDA kernel for vector addition with different
  \texttt{threads\_per\_block} values (32, 128, 256, 512, 1024). Measure
  runtime and see which is fastest.
\item
  Use \texttt{nvprof} or \texttt{nsys} to inspect occupancy of kernels
  during GPT-2 training. Note which kernels run at \textless50\%
  occupancy.
\item
  Experiment with a softmax kernel: launch one block per row
  vs.~multiple blocks per row. Compare performance and memory use.
\item
  Explore how shared memory allocation per block affects occupancy by
  artificially increasing shared memory usage.
\item
  Compare cuBLAS GEMM (matrix multiply) performance to a naive CUDA
  implementation and observe how kernel configuration explains the speed
  difference.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-57}

Kernel launch configuration is the hidden lever of GPU performance. By
adjusting how threads and blocks are assigned, you control how much of
the GPU is kept busy, how well memory bandwidth is used, and how
smoothly computations flow. For models like GPT-2, libraries handle most
kernels, but knowing what's happening under the hood is key to writing
or debugging efficient CUDA code.

\subsection{69. CUDA Error Handling and
Debugging}\label{cuda-error-handling-and-debugging}

When writing or running CUDA code, one of the most frustrating parts is
that errors often don't show up immediately. Unlike CPU code, where an
invalid pointer or division by zero can crash right away, CUDA launches
kernels asynchronously. This means the host code (running on the CPU)
queues up GPU work and moves on, while the GPU processes it in the
background. If something goes wrong inside the kernel, the error might
not be visible until later---sometimes only after you try to
synchronize.

\subsubsection{Common Error Sources}\label{common-error-sources}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Out-of-bounds memory access A kernel thread tries to read or write
  past the end of an array. This can silently produce incorrect results
  or crash the program.
\item
  Invalid memory alignment Some CUDA operations require pointers to be
  aligned. Misaligned access can degrade performance or trigger errors.
\item
  Illegal instruction or unsupported hardware feature Using Tensor Cores
  on an older GPU, or using instructions not supported by your GPU's
  compute capability, can fail.
\item
  Out of memory (OOM) Allocating more GPU memory than available causes
  runtime errors. Unlike CPU memory, GPUs cannot ``swap'' to disk.
\item
  Race conditions Threads within a block or across blocks accessing the
  same memory location without synchronization can corrupt results.
\end{enumerate}

\subsubsection{How CUDA Reports Errors}\label{how-cuda-reports-errors}

Every CUDA runtime API call returns an error code. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cudaError\_t err }\OperatorTok{=}\NormalTok{ cudaMalloc}\OperatorTok{(\&}\NormalTok{ptr}\OperatorTok{,}\NormalTok{ size}\OperatorTok{);}
\ControlFlowTok{if} \OperatorTok{(}\NormalTok{err }\OperatorTok{!=}\NormalTok{ cudaSuccess}\OperatorTok{)} \OperatorTok{\{}
\NormalTok{    printf}\OperatorTok{(}\StringTok{"Error: }\SpecialCharTok{\%s\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ cudaGetErrorString}\OperatorTok{(}\NormalTok{err}\OperatorTok{));}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

Similarly, after launching a kernel, you should check:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_kernel}\OperatorTok{\textless{}\textless{}\textless{}}\NormalTok{blocks}\OperatorTok{,}\NormalTok{ threads}\OperatorTok{\textgreater{}\textgreater{}\textgreater{}(...);}
\NormalTok{cudaError\_t err }\OperatorTok{=}\NormalTok{ cudaGetLastError}\OperatorTok{();}
\ControlFlowTok{if} \OperatorTok{(}\NormalTok{err }\OperatorTok{!=}\NormalTok{ cudaSuccess}\OperatorTok{)} \OperatorTok{\{}
\NormalTok{    printf}\OperatorTok{(}\StringTok{"Kernel launch failed: }\SpecialCharTok{\%s\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ cudaGetErrorString}\OperatorTok{(}\NormalTok{err}\OperatorTok{));}
\OperatorTok{\}}
\NormalTok{cudaDeviceSynchronize}\OperatorTok{();} \CommentTok{// forces the GPU to finish and report errors}
\end{Highlighting}
\end{Shaded}

This pattern ensures that if something fails, you see it quickly instead
of later.

\subsubsection{Debugging Tools}\label{debugging-tools}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  cuda-gdb A GPU-aware debugger. Lets you step through CUDA kernels much
  like gdb on CPU code.
\item
  cuda-memcheck Detects out-of-bounds accesses, race conditions, and
  misaligned memory operations. Essential when kernels produce
  ``mysterious'' wrong outputs.
\item
  Nsight Systems / Nsight Compute Profiling tools that show kernel
  timelines, occupancy, memory throughput, and errors.
\item
  Sanity checks in code Often, simply inserting assertions
  (\texttt{assert(i\ \textless{}\ N)}) or zero-initializing memory can
  catch problems earlier.
\end{enumerate}

\subsubsection{\texorpdfstring{Debugging in
\emph{llm.c}}{Debugging in llm.c}}\label{debugging-in-llm.c}

In \emph{llm.c}, most of the CUDA-heavy lifting is handled by cuBLAS and
cuDNN. But when experimenting with custom kernels (e.g., softmax,
masking, or layernorm), debugging becomes crucial. A small indexing
mistake could make training diverge or crash with \texttt{nan} losses.
By adding \texttt{cudaGetLastError()} checks after every kernel launch,
you can catch issues right where they happen.

\subsubsection{Example: A Softmax Bug}\label{example-a-softmax-bug}

Imagine a kernel computing softmax across 1024 tokens per row. If one
thread index accidentally runs past 1024, it may read garbage memory.
Without error checking, you might just see ``loss is NaN'' 100 steps
later. With \texttt{cuda-memcheck}, you'd immediately see:

\begin{verbatim}
Invalid global read of size 4
    at softmax.cu:42
    by thread (1025,0,0) in block (1,0,0)
\end{verbatim}

Now you know exactly where to fix the bug.

\subsubsection{Why It Matters}\label{why-it-matters-45}

Training large models is expensive. A single bug in a CUDA kernel can
waste hours of GPU time, produce invalid gradients, or silently corrupt
weights. Robust error handling and debugging practices save not only
frustration but also significant cost.

\subsubsection{Try It Yourself}\label{try-it-yourself-57}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a CUDA kernel with an intentional bug (e.g., forget to check
  array bounds). Run it with and without \texttt{cuda-memcheck} to see
  the difference.
\item
  Add \texttt{cudaGetLastError()} after every kernel in a simple project
  and watch how it pinpoints issues earlier.
\item
  Experiment with Nsight Systems: run GPT-2 training and inspect kernel
  launches, checking for errors or unexpected stalls.
\item
  Train with bad initialization (e.g., NaNs in inputs) and see how error
  checking reports failures.
\item
  Introduce a race condition by having two threads update the same
  memory without \texttt{\_\_syncthreads()}. Debug using
  \texttt{cuda-memcheck}.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-58}

CUDA's asynchronous nature makes error handling less straightforward
than CPU programming. But with the right tools---error codes,
synchronization, cuda-memcheck, and debuggers---you can systematically
catch and fix problems. In \emph{llm.c}, this discipline ensures that
CUDA kernels not only run fast but also run correctly, which is just as
important when training large-scale models.

\subsection{\texorpdfstring{70. \texttt{dev/cuda/}: From Simple Kernels
to High
Performance}{70. dev/cuda/: From Simple Kernels to High Performance}}\label{devcuda-from-simple-kernels-to-high-performance}

Inside the \emph{llm.c} repository, there is a folder called
\texttt{dev/cuda/}. At first glance it may look like a side experiment,
but it's actually one of the most instructive parts of the project. The
main training files (\texttt{train\_gpt2.cu},
\texttt{train\_gpt2\_fp32.cu}) rely heavily on cuBLAS and
cuDNN---optimized libraries that already deliver near-peak performance.
But if you want to understand how CUDA really works under the hood, you
have to look at how kernels are written from scratch, and that's exactly
what this folder shows.

\subsubsection{Why This Folder Exists}\label{why-this-folder-exists}

The goal of \texttt{dev/cuda/} is not to replace cuBLAS or cuDNN.
Instead, it acts as a sandbox for:

\begin{itemize}
\tightlist
\item
  Building intuition about how GPU kernels are structured.
\item
  Experimenting with small-scale implementations of operations like
  vector addition, matrix multiplication, or normalization.
\item
  Comparing naive CUDA implementations to highly optimized library
  calls.
\item
  Teaching developers how memory layout, thread synchronization, and
  shared memory affect performance.
\end{itemize}

It's a bridge: start simple with ``hello world'' style kernels, then
step closer to the performance tricks used by NVIDIA's professional
libraries.

\subsubsection{A Journey from Naive to
Optimized}\label{a-journey-from-naive-to-optimized}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Simple Elementwise Kernels The first step is usually a kernel where
  each thread processes one element. For example, adding two vectors
  \texttt{C{[}i{]}\ =\ A{[}i{]}\ +\ B{[}i{]}}. This teaches indexing,
  memory coalescing, and the idea of grids and blocks.
\item
  Reduction Kernels Next, you move to slightly harder tasks like summing
  an array. Now you need thread cooperation and synchronization
  (\texttt{\_\_syncthreads()}), plus shared memory usage.
\item
  Matrix Multiplication (GEMM) A naive kernel might have each thread
  compute one output element by looping over the input dimension. It
  works, but is slow because it reloads data from global memory
  repeatedly. The optimized version uses tiling: load a tile of the
  matrix into shared memory, let threads reuse it many times, and then
  move to the next tile. This can speed up performance by 10× or more.
\item
  Advanced Optimizations Later examples may add warp-level primitives,
  vectorized loads, and Tensor Core usage. These bring performance
  closer to cuBLAS.
\end{enumerate}

\subsubsection{Educational Value}\label{educational-value}

Seeing these steps side by side makes the performance story very
tangible:

\begin{itemize}
\tightlist
\item
  A naive GEMM kernel might achieve 1\% of cuBLAS speed.
\item
  A tiled shared-memory GEMM can jump to 30--40\%.
\item
  With careful warp scheduling, it can reach 60--70\%.
\item
  cuBLAS goes further with hand-tuned assembly and Tensor Cores, pushing
  90--95\%.
\end{itemize}

This teaches that optimization is not magic---it's a sequence of logical
improvements, each shaving off inefficiencies.

\subsubsection{Why It Matters for GPT-2
Training}\label{why-it-matters-for-gpt-2-training}

Even if you never plan to reimplement matrix multiplication yourself,
understanding what happens in \texttt{dev/cuda/} helps explain why the
main training loop in \texttt{train\_gpt2.cu} is so fast. You see why
cuBLAS/cuDNN kernels are black boxes of efficiency: because writing your
own at that level is extremely hard.

But this also means you're better prepared to write custom kernels when
you need them. For example, maybe you want to test a new activation
function or a different attention mechanism. By borrowing patterns from
the experimental kernels, you can build your own, test them, and compare
to baselines.

\subsubsection{Example: Vector Add
Kernel}\label{example-vector-add-kernel}

Here's a simple kernel you might find in this folder:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_\_global\_\_ }\DataTypeTok{void}\NormalTok{ vector\_add}\OperatorTok{(}\AttributeTok{const} \DataTypeTok{float}\OperatorTok{*}\NormalTok{ A}\OperatorTok{,} \AttributeTok{const} \DataTypeTok{float}\OperatorTok{*}\NormalTok{ B}\OperatorTok{,} \DataTypeTok{float}\OperatorTok{*}\NormalTok{ C}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ N}\OperatorTok{)} \OperatorTok{\{}
    \DataTypeTok{int}\NormalTok{ i }\OperatorTok{=}\NormalTok{ blockIdx}\OperatorTok{.}\NormalTok{x }\OperatorTok{*}\NormalTok{ blockDim}\OperatorTok{.}\NormalTok{x }\OperatorTok{+}\NormalTok{ threadIdx}\OperatorTok{.}\NormalTok{x}\OperatorTok{;}
    \ControlFlowTok{if} \OperatorTok{(}\NormalTok{i }\OperatorTok{\textless{}}\NormalTok{ N}\OperatorTok{)} \OperatorTok{\{}
\NormalTok{        C}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{=}\NormalTok{ A}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{+}\NormalTok{ B}\OperatorTok{[}\NormalTok{i}\OperatorTok{];}
    \OperatorTok{\}}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

It's trivial compared to GPT-2's attention, but this is where everyone
starts. From here you scale up to 2D indexing for matrices, then to
tiled shared-memory patterns.

\subsubsection{Try It Yourself}\label{try-it-yourself-58}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run a kernel from \texttt{dev/cuda/} that does naive matrix
  multiplication. Compare its runtime to cuBLAS for the same dimensions.
\item
  Modify the naive GEMM to use tiling with shared memory. Measure how
  performance improves.
\item
  Inspect PTX (the intermediate assembly) generated by NVCC for a simple
  kernel. Observe how memory loads are translated.
\item
  Add timing code around kernels to see how much performance scales with
  different block sizes.
\item
  Implement a new custom kernel (e.g., ReLU activation) and compare its
  speed to applying ReLU via cuDNN.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-59}

The \texttt{dev/cuda/} folder is not about production training. It's
about learning and experimenting. It starts with the simplest CUDA
kernels and builds up to performance-conscious designs. This progression
mirrors how professional libraries achieve their speed. By studying and
experimenting here, you gain a deeper appreciation of what it takes to
make GPUs run at full tilt---and you gain the skills to write your own
kernels when the libraries don't provide what you need.

\section{Chapter 8. Multi-GPU and Multi-node
training}\label{chapter-8.-multi-gpu-and-multi-node-training}

\subsection{\texorpdfstring{71. Data Parallelism in
\emph{llm.c}}{71. Data Parallelism in llm.c}}\label{data-parallelism-in-llm.c}

When you want to train a large model, a single GPU often isn't enough.
Either the model doesn't fit in memory, or the training takes too long.
One of the simplest and most widely used ways to scale training across
multiple GPUs is data parallelism. The idea is conceptually simple:
instead of giving all the training data to one GPU, you split it into
smaller batches, send each GPU a piece, let them process it
independently, and then combine their results.

\subsubsection{The Core Idea}\label{the-core-idea-3}

Imagine you have a batch of 128 sequences and 4 GPUs. In data
parallelism:

\begin{itemize}
\tightlist
\item
  GPU 0 sees sequences 0--31
\item
  GPU 1 sees sequences 32--63
\item
  GPU 2 sees sequences 64--95
\item
  GPU 3 sees sequences 96--127
\end{itemize}

Each GPU runs the forward pass, computes the loss, and calculates
gradients for its slice. At the end of the step, the gradients are
averaged across GPUs, ensuring that all models stay synchronized. Every
GPU holds a full copy of the model parameters, so they are always
consistent after gradient averaging.

\subsubsection{\texorpdfstring{In
\emph{llm.c}}{In llm.c}}\label{in-llm.c-1}

The \emph{llm.c} repository keeps things minimal, so there isn't a
full-fledged DeepSpeed or PyTorch DDP implementation. But the same
principle applies:

\begin{itemize}
\tightlist
\item
  Each GPU gets a copy of the GPT-2 model.
\item
  The batch is split across devices.
\item
  After the backward pass, gradients from all GPUs must be synchronized.
\end{itemize}

This synchronization is usually done using NCCL all-reduce (covered in
the next section), but the design remains data parallel at heart.

\subsubsection{Why Data Parallelism
Works}\label{why-data-parallelism-works}

The forward and backward passes are embarrassingly parallel across
different data samples. A token in sequence A doesn't need to know about
a token in sequence B when computing gradients. As long as all GPUs
agree on parameter updates after each step, splitting the batch is
perfectly valid.

\subsubsection{Example Walkthrough}\label{example-walkthrough-4}

Let's say we're training GPT-2 on TinyStories with batch size
\texttt{B\ =\ 32} and sequence length \texttt{T\ =\ 64}.

\begin{itemize}
\tightlist
\item
  On a single GPU, the forward pass computes embeddings, attention, MLP,
  and loss for all 32 sequences.
\item
  With 2 GPUs, we set \texttt{B\ =\ 16} on each. Each GPU processes 16
  sequences in parallel.
\item
  After backpropagation, both GPUs hold gradients for their half of the
  batch. Before applying the optimizer, the gradients are averaged so
  that the weight update is equivalent to training with the full batch
  of 32.
\end{itemize}

From the model's perspective, it's as if nothing changed---it just sees
gradients from the whole batch.

\subsubsection{Memory and Speed
Benefits}\label{memory-and-speed-benefits}

\begin{itemize}
\tightlist
\item
  Memory: Each GPU stores only the activations for its local batch. This
  reduces per-GPU memory use, making it possible to train with larger
  global batches.
\item
  Speed: Training steps finish faster because multiple GPUs share the
  work. For example, doubling the number of GPUs often cuts training
  time per step nearly in half, though communication overhead prevents
  perfect scaling.
\end{itemize}

\subsubsection{Limitations}\label{limitations}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Communication Overhead Synchronizing gradients across GPUs can become
  expensive, especially with large models or when running across
  multiple nodes.
\item
  I/O Bottlenecks Feeding data to multiple GPUs fast enough requires
  efficient dataloaders and prefetching.
\item
  Optimizer State Replication With AdamW, each GPU also needs to store
  optimizer states (m and v). This means memory scales with the number
  of GPUs instead of shrinking.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-46}

Data parallelism is the workhorse of deep learning scaling. It's
conceptually easy to understand, straightforward to implement, and works
well even for large models. In practice, nearly all large-scale GPT
training begins with data parallelism, often enhanced by techniques like
gradient accumulation or mixed precision.

\subsubsection{Try It Yourself}\label{try-it-yourself-59}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train GPT-2 in \emph{llm.c} on a single GPU, then split the batch
  across two GPUs using \texttt{CUDA\_VISIBLE\_DEVICES}. Compare
  throughput and loss curves.
\item
  Experiment with increasing global batch size while keeping per-GPU
  batch size fixed. Notice how validation loss behaves.
\item
  Simulate gradient averaging by writing a simple script that averages
  arrays from two processes. Connect this idea back to how NCCL
  all-reduce works.
\item
  Measure the difference in memory usage per GPU when training with 1 vs
  2 GPUs.
\item
  Run a small experiment with different numbers of GPUs (1, 2, 4) and
  plot how training time per step changes.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-60}

Data parallelism splits the workload across GPUs by dividing the batch.
Each GPU trains a full model replica on part of the data, then
synchronizes gradients so that updates are consistent. It's simple but
powerful, forming the foundation of scaling strategies in \emph{llm.c}
and in most deep learning frameworks. Without it, training GPT-2 and
larger models on modern datasets would be impractical.

\subsection{72. MPI Process Model and GPU
Affinity}\label{mpi-process-model-and-gpu-affinity}

When you scale training beyond a single GPU, you need a way to manage
multiple processes and devices. In the \emph{llm.c} codebase, the
minimalist approach relies on MPI (Message Passing Interface), a library
that has been around for decades in high-performance computing. MPI
provides a simple abstraction: you launch multiple processes, each
assigned a rank (an ID number), and they can communicate with each other
by sending and receiving messages.

In distributed deep learning, MPI typically works alongside NCCL (NVIDIA
Collective Communications Library). MPI handles process
management---spawning workers, assigning GPUs, setting up environment
variables---while NCCL handles the actual gradient synchronization.

\subsubsection{MPI Processes and Ranks}\label{mpi-processes-and-ranks}

Suppose you want to train on 4 GPUs. MPI will start 4 processes. Each
process:

\begin{itemize}
\tightlist
\item
  Loads the same GPT-2 model code.
\item
  Initializes CUDA on one GPU.
\item
  Reads a shard of the training data or the same dataset, depending on
  setup.
\end{itemize}

Each process gets a rank:

\begin{itemize}
\tightlist
\item
  Rank 0 → GPU 0
\item
  Rank 1 → GPU 1
\item
  Rank 2 → GPU 2
\item
  Rank 3 → GPU 3
\end{itemize}

Ranks are important because they determine roles. For example, rank 0
often acts as the ``master,'' printing logs or handling checkpoints,
while the others focus purely on computation.

\subsubsection{GPU Affinity}\label{gpu-affinity}

If you don't explicitly map processes to GPUs, they can all try to use
the same device. That leads to oversubscription---multiple processes
fighting for one GPU while the others sit idle. To prevent this, you set
GPU affinity.

The environment variable \texttt{CUDA\_VISIBLE\_DEVICES} is the simplest
way to do this. For example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Run with 4 GPUs}
\ExtensionTok{mpirun} \AttributeTok{{-}np}\NormalTok{ 4 }\DataTypeTok{\textbackslash{}}
  \AttributeTok{{-}x}\NormalTok{ CUDA\_VISIBLE\_DEVICES=0,1,2,3 }\DataTypeTok{\textbackslash{}}
\NormalTok{  ./train\_gpt2\_mpi}
\end{Highlighting}
\end{Shaded}

MPI automatically assigns process 0 to GPU 0, process 1 to GPU 1, and so
on. Inside the code, you can confirm this by calling
\texttt{cudaSetDevice(rank)}.

On multi-node clusters, GPU affinity also needs to consider network
topology. You want each process close to its GPU and ideally aligned
with the node's network card for faster NCCL communication.

\subsubsection{Synchronization and
Communication}\label{synchronization-and-communication}

After each forward and backward pass, each MPI process has its local
gradients. These must be averaged across processes to keep the model
weights consistent. MPI itself offers collective operations like
\texttt{MPI\_Allreduce}, but in practice, \emph{llm.c} uses NCCL for
GPU-to-GPU communication because it is faster and topology-aware. MPI
sets up the group, NCCL does the heavy lifting.

\subsubsection{Example Workflow}\label{example-workflow}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Launch: \texttt{mpirun\ -np\ 4\ ./train\_gpt2} starts 4 processes.
\item
  Initialization: Each process determines its rank and sets its GPU with
  \texttt{cudaSetDevice(rank)}.
\item
  Training loop:

  \begin{itemize}
  \tightlist
  \item
    Forward pass on each process's GPU.
  \item
    Backward pass to compute gradients.
  \item
    Gradients averaged with NCCL All-Reduce.
  \end{itemize}
\item
  Update: Every process updates its copy of the weights.
\item
  Sync: At the next step, all model replicas are identical.
\end{enumerate}

\subsubsection{Debugging GPU Affinity
Issues}\label{debugging-gpu-affinity-issues}

If you accidentally misconfigure GPU affinity, symptoms include:

\begin{itemize}
\tightlist
\item
  Two processes trying to use the same GPU → out-of-memory errors.
\item
  GPUs left idle because no process is assigned.
\item
  Slowdowns because processes are spread inefficiently across sockets or
  PCIe lanes.
\end{itemize}

A quick way to debug is to print the rank and the GPU ID at startup:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{int}\NormalTok{ rank}\OperatorTok{;}
\NormalTok{MPI\_Comm\_rank}\OperatorTok{(}\NormalTok{MPI\_COMM\_WORLD}\OperatorTok{,} \OperatorTok{\&}\NormalTok{rank}\OperatorTok{);}
\DataTypeTok{int}\NormalTok{ device}\OperatorTok{;}
\NormalTok{cudaGetDevice}\OperatorTok{(\&}\NormalTok{device}\OperatorTok{);}
\NormalTok{printf}\OperatorTok{(}\StringTok{"Process }\SpecialCharTok{\%d}\StringTok{ is using GPU }\SpecialCharTok{\%d\textbackslash{}n}\StringTok{"}\OperatorTok{,}\NormalTok{ rank}\OperatorTok{,}\NormalTok{ device}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

\subsubsection{Why It Matters}\label{why-it-matters-47}

MPI and GPU affinity might feel like low-level plumbing, but they are
critical for scaling. If you don't get this right, training may run at a
fraction of expected speed or crash outright. For small setups (2--4
GPUs), it may feel like overkill, but for larger clusters with 8, 16, or
64 GPUs, careful mapping is the difference between success and wasted
compute time.

\subsubsection{Try It Yourself}\label{try-it-yourself-60}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train GPT-2 with \texttt{mpirun\ -np\ 2} and verify each process
  prints a different GPU ID.
\item
  Intentionally misconfigure \texttt{CUDA\_VISIBLE\_DEVICES} so both
  processes map to GPU 0, then observe the OOM error.
\item
  On a multi-GPU machine, experiment with running processes pinned to
  different GPUs. Measure training throughput.
\item
  Use \texttt{nvidia-smi\ topo\ -m} to view the PCIe topology of your
  GPUs. Try to align MPI ranks with nearby GPUs for better performance.
\item
  Print the time spent in all-reduce with different mappings to see how
  GPU affinity affects communication overhead.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-61}

MPI is the backbone for managing multiple processes in distributed
training, and GPU affinity ensures each process has exclusive access to
the right device. Together, they lay the groundwork for efficient
multi-GPU training in \emph{llm.c}. Get these details right, and scaling
is smooth; get them wrong, and you run into memory crashes, idle GPUs,
or bottlenecked communication.

\subsection{73. NCCL All-Reduce for Gradient
Sync}\label{nccl-all-reduce-for-gradient-sync}

Once each GPU finishes its forward and backward pass, it has a set of
gradients that reflect only its portion of the training data. To keep
the model parameters consistent across all GPUs, these gradients must be
synchronized. The standard way to do this in modern deep learning
systems is with All-Reduce, and NVIDIA's NCCL (NVIDIA Collective
Communications Library, pronounced ``Nickel'') provides the optimized
implementation.

\subsubsection{What All-Reduce Does}\label{what-all-reduce-does}

All-Reduce is a collective communication operation. Every process (GPU)
starts with its own local buffer of values---here, the gradients---and
the operation combines them (using a reduction, usually summation) and
distributes the result back to all processes.

Mathematically, if GPU 0 has \texttt{g0}, GPU 1 has \texttt{g1}, GPU 2
has \texttt{g2}, and GPU 3 has \texttt{g3}, then after All-Reduce, each
GPU has the same result:

\begin{verbatim}
g_all = (g0 + g1 + g2 + g3) / 4
\end{verbatim}

The division by 4 is optional---it depends whether you want a sum or an
average---but averaging is common for gradient updates.

This ensures that every GPU applies the same weight update and stays
synchronized with the others.

\subsubsection{Why NCCL?}\label{why-nccl}

While MPI provides an All-Reduce primitive, NCCL is specifically
optimized for GPUs. It knows about PCIe, NVLink, NVSwitch, and
Infiniband topologies and arranges communication to maximize bandwidth
and minimize latency. Some of its key strategies include:

\begin{itemize}
\tightlist
\item
  Ring All-Reduce: GPUs are arranged in a ring. Each GPU sends its data
  to the next while receiving from the previous, accumulating partial
  sums as the data flows. This scales well with many GPUs.
\item
  Tree All-Reduce: Organizes communication as a tree, reducing depth
  (latency) at the cost of bandwidth.
\item
  Hybrid schemes: NCCL dynamically chooses strategies depending on GPU
  count and topology.
\end{itemize}

By exploiting topology awareness, NCCL can saturate available
communication channels.

\subsubsection{\texorpdfstring{Example in \emph{llm.c}
Training}{Example in llm.c Training}}\label{example-in-llm.c-training}

In the CPU-only training loop, gradients are updated directly without
communication. In the multi-GPU CUDA path, after backpropagation
(\texttt{gpt2\_backward}), each GPU has its local gradients in memory.
At this point:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ncclAllReduce}\OperatorTok{(}\NormalTok{model}\OperatorTok{.}\NormalTok{grads\_memory}\OperatorTok{,}
\NormalTok{              model}\OperatorTok{.}\NormalTok{grads\_memory}\OperatorTok{,}
\NormalTok{              model}\OperatorTok{.}\NormalTok{num\_parameters}\OperatorTok{,}
\NormalTok{              ncclFloat32}\OperatorTok{,}  \CommentTok{// or half precision}
\NormalTok{              ncclSum}\OperatorTok{,}
\NormalTok{              comm}\OperatorTok{,}\NormalTok{ stream}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

After this call, \texttt{model.grads\_memory} on every GPU contains the
summed gradients across all GPUs. Dividing by the number of GPUs turns
it into the average.

\subsubsection{Why Gradient Sync
Matters}\label{why-gradient-sync-matters}

Without gradient synchronization, each GPU would drift apart, updating
weights independently. This would be equivalent to training multiple
smaller models rather than one unified model. Synchronization makes sure
all replicas behave like a single large-batch training job.

\subsubsection{Memory and Performance
Considerations}\label{memory-and-performance-considerations}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Bandwidth-bound: Gradient synchronization often dominates runtime as
  model size grows. For GPT-2 774M, gradients alone can be several GB
  per step.
\item
  Overlapping Communication with Compute: Advanced systems overlap
  gradient exchange with backward computation. While later layers are
  computing gradients, earlier layers are already being synchronized.
\item
  Precision: Gradients can be synchronized in FP16/BF16 to cut
  communication bandwidth in half. This is called gradient compression.
\end{enumerate}

\subsubsection{Analogy}\label{analogy-4}

Think of four chefs cooking the same dish in separate kitchens. Each
chef tastes their own version and suggests adjustments (gradients). If
they don't talk, their recipes diverge. With All-Reduce, the chefs share
notes, average their adjustments, and apply the same changes---so all
four kitchens end up cooking the same dish.

\subsubsection{Try It Yourself}\label{try-it-yourself-61}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run training on 2 GPUs with and without gradient synchronization (by
  commenting out All-Reduce). Watch how quickly the models diverge in
  loss.
\item
  Use NCCL's \texttt{NCCL\_DEBUG=INFO} environment variable to print
  communication patterns. Observe the chosen ring/tree strategies.
\item
  Experiment with FP32 vs FP16 gradient synchronization and measure
  bandwidth savings.
\item
  Profile training with \texttt{nsys} or \texttt{nvprof} to see how much
  time is spent in All-Reduce.
\item
  Scale from 2 GPUs to 4 or 8 and measure how synchronization overhead
  grows.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-62}

NCCL All-Reduce is the backbone of multi-GPU training in \emph{llm.c}.
It ensures that gradients computed on separate GPUs are combined into a
single, consistent update. By leveraging topology-aware algorithms like
ring and tree reductions, NCCL keeps synchronization efficient even as
models and GPU counts scale up. Without it, distributed training would
produce inconsistent, drifting models rather than a unified one.

\subsection{74. Building and Running Multi-GPU
Trainers}\label{building-and-running-multi-gpu-trainers}

Getting multiple GPUs to cooperate isn't automatic---you need to set up
the environment, initialize communication, and make sure each process
knows which GPU to use. In \emph{llm.c}, the design is intentionally
minimalist, but it still has to integrate with MPI (Message Passing
Interface) and NCCL to allow training across several GPUs.

\subsubsection{Step 1: MPI Launch}\label{step-1-mpi-launch}

Multi-GPU training starts with MPI. You don't run the program once; you
launch it with \texttt{mpirun} or \texttt{mpiexec}, which spawns one
process per GPU. For example:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{mpirun} \AttributeTok{{-}np}\NormalTok{ 4 ./train\_gpt2\_cu}
\end{Highlighting}
\end{Shaded}

Here, \texttt{-np\ 4} starts four processes. Each process will attach
itself to one GPU.

MPI provides:

\begin{itemize}
\tightlist
\item
  Rank: a unique ID for each process (0, 1, 2, 3).
\item
  World size: the total number of processes (here, 4).
\end{itemize}

Each process knows who it is and how many peers it has.

\subsubsection{Step 2: GPU Assignment}\label{step-2-gpu-assignment}

Once MPI assigns ranks, each process must select a GPU. This is often
done with:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cudaSetDevice}\OperatorTok{(}\NormalTok{rank}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

So process 0 gets GPU 0, process 1 gets GPU 1, and so on. Without this
step, processes might all pile onto the same GPU, leading to chaos.

\subsubsection{Step 3: NCCL
Communicator}\label{step-3-nccl-communicator}

Next, the code creates an NCCL communicator. Think of it as a
``conference call'' between all GPUs. NCCL sets up the communication
paths (rings, trees) across devices. A typical setup looks like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ncclCommInitRank}\OperatorTok{(\&}\NormalTok{comm}\OperatorTok{,}\NormalTok{ world\_size}\OperatorTok{,}\NormalTok{ nccl\_id}\OperatorTok{,}\NormalTok{ rank}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

Here:

\begin{itemize}
\tightlist
\item
  \texttt{world\_size} is the number of GPUs.
\item
  \texttt{nccl\_id} is a shared identifier obtained via MPI (all
  processes must use the same one).
\item
  \texttt{rank} is the local ID.
\end{itemize}

Now the GPUs can talk to each other.

\subsubsection{Step 4: Training Loop
Integration}\label{step-4-training-loop-integration}

Once communication is established, the training loop doesn't look
dramatically different. Each GPU:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Loads its own batch of data (so the dataset is divided across GPUs).
\item
  Runs the forward pass.
\item
  Runs the backward pass.
\item
  Calls NCCL All-Reduce to sync gradients.
\item
  Updates parameters.
\end{enumerate}

The only new ingredient is step 4. Without it, each GPU would wander off
with its own gradients.

\subsubsection{Example Command}\label{example-command}

Suppose you have 2 GPUs on your machine. You can train with:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{mpirun} \AttributeTok{{-}np}\NormalTok{ 2 ./train\_gpt2\_cu }\AttributeTok{{-}batch\_size}\NormalTok{ 8 }\AttributeTok{{-}seq\_len}\NormalTok{ 128}
\end{Highlighting}
\end{Shaded}

Each GPU trains on 8 sequences of 128 tokens. Combined, it's like
training with batch size 16, but split across GPUs.

\subsubsection{Common Pitfalls}\label{common-pitfalls-1}

\begin{itemize}
\tightlist
\item
  Forgetting to set device by rank: all processes fight for GPU 0.
\item
  Mismatched NCCL IDs: communicator fails to initialize.
\item
  MPI vs NCCL versions: some builds are picky, and you may need to
  recompile with matching CUDA/NCCL.
\item
  Networking issues: on multi-node setups, firewalls or missing
  InfiniBand drivers can block communication.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-48}

Building a multi-GPU trainer is the gateway to scaling. A single GPU may
take weeks to train a large model, but spreading the work across 4, 8,
or 16 GPUs cuts the time dramatically. The simplicity of \emph{llm.c}
shows that distributed training doesn't require a massive
framework---just careful use of MPI and NCCL.

\subsubsection{Try It Yourself}\label{try-it-yourself-62}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Launch training with 1 GPU and then 2 GPUs, keeping the global batch
  size the same. Compare training speed.
\item
  Launch with 2 GPUs but forget All-Reduce. Notice how validation loss
  behaves differently on each GPU.
\item
  Use \texttt{NCCL\_DEBUG=INFO} to see how NCCL sets up communication.
\item
  Try deliberately mismatching ranks and devices---observe the crash to
  understand why assignment matters.
\item
  Measure GPU utilization with \texttt{nvidia-smi} during training to
  confirm both GPUs are working.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-63}

Multi-GPU trainers in \emph{llm.c} are built around three pillars: MPI
to manage processes, NCCL to synchronize gradients, and CUDA to run the
math. Once these are in place, the training loop remains familiar, but
the computation spreads across GPUs seamlessly. This design keeps the
code minimal while still unlocking significant scaling power.

\subsection{75. Multi-Node Bootstrapping with
MPI}\label{multi-node-bootstrapping-with-mpi}

So far, running across multiple GPUs on a single machine is relatively
straightforward: every process talks through shared memory or high-speed
interconnects like NVLink. Things become more interesting when training
has to scale across multiple machines (often called ``nodes'')---for
example, when you want to run on 2 servers, each with 4 GPUs, to make a
total of 8.

\subsubsection{The MPI World}\label{the-mpi-world}

MPI was designed for this. When you run:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{mpirun} \AttributeTok{{-}np}\NormalTok{ 8 }\AttributeTok{{-}hostfile}\NormalTok{ myhosts ./train\_gpt2\_cu}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{-np\ 8} says you want 8 processes.
\item
  \texttt{-hostfile\ myhosts} lists the machines (and how many processes
  to run on each).
\end{itemize}

MPI then launches processes across nodes and assigns each one a rank.
From the program's perspective, it doesn't matter if two ranks are on
the same machine or different machines---they all see a global
communicator of size 8.

\subsubsection{Setting Up NCCL Across
Nodes}\label{setting-up-nccl-across-nodes}

NCCL doesn't know how to find other machines by itself. It relies on MPI
to exchange a unique NCCL ID. The typical flow is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Rank 0 creates a new NCCL ID.
\item
  Rank 0 broadcasts the ID to all other ranks using MPI.
\item
  Each process calls \texttt{ncclCommInitRank} with the shared ID, total
  world size, and its own rank.
\end{enumerate}

This ensures all GPUs, even across different machines, join the same
``conference call.''

\subsubsection{Networking
Considerations}\label{networking-considerations}

When scaling across nodes, networking becomes critical:

\begin{itemize}
\item
  Ethernet vs InfiniBand: Standard Ethernet works but can be slow.
  High-performance clusters use InfiniBand for much higher bandwidth and
  lower latency.
\item
  Firewall rules: NCCL needs open ports to connect nodes. Firewalls or
  strict security settings can block communication.
\item
  Environment variables: Variables like \texttt{NCCL\_SOCKET\_IFNAME}
  (to pick the right network interface) often need to be set. For
  example:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{export} \VariableTok{NCCL\_SOCKET\_IFNAME}\OperatorTok{=}\NormalTok{eth0}
\end{Highlighting}
\end{Shaded}
\end{itemize}

\subsubsection{Example Hostfile}\label{example-hostfile}

A simple \texttt{myhosts} file could look like this:

\begin{verbatim}
node1 slots=4
node2 slots=4
\end{verbatim}

This says node1 and node2 each have 4 GPUs. MPI will launch 4 processes
on each, totaling 8.

\subsubsection{Synchronization Across
Nodes}\label{synchronization-across-nodes}

Because communication now spans machines, synchronization overhead
becomes more visible. Gradient All-Reduce has to move data not only
between GPUs in one server but also across the network. Efficient
scaling depends on:

\begin{itemize}
\tightlist
\item
  Large enough batch sizes (so compute time outweighs communication).
\item
  Overlapping communication with computation (advanced optimization).
\item
  Fast interconnects between machines.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-49}

Training large models rarely happens on a single machine. Multi-node
training is how researchers and companies scale models to billions of
parameters. By showing how to bootstrap MPI and NCCL across nodes,
\emph{llm.c} demonstrates the foundation of distributed AI training
systems, but in a minimal and transparent way.

\subsubsection{Try It Yourself}\label{try-it-yourself-63}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Prepare two machines with CUDA and NCCL installed, connected via the
  same network.
\item
  Write a hostfile listing both machines, then launch with
  \texttt{mpirun}.
\item
  Set \texttt{NCCL\_DEBUG=INFO} to watch how NCCL connects across nodes.
\item
  Compare throughput between single-node and two-node runs with the same
  number of GPUs.
\item
  Experiment with environment variables like
  \texttt{NCCL\_SOCKET\_IFNAME} or \texttt{NCCL\_IB\_DISABLE=1} to see
  how network choices affect speed.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-64}

Bootstrapping multi-node training is about extending the same principles
as single-node multi-GPU training, but with networking in the mix. MPI
handles process management, NCCL sets up communication, and CUDA runs
the math. With just a few lines of setup, \emph{llm.c} can stretch from
one GPU on your laptop to dozens of GPUs spread across multiple servers.

\subsection{76. SLURM and PMIx Caveats}\label{slurm-and-pmix-caveats}

On many research clusters or supercomputers, you don't launch jobs
manually with \texttt{mpirun} and a hostfile. Instead, you interact with
a job scheduler, most commonly SLURM. SLURM takes care of allocating
resources, starting processes across nodes, and enforcing quotas. While
this saves you from manually managing hostfiles, it introduces its own
set of details that you need to understand.

\subsubsection{SLURM Basics}\label{slurm-basics}

In SLURM, you typically request GPUs and nodes using a script or command
like:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{salloc} \AttributeTok{{-}N}\NormalTok{ 2 }\AttributeTok{{-}G}\NormalTok{ 8 }\AttributeTok{{-}{-}time}\OperatorTok{=}\NormalTok{01:00:00}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{-N\ 2} asks for 2 nodes.
\item
  \texttt{-G\ 8} requests 8 GPUs (across those nodes).
\item
  \texttt{-\/-time=01:00:00} sets a one-hour time limit.
\end{itemize}

Once the job starts, SLURM sets environment variables such as:

\begin{itemize}
\tightlist
\item
  \texttt{SLURM\_NTASKS}: total number of tasks (processes).
\item
  \texttt{SLURM\_PROCID}: the rank of the current process.
\item
  \texttt{SLURM\_NODEID}: which node this process is running on.
\end{itemize}

MPI implementations (OpenMPI, MPICH) and NCCL can use these to bootstrap
communication automatically.

\subsubsection{PMIx Integration}\label{pmix-integration}

Modern SLURM often works with PMIx (Process Management Interface for
Exascale). PMIx allows MPI and other runtimes to query process
information directly from SLURM without relying on older launchers. In
practice, this means:

\begin{itemize}
\item
  You might not use \texttt{mpirun} at all. Instead, SLURM provides
  \texttt{srun}.
\item
  For example:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{srun} \AttributeTok{{-}n}\NormalTok{ 8 ./train\_gpt2\_cu}
\end{Highlighting}
\end{Shaded}

  Here \texttt{-n\ 8} launches 8 tasks across your allocated nodes.
  SLURM/PMIx handles the rank assignments.
\end{itemize}

\subsubsection{Common Pitfalls}\label{common-pitfalls-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  MPI version mismatch If your cluster has multiple MPI libraries
  installed, you may accidentally compile with one and run with another.
  Always confirm that the \texttt{mpicc} and \texttt{mpirun} you're
  using match the library your job is linking against.
\item
  Environment variable propagation NCCL relies on environment variables
  like \texttt{NCCL\_DEBUG}, \texttt{NCCL\_SOCKET\_IFNAME}, and
  \texttt{NCCL\_IB\_HCA}. Sometimes SLURM doesn't forward these to all
  nodes unless you configure it to. Using \texttt{-\/-export=ALL} or
  adding exports in your job script can fix this.
\item
  GPU visibility SLURM manages GPU allocation via
  \texttt{CUDA\_VISIBLE\_DEVICES}. Each process only ``sees'' the GPUs
  it was assigned. If your code assumes a global view of all GPUs, it
  may break. In \emph{llm.c}, the mapping between rank and GPU ID needs
  to respect this.
\item
  Network fabric mismatches On big clusters, you may have multiple
  network fabrics (Ethernet, InfiniBand). If NCCL picks the wrong one,
  performance plummets. Explicitly setting \texttt{NCCL\_SOCKET\_IFNAME}
  or \texttt{NCCL\_IB\_DISABLE} can solve this.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-50}

Learning to run across nodes with SLURM is essential if you want to
scale training beyond a single server. While local \texttt{mpirun}
commands work for development, almost all serious training
runs---whether academic or industrial---happen under SLURM or a similar
workload manager. Understanding the quirks of SLURM and PMIx ensures
that your code scales smoothly without mysterious hangs or slowdowns.

\subsubsection{Try It Yourself}\label{try-it-yourself-64}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a small SLURM job script that requests 2 GPUs for 10 minutes and
  runs a dummy \emph{llm.c} training loop.
\item
  Use \texttt{srun} to launch the program and print out the
  \texttt{SLURM\_PROCID} and \texttt{SLURM\_NODEID} for each process.
\item
  Set \texttt{NCCL\_DEBUG=INFO} in your job script and observe how NCCL
  initializes communication.
\item
  Experiment with \texttt{srun\ -\/-ntasks-per-node} to control how many
  processes land on each node.
\item
  Intentionally misconfigure \texttt{CUDA\_VISIBLE\_DEVICES} to see how
  it affects rank-to-GPU mapping.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-65}

SLURM and PMIx streamline distributed training on large clusters, but
they add another layer of complexity. The principles remain the
same---MPI ranks, NCCL communicators, and CUDA kernels---but the
scheduler decides how processes are placed and how environments are set
up. With a bit of practice, these tools allow \emph{llm.c} to move from
simple multi-GPU experiments to scalable cluster-wide training runs.

\subsection{77. Debugging Multi-GPU Hangs and
Stalls}\label{debugging-multi-gpu-hangs-and-stalls}

When training on multiple GPUs, one of the most frustrating experiences
is a job that simply hangs --- no errors, no crashes, just frozen
processes. In distributed deep learning, hangs are almost always related
to synchronization mismatches. Every GPU worker is supposed to ``meet
up'' at communication points (like gradient all-reduce), and if even one
process gets lost, the whole group stalls.

\subsubsection{Common Causes of Hangs}\label{common-causes-of-hangs}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Mismatched Collective Calls If one rank calls \texttt{ncclAllReduce}
  while another rank skips it or calls \texttt{ncclBroadcast}, the
  system deadlocks. All GPUs wait forever because they're not speaking
  the same ``language'' at that step.
\item
  Uneven Batch Sizes If the training data isn't perfectly divisible
  across GPUs, one process might run out of data earlier than others.
  The code tries to sync gradients, but some ranks never reach that
  point.
\item
  CUDA Errors Silently Ignored A kernel launch failure on one GPU can
  prevent it from reaching synchronization. If error checks are missing,
  you won't see the failure until the program is stuck.
\item
  Networking Issues NCCL depends on reliable network connections. If one
  node has a bad InfiniBand card, firewall rule, or misconfigured
  interface, communication halts.
\end{enumerate}

\subsubsection{Debugging Strategies}\label{debugging-strategies}

\begin{itemize}
\item
  Enable NCCL Debugging Set:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{export} \VariableTok{NCCL\_DEBUG}\OperatorTok{=}\NormalTok{INFO}
\BuiltInTok{export} \VariableTok{NCCL\_DEBUG\_SUBSYS}\OperatorTok{=}\NormalTok{ALL}
\end{Highlighting}
\end{Shaded}

  This produces logs showing when each rank enters and leaves collective
  operations. By comparing ranks, you can see who got stuck.
\item
  Check CUDA Errors Always wrap CUDA calls with error checks, or run
  with:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{cuda{-}memcheck}\NormalTok{ ./train\_gpt2\_cu}
\end{Highlighting}
\end{Shaded}

  This detects invalid memory access or kernel failures that might lead
  to stalls.
\item
  Simplify the Setup Start with 2 GPUs on a single node. If it works,
  increase to 4 GPUs, then expand to multiple nodes. This isolates
  whether the bug is in GPU logic or network communication.
\item
  Timeouts and Watchdogs NCCL provides environment variables like
  \texttt{NCCL\_TIMEOUT} that can help detect when a collective is
  stalled. Although it won't fix the hang, it prevents wasting hours
  waiting for nothing.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-51}

In multi-GPU training, hangs are not rare --- they are part of the
debugging journey. Understanding that hangs usually mean ``one rank is
out of sync'' helps you approach the problem methodically. By checking
logs, validating batch sizes, and carefully testing collective calls,
you avoid endless frustration and wasted GPU hours.

\subsubsection{Try It Yourself}\label{try-it-yourself-65}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run a 2-GPU training job and intentionally misconfigure the code so
  only one rank calls \texttt{gpt2\_backward}. Observe how the system
  hangs.
\item
  Enable \texttt{NCCL\_DEBUG=INFO} and compare logs between the two
  ranks.
\item
  Modify the dataloader so that one GPU gets fewer batches than the
  other. Watch how training stalls at the first gradient sync.
\item
  Experiment with \texttt{cuda-memcheck} to catch silent CUDA errors in
  a simple kernel.
\item
  Practice scaling up from 1 node to 2 nodes to see where hangs are more
  likely to appear.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-66}

Hangs in distributed training almost always trace back to mismatched
synchronization, unbalanced workloads, or hidden errors. By using NCCL's
debug tools, adding error checks, and testing systematically, you can
turn mysterious freezes into solvable problems. Multi-GPU training isn't
just about raw speed --- it's about learning to keep many moving parts
in lockstep.

\subsection{78. Scaling Stories: GPT-2 124M → 774M →
1.6B}\label{scaling-stories-gpt-2-124m-774m-1.6b}

One of the most exciting parts of \emph{llm.c} is that it doesn't just
stop at toy models. The same code that trains a small GPT-2 model on
Tiny Shakespeare can be scaled up to much larger models, like GPT-2 774M
and even 1.6B. But scaling isn't just about making the numbers bigger
--- it changes almost everything about how you train: memory
requirements, communication costs, optimizer stability, and even your
workflow.

\subsubsection{Starting Small: GPT-2
124M}\label{starting-small-gpt-2-124m}

The 124M parameter model is the ``hello world'' of GPT-2 training. It
fits comfortably on a single modern GPU, and you can even run a
trimmed-down version on CPU. At this size:

\begin{itemize}
\tightlist
\item
  Batch sizes can stay small (e.g., 4--8).
\item
  Memory requirements are modest --- a few gigabytes of VRAM.
\item
  Training speed is relatively fast, so you can iterate quickly.
\item
  Purpose: sanity checks, debugging kernels, verifying correctness.
\end{itemize}

Think of 124M as the training wheels stage: you're learning to balance,
not yet racing.

\subsubsection{Moving to GPT-2 774M}\label{moving-to-gpt-2-774m}

At \textasciitilde774M parameters, the picture changes:

\begin{itemize}
\tightlist
\item
  A single GPU can still \emph{fit} the model, but training speed slows
  dramatically.
\item
  Gradient synchronization across multiple GPUs becomes essential to get
  reasonable throughput.
\item
  Communication costs start to matter: an all-reduce of hundreds of
  megabytes per step stresses both PCIe and network bandwidth.
\item
  Stability becomes more sensitive: learning rates and warmup schedules
  need more careful tuning.
\end{itemize}

Here, training is less about ``does the code run?'' and more about
``does the system scale?'' This size is often used in academic
replications of GPT-2 because it's large enough to be interesting but
not impossibly expensive.

\subsubsection{GPT-2 1.6B: Scaling to the
Edge}\label{gpt-2-1.6b-scaling-to-the-edge}

At 1.6B parameters, the model is too large for a single GPU to train
efficiently. You need:

\begin{itemize}
\tightlist
\item
  Multi-GPU setups with NCCL all-reduce to share gradient updates.
\item
  Multi-node training on clusters when even 8 GPUs aren't enough.
\item
  Careful optimizer tuning --- without proper settings for AdamW and
  schedulers, the model may diverge.
\item
  Memory tricks like mixed precision (FP16/BF16) and gradient
  checkpointing to fit activations in memory.
\end{itemize}

Training GPT-2 1.6B is a significant engineering challenge, but it
proves that \emph{llm.c} is not just a toy project --- it's a minimal
yet real implementation that can push to billion-parameter scale.

\subsubsection{Scaling Lessons}\label{scaling-lessons}

As you climb from 124M to 774M to 1.6B, several lessons emerge:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Debug small, scale big --- always test on 124M before attempting
  larger models.
\item
  Communication dominates --- at 774M and beyond, time spent moving
  gradients often exceeds compute time.
\item
  Hyperparameters evolve --- a learning rate that works for 124M may
  explode the loss at 1.6B.
\item
  Infrastructure matters --- GPUs, interconnects, and schedulers become
  as important as code.
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-52}

Scaling stories show that deep learning isn't just about writing a
clever algorithm --- it's about making that algorithm work under
increasingly heavy loads. Each jump in size uncovers new bottlenecks and
new engineering challenges. By following this path, you gain intuition
for how large models are really trained in practice.

\subsubsection{Try It Yourself}\label{try-it-yourself-66}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train GPT-2 124M on Tiny Shakespeare until the loss stabilizes. Record
  how long each step takes.
\item
  Attempt the same experiment on OpenWebText with 124M --- watch how
  dataset size now becomes the limiting factor.
\item
  Scale up to GPT-2 355M or 774M if you have access to multiple GPUs.
  Measure how much time is spent in NCCL all-reduce compared to compute.
\item
  If you have cluster access, try running 774M with \texttt{srun} or
  \texttt{mpirun} across nodes.
\item
  Study published training logs for GPT-2 1.6B and compare them to your
  own --- how does scaling change the shape of the loss curve?
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-67}

Scaling isn't just ``bigger models need bigger GPUs.'' Each increase in
model size reshapes the training process, introducing new bottlenecks
and requiring new techniques. \emph{llm.c} is valuable precisely because
it makes these transitions transparent: you can start with a tiny model
and gradually experience the real engineering hurdles of training
state-of-the-art language models.

\subsection{79. NCCL Tuning and Overlap
Opportunities}\label{nccl-tuning-and-overlap-opportunities}

Once your training runs extend beyond a single GPU, communication
overhead becomes a central challenge. Every training step requires
gradients to be exchanged among GPUs so that the optimizer updates stay
synchronized. This is where NCCL (NVIDIA Collective Communications
Library) comes in. NCCL provides efficient implementations of collective
operations like all-reduce, all-gather, and broadcast. But simply using
NCCL isn't enough: how you tune it, and how you overlap communication
with computation, can make the difference between sluggish training and
near-linear scaling.

\subsubsection{How NCCL Works in
Training}\label{how-nccl-works-in-training}

In \emph{llm.c}, when multiple GPUs train together, each GPU computes
its local gradients during backpropagation. At the end of the backward
pass, NCCL's all-reduce combines gradients across GPUs so that every GPU
ends up with the same values. Only then can the optimizer step forward.

Without NCCL, you'd have to write custom point-to-point code with
\texttt{cudaMemcpyPeer} or MPI, which would be both slower and harder to
maintain. NCCL ensures the communication pattern is efficient for the
underlying hardware --- PCIe, NVLink, or InfiniBand.

\subsubsection{Key NCCL Tuning
Parameters}\label{key-nccl-tuning-parameters}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  NCCL\_DEBUG Setting \texttt{NCCL\_DEBUG=INFO} helps you understand
  what NCCL is doing. For performance tuning, logs are essential.
\item
  NCCL\_SOCKET\_IFNAME On multi-node clusters, this decides which
  network interface NCCL binds to. Using the wrong interface (like
  Ethernet instead of InfiniBand) can slow training by orders of
  magnitude.
\item
  NCCL\_ALGO Determines how collectives are executed:

  \begin{itemize}
  \tightlist
  \item
    \emph{Ring}: good for large message sizes, stable performance.
  \item
    \emph{Tree}: faster for small messages, less latency. Some training
    runs benefit from experimenting with both.
  \end{itemize}
\item
  NCCL\_IB\_DISABLE Useful if you want to force NCCL to avoid InfiniBand
  and stick with TCP/IP, usually for debugging network issues.
\end{enumerate}

\subsubsection{Overlapping Communication with
Computation}\label{overlapping-communication-with-computation}

The backward pass doesn't need to wait until all gradients are computed
before starting to communicate. In fact, gradients for earlier layers
can start their all-reduce while later layers are still computing
gradients. This is called communication-computation overlap.

For example:

\begin{itemize}
\tightlist
\item
  Without overlap: compute gradients for all layers → run NCCL
  all-reduce for all gradients → update parameters.
\item
  With overlap: while gradients for higher layers are still being
  computed, start the all-reduce for earlier layers.
\end{itemize}

This reduces idle time and often leads to substantial throughput gains.
Some frameworks (like PyTorch's DistributedDataParallel) implement this
automatically. In a low-level system like \emph{llm.c}, this would
require careful kernel launch ordering and stream management.

\subsubsection{Practical Example}\label{practical-example}

Imagine you're training GPT-2 774M across 8 GPUs. Each backward pass
produces \textasciitilde3 GB of gradients. If you wait until all
gradients are ready before syncing, the all-reduce might take 200 ms. If
your compute step also takes 200 ms, then half of your training time is
spent idle. With overlap, you can hide much of that communication inside
compute time, potentially cutting step time almost in half.

\subsubsection{Why It Matters}\label{why-it-matters-53}

As model sizes increase, communication costs can rival or exceed
computation. Without tuning, GPUs spend more time waiting for data to
arrive than actually training the model. By understanding NCCL and
applying overlap techniques, you unlock the ability to scale efficiently
to dozens or even hundreds of GPUs.

\subsubsection{Try It Yourself}\label{try-it-yourself-67}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run a multi-GPU training job with \texttt{NCCL\_DEBUG=INFO} enabled
  and watch the communication patterns.
\item
  Change \texttt{NCCL\_ALGO} between \texttt{Ring} and \texttt{Tree} and
  measure the effect on step times.
\item
  Experiment with setting \texttt{CUDA\_LAUNCH\_BLOCKING=1} to remove
  overlap, then remove it again to see how communication and computation
  interleave.
\item
  If you have a cluster, try forcing NCCL to use Ethernet instead of
  InfiniBand and compare bandwidth.
\item
  Profile a multi-GPU run using \texttt{nvprof} or Nsight Systems and
  check whether NCCL collectives overlap with kernel execution.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-68}

Efficient distributed training is not only about having more GPUs ---
it's about keeping them busy. NCCL provides the communication backbone,
but how you configure and overlap its operations determines whether you
get close to linear scaling or waste resources. Mastering these details
transforms multi-GPU training from ``just working'' into truly efficient
large-scale computation.

\subsection{80. Common Multi-GPU Errors and
Fixes}\label{common-multi-gpu-errors-and-fixes}

When running \emph{llm.c} across multiple GPUs, errors can range from
confusing hangs to cryptic NCCL messages. These problems are normal in
distributed training, but they can eat up hours unless you recognize the
patterns. The good news is that most errors fall into a handful of
common categories, and once you learn the typical causes, they're easier
to diagnose and fix.

\subsubsection{Error Type 1: Process Hangs with No
Output}\label{error-type-1-process-hangs-with-no-output}

Symptom: Training starts but then freezes. No error message, no crash,
just silence. Cause: Usually, one or more ranks are out of sync. This
could mean:

\begin{itemize}
\tightlist
\item
  Different batch sizes on each rank (one rank has fewer tokens left).
\item
  A mismatch in collective calls --- for example, one GPU calls
  \texttt{all\_reduce} while another skips it.
\item
  A CUDA error in one process that prevents it from reaching
  synchronization. Fix:
\item
  Check that dataloaders feed the same number of steps to every rank.
\item
  Add error checking to CUDA calls.
\item
  Enable \texttt{NCCL\_DEBUG=INFO} to trace which rank got stuck.
\end{itemize}

\subsubsection{\texorpdfstring{Error Type 2:
\texttt{NCCL\ WARN\ Net:\ no\ interface\ found}}{Error Type 2: NCCL WARN Net: no interface found}}\label{error-type-2-nccl-warn-net-no-interface-found}

Symptom: NCCL reports it can't find a network interface, or training is
extremely slow. Cause: NCCL can't discover the correct interface to use
for inter-node communication. By default, it may try Ethernet instead of
InfiniBand. Fix:

\begin{itemize}
\item
  Set \texttt{NCCL\_SOCKET\_IFNAME} to the right interface, e.g.:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{export} \VariableTok{NCCL\_SOCKET\_IFNAME}\OperatorTok{=}\NormalTok{ib0}
\end{Highlighting}
\end{Shaded}
\item
  Confirm with your sysadmin which network interfaces are available for
  high-performance GPU communication.
\end{itemize}

\subsubsection{\texorpdfstring{Error Type 3:
\texttt{CUDA\_ERROR\_OUT\_OF\_MEMORY}}{Error Type 3: CUDA\_ERROR\_OUT\_OF\_MEMORY}}\label{error-type-3-cuda_error_out_of_memory}

Symptom: Processes crash when allocating model weights or during the
backward pass. Cause:

\begin{itemize}
\tightlist
\item
  Model too large for the available GPU memory.
\item
  Batch size too high.
\item
  Memory fragmentation from repeated allocations. Fix:
\item
  Reduce batch size \texttt{B} or sequence length \texttt{T}.
\item
  Try mixed precision (FP16/BF16) if supported.
\item
  Restart processes to clear memory fragmentation.
\end{itemize}

\subsubsection{\texorpdfstring{Error Type 4:
\texttt{unhandled\ system\ error,\ NCCL\ version\ mismatch}}{Error Type 4: unhandled system error, NCCL version mismatch}}\label{error-type-4-unhandled-system-error-nccl-version-mismatch}

Symptom: One process logs NCCL version \texttt{2.17} and another logs
\texttt{2.14}. Training fails. Cause: Different NCCL libraries are being
used across nodes. This happens when software environments aren't
identical. Fix:

\begin{itemize}
\tightlist
\item
  Use the same container or module environment on all nodes.
\item
  Confirm NCCL versions with \texttt{ldd} or \texttt{conda\ list}.
\end{itemize}

\subsubsection{Error Type 5: Validation Loss Diverges on Multi-GPU but
Not on Single
GPU}\label{error-type-5-validation-loss-diverges-on-multi-gpu-but-not-on-single-gpu}

Symptom: Loss values explode only when running across multiple GPUs.
Cause: Gradient synchronization may be broken --- for example, only a
subset of parameters are being all-reduced. Another possibility is using
a different effective learning rate because of batch size scaling. Fix:

\begin{itemize}
\tightlist
\item
  Confirm that all parameters participate in gradient synchronization.
\item
  Scale the learning rate properly: if you double the global batch size
  by using more GPUs, you may need to adjust the learning rate.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-54}

Multi-GPU training is powerful but unforgiving: even tiny mismatches in
environment, data, or synchronization can cause errors. Instead of
treating these as random mysteries, it's helpful to recognize the
patterns. Each error message or hang has a likely cause, and learning to
map symptoms to fixes will make distributed training much smoother.

\subsubsection{Try It Yourself}\label{try-it-yourself-68}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Intentionally reduce the dataset size so one rank runs out of data
  early --- observe the hang.
\item
  Launch a multi-node run without setting \texttt{NCCL\_SOCKET\_IFNAME}
  and watch how performance collapses.
\item
  Increase the batch size until you trigger
  \texttt{CUDA\_ERROR\_OUT\_OF\_MEMORY}, then reduce it step by step to
  see the limit.
\item
  Experiment with mismatched NCCL versions across nodes if you have
  access to multiple environments, then fix it by standardizing.
\item
  Run a small model with different batch sizes per rank and study how
  the validation loss diverges.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-69}

Most multi-GPU errors aren't mysterious once you understand what's
happening under the hood. They usually boil down to synchronization
mismatches, network misconfigurations, memory limits, or environment
inconsistencies. With the right debugging tools and a systematic
mindset, you can fix these problems quickly and keep your training runs
moving forward.

\section{Chapter 9. Extending the
codebase}\label{chapter-9.-extending-the-codebase}

\subsection{\texorpdfstring{81. The \texttt{dev/cuda} Library for Custom
Kernels}{81. The dev/cuda Library for Custom Kernels}}\label{the-devcuda-library-for-custom-kernels}

So far, most of the CUDA logic in \emph{llm.c} has relied on NVIDIA's
optimized libraries like cuBLAS and cuDNN. These libraries are extremely
powerful and efficient, but sometimes you want more control: maybe
you're experimenting with a new attention mechanism, or maybe you want
to fuse multiple operations into a single kernel to reduce memory
traffic. That's where the \texttt{dev/cuda} directory comes in. It's the
playground for custom kernels.

\subsubsection{\texorpdfstring{What Lives in
\texttt{dev/cuda}}{What Lives in dev/cuda}}\label{what-lives-in-devcuda}

If you look at the repository structure, you'll notice a folder named
\texttt{dev/cuda}. This is not part of the minimal training path --- you
can train GPT-2 models without ever touching it. Instead, it contains
experimental kernels that showcase how to move from simple CUDA examples
toward more advanced, production-level implementations.

Inside, you'll typically find:

\begin{itemize}
\tightlist
\item
  Hello World kernels: basic examples like elementwise addition to get
  familiar with CUDA.
\item
  Fused operations: simple prototypes for combining steps like bias
  addition + activation.
\item
  Benchmark code: small programs that measure kernel performance
  compared to cuBLAS/cuDNN.
\end{itemize}

These files are not polished production code. They're meant to be read,
modified, and played with --- like lab notebooks for CUDA development.

\subsubsection{Why Custom Kernels
Matter}\label{why-custom-kernels-matter}

Libraries like cuBLAS are designed to cover a wide range of use cases,
but they don't always hit the sweet spot for your specific workload.
Writing custom kernels allows you to:

\begin{itemize}
\tightlist
\item
  Fuse operations: Instead of launching separate kernels for bias
  addition, activation, and dropout, you can do all three in one kernel,
  saving time on memory reads/writes.
\item
  Experiment with new algorithms: If you invent a new type of attention
  or normalization, you can't rely on cuDNN --- you need to implement it
  yourself.
\item
  Learn how GPUs actually work: Reading and writing custom kernels
  teaches you about thread blocks, memory hierarchy, and warp
  scheduling, all of which deepen your understanding of GPU programming.
\end{itemize}

\subsubsection{Example: A Simple Elementwise
Kernel}\label{example-a-simple-elementwise-kernel}

Here's a very small kernel from a toy example you might find in
\texttt{dev/cuda}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_\_global\_\_ void add\_kernel(const float* a, const float* b, float* out, int N) \{}
\NormalTok{    int i = blockIdx.x * blockDim.x + threadIdx.x;}
\NormalTok{    if (i \textless{} N) \{}
\NormalTok{        out[i] = a[i] + b[i];}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This kernel adds two arrays \texttt{a} and \texttt{b} elementwise. While
trivial compared to attention mechanisms, it illustrates the GPU
execution model:

\begin{itemize}
\tightlist
\item
  Each thread handles one index \texttt{i}.
\item
  Threads are grouped into blocks, and blocks form a grid.
\item
  Memory access is explicit: you control exactly how \texttt{out} is
  written.
\end{itemize}

Scaling this up to real workloads means adding more complexity ---
shared memory, warp shuffles, half-precision math --- but the principles
remain the same.

\subsubsection{Why It Matters}\label{why-it-matters-55}

The \texttt{dev/cuda} directory is not just for fun experiments. It's a
bridge between ``using GPU libraries'' and ``designing GPU algorithms.''
By learning to write kernels here, you gain the freedom to customize and
optimize beyond what standard libraries provide. That skill becomes
essential if you want to innovate in model architectures or squeeze the
last bit of performance out of your hardware.

\subsubsection{Try It Yourself}\label{try-it-yourself-69}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Open one of the example \texttt{.cu} files in \texttt{dev/cuda} and
  compile it with \texttt{nvcc}.
\item
  Modify the elementwise kernel so it performs
  \texttt{out{[}i{]}\ =\ a{[}i{]}\ *\ b{[}i{]}} instead of addition.
\item
  Benchmark your kernel against the equivalent cuBLAS call (e.g.,
  \texttt{cublasSaxpy}) and compare performance.
\item
  Write a fused kernel that does bias addition followed by ReLU
  activation in one pass.
\item
  Use \texttt{nvprof} or Nsight Systems to measure how many kernel
  launches occur in a forward pass and imagine how custom fusion might
  reduce them.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-70}

The \texttt{dev/cuda} library is your sandbox for learning and
experimenting with CUDA. It's not required for running GPT-2, but it's
where you build the skills to go beyond libraries and design your own
GPU operations. Whether you're optimizing for speed or testing new
research ideas, this directory is where theory meets practice in GPU
programming.

\subsection{\texorpdfstring{82. Adding New Dataset Pipelines
(\texttt{dev/data/*})}{82. Adding New Dataset Pipelines (dev/data/*)}}\label{adding-new-dataset-pipelines-devdata}

Training a language model is not just about having a clever model ---
it's equally about the data you feed it. In \emph{llm.c}, datasets are
handled in a very lightweight way compared to frameworks like PyTorch.
Instead of complicated abstractions, the project keeps things simple:
tokenize your text once, save it as a \texttt{.bin} file, and then
stream batches of tokens into the model.

The \texttt{dev/data/} directory is where this happens. It contains
scripts and utilities for preparing different datasets, from tiny toy
corpora like Tiny Shakespeare to larger collections like TinyStories or
OpenWebText subsets. Understanding how this directory works is the key
to plugging in your own datasets.

\subsubsection{\texorpdfstring{How Data Pipelines Work in
\emph{llm.c}}{How Data Pipelines Work in llm.c}}\label{how-data-pipelines-work-in-llm.c}

At a high level, the pipeline follows three steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Download or provide raw text data. For example,
  \texttt{tiny\_shakespeare.txt} is just a plain text file with plays
  concatenated.
\item
  Tokenize the data once using the GPT-2 tokenizer. The tokenizer
  converts text into integers according to \texttt{gpt2\_tokenizer.bin}.
\item
  Write the tokens to a binary file (\texttt{.bin}). This is a flat
  array of integers stored as 32-bit values, which makes it fast to
  memory-map and stream during training.
\end{enumerate}

Once the \texttt{.bin} files exist, \texttt{dataloader\_init} can open
them, divide them into training and validation splits, and generate
batches of shape \texttt{(B,\ T)} for the model.

\subsubsection{\texorpdfstring{What's Inside
\texttt{dev/data/}}{What's Inside dev/data/}}\label{whats-inside-devdata-1}

The folder contains small scripts like:

\begin{itemize}
\tightlist
\item
  \texttt{download\_starter\_pack.sh} --- downloads Tiny Shakespeare and
  TinyStories.
\item
  Tokenization scripts --- often small Python snippets that run the
  GPT-2 tokenizer over raw text.
\item
  Prebuilt \texttt{.bin} files --- these are used in the quickstart so
  you don't need to regenerate them yourself.
\end{itemize}

The design choice here is minimalism: instead of a heavy dataset
framework, you get plain files and short scripts. You can read and
understand everything in a few minutes.

\subsubsection{Adding Your Own Dataset}\label{adding-your-own-dataset}

Suppose you want to train on your company's support chat logs or a new
dataset you've found. The process looks like this:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Prepare raw text in a simple format (one text file is fine).
\item
  Run the tokenizer from \emph{llm.c} on it:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{python}\NormalTok{ dev/data/tokenizer.py }\AttributeTok{{-}{-}input}\NormalTok{ my\_corpus.txt }\AttributeTok{{-}{-}output}\NormalTok{ my\_corpus.bin}
\end{Highlighting}
\end{Shaded}

  This produces a binary token file.
\item
  Drop the file into \texttt{dev/data/}. You might name it
  \texttt{my\_corpus\_train.bin} and \texttt{my\_corpus\_val.bin}.
\item
  Point the dataloader at it in your training code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataloader\_init}\OperatorTok{(\&}\NormalTok{train\_loader}\OperatorTok{,} \StringTok{"dev/data/my\_corpus\_train.bin"}\OperatorTok{,}\NormalTok{ B}\OperatorTok{,}\NormalTok{ T}\OperatorTok{,} \DecValTok{0}\OperatorTok{,} \DecValTok{1}\OperatorTok{,} \DecValTok{1}\OperatorTok{);}
\NormalTok{dataloader\_init}\OperatorTok{(\&}\NormalTok{val\_loader}\OperatorTok{,} \StringTok{"dev/data/my\_corpus\_val.bin"}\OperatorTok{,}\NormalTok{ B}\OperatorTok{,}\NormalTok{ T}\OperatorTok{,} \DecValTok{0}\OperatorTok{,} \DecValTok{1}\OperatorTok{,} \DecValTok{0}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}
\end{enumerate}

That's it --- you now have a new dataset pipeline integrated with the
same training loop.

\subsubsection{Why It Matters}\label{why-it-matters-56}

Many frameworks hide data preprocessing behind layers of abstractions.
\emph{llm.c} takes the opposite approach: it makes the process
transparent. You see exactly how text becomes tokens, how those tokens
become batches, and how the model consumes them. This transparency makes
it easier to debug, extend, and customize. Adding a new dataset is no
longer a mystery --- it's just a matter of writing one file and updating
a path.

\subsubsection{Try It Yourself}\label{try-it-yourself-70}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Explore the \texttt{dev/data/} directory and read through the provided
  scripts.
\item
  Tokenize a new small dataset of your choice (a novel, a set of
  Wikipedia pages, or your own text).
\item
  Train a 124M model on your new dataset and observe the loss curve.
\item
  Compare validation loss between Tiny Shakespeare and your dataset ---
  how does the model behave differently?
\item
  Try increasing sequence length \texttt{T} to see how batching
  interacts with longer documents.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-71}

The \texttt{dev/data} folder is where you connect language models to the
real world. It shows how raw text becomes training-ready binary files
with almost no overhead. By learning to add your own pipelines, you gain
the ability to train \emph{llm.c} on any dataset --- from classic
literature to domain-specific corpora --- while keeping the workflow
fast and understandable.

\subsection{83. Adding a New Optimizer to the
Codebase}\label{adding-a-new-optimizer-to-the-codebase}

So far, \emph{llm.c} has focused on AdamW, which is the workhorse
optimizer for training transformer models. But deep learning is a
fast-moving field: new optimizers appear, older ones sometimes
resurface, and certain workloads benefit from alternatives. The
simplicity of \emph{llm.c} makes it a great environment to learn how to
implement and experiment with optimizers.

\subsubsection{\texorpdfstring{Where Optimizers Live in
\emph{llm.c}}{Where Optimizers Live in llm.c}}\label{where-optimizers-live-in-llm.c}

In the CPU training path, the optimizer logic is implemented directly in
C in the function \texttt{gpt2\_update}. This function iterates through
every parameter, applies AdamW's moment updates, applies bias
correction, and then modifies the parameter values in place.

Because the parameters, gradients, and optimizer states are all stored
as contiguous arrays in memory (\texttt{params\_memory},
\texttt{grads\_memory}, \texttt{m\_memory}, \texttt{v\_memory}), adding
a new optimizer usually means:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Allocating any new state arrays you need.
\item
  Defining the update rule in the training loop.
\item
  Adding function calls for your new optimizer, similar to
  \texttt{gpt2\_update}.
\end{enumerate}

\subsubsection{Example: Implementing SGD with
Momentum}\label{example-implementing-sgd-with-momentum}

Stochastic Gradient Descent (SGD) with momentum is much simpler than
AdamW. The update rule looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{// SGD with momentum update}
\DataTypeTok{void}\NormalTok{ gpt2\_update\_sgd}\OperatorTok{(}\NormalTok{GPT2 }\OperatorTok{*}\NormalTok{model}\OperatorTok{,} \DataTypeTok{float}\NormalTok{ learning\_rate}\OperatorTok{,} \DataTypeTok{float}\NormalTok{ momentum}\OperatorTok{)} \OperatorTok{\{}
    \ControlFlowTok{if} \OperatorTok{(}\NormalTok{model}\OperatorTok{{-}\textgreater{}}\NormalTok{m\_memory }\OperatorTok{==}\NormalTok{ NULL}\OperatorTok{)} \OperatorTok{\{}
\NormalTok{        model}\OperatorTok{{-}\textgreater{}}\NormalTok{m\_memory }\OperatorTok{=} \OperatorTok{(}\DataTypeTok{float}\OperatorTok{*)}\NormalTok{calloc}\OperatorTok{(}\NormalTok{model}\OperatorTok{{-}\textgreater{}}\NormalTok{num\_parameters}\OperatorTok{,} \KeywordTok{sizeof}\OperatorTok{(}\DataTypeTok{float}\OperatorTok{));}
    \OperatorTok{\}}

    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{size\_t}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}}\NormalTok{ model}\OperatorTok{{-}\textgreater{}}\NormalTok{num\_parameters}\OperatorTok{;}\NormalTok{ i}\OperatorTok{++)} \OperatorTok{\{}
        \DataTypeTok{float}\NormalTok{ grad }\OperatorTok{=}\NormalTok{ model}\OperatorTok{{-}\textgreater{}}\NormalTok{grads\_memory}\OperatorTok{[}\NormalTok{i}\OperatorTok{];}
        \DataTypeTok{float}\NormalTok{ m }\OperatorTok{=}\NormalTok{ momentum }\OperatorTok{*}\NormalTok{ model}\OperatorTok{{-}\textgreater{}}\NormalTok{m\_memory}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{+}\NormalTok{ grad}\OperatorTok{;}
\NormalTok{        model}\OperatorTok{{-}\textgreater{}}\NormalTok{m\_memory}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{=}\NormalTok{ m}\OperatorTok{;}
\NormalTok{        model}\OperatorTok{{-}\textgreater{}}\NormalTok{params\_memory}\OperatorTok{[}\NormalTok{i}\OperatorTok{]} \OperatorTok{{-}=}\NormalTok{ learning\_rate }\OperatorTok{*}\NormalTok{ m}\OperatorTok{;}
    \OperatorTok{\}}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

Here, \texttt{m\_memory} stores the velocity (the exponentially decayed
average of past gradients). There's no second moment estimate like in
AdamW, so it's leaner in both code and memory usage.

\subsubsection{Comparing Optimizers}\label{comparing-optimizers}

Adding new optimizers lets you experiment and compare behaviors:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1863}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3627}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1176}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Optimizer
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Weaknesses
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Memory Needs
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
SGD & Simple, stable, fewer hyperparameters & Slow convergence on large
models & Low \\
SGD + Momentum & Faster convergence, smooths updates & Still less
adaptive than Adam & Low \\
Adam & Adapts learning rates per parameter & Can overfit small datasets
& Medium \\
AdamW & Same as Adam + correct weight decay & More complex & Medium \\
Adagrad/RMSProp & Good for sparse features & Not always stable for
transformers & Medium \\
\end{longtable}

In \emph{llm.c}, each optimizer is just a loop over parameters with a
few math operations. That makes it the perfect playground to see how
different optimizers actually behave in practice.

\subsubsection{Why It Matters}\label{why-it-matters-57}

Optimizers control how your model learns. While architectures like GPT-2
get a lot of attention, small changes in optimization can make the
difference between a model that converges smoothly and one that
diverges. By adding your own optimizers, you get a clearer understanding
of this often ``black box'' part of deep learning.

\subsubsection{Try It Yourself}\label{try-it-yourself-71}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implement the SGD with momentum function shown above and swap it into
  the training loop instead of AdamW.
\item
  Run training on Tiny Shakespeare and compare how many steps it takes
  for the loss to reach 2.0.
\item
  Modify the code to implement RMSProp (similar to Adam, but without
  momentum on the first moment).
\item
  Benchmark memory usage: notice how AdamW allocates both
  \texttt{m\_memory} and \texttt{v\_memory}, while SGD only uses one.
\item
  Try running AdamW with a very small dataset versus SGD --- does one
  overfit faster?
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-72}

Optimizers are just math on arrays. By writing and testing new ones
inside \emph{llm.c}, you'll demystify how learning actually happens at
the parameter level. This makes it easier to appreciate why AdamW became
the default for transformers, and also gives you the tools to explore
alternatives in a clean, transparent environment.

\subsection{84. Adding a New Scheduler (cosine, step,
etc.)}\label{adding-a-new-scheduler-cosine-step-etc.}

Training isn't only about choosing an optimizer; the way you adjust the
learning rate over time is just as important. A scheduler tells the
optimizer \emph{how fast to learn} at each step. Without a scheduler,
you'd use a fixed learning rate, which often works poorly for large
models. In \emph{llm.c}, schedulers are kept intentionally simple so you
can see exactly how they influence training.

\subsubsection{Where Schedulers Fit}\label{where-schedulers-fit}

If you look at the training loop, every step calls the optimizer like
this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gpt2\_update}\OperatorTok{(\&}\NormalTok{model}\OperatorTok{,}\NormalTok{ lr}\OperatorTok{,}\NormalTok{ beta1}\OperatorTok{,}\NormalTok{ beta2}\OperatorTok{,}\NormalTok{ eps}\OperatorTok{,}\NormalTok{ weight\_decay}\OperatorTok{,}\NormalTok{ step}\OperatorTok{+}\DecValTok{1}\OperatorTok{);}
\end{Highlighting}
\end{Shaded}

The \texttt{lr} here doesn't have to be constant. Instead, a scheduler
function can compute it based on the step number. In \emph{llm.c}, this
logic lives in \texttt{schedulers.h} and related helpers.

\subsubsection{Common Schedulers You Can
Add}\label{common-schedulers-you-can-add}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Step Decay Reduce the learning rate by a fixed factor every N steps.

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{float}\NormalTok{ step\_decay}\OperatorTok{(}\DataTypeTok{int}\NormalTok{ step}\OperatorTok{,} \DataTypeTok{float}\NormalTok{ base\_lr}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ decay\_every}\OperatorTok{,} \DataTypeTok{float}\NormalTok{ decay\_factor}\OperatorTok{)} \OperatorTok{\{}
    \DataTypeTok{int}\NormalTok{ k }\OperatorTok{=}\NormalTok{ step }\OperatorTok{/}\NormalTok{ decay\_every}\OperatorTok{;}
    \ControlFlowTok{return}\NormalTok{ base\_lr }\OperatorTok{*}\NormalTok{ powf}\OperatorTok{(}\NormalTok{decay\_factor}\OperatorTok{,}\NormalTok{ k}\OperatorTok{);}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}
\item
  Cosine Decay Smoothly decrease the learning rate following a cosine
  curve.

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{float}\NormalTok{ cosine\_decay}\OperatorTok{(}\DataTypeTok{int}\NormalTok{ step}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ max\_steps}\OperatorTok{,} \DataTypeTok{float}\NormalTok{ base\_lr}\OperatorTok{)} \OperatorTok{\{}
    \DataTypeTok{float}\NormalTok{ progress }\OperatorTok{=} \OperatorTok{(}\DataTypeTok{float}\OperatorTok{)}\NormalTok{step }\OperatorTok{/}\NormalTok{ max\_steps}\OperatorTok{;}
    \ControlFlowTok{return}\NormalTok{ base\_lr }\OperatorTok{*} \FloatTok{0.5}\BuiltInTok{f} \OperatorTok{*} \OperatorTok{(}\FloatTok{1.0}\BuiltInTok{f} \OperatorTok{+}\NormalTok{ cosf}\OperatorTok{(}\NormalTok{M\_PI }\OperatorTok{*}\NormalTok{ progress}\OperatorTok{));}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}
\item
  Linear Warmup + Cosine Decay Start with a gradual increase (warmup) to
  avoid instability, then switch to cosine decay. This is the most
  common choice for transformers.
\end{enumerate}

\subsubsection{Example: Cosine with
Warmup}\label{example-cosine-with-warmup}

Here's how you might implement cosine with warmup in \emph{llm.c}:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{float}\NormalTok{ lr\_scheduler}\OperatorTok{(}\DataTypeTok{int}\NormalTok{ step}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ warmup\_steps}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ max\_steps}\OperatorTok{,} \DataTypeTok{float}\NormalTok{ base\_lr}\OperatorTok{)} \OperatorTok{\{}
    \ControlFlowTok{if} \OperatorTok{(}\NormalTok{step }\OperatorTok{\textless{}}\NormalTok{ warmup\_steps}\OperatorTok{)} \OperatorTok{\{}
        \ControlFlowTok{return}\NormalTok{ base\_lr }\OperatorTok{*} \OperatorTok{(}\DataTypeTok{float}\OperatorTok{)(}\NormalTok{step }\OperatorTok{+} \DecValTok{1}\OperatorTok{)} \OperatorTok{/}\NormalTok{ warmup\_steps}\OperatorTok{;}
    \OperatorTok{\}} \ControlFlowTok{else} \OperatorTok{\{}
        \DataTypeTok{float}\NormalTok{ progress }\OperatorTok{=} \OperatorTok{(}\DataTypeTok{float}\OperatorTok{)(}\NormalTok{step }\OperatorTok{{-}}\NormalTok{ warmup\_steps}\OperatorTok{)} \OperatorTok{/} \OperatorTok{(}\NormalTok{max\_steps }\OperatorTok{{-}}\NormalTok{ warmup\_steps}\OperatorTok{);}
        \ControlFlowTok{return}\NormalTok{ base\_lr }\OperatorTok{*} \FloatTok{0.5}\BuiltInTok{f} \OperatorTok{*} \OperatorTok{(}\FloatTok{1.0}\BuiltInTok{f} \OperatorTok{+}\NormalTok{ cosf}\OperatorTok{(}\NormalTok{M\_PI }\OperatorTok{*}\NormalTok{ progress}\OperatorTok{));}
    \OperatorTok{\}}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

This means:

\begin{itemize}
\tightlist
\item
  Steps 0--\texttt{warmup\_steps}: linearly scale from 0 →
  \texttt{base\_lr}.
\item
  After warmup: smoothly decay the learning rate using cosine.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-58}

Schedulers help stabilize training. At the beginning, gradients can be
very noisy, so warming up slowly prevents divergence. At the end,
lowering the learning rate helps the model converge instead of bouncing
around minima. Without schedulers, you'd often need more tuning to get
the same results.

\subsubsection{Try It Yourself}\label{try-it-yourself-72}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train with a constant learning rate on Tiny Shakespeare and record the
  loss curve.
\item
  Switch to a step decay scheduler and see if convergence improves.
\item
  Implement cosine decay with warmup and compare against constant LR ---
  which reaches a lower validation loss?
\item
  Experiment with different warmup lengths (e.g., 10 steps vs 100 steps)
  and watch how training stability changes.
\item
  Try running the same experiment on TinyStories and see if dataset size
  affects which scheduler works best.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-73}

Schedulers are small pieces of code with big impact. They don't change
the model or the optimizer, but they control the \emph{pace of
learning}. By adding new schedulers to \emph{llm.c}, you get a hands-on
way to see why modern training recipes almost always combine warmup with
a smooth decay schedule.

\subsection{85. Alternative Attention
Mechanisms}\label{alternative-attention-mechanisms}

Transformers became famous because of the self-attention mechanism, but
``attention'' is not a single fixed formula. Researchers have explored
many alternatives that trade off memory use, speed, and accuracy. In
\emph{llm.c}, the default is scaled dot-product attention, but nothing
prevents you from experimenting with new approaches.

\subsubsection{The Default: Scaled Dot-Product
Attention}\label{the-default-scaled-dot-product-attention}

The standard attention formula looks like this:

\[
\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right) V
\]

\begin{itemize}
\tightlist
\item
  Q = queries
\item
  K = keys
\item
  V = values
\item
  \(d_k\) = key dimension (used for scaling)
\end{itemize}

In \emph{llm.c}, this is implemented using matrix multiplications and
masking to enforce causality. It's correct and faithful to GPT-2, but
has quadratic cost in sequence length \(T\).

\subsubsection{Variants You Could Add}\label{variants-you-could-add}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Sparse Attention Instead of attending to every token, restrict
  attention to a local window or a set of important positions.

  \begin{itemize}
  \tightlist
  \item
    Good for long sequences.
  \item
    Saves compute and memory.
  \item
    Example: ``sliding window'' attention where each token only looks
    back 128 steps.
  \end{itemize}
\item
  Linformer / Low-Rank Attention Approximate \(QK^T\) using low-rank
  projections.

  \begin{itemize}
  \tightlist
  \item
    Reduces memory from \(O(T^2)\) to \(O(T)\).
  \item
    Works well when redundancy exists in sequences.
  \end{itemize}
\item
  Performer (Linear Attention) Replace the softmax with kernel
  approximations so attention becomes linear in sequence length.

  \begin{itemize}
  \tightlist
  \item
    Trades exactness for scalability.
  \item
    Allows much longer sequences on the same hardware.
  \end{itemize}
\item
  ALiBi (Attention with Linear Biases) Adds simple position-dependent
  biases instead of full positional embeddings.

  \begin{itemize}
  \tightlist
  \item
    Extremely efficient.
  \item
    Helps extrapolate to longer sequences than seen in training.
  \end{itemize}
\end{enumerate}

\subsubsection{\texorpdfstring{How to Experiment in
\emph{llm.c}}{How to Experiment in llm.c}}\label{how-to-experiment-in-llm.c}

The attention implementation lives inside the
\texttt{attention\_forward} and \texttt{attention\_backward} routines in
\texttt{train\_gpt2.c} (and their CUDA equivalents). To try an
alternative:

\begin{itemize}
\tightlist
\item
  Replace the part where \(QK^T\) is computed with your chosen method.
\item
  Keep the interface the same: given inputs Q, K, V, return outputs
  shaped \texttt{(B,\ T,\ C)}.
\item
  Run unit tests (\texttt{test\_gpt2.c}) against the baseline to make
  sure the outputs stay reasonable.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-59}

Attention is often the bottleneck in transformers. Quadratic time and
memory usage limit how long your sequences can be. By experimenting with
alternatives, you not only improve efficiency but also learn how new
research ideas are implemented in practice. Many of today's ``efficient
transformers'' came from simple tweaks to this block.

\subsubsection{Try It Yourself}\label{try-it-yourself-73}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Modify attention so each token only attends to the last 16 tokens (a
  toy form of sparse attention). Train on Tiny Shakespeare and compare
  speed vs.~accuracy.
\item
  Implement ALiBi by adding linear position-dependent bias terms and see
  if your model generalizes better to longer text.
\item
  Benchmark the GPU memory footprint of standard attention vs.~your
  custom version using Nsight Systems or \texttt{nvidia-smi}.
\item
  Try removing the scaling factor \(1/\sqrt{d_k}\) --- does training
  become unstable?
\item
  Replace softmax with a simple ReLU and see how the model behaves
  (hint: it usually diverges, but it teaches why softmax is important).
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-74}

The attention block is where much of the magic happens in transformers,
but it's also the biggest bottleneck. By experimenting with alternatives
in \emph{llm.c}, you'll gain a deeper understanding of why the standard
formula works, what its weaknesses are, and how new ideas from research
can be tested directly in code.

\subsection{86. Profiling and Testing New
Kernels}\label{profiling-and-testing-new-kernels}

When you start adding custom CUDA kernels or experimenting with new
attention mechanisms, the next big question is: how do you know if
they're correct and efficient? That's where profiling and testing come
in. \emph{llm.c} keeps this process minimal but transparent so you can
see exactly how to validate both correctness and performance.

\subsubsection{Correctness First: Testing Against a
Reference}\label{correctness-first-testing-against-a-reference}

Any new kernel you write should be compared against a known-good
implementation. In \emph{llm.c}, PyTorch usually serves as the
``oracle.'' For example:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generate random input tensors in both \emph{llm.c} and PyTorch.
\item
  Run your custom kernel in \emph{llm.c}.
\item
  Run the equivalent operation in PyTorch.
\item
  Compare the outputs within a small tolerance (e.g., differences less
  than \texttt{1e-5}).
\end{enumerate}

This ensures your kernel doesn't silently compute the wrong thing.
Without this step, you might train for hours before realizing your model
is learning nonsense.

\subsubsection{Performance: Profiling the
Kernels}\label{performance-profiling-the-kernels}

Once correctness is established, the next step is performance. NVIDIA
provides several tools:

\begin{itemize}
\tightlist
\item
  nvprof (older): still widely used, easy to launch.
\item
  Nsight Systems / Nsight Compute (modern): more detailed, lets you see
  kernel timings, memory transfers, occupancy, and more.
\end{itemize}

In practice:

\begin{itemize}
\tightlist
\item
  Run your training loop with profiling enabled.
\item
  Identify which kernels take the most time.
\item
  Check if your custom kernel is faster than the baseline (e.g., cuBLAS
  or cuDNN).
\end{itemize}

\subsubsection{Common Metrics to Watch}\label{common-metrics-to-watch}

\begin{itemize}
\tightlist
\item
  Kernel time (how long each launch takes).
\item
  Occupancy (how many CUDA cores are active relative to maximum).
\item
  Memory throughput (are you saturating memory bandwidth?).
\item
  Launch count (do you call your kernel too many times instead of fusing
  operations?).
\end{itemize}

Even a correct kernel can be slower than a library implementation if it
doesn't use the GPU efficiently.

\subsubsection{Example Workflow}\label{example-workflow-1}

Suppose you write a fused bias + ReLU kernel. You can test it like this:

\begin{itemize}
\tightlist
\item
  Generate a random tensor in C and in PyTorch.
\item
  Apply your fused kernel vs.~PyTorch's separate \texttt{+} and
  \texttt{ReLU} ops.
\item
  Compare results for correctness.
\item
  Profile both approaches: is your kernel faster? Did it reduce kernel
  launch overhead?
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-60}

Custom kernels are fun to write, but without testing and profiling
they're just guesswork. Many research ideas look promising in theory but
fall apart in practice because they run slower or break correctness. By
learning to systematically test and profile, you can separate ideas that
are genuinely useful from those that are just experiments.

\subsubsection{Try It Yourself}\label{try-it-yourself-74}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a simple fused kernel for bias + ReLU. Compare it against
  PyTorch's \texttt{x\ +\ bias} followed by \texttt{relu(x)}.
\item
  Use \texttt{nvprof} to check how many kernel launches happen in each
  version.
\item
  Run Nsight Systems and look at the timeline: do you see your fused
  kernel overlapping better with other GPU activity?
\item
  Try scaling sequence length \texttt{T} to very large values --- does
  your kernel still perform well?
\item
  Record memory usage before and after your kernel runs. Is there a
  difference compared to the unfused version?
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-75}

Profiling and testing turn kernel hacking from random tinkering into
real engineering. With a reference for correctness and tools for
performance measurement, you can iterate confidently, knowing when your
changes are truly improvements. This is how \emph{llm.c} bridges the gap
between a learning project and real GPU systems work.

\subsection{87. Using PyTorch Reference as
Oracle}\label{using-pytorch-reference-as-oracle}

One of the guiding principles of \emph{llm.c} is to stay small,
readable, and minimal --- but that doesn't mean you're left without a
safety net. When implementing something as mathematically dense as a
transformer, how do you know your C or CUDA code is doing the right
thing? The answer is to use PyTorch as a reference implementation, often
called an ``oracle.''

\subsubsection{What Does ``Oracle'' Mean
Here?}\label{what-does-oracle-mean-here}

An oracle is simply a trusted system you compare against. PyTorch is
trusted because:

\begin{itemize}
\tightlist
\item
  It's widely used in production and research.
\item
  Its operators (matrix multiply, attention, layernorm, etc.) have been
  thoroughly tested.
\item
  It gives you both CPU and GPU implementations with stable numerical
  behavior.
\end{itemize}

If your \emph{llm.c} forward or backward pass matches PyTorch's within a
small error tolerance, you can be confident that your implementation is
correct.

\subsubsection{How the Comparison Works}\label{how-the-comparison-works}

The workflow usually looks like this:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Set up the same model in both PyTorch and \emph{llm.c} with identical
  weights.
\item
  Feed the same inputs to both models.
\item
  Compare outputs --- logits, losses, or gradients.
\item
  Allow for tiny differences due to floating point arithmetic, usually
  within \texttt{1e-5} to \texttt{1e-6}.
\end{enumerate}

For example, \texttt{test\_gpt2.c} in the repository runs a forward pass
in C and compares the logits to those produced by a PyTorch GPT-2
checkpoint.

\subsubsection{Example}\label{example-1}

Suppose you're testing the embedding lookup. In PyTorch you might write:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{from}\NormalTok{ transformers }\ImportTok{import}\NormalTok{ GPT2Model, GPT2Tokenizer}

\NormalTok{tokenizer }\OperatorTok{=}\NormalTok{ GPT2Tokenizer.from\_pretrained(}\StringTok{"gpt2"}\NormalTok{)}
\NormalTok{model }\OperatorTok{=}\NormalTok{ GPT2Model.from\_pretrained(}\StringTok{"gpt2"}\NormalTok{)}
\NormalTok{tokens }\OperatorTok{=}\NormalTok{ torch.tensor([[}\DecValTok{50256}\NormalTok{, }\DecValTok{200}\NormalTok{]])  }\CommentTok{\# EOT and an example token}
\NormalTok{out }\OperatorTok{=}\NormalTok{ model.wte(tokens)  }\CommentTok{\# word token embeddings}
\BuiltInTok{print}\NormalTok{(out.detach().numpy())}
\end{Highlighting}
\end{Shaded}

In \emph{llm.c}, you'd run the same two tokens through the embedding
lookup and compare the resulting vectors element by element. If they
match, your embedding implementation is correct.

\subsubsection{Why PyTorch is the Perfect
Oracle}\label{why-pytorch-is-the-perfect-oracle}

\begin{itemize}
\tightlist
\item
  Transparency: it's easy to extract weights from PyTorch checkpoints
  (\texttt{.bin} files).
\item
  Flexibility: you can test individual layers, not just the whole model.
\item
  Debuggability: if something goes wrong, you can isolate which layer
  diverges first.
\end{itemize}

This last point is crucial --- instead of training for days only to find
your loss curve diverges, you can catch mismatches immediately at the
layer level.

\subsubsection{Why It Matters}\label{why-it-matters-61}

Deep learning models are fragile. Even a tiny mistake in normalization,
masking, or gradient flow can ruin training. By anchoring your work to
PyTorch, you avoid ``trusting your gut'' and instead rely on a
battle-tested baseline. This practice isn't unique to \emph{llm.c} ---
many professional frameworks (Megatron-LM, DeepSpeed) also validate
against PyTorch during development.

\subsubsection{Try It Yourself}\label{try-it-yourself-75}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Extract GPT-2 weights from Hugging Face Transformers and load them
  into \emph{llm.c}.
\item
  Run a forward pass in both PyTorch and \emph{llm.c} with the same
  input tokens. Compare outputs numerically.
\item
  Focus on a single block: check whether the attention output matches
  PyTorch's.
\item
  Modify a kernel (e.g., change softmax to ReLU) and watch how quickly
  the outputs diverge from PyTorch's.
\item
  Use PyTorch to verify gradients by calling \texttt{.backward()} and
  comparing with \texttt{gpt2\_backward} in \emph{llm.c}.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-76}

PyTorch is your map and compass when navigating the dense jungle of
transformer internals. By treating it as an oracle, you can move
confidently from one layer to the next, knowing that your small,
hand-written code matches the behavior of a full-featured deep learning
framework. This practice turns \emph{llm.c} into more than a toy project
--- it becomes a faithful, verifiable reimplementation of GPT-2.

\subsection{88. Exploring Beyond GPT-2: LLaMA
Example}\label{exploring-beyond-gpt-2-llama-example}

While \emph{llm.c} focuses on GPT-2 for clarity, the same framework can
extend to newer, larger, and more modern models such as LLaMA. LLaMA,
released by Meta, uses many of the same building blocks as GPT-2 ---
embeddings, attention layers, MLPs, normalization, and residual streams
--- but with tweaks that improve efficiency and scaling. Looking at
LLaMA through the lens of \emph{llm.c} helps you see how language model
designs evolve while still sharing the same DNA.

\subsubsection{What Stays the Same}\label{what-stays-the-same}

\begin{itemize}
\tightlist
\item
  Token embeddings: both GPT-2 and LLaMA use lookup tables to turn token
  IDs into dense vectors.
\item
  Transformer blocks: the fundamental loop of attention → MLP →
  residuals is unchanged.
\item
  Autoregressive training: predict the next token given all previous
  ones, using causal masking.
\end{itemize}

This means much of the code in \emph{llm.c} --- dataloaders, embeddings,
forward loops --- would work with LLaMA almost unchanged.

\subsubsection{What's Different}\label{whats-different}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Normalization

  \begin{itemize}
  \tightlist
  \item
    GPT-2 uses LayerNorm before each block output.
  \item
    LLaMA uses RMSNorm, which normalizes using only the root mean square
    of activations (no mean subtraction).
  \item
    This reduces compute slightly and improves stability.
  \end{itemize}
\item
  Positional Encoding

  \begin{itemize}
  \tightlist
  \item
    GPT-2 has learned positional embeddings.
  \item
    LLaMA uses Rotary Position Embeddings (RoPE), which rotate queries
    and keys in attention space to encode positions.
  \item
    RoPE scales better to longer contexts.
  \end{itemize}
\item
  Vocabulary

  \begin{itemize}
  \tightlist
  \item
    GPT-2's vocab size is 50,257.
  \item
    LLaMA uses a different tokenizer (SentencePiece/BPE) with a larger
    vocabulary, closer to 32k for LLaMA-2.
  \end{itemize}
\item
  Model Scale

  \begin{itemize}
  \tightlist
  \item
    GPT-2 tops out at 1.6B parameters.
  \item
    LLaMA-2 and LLaMA-3 scale from 7B up to 70B+. This makes distributed
    training mandatory, with mixed precision and checkpointing as
    standard.
  \end{itemize}
\end{enumerate}

\subsubsection{\texorpdfstring{Adapting \emph{llm.c} to
LLaMA}{Adapting llm.c to LLaMA}}\label{adapting-llm.c-to-llama}

If you wanted to modify \emph{llm.c} to approximate LLaMA, the main
tasks would be:

\begin{itemize}
\tightlist
\item
  Replace LayerNorm with an RMSNorm implementation.
\item
  Add RoPE into the attention mechanism. This means modifying the step
  where Q and K vectors are built, applying a rotation based on token
  positions.
\item
  Swap out the GPT-2 tokenizer with a SentencePiece tokenizer trained on
  the desired vocabulary.
\end{itemize}

The rest of the pipeline --- optimizer, schedulers, dataloaders,
multi-GPU support --- would remain valid.

\subsubsection{Why It Matters}\label{why-it-matters-62}

By studying LLaMA in the context of GPT-2, you see that modern LLMs
aren't completely alien. They're evolutionary improvements on the same
transformer backbone. Recognizing these small architectural changes
(RMSNorm, RoPE, scaling) helps demystify why newer models outperform
older ones, and it shows you exactly what you'd need to tweak in
\emph{llm.c} to explore beyond GPT-2.

\subsubsection{Try It Yourself}\label{try-it-yourself-76}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implement RMSNorm in C by adapting the LayerNorm code in \emph{llm.c}.
\item
  Add a simplified version of RoPE to the attention kernel and run it on
  Tiny Shakespeare.
\item
  Swap the GPT-2 tokenizer for a SentencePiece model and train a small
  LLaMA-like model on your own dataset.
\item
  Compare training stability between LayerNorm and RMSNorm --- does the
  loss curve look different?
\item
  Study the memory use of GPT-2 vs.~a small LLaMA-style variant and see
  how scaling behaves.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-77}

Exploring LLaMA through \emph{llm.c} shows how flexible the codebase
really is. With only a few targeted changes --- normalization,
positional encoding, tokenizer --- you can shift from replicating GPT-2
to experimenting with the building blocks of modern LLMs. This makes
\emph{llm.c} not just a study tool for one model, but a foundation for
understanding the entire lineage of transformers.

\subsection{89. Porting Playbook: C →
Go/Rust/Metal}\label{porting-playbook-c-gorustmetal}

The \emph{llm.c} codebase is written in plain C for maximum readability
and minimal dependencies. But in practice, many developers want to
experiment with other languages or platforms --- for example, writing a
Go or Rust version for better tooling, or targeting Apple's Metal API
for GPU acceleration on Macs. Porting is not just a copy-paste exercise;
it requires careful thinking about how low-level memory, math
operations, and parallelism map across ecosystems.

\subsubsection{Why Port at All?}\label{why-port-at-all}

\begin{itemize}
\tightlist
\item
  Go: strong concurrency model (goroutines, channels), good for building
  training services or distributed experiments.
\item
  Rust: memory safety and performance without garbage collection, ideal
  for writing reliable numerical kernels.
\item
  Metal (Apple): GPU acceleration on macOS/iOS, a must if you want to
  train or run models efficiently on Apple Silicon.
\end{itemize}

Each ecosystem has strengths that make \emph{llm.c}'s concepts more
approachable or more production-ready.

\subsubsection{Mapping the Core
Components}\label{mapping-the-core-components}

Let's look at how the key pieces of \emph{llm.c} translate:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1458}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1389}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2431}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2431}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2292}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
C (llm.c)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Go Equivalent
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Rust Equivalent
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Metal Equivalent
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Memory allocation & \texttt{malloc}, \texttt{calloc} & \texttt{make},
slices, \texttt{unsafe.Pointer} &
\texttt{Vec\textless{}T\textgreater{}}, \texttt{Box}, \texttt{unsafe} if
needed & Buffers allocated on GPU \\
Math kernels & manual loops, OpenMP & loops or cgo bindings to BLAS &
loops with iterators, Rayon for CPU & Metal compute shaders \\
Tokenizer & \texttt{fread} binary file & standard file I/O,
\texttt{encoding/json} & \texttt{serde}, binary read & Preprocessing on
CPU, feed to GPU \\
Training loop & for-loops, structs & goroutines for dataloader + trainer
& async tasks, channels & CPU driver, GPU kernels \\
Parallelism & \texttt{\#pragma\ omp} & goroutines + sync primitives &
Rayon or explicit threads & Warp/thread groups in Metal \\
\end{longtable}

\subsubsection{Example: LayerNorm in
Rust}\label{example-layernorm-in-rust}

Here's a small Rust sketch of how a forward pass for LayerNorm might
look:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{fn}\NormalTok{ layernorm\_forward(out}\OperatorTok{:} \OperatorTok{\&}\KeywordTok{mut}\NormalTok{ [}\DataTypeTok{f32}\NormalTok{]}\OperatorTok{,}\NormalTok{ inp}\OperatorTok{:} \OperatorTok{\&}\NormalTok{[}\DataTypeTok{f32}\NormalTok{]}\OperatorTok{,}\NormalTok{ weight}\OperatorTok{:} \OperatorTok{\&}\NormalTok{[}\DataTypeTok{f32}\NormalTok{]}\OperatorTok{,}\NormalTok{ bias}\OperatorTok{:} \OperatorTok{\&}\NormalTok{[}\DataTypeTok{f32}\NormalTok{]}\OperatorTok{,}\NormalTok{ c}\OperatorTok{:} \DataTypeTok{usize}\NormalTok{) }\OperatorTok{\{}
    \KeywordTok{let}\NormalTok{ mean}\OperatorTok{:} \DataTypeTok{f32} \OperatorTok{=}\NormalTok{ inp}\OperatorTok{.}\NormalTok{iter()}\OperatorTok{.}\PreprocessorTok{sum::}\OperatorTok{\textless{}}\DataTypeTok{f32}\OperatorTok{\textgreater{}}\NormalTok{() }\OperatorTok{/}\NormalTok{ c }\KeywordTok{as} \DataTypeTok{f32}\OperatorTok{;}
    \KeywordTok{let}\NormalTok{ var}\OperatorTok{:} \DataTypeTok{f32} \OperatorTok{=}\NormalTok{ inp}\OperatorTok{.}\NormalTok{iter()}\OperatorTok{.}\NormalTok{map(}\OperatorTok{|}\NormalTok{x}\OperatorTok{|}\NormalTok{ (x }\OperatorTok{{-}}\NormalTok{ mean)}\OperatorTok{.}\NormalTok{powi(}\DecValTok{2}\NormalTok{))}\OperatorTok{.}\PreprocessorTok{sum::}\OperatorTok{\textless{}}\DataTypeTok{f32}\OperatorTok{\textgreater{}}\NormalTok{() }\OperatorTok{/}\NormalTok{ c }\KeywordTok{as} \DataTypeTok{f32}\OperatorTok{;}
    \KeywordTok{let}\NormalTok{ rstd }\OperatorTok{=} \DecValTok{1.0} \OperatorTok{/}\NormalTok{ (var }\OperatorTok{+} \DecValTok{1e{-}5}\NormalTok{)}\OperatorTok{.}\NormalTok{sqrt()}\OperatorTok{;}

    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \DecValTok{0}\OperatorTok{..}\NormalTok{c }\OperatorTok{\{}
        \KeywordTok{let}\NormalTok{ norm }\OperatorTok{=}\NormalTok{ (inp[i] }\OperatorTok{{-}}\NormalTok{ mean) }\OperatorTok{*}\NormalTok{ rstd}\OperatorTok{;}
\NormalTok{        out[i] }\OperatorTok{=}\NormalTok{ norm }\OperatorTok{*}\NormalTok{ weight[i] }\OperatorTok{+}\NormalTok{ bias[i]}\OperatorTok{;}
    \OperatorTok{\}}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

This looks strikingly similar to the C code, but benefits from Rust's
type safety and the absence of manual memory management bugs.

\subsubsection{Example: Attention Kernel in
Metal}\label{example-attention-kernel-in-metal}

Metal would handle attention differently --- you'd write a compute
shader in \texttt{.metal} language:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kernel void attention\_forward(}
\NormalTok{    device float* out [[buffer(0)]],}
\NormalTok{    device float* qkv [[buffer(1)]],}
\NormalTok{    uint id [[thread\_position\_in\_grid]]}
\NormalTok{) \{}
\NormalTok{    // compute a dot product between query and key vectors}
\NormalTok{    // accumulate into out, using threadgroup memory for efficiency}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This isn't a line-for-line port, but it shows how the concept ---
multiply queries with keys, apply softmax, weight values --- remains the
same while the implementation moves into GPU-land.

\subsubsection{Challenges You'll Face}\label{challenges-youll-face}

\begin{itemize}
\tightlist
\item
  Numerical libraries: C often leans on BLAS/LAPACK or just hand-written
  loops. In Go and Rust, you'll either bind to these libraries or
  reimplement them.
\item
  Performance portability: getting code that runs fast both on CPU and
  GPU isn't trivial. What works in C with OpenMP won't directly
  translate.
\item
  Tokenizer compatibility: making sure tokenization matches
  byte-for-byte is essential. One mismatch can ruin training
  reproducibility.
\end{itemize}

\subsubsection{Why It Matters}\label{why-it-matters-63}

Porting \emph{llm.c} forces you to understand what each piece of the
code is doing --- you can't just rely on \texttt{torch.nn.LayerNorm} or
\texttt{torch.nn.MultiheadAttention}. This makes it an excellent
exercise for truly learning transformers, while also giving you
practical implementations in different environments.

\subsubsection{Try It Yourself}\label{try-it-yourself-77}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Reimplement one kernel (like LayerNorm or matmul) in Go or Rust. Test
  it against the C version.
\item
  Write a minimal Metal kernel that adds two vectors, then extend it to
  matrix multiplication.
\item
  Build a Rust tokenizer reader that loads \texttt{gpt2\_tokenizer.bin}
  and decodes IDs back into text.
\item
  Compare training speed: C with OpenMP vs.~Rust with Rayon vs.~Go with
  goroutines.
\item
  Try porting just the forward pass first --- inference is easier than
  training because you don't need backprop.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-78}

The \emph{llm.c} design is portable by nature --- it doesn't hide behind
opaque frameworks. Porting it to Go, Rust, or Metal is not just about
performance or language preference. It's about proving to yourself that
the transformer algorithm is universal, and you can implement it
anywhere once you truly understand it.

\subsection{90. Keeping the Repo Minimal and
Clean}\label{keeping-the-repo-minimal-and-clean}

One of the defining features of \emph{llm.c} is its minimalism. The
repository avoids the sprawling complexity of large frameworks and
instead sticks to a small, readable core. This design choice isn't an
accident---it's a philosophy. By keeping the codebase small and clean,
contributors can focus on understanding the fundamentals of transformers
rather than navigating thousands of lines of boilerplate.

\subsubsection{The Philosophy of
Minimalism}\label{the-philosophy-of-minimalism}

\begin{itemize}
\tightlist
\item
  Readability over performance: While production frameworks like PyTorch
  or TensorFlow optimize aggressively, \emph{llm.c} intentionally trades
  some performance for clarity. A loop in plain C is easier to study
  than a chain of optimized CUDA calls hidden behind macros.
\item
  Portability: A smaller codebase can be ported more easily to new
  environments (Go, Rust, Metal) without pulling in dozens of
  dependencies.
\item
  Learning-first design: Every line of code has a clear purpose. There
  are no abstractions ``just in case.''
\end{itemize}

This philosophy turns \emph{llm.c} into both a working training
framework and an educational resource.

\subsubsection{How Minimalism Is
Enforced}\label{how-minimalism-is-enforced}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Flat structure: The repository avoids deep directory hierarchies. Most
  files live directly under \texttt{llmc/} or \texttt{dev/}.
\item
  No external libraries unless critical: You'll see OpenMP, cuBLAS, or
  cuDNN for performance, but not sprawling dependency chains.
\item
  One feature, one file: Tokenization, dataloading, schedulers, and
  samplers each get their own small C file. This prevents ``god files''
  where too much is lumped together.
\item
  Consistent naming: Functions and structs use clear, descriptive names
  (e.g., \texttt{dataloader\_next\_batch}, \texttt{tokenizer\_decode})
  so readers don't get lost.
\end{enumerate}

\subsubsection{Practical Examples}\label{practical-examples}

If you open \texttt{train\_gpt2.c}, you'll find that it builds the
model, initializes dataloaders, and runs the training loop. It doesn't
try to handle every possible model configuration, dataset format, or
distributed scenario. Those belong in specialized files or external
tools.

Likewise, \texttt{llmc/utils.h} only defines the absolute essentials:
safe file I/O wrappers (\texttt{fopenCheck}, \texttt{freadCheck}) and
memory allocators. It's not bloated with generic helpers unrelated to
training GPT-2.

\subsubsection{Why It Matters}\label{why-it-matters-64}

A minimal repo lowers the barrier to entry. Beginners can trace
execution from \texttt{main()} all the way to the optimizer update
without detours. Researchers can fork the code and modify it without
worrying about breaking dozens of interconnected modules. Even advanced
developers benefit because the simplicity forces clarity in reasoning
about algorithms.

\subsubsection{Try It Yourself}\label{try-it-yourself-78}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Pick any file in the repo and count how many lines it has. Most are
  under a few hundred. Compare this with a similar file in PyTorch or
  TensorFlow.
\item
  Try adding a new feature---say, a different activation function.
  Notice how easy it is to slot it in because the structure is clean.
\item
  Explore what happens if you make the repo ``heavier.'' Add too many
  helpers, abstractions, or configs. Does it make the code harder to
  read?
\item
  Practice explaining the training loop to a friend. If the repo is
  simple, you should be able to walk them through without glossing over
  details.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-79}

Keeping \emph{llm.c} minimal is not just about saving lines of code.
It's about preserving clarity, ensuring reproducibility, and making the
repository a place where anyone---from curious learners to experienced
engineers---can open a file and \emph{understand what's happening}. The
simplicity is the point, and that's what makes \emph{llm.c} a rare and
valuable resource in a world of bloated ML frameworks.

\section{Chapter 10. Reproduction, community and
roadmap}\label{chapter-10.-reproduction-community-and-roadmap}

\subsection{91. Reproducing GPT-2 124M on Single
Node}\label{reproducing-gpt-2-124m-on-single-node}

The first major milestone for anyone exploring \emph{llm.c} is to
reproduce the training of GPT-2 124M, the smallest version of the GPT-2
family. This model has around 124 million parameters, which is large
enough to be interesting, but still small enough to train on a single
modern GPU---or even slowly on CPU for demonstration purposes.

\subsubsection{Why Start with 124M?}\label{why-start-with-124m}

GPT-2 comes in multiple sizes: 124M, 355M, 774M, and 1.6B parameters.
Training the largest requires clusters of GPUs and serious compute
budgets. The 124M version, however, fits comfortably on consumer-grade
GPUs like an NVIDIA 3090, and can even be run on a laptop CPU if you're
patient. It's the ``hello world'' of transformer reproduction: small,
approachable, and still real.

\subsubsection{What the Training Setup Looks
Like}\label{what-the-training-setup-looks-like}

Training GPT-2 124M involves a few key steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Model Configuration The config for 124M is baked into \emph{llm.c}
  with 12 layers, 12 heads, hidden dimension of 768, and max sequence
  length of 1024.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GPT2Config config }\OperatorTok{=} \OperatorTok{\{}
    \OperatorTok{.}\NormalTok{max\_seq\_len }\OperatorTok{=} \DecValTok{1024}\OperatorTok{,}
    \OperatorTok{.}\NormalTok{vocab\_size }\OperatorTok{=} \DecValTok{50257}\OperatorTok{,}
    \OperatorTok{.}\NormalTok{num\_layers }\OperatorTok{=} \DecValTok{12}\OperatorTok{,}
    \OperatorTok{.}\NormalTok{num\_heads }\OperatorTok{=} \DecValTok{12}\OperatorTok{,}
    \OperatorTok{.}\NormalTok{channels }\OperatorTok{=} \DecValTok{768}
\OperatorTok{\};}
\end{Highlighting}
\end{Shaded}
\item
  Dataset You can train on small datasets like Tiny Shakespeare or Tiny
  Stories for quick runs. For more realistic reproduction, you need
  something closer to OpenWebText. The data is tokenized with the GPT-2
  tokenizer (\texttt{gpt2\_tokenizer.bin}) and stored in \texttt{.bin}
  files.
\item
  Batch Size and Sequence Length A common setting is
  \texttt{B\ =\ 8,\ T\ =\ 1024}, meaning 8 sequences, each of length
  1024 tokens. Adjust these based on available memory.
\item
  Optimizer AdamW with a learning rate around \texttt{3e-4} is the
  default. Warmup and cosine decay scheduling can be enabled to match
  published GPT-2 training curves.
\end{enumerate}

\subsubsection{What to Expect in
Practice}\label{what-to-expect-in-practice}

On CPU, training a single step may take several seconds. On a single GPU
with CUDA, each step may take under 100 milliseconds. With 124M
parameters, training from scratch on a dataset the size of OpenWebText
still takes days, but you can reproduce key dynamics (loss curve, sample
generations) on smaller datasets in hours.

As an example, here's the type of log you might see:

\begin{verbatim}
step 0: train loss 6.9321 (took 421.5 ms)
step 100: train loss 4.2137 (took 95.2 ms)
step 200: train loss 3.8914 (took 94.8 ms)
val loss 3.7725
\end{verbatim}

Even within a few hundred steps, the model begins to generate text
resembling English rather than pure noise.

\subsubsection{Why It Matters}\label{why-it-matters-65}

Reproducing GPT-2 124M is a confidence check. If your setup is correct,
your loss curves should match those in the original OpenAI paper or the
PyTorch reference implementation. This validates that \emph{llm.c} is a
faithful reproduction, not just a toy. It also teaches you how much
compute and data go into even the smallest GPT-2 model, building
intuition about scaling laws.

\subsubsection{Try It Yourself}\label{try-it-yourself-79}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train GPT-2 124M for 1000 steps on Tiny Shakespeare. Watch how the
  generated text improves.
\item
  Change the batch size \texttt{B} from 8 to 4. What happens to the
  speed and the stability of training?
\item
  Run training on CPU vs.~GPU. Compare how long each step takes.
\item
  Track both training and validation loss. Notice how they diverge when
  the model begins to overfit.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-80}

The GPT-2 124M run is more than just a demo---it's your gateway into
real LLM training. You see how data, model size, optimizer, and hardware
come together. Once you've mastered this reproduction, you're ready to
push toward larger models and more complex setups. It's the foundation
on which everything else in \emph{llm.c} builds.

\subsection{92. Reproducing GPT-2 355M (Constraints and
Tricks)}\label{reproducing-gpt-2-355m-constraints-and-tricks}

Once GPT-2 124M has been successfully reproduced, the next logical step
is scaling up to GPT-2 355M. This version is roughly three times larger,
with about 355 million parameters. It introduces new challenges that
don't appear at the smaller scale: memory pressure, training stability,
and compute cost.

\subsubsection{Model Configuration}\label{model-configuration}

The 355M model still uses 1024 tokens as the maximum sequence length and
the same GPT-2 tokenizer. The difference is in the depth and width of
the network:

\begin{itemize}
\tightlist
\item
  Layers: 24 transformer blocks instead of 12
\item
  Hidden dimension (channels): 1024 instead of 768
\item
  Heads: 16 instead of 12
\end{itemize}

The total parameter count rises from \textasciitilde124M to
\textasciitilde355M. That means not just three times more math per step,
but also more memory needed for parameters, gradients, and optimizer
state.

\subsubsection{The Compute Challenge}\label{the-compute-challenge}

With 124M, a single GPU with 8--12 GB of VRAM is enough. For 355M, you
need at least 16 GB to run comfortably with sequence length 1024 and
batch size of 8. On smaller GPUs, you'll quickly hit ``CUDA out of
memory'' errors.

One trick is to reduce batch size (B) or sequence length (T). For
example, instead of training with \texttt{(B=8,\ T=1024)}, you might use
\texttt{(B=4,\ T=512)}. This halves the memory footprint but still lets
you test scaling dynamics.

Another approach is to use gradient accumulation: simulate a larger
batch size by running multiple small steps and accumulating gradients
before updating.

\subsubsection{Training Stability}\label{training-stability}

Larger models are more sensitive to hyperparameters. The AdamW optimizer
still works, but the learning rate schedule becomes more important. Many
practitioners use:

\begin{itemize}
\tightlist
\item
  Learning rate: \textasciitilde3e-4 peak
\item
  Warmup steps: a few thousand
\item
  Cosine decay: to taper the learning rate gradually
\end{itemize}

If you skip warmup, the larger model may diverge early (loss exploding
instead of decreasing).

\subsubsection{Tricks for Feasibility}\label{tricks-for-feasibility}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Mixed Precision Training (FP16 or BF16): Cuts memory use nearly in
  half. Supported in CUDA paths of \emph{llm.c}.
\item
  Activation Checkpointing: Save memory by recomputing activations
  during backpropagation. Slower, but lets you fit bigger models.
\item
  Smaller Dataset Runs: Train on Tiny Shakespeare or Tiny Stories to
  sanity-check the setup, then scale to OpenWebText-like data.
\end{enumerate}

\subsubsection{Example Logs}\label{example-logs}

Running GPT-2 355M for a short demo might look like:

\begin{verbatim}
step 0: train loss 7.1032 (took 321.8 ms)
step 50: train loss 5.4231 (took 310.4 ms)
step 100: train loss 4.8217 (took 311.0 ms)
val loss 4.7322
\end{verbatim}

The loss drops more slowly than with 124M because the model has more
capacity to learn, but also needs more data to generalize.

\subsubsection{Why It Matters}\label{why-it-matters-66}

355M is the first step into ``medium-sized'' LLMs. You start to feel the
bottlenecks that dominate larger models: VRAM limits, training speed,
and hyperparameter tuning. Solving these prepares you for the 774M and
1.6B experiments, where such problems become even more pronounced.

\subsubsection{Try It Yourself}\label{try-it-yourself-80}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train GPT-2 355M with batch size 4 and sequence length 512. Record how
  long each step takes.
\item
  Experiment with warmup steps: run once with warmup=0, and once with
  warmup=2000. Compare stability.
\item
  Enable mixed precision if you have a CUDA-capable GPU. Measure memory
  usage before and after.
\item
  Try training on Tiny Shakespeare vs.~Tiny Stories. Does the model
  overfit faster on the smaller dataset?
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-81}

Reproducing GPT-2 355M is all about learning how to stretch limited
resources. You'll discover memory-saving tricks, the importance of
learning rate schedules, and the role of data scale. It's a practical
exercise in resource management---just like the real challenges faced
when training today's billion-parameter models.

\subsection{93. Reproducing GPT-2 774M (Scaling
Up)}\label{reproducing-gpt-2-774m-scaling-up}

The 774M parameter version of GPT-2 is often called ``GPT-2 Medium.''
This is the point where training transitions from a personal experiment
to a small-scale research project. It's about six times larger than the
124M baseline, and roughly twice the size of 355M. Running it requires
careful planning of hardware, memory, and software tricks.

\subsubsection{Model Configuration}\label{model-configuration-1}

For 774M, the architecture expands again:

\begin{itemize}
\tightlist
\item
  Layers (transformer blocks): 36
\item
  Hidden size (channels): 1280
\item
  Attention heads: 20
\item
  Maximum sequence length: 1024 (unchanged)
\end{itemize}

This jump in size increases both the parameter storage and the number of
activations that must be kept during training. Optimizer states (AdamW's
\emph{m} and \emph{v} vectors) alone consume several gigabytes.

\subsubsection{Hardware Requirements}\label{hardware-requirements}

Running GPT-2 774M from scratch generally requires GPUs with 24 GB VRAM
or more (e.g., NVIDIA RTX 3090/4090, A100, or H100). With smaller cards,
you'll almost certainly hit memory errors unless you aggressively reduce
batch size and use techniques like activation checkpointing.

On CPUs, training is technically possible but far too slow to be
practical---steps that take milliseconds on GPUs might take many seconds
or even minutes.

\subsubsection{Practical Constraints}\label{practical-constraints}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Batch Size: In practice, you may need to lower \texttt{B} to 2 or even
  1 with sequence length 1024.
\item
  Gradient Accumulation: A must-have to simulate larger batch sizes and
  stabilize training.
\item
  Mixed Precision: FP16 or BF16 reduces memory by about half, without
  hurting convergence much.
\item
  Checkpointing: Recomputes intermediate results instead of storing
  them, trading time for memory.
\end{enumerate}

\subsubsection{Training Dynamics}\label{training-dynamics}

The loss curve for GPT-2 774M drops more steadily and requires much more
data to reach its potential. If you only train on Tiny Shakespeare or
Tiny Stories, it will quickly overfit: the model is too large for such a
small dataset. For meaningful reproduction, you need a dataset similar
in scale to OpenWebText.

Training logs for a sanity check run might look like:

\begin{verbatim}
step 0: train loss 8.0123 (took 512.4 ms)
step 50: train loss 5.7892 (took 490.7 ms)
step 100: train loss 5.1428 (took 495.1 ms)
val loss 5.0039
\end{verbatim}

Notice that losses start higher (due to the larger random initialization
space) but decrease predictably once training gets underway.

\subsubsection{Why It Matters}\label{why-it-matters-67}

The 774M model is the sweet spot where scaling laws become obvious.
Compared to 124M and 355M, it generalizes better, generates more fluent
text, and demonstrates the benefits of parameter growth. But it also
shows why infrastructure matters: without careful management, it's
nearly impossible to train this model on consumer-grade hardware.

This is also where distributed training (covered in Chapter 8) becomes
relevant, because one GPU is often not enough for efficient scaling.

\subsubsection{Try It Yourself}\label{try-it-yourself-81}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train GPT-2 774M with \texttt{(B=1,\ T=1024)} and gradient
  accumulation over 8 steps. Watch how it simulates a batch size of 8.
\item
  Compare training with FP32 vs.~mixed precision. Measure both memory
  use and speed.
\item
  Run a short fine-tuning experiment on Tiny Stories. Observe how
  quickly the model memorizes the dataset.
\item
  Plot the training and validation loss curves. Does the larger model
  overfit faster or slower than 355M?
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-82}

Reproducing GPT-2 774M is about scaling into the territory of real
research workloads. You face serious memory constraints, dataset
requirements, and compute costs. But if you succeed, you'll see
firsthand why the machine learning community kept pushing toward
billion-parameter models: larger networks unlock noticeably stronger
capabilities, even with the same architecture.

\subsection{94. Reproducing GPT-2 1.6B on 8×H100 (24h
Run)}\label{reproducing-gpt-2-1.6b-on-8h100-24h-run}

The largest GPT-2 model, with 1.6 billion parameters, represents the
upper bound of the original GPT-2 family. Training this model from
scratch is not something you can casually attempt on a single
workstation. It demands cluster-scale resources, distributed training
software, and careful tuning to keep everything stable. In this section,
we'll walk through what makes the 1.6B model special, what
infrastructure is required, and how a full reproduction might look.

\subsubsection{Model Configuration}\label{model-configuration-2}

The jump from 774M to 1.6B doubles the parameter count and makes the
network both deeper and wider:

\begin{itemize}
\tightlist
\item
  Layers (transformer blocks): 48
\item
  Hidden size (channels): 1600
\item
  Attention heads: 25
\item
  Sequence length: still 1024
\end{itemize}

With these dimensions, every forward and backward pass requires massive
amounts of memory and compute. Just storing the parameters in FP32 takes
around 6.4 GB. Once you add gradients, optimizer states (AdamW's
\emph{m} and \emph{v}), and activations, the memory footprint easily
exceeds 100 GB.

\subsubsection{Hardware Setup}\label{hardware-setup}

To reproduce this model realistically, you need access to high-end
accelerators such as NVIDIA A100s or H100s. A common baseline is 8 GPUs
with 80 GB each. With this setup, it is possible to train GPT-2 1.6B in
under 24 hours, assuming efficient utilization.

Without multi-GPU, training is impractical. Even if you could somehow
fit the model on one GPU, the runtime would be weeks or months.

\subsubsection{Distributed Training}\label{distributed-training}

The main strategy is data parallelism: each GPU processes a different
mini-batch of data, and gradients are averaged across all devices with
NCCL all-reduce. The code paths in \emph{llm.c} support this via MPI
integration, so you can scale from single-GPU to multi-node setups.

The training loop looks nearly identical to smaller models, but behind
the scenes, every parameter update is coordinated across devices.

\subsubsection{Training Dynamics}\label{training-dynamics-1}

The loss curve for 1.6B smooths out compared to smaller models. With
enough data, the model continues to improve where 774M starts to
plateau. This was one of the key insights from the original GPT-2 paper:
scaling laws hold, and performance improves predictably with size, data,
and compute.

Logs from a distributed run might look like this:

\begin{verbatim}
[rank 0] step 0: train loss 8.5029 (took 312.6 ms)
[rank 0] step 50: train loss 6.3121 (took 308.2 ms)
[rank 0] step 100: train loss 5.7210 (took 309.4 ms)
[rank 0] val loss 5.5347
\end{verbatim}

Notice the speed: each step still takes only a few hundred milliseconds
despite the massive size, thanks to parallelism across multiple H100s.

\subsubsection{Why It Matters}\label{why-it-matters-68}

Reproducing GPT-2 1.6B is less about training a useful model today and
more about understanding the scaling challenges of large language
models. This exercise demonstrates how compute, memory, and distributed
infrastructure become the limiting factors as models grow. It also shows
why modern research labs design entire pipelines around multi-GPU and
multi-node scaling.

\subsubsection{Try It Yourself}\label{try-it-yourself-82}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simulate a multi-GPU run with fewer resources by reducing the model
  size but using the same parallel training setup. For example, train
  GPT-2 124M across 2 GPUs to practice the workflow.
\item
  Experiment with gradient accumulation to mimic large global batch
  sizes even on smaller clusters.
\item
  Try enabling and disabling mixed precision. Watch how memory use drops
  dramatically with FP16/BF16.
\item
  Plot validation loss curves for 124M, 355M, 774M, and 1.6B side by
  side. Notice how the larger models sustain improvements longer.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-83}

The GPT-2 1.6B reproduction is the capstone project of \emph{llm.c}. It
forces you to combine everything you've learned: data pipelines,
optimizers, schedulers, distributed training, and system-level
debugging. While few people will actually train 1.6B themselves,
understanding what it takes provides a window into the engineering
behind state-of-the-art LLMs and prepares you to engage with even larger
modern models.

\subsection{95. CPU-only Fine-Tune Demo (Tiny
Shakespeare)}\label{cpu-only-fine-tune-demo-tiny-shakespeare}

Not everyone has access to powerful GPUs or large compute clusters. One
of the strengths of \emph{llm.c} is that it provides a clean, minimal
CPU-only path that lets you run real experiments---even if they are
small and slow. A practical way to explore this is by fine-tuning GPT-2
on a small dataset like Tiny Shakespeare, which has only about 1 MB of
text.

\subsubsection{Why Tiny Shakespeare?}\label{why-tiny-shakespeare}

Tiny Shakespeare is a classic toy dataset in machine learning. It's
small enough to fit in memory, yet it contains a rich variety of words,
characters, and structures. Fine-tuning GPT-2 on this dataset allows the
model to mimic Shakespearean style in just a few thousand steps. It's
not about building a state-of-the-art model---it's about seeing the
training process work end-to-end on modest hardware.

\subsubsection{Setup}\label{setup}

The fine-tuning process uses the same \texttt{train\_gpt2.c} CPU path,
but with fewer steps, smaller batch sizes, and lower sequence lengths to
keep things fast. A typical setup looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\DataTypeTok{int}\NormalTok{ B }\OperatorTok{=} \DecValTok{4}\OperatorTok{;}    \CommentTok{// batch size}
\DataTypeTok{int}\NormalTok{ T }\OperatorTok{=} \DecValTok{64}\OperatorTok{;}   \CommentTok{// sequence length}
\DataTypeTok{int}\NormalTok{ steps }\OperatorTok{=} \DecValTok{1000}\OperatorTok{;}  \CommentTok{// training iterations}
\end{Highlighting}
\end{Shaded}

The dataset is tokenized once using \texttt{gpt2\_tokenizer.bin} and
stored in binary \texttt{.bin} files:

\begin{verbatim}
dev/data/tinyshakespeare/tiny_shakespeare_train.bin
dev/data/tinyshakespeare/tiny_shakespeare_val.bin
\end{verbatim}

These files are only a few megabytes, making them perfect for quick
experiments.

\subsubsection{Training Dynamics}\label{training-dynamics-2}

When you fine-tune on Tiny Shakespeare, the training logs may look like
this:

\begin{verbatim}
step 0: train loss 6.9312 (took 1220.4 ms)
step 50: train loss 4.3217 (took 1175.1 ms)
step 100: train loss 3.7120 (took 1169.4 ms)
val loss 3.5894
\end{verbatim}

Within a few hundred steps, the loss drops rapidly. By the time you've
run 1000 steps, the model starts producing text that looks recognizably
Shakespearean---complete with archaic words, unusual punctuation, and
rhythmic patterns.

\subsubsection{Example Output}\label{example-output}

Here's a short sample from a fine-tuned run:

\begin{verbatim}
generating:

ROMEO: But hark, what light through yonder window breaks?
JULIET: Ay me! the time is near, and I must away.
ROMEO: Fear not, sweet love, for night shall bring us peace.
\end{verbatim}

It isn't perfect, but it captures the ``feel'' of Shakespeare, which is
remarkable given the tiny dataset and limited compute.

\subsubsection{Why It Matters}\label{why-it-matters-69}

The CPU-only Tiny Shakespeare demo proves that LLMs are not just for
massive data centers. With a minimal setup, you can watch the model
learn, generate text, and overfit to a dataset. This hands-on practice
builds intuition about what training does, how loss curves behave, and
why scaling up matters.

\subsubsection{Try It Yourself}\label{try-it-yourself-83}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change the sequence length from 64 to 128. How does training speed and
  loss change?
\item
  Reduce the number of training steps to 200. Do you still see
  Shakespeare-like text in generation?
\item
  Increase the batch size to 8. Does the loss curve become smoother or
  noisier?
\item
  Fine-tune again but initialize from scratch (random weights). Compare
  the results to fine-tuning from pretrained GPT-2 124M.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-84}

Fine-tuning GPT-2 on Tiny Shakespeare with CPU-only training is a simple
yet powerful demonstration. You don't need GPUs to understand the
mechanics of transformers. Even a modest laptop can teach you how
training works, why overfitting happens, and how LLMs adapt to new
domains. It's a reminder that the best way to learn machine learning is
by rolling up your sleeves and running experiments---even small ones.

\subsection{96. Cost and Time Estimation for
Runs}\label{cost-and-time-estimation-for-runs}

One of the most eye-opening parts of working with large language models
is realizing how expensive and time-consuming training can be. While
\emph{llm.c} makes it possible to run models of all sizes with simple,
minimal C code, the hardware requirements grow quickly as you scale from
GPT-2 124M to GPT-2 1.6B. Understanding cost and time estimation helps
set realistic expectations, whether you're running on a laptop CPU, a
single GPU, or a rented cluster of accelerators.

\subsubsection{Key Factors Affecting Training
Time}\label{key-factors-affecting-training-time}

Several components determine how long training takes and how much it
costs:

\begin{itemize}
\tightlist
\item
  Model size (parameters): Larger models mean more multiplications per
  forward/backward pass, and more memory for parameters, gradients, and
  optimizer states.
\item
  Batch size and sequence length: Increasing either multiplies the
  amount of work per step.
\item
  Dataset size: Bigger datasets require more steps to complete one
  epoch.
\item
  Hardware speed: CPUs are far slower than GPUs; high-end GPUs like
  H100s can be 100× faster than CPUs for this workload.
\item
  Parallelism: Multi-GPU or multi-node setups let you divide the work,
  reducing time per step.
\end{itemize}

\subsubsection{Rough Time Estimates}\label{rough-time-estimates}

Here's a simplified view of how long it might take to train GPT-2 models
depending on hardware and setup. These are very approximate, assuming
full training on a dataset like OpenWebText:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1493}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1493}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2239}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1940}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2836}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Parameters
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Hardware
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Time per Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Total Training Time
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
GPT-2 124M & \textasciitilde124M & Laptop CPU & 1--2 s & Months \\
GPT-2 124M & \textasciitilde124M & Single RTX 3090 & \textasciitilde100
ms & \textasciitilde2--4 weeks \\
GPT-2 355M & \textasciitilde355M & Single RTX 3090 & \textasciitilde300
ms & \textasciitilde6--8 weeks \\
GPT-2 774M & \textasciitilde774M & 2× A100 40GB & \textasciitilde200 ms
& \textasciitilde4--6 weeks \\
GPT-2 1.6B & \textasciitilde1.6B & 8× H100 80GB & \textasciitilde300 ms
& \textasciitilde24 hours \\
\end{longtable}

These numbers show why scaling matters. The larger models don't just
need more compute per step---they also need more data to reach their
potential. That means the total cost balloons unless you have a cluster
of top-tier GPUs.

\subsubsection{Cost in Cloud
Environments}\label{cost-in-cloud-environments}

If you run these experiments on cloud providers like AWS, GCP, or Azure,
costs can add up quickly. For example:

\begin{itemize}
\tightlist
\item
  An NVIDIA A100 40GB instance costs around \$2--3 per hour (spot
  pricing can be cheaper).
\item
  Training GPT-2 124M for a week might cost \$500--1,000.
\item
  Training GPT-2 1.6B for 24 hours on 8× H100s might cost
  \$5,000--10,000, depending on the provider.
\end{itemize}

This is why many researchers test code paths on small datasets and small
models first, then only scale up when absolutely necessary.

\subsubsection{Why It Matters}\label{why-it-matters-70}

Estimating cost and time prevents frustration and wasted money. It
teaches you to prototype at small scale (CPU or 124M runs), validate
your setup, and only then scale up to medium (355M, 774M) or large
(1.6B) models. It also gives you a realistic appreciation for the
engineering and budget that went into OpenAI's original GPT-2 training
runs.

\subsubsection{Try It Yourself}\label{try-it-yourself-84}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Time how long a single training step takes on your hardware for GPT-2
  124M. Multiply by 1000 to estimate training time for 1000 steps.
\item
  Reduce the batch size by half. Does time per step decrease linearly,
  or not?
\item
  Run a short fine-tune (e.g., 200 steps) and measure the electricity
  cost if you're on a home machine.
\item
  Use a cloud GPU for one hour. Compare the cost and speed to your local
  CPU.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-85}

Training large language models is a balancing act between ambition,
hardware, and budget. \emph{llm.c} gives you the tools to explore
everything from toy demos to billion-parameter reproductions, but you'll
quickly see why the field has shifted toward big labs and shared
infrastructure. With careful planning, though, you can still learn a
tremendous amount by running smaller experiments and scaling them up
thoughtfully.

\subsection{\texorpdfstring{97. Hyperparameter Sweeps
(\texttt{sweep.sh})}{97. Hyperparameter Sweeps (sweep.sh)}}\label{hyperparameter-sweeps-sweep.sh}

Getting a model like GPT-2 to train well isn't just about writing the
code or having enough compute---it's also about finding the right
hyperparameters. These include the learning rate, batch size, weight
decay, dropout rate, and scheduler configuration. A setting that works
well for one dataset or model size might completely fail for another.
That's where hyperparameter sweeps come in: systematically trying
different configurations to see which ones give the best results.

\subsubsection{\texorpdfstring{The Role of
\texttt{sweep.sh}}{The Role of sweep.sh}}\label{the-role-of-sweep.sh}

In \emph{llm.c}, there's a simple shell script called \texttt{sweep.sh}
designed to automate hyperparameter testing. It's not a complex
experiment management system like Ray Tune or Optuna; instead, it's
lightweight, transparent, and easy to adapt. The script usually looks
like a loop over different learning rates or batch sizes, running the
training executable with each setting, and logging the output.

A very simplified version might look like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#!/bin/bash}
\ControlFlowTok{for}\NormalTok{ lr }\KeywordTok{in}\NormalTok{ 1e{-}3 5e{-}4 1e{-}4}
\ControlFlowTok{do}
    \BuiltInTok{echo} \StringTok{"Running with learning rate }\VariableTok{$lr}\StringTok{"}
    \ExtensionTok{./train\_gpt2} \AttributeTok{{-}lr} \VariableTok{$lr} \AttributeTok{{-}epochs}\NormalTok{ 5 }\OperatorTok{\textgreater{}}\NormalTok{ logs/lr\_}\VariableTok{$lr}\NormalTok{.txt}
\ControlFlowTok{done}
\end{Highlighting}
\end{Shaded}

This way, you can launch multiple experiments with a single command and
later compare validation losses to decide which hyperparameters are
best.

\subsubsection{Why Sweeps Are Important}\label{why-sweeps-are-important}

Training a transformer is highly sensitive to hyperparameters. For
example:

\begin{itemize}
\tightlist
\item
  If the learning rate is too high, loss might explode.
\item
  If it's too low, training will be painfully slow.
\item
  Too much weight decay can hurt performance, while too little can cause
  overfitting.
\item
  The number of warmup steps can make the difference between stable
  convergence and failure in the first few hundred iterations.
\end{itemize}

Instead of guessing, sweeps let you see patterns. For instance, you
might discover that \texttt{3e-4} is optimal for GPT-2 124M, but GPT-2
355M prefers \texttt{2e-4}.

\subsubsection{Example of a Sweep in
Practice}\label{example-of-a-sweep-in-practice}

Suppose you want to test three learning rates and two batch sizes. You
can write a nested loop:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ lr }\KeywordTok{in}\NormalTok{ 3e{-}4 2e{-}4 1e{-}4}
\ControlFlowTok{do}
    \ControlFlowTok{for}\NormalTok{ B }\KeywordTok{in}\NormalTok{ 4 8}
    \ControlFlowTok{do}
        \BuiltInTok{echo} \StringTok{"Running with lr=}\VariableTok{$lr}\StringTok{ and batch\_size=}\VariableTok{$B}\StringTok{"}
        \ExtensionTok{./train\_gpt2} \AttributeTok{{-}lr} \VariableTok{$lr} \AttributeTok{{-}B} \VariableTok{$B} \AttributeTok{{-}steps}\NormalTok{ 500 }\OperatorTok{\textgreater{}}\NormalTok{ logs/lr\_}\VariableTok{$\{lr\}}\NormalTok{\_B}\VariableTok{$\{B\}}\NormalTok{.txt}
    \ControlFlowTok{done}
\ControlFlowTok{done}
\end{Highlighting}
\end{Shaded}

Afterward, you could open the logs and compare validation loss at the
end of each run. This gives you data-driven evidence about what works
best.

\subsubsection{Why It Matters}\label{why-it-matters-71}

Hyperparameter sweeps are a cornerstone of practical machine learning.
Even though \emph{llm.c} is a minimalist project, the ability to quickly
test and compare runs is essential. It transforms training from
guesswork into an empirical process. You don't just hope your model will
converge---you verify it across multiple settings and pick the winner.

\subsubsection{Try It Yourself}\label{try-it-yourself-85}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run a sweep over learning rates \texttt{{[}1e-3,\ 3e-4,\ 1e-4{]}} for
  GPT-2 124M on Tiny Shakespeare. Which one converges fastest?
\item
  Try sweeping over sequence length (\texttt{T=32,\ 64,\ 128}). How does
  it affect speed and loss?
\item
  Compare runs with and without weight decay. Which generalizes better
  to the validation set?
\item
  Extend the sweep to test different schedulers (cosine vs.~step decay).
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-86}

Hyperparameter sweeps are the ``experimentation muscle'' of training
LLMs. They teach you that no single setting works everywhere and that
systematic testing is far more effective than intuition alone. With a
simple script like \texttt{sweep.sh}, you can explore dozens of setups
in a reproducible way and build confidence that your model is training
as well as it can.

\subsection{98. Validating Evaluation and Loss
Curves}\label{validating-evaluation-and-loss-curves}

Training logs---loss values printed after each step---are just numbers.
To really understand whether your model is learning, you need to
validate those numbers, plot them, and compare them across runs. This
process of analyzing evaluation and loss curves is one of the most
important skills in machine learning. It's how you know whether your
model is converging, overfitting, or failing entirely.

\subsubsection{Training Loss vs.~Validation
Loss}\label{training-loss-vs.-validation-loss}

There are two kinds of loss curves to pay attention to:

\begin{itemize}
\tightlist
\item
  Training loss: computed on the batches the model actually sees during
  training.
\item
  Validation loss: computed on a held-out dataset (like
  \texttt{*\_val.bin} in \emph{llm.c}).
\end{itemize}

Training loss almost always goes down steadily. Validation loss is the
real test: if it decreases alongside training loss, the model is
learning useful patterns. If it stalls or increases while training loss
keeps dropping, the model is overfitting.

\subsubsection{Plotting Loss Curves}\label{plotting-loss-curves}

Even though \emph{llm.c} is a pure C project, you can redirect its
training logs to a file and then use tools like Python + matplotlib to
visualize. For example:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{./train\_gpt2} \OperatorTok{\textgreater{}}\NormalTok{ logs/run1.txt}
\end{Highlighting}
\end{Shaded}

Then parse \texttt{logs/run1.txt} in Python:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ re, matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{steps, train\_loss }\OperatorTok{=}\NormalTok{ [], []}
\ControlFlowTok{for}\NormalTok{ line }\KeywordTok{in} \BuiltInTok{open}\NormalTok{(}\StringTok{"logs/run1.txt"}\NormalTok{):}
\NormalTok{    match }\OperatorTok{=}\NormalTok{ re.match(}\VerbatimStringTok{r"step }\KeywordTok{(}\DecValTok{\textbackslash{}d}\OperatorTok{+}\KeywordTok{)}\VerbatimStringTok{: train loss }\KeywordTok{(}\PreprocessorTok{[0{-}9.]}\OperatorTok{+}\KeywordTok{)}\VerbatimStringTok{"}\NormalTok{, line)}
    \ControlFlowTok{if}\NormalTok{ match:}
\NormalTok{        steps.append(}\BuiltInTok{int}\NormalTok{(match.group(}\DecValTok{1}\NormalTok{)))}
\NormalTok{        train\_loss.append(}\BuiltInTok{float}\NormalTok{(match.group(}\DecValTok{2}\NormalTok{)))}

\NormalTok{plt.plot(steps, train\_loss, label}\OperatorTok{=}\StringTok{"train loss"}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

Adding validation loss to the same plot makes it even more useful:
you'll see both curves and their relationship over time.

\subsubsection{What a Healthy Curve Looks
Like}\label{what-a-healthy-curve-looks-like}

\begin{itemize}
\tightlist
\item
  Early phase: Both training and validation loss drop quickly.
\item
  Middle phase: Training loss continues downward, validation loss drops
  more slowly.
\item
  Late phase: Training loss may keep decreasing, but validation loss
  stabilizes or begins to rise. That's the point of overfitting.
\end{itemize}

For example, a good curve might look like this:

\begin{verbatim}
step 0: train loss 6.92, val loss 6.85
step 100: train loss 4.31, val loss 4.52
step 200: train loss 3.78, val loss 4.01
step 500: train loss 2.91, val loss 3.95
\end{verbatim}

Training loss keeps going down, but validation loss plateaus around step
500. That's a signal to stop training or adjust hyperparameters.

\subsubsection{Why It Matters}\label{why-it-matters-72}

Loss curves are your window into model behavior. They reveal whether
your model is underfitting (loss too high), overfitting (gap between
train and val too large), or training stably (both losses decreasing
together). Without them, you're flying blind---just staring at numbers
without context.

\subsubsection{Try It Yourself}\label{try-it-yourself-86}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train GPT-2 124M for 500 steps on Tiny Shakespeare and plot both
  training and validation loss.
\item
  Reduce the dataset size by half. Watch how validation loss worsens
  earlier due to overfitting.
\item
  Run two experiments with different learning rates. Plot both curves
  and compare stability.
\item
  Extend plotting to multiple runs (e.g., GPT-2 124M vs.~355M) to see
  scaling effects.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-87}

Validating evaluation and loss curves turns raw logs into insight. It
helps you decide when to stop training, how to tune hyperparameters, and
whether scaling up is worth it. In \emph{llm.c}, even though the project
is minimal, capturing and plotting these curves is the single most
effective way to understand what's happening inside your model.

\subsection{99. Future Work: Kernel Library, Less cuDNN
Dependence}\label{future-work-kernel-library-less-cudnn-dependence}

The CUDA path in \emph{llm.c} already uses cuBLAS and cuDNN, NVIDIA's
high-performance math libraries, to handle the heavy lifting of matrix
multiplications and attention operations. These libraries are
battle-tested and extremely fast, but they also act as a ``black box'':
you call into them, they do the work, and you get results without seeing
what's inside. While this is convenient, it limits flexibility and makes
it harder to experiment with novel optimizations.

That's why one of the most exciting areas of future work is building a
lightweight custom kernel library for \emph{llm.c}. This would mean
replacing parts of cuDNN with hand-written CUDA kernels for operations
like attention, normalization, and activation functions.

\subsubsection{Why Reduce cuDNN
Dependence?}\label{why-reduce-cudnn-dependence}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Transparency: With custom kernels, you see exactly how operations are
  implemented, which is great for learning and debugging.
\item
  Flexibility: You can experiment with new ideas (e.g., alternative
  attention mechanisms, sparsity tricks) without waiting for cuDNN
  support.
\item
  Portability: cuDNN is NVIDIA-specific. A custom kernel library could
  make it easier to port \emph{llm.c} to other backends, like AMD GPUs
  (HIP) or even Metal for Apple silicon.
\item
  Performance Tuning: For small to medium models, hand-tuned kernels can
  sometimes outperform generic library calls because they're tailored to
  the workload.
\end{enumerate}

\subsubsection{What a Kernel Library Might
Include}\label{what-a-kernel-library-might-include}

A first version of a kernel library for \emph{llm.c} might implement:

\begin{itemize}
\tightlist
\item
  Matrix multiplication (the workhorse of transformers) using tiling and
  shared memory.
\item
  Softmax kernels with numerical stability built in.
\item
  LayerNorm kernels for both forward and backward passes.
\item
  Attention kernels that integrate matmul + masking + softmax in one
  fused operation.
\item
  Activation functions like GELU or ReLU in fused forms.
\end{itemize}

For example, a very simplified CUDA kernel for vector addition might
look like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_\_global\_\_ void vec\_add(float* a, float* b, float* c, int N) \{}
\NormalTok{    int i = blockIdx.x * blockDim.x + threadIdx.x;}
\NormalTok{    if (i \textless{} N) \{}
\NormalTok{        c[i] = a[i] + b[i];}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

While trivial, this illustrates the idea: instead of calling a library,
you write the math yourself and control how threads are launched and
memory is accessed.

\subsubsection{The Path Forward}\label{the-path-forward}

Developing a kernel library is a long-term effort. It requires
profiling, benchmarking, and iterative tuning. At first, custom kernels
may be slower than cuDNN, but over time, they can evolve into a compact,
educational library of building blocks for transformer training.

\subsubsection{Why It Matters}\label{why-it-matters-73}

Reducing dependence on cuDNN isn't just about performance---it's about
control and portability. By having your own kernels, you gain the
freedom to run \emph{llm.c} on more platforms, test new research ideas,
and understand exactly what's happening inside the GPU. For a minimal,
educational project like this one, that's a natural next step.

\subsubsection{Try It Yourself}\label{try-it-yourself-87}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a simple CUDA kernel for vector addition, like the one above,
  and call it from a C program. Compare its performance to
  \texttt{cublasSaxpy}.
\item
  Replace one small piece of \emph{llm.c} (e.g., softmax) with a custom
  kernel. Check if the outputs match cuDNN.
\item
  Benchmark a custom kernel against cuDNN on a small input size. Does
  cuDNN still dominate?
\item
  Try porting your kernel to HIP (for AMD) or Metal (for Apple GPUs).
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-88}

Building a kernel library is about moving from consumer of black-box
libraries to creator of transparent, flexible tools. It's a lot of work,
but it transforms \emph{llm.c} into not just a project for using LLMs,
but also a platform for learning the deep internals of GPU programming.
By reducing cuDNN dependence, you open the door to true end-to-end
control of model training.

\subsection{100. Community, GitHub Discussions, and Suggested Learning
Path}\label{community-github-discussions-and-suggested-learning-path}

Large language models are complicated, but one of the goals of
\emph{llm.c} is to make them accessible. The code is clean, minimal, and
approachable---but learning doesn't stop at reading code. The broader
community around \emph{llm.c} plays a huge role in helping people
understand, experiment, and grow. This section highlights where to
connect with others, how to contribute, and how to build your own
learning journey.

\subsubsection{GitHub as the Hub}\label{github-as-the-hub}

The central place for \emph{llm.c} discussions is the
\href{https://github.com/karpathy/llm.c}{GitHub repository}. There,
you'll find:

\begin{itemize}
\tightlist
\item
  Issues: where users ask questions, report bugs, or propose
  improvements.
\item
  Discussions: an open forum for sharing results, asking ``how do
  I\ldots?'' questions, and comparing training logs.
\item
  Pull Requests: contributions ranging from bug fixes to new features,
  often with valuable code reviews.
\end{itemize}

Even just browsing issues and discussions can be an educational
experience. Many questions you might have---about CUDA errors, dataset
preparation, or optimizer quirks---have already been asked and answered.

\subsubsection{The Value of Community}\label{the-value-of-community}

Working alone on language models can feel overwhelming. By engaging with
the community, you:

\begin{itemize}
\tightlist
\item
  See how others run experiments with different datasets and hardware.
\item
  Learn troubleshooting strategies for common problems (e.g.,
  out-of-memory errors).
\item
  Get inspiration for extensions---like custom kernels, new optimizers,
  or non-GPT architectures.
\item
  Find collaborators for experiments that go beyond what one person can
  do.
\end{itemize}

\subsubsection{Suggested Learning Path}\label{suggested-learning-path}

Because \emph{llm.c} is minimal, it works well as a self-study tool.
Here's a suggested path to build up your knowledge:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Start Small

  \begin{itemize}
  \tightlist
  \item
    Train GPT-2 124M on Tiny Shakespeare using CPU-only mode.
  \item
    Inspect training logs and watch how loss decreases.
  \item
    Generate text and see how quickly the model memorizes.
  \end{itemize}
\item
  Step Into CUDA

  \begin{itemize}
  \tightlist
  \item
    Switch to \texttt{train\_gpt2.cu} and train with GPU acceleration.
  \item
    Try mixed precision (\texttt{FP16}/\texttt{BF16}) and observe memory
    savings.
  \end{itemize}
\item
  Scale Up

  \begin{itemize}
  \tightlist
  \item
    Attempt GPT-2 355M or 774M on your hardware (or cloud GPUs).
  \item
    Learn how to use gradient accumulation and checkpointing.
  \end{itemize}
\item
  Experiment

  \begin{itemize}
  \tightlist
  \item
    Modify the training loop: try new schedulers, tweak optimizer
    hyperparameters.
  \item
    Add your own datasets (e.g., your personal text corpus).
  \end{itemize}
\item
  Explore Internals

  \begin{itemize}
  \tightlist
  \item
    Step through forward and backward passes in \texttt{train\_gpt2.c}.
  \item
    Write small experiments to isolate key concepts (e.g., LayerNorm).
  \end{itemize}
\item
  Join the Discussion

  \begin{itemize}
  \tightlist
  \item
    Share your results on GitHub Discussions.
  \item
    Contribute improvements, even small ones---like documentation fixes.
  \end{itemize}
\end{enumerate}

\subsubsection{Why It Matters}\label{why-it-matters-74}

The journey of learning LLM internals isn't just about reading
code---it's about active practice, asking questions, and comparing
experiences with others. Community provides the feedback loop that
accelerates learning and keeps motivation alive.

\subsubsection{Try It Yourself}\label{try-it-yourself-88}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Clone the \emph{llm.c} repository and explore open issues. Can you
  answer one for someone else?
\item
  Run a training experiment and share your loss curve in GitHub
  Discussions.
\item
  Contribute a small improvement (like a new dataset script) as a pull
  request.
\item
  Create your own ``learning log'' to track experiments, much like a
  public notebook.
\end{enumerate}

\subsubsection{The Takeaway}\label{the-takeaway-89}

\emph{llm.c} isn't just a codebase---it's an invitation to join a
learning community. By engaging with GitHub, trying experiments, and
sharing your results, you move from passive reader to active
participant. That's where the deepest understanding comes from: learning
together, not alone.




\end{document}
